next.js 13 was recently released,  and see the . version 13 also introduces beta features like the  that works alongside the  directory (stable) for incremental adoption. you can continue using  in next.js 13, but if you want to try the new  features, .
getting started
welcome to the next.js documentation!
if you're new to next.js, we recommend starting with the . the interactive course with quizzes will guide you through everything you need to know to use next.js.
if you have questions about anything related to next.js, you're always welcome to ask our community on .
or newer
macos, windows (including wsl), and linux are supported
we recommend creating a new next.js app using , which sets up everything automatically for you. to create a project, run:
if you want to start with a typescript project you can use the  flag:
after the installation is complete:
run  or  or  to start the development server on
visit  to view your application
edit  and see the updated result in your browser
for more information on how to use , you can review the .
install ,  and  in your project:
open  and add the following :
these scripts refer to the different stages of developing an application:
- runs  to start next.js in development mode
- runs  to build the application for production usage
- runs  to start a next.js production server
- runs  to set up next.js' built-in eslint configuration
create two directories  and  at the root of your application:
- associated with a route based on their file name. for example  is mapped to
- stores static assets such as images, fonts, etc. files inside  directory can then be referenced by your code starting from the base url ().
next.js is built around the concept of . a page is a  exported from a , , , or  file in the  directory. you can even add  parameters with the filename.
inside the  directory add the  file to get started. this is the page that is rendered when the user visits the root of your application.
populate  with the following contents:
after the set up is complete:
so far, we get:
automatic compilation and
through  which is mapped to the base url ()
in addition, any next.js application is ready for production from the start. read more in our .
for more information on what to do next, we recommend the following sections:
pages
note: next.js 13 introduces the  directory (beta). this new directory has support for layouts, nested routes, and uses server components by default. inside , you can fetch data for your entire application inside layouts, including support for more granular nested layouts (with ).
in next.js, a page is a  exported from a , , , or  file in the  directory. each page is associated with a route based on its file name.
example: if you create  that exports a react component like below, it will be accessible at .
next.js supports pages with dynamic routes. for example, if you create a file called , then it will be accessible at , , etc.
to learn more about dynamic routing, check the .
by default, next.js pre-renders every page. this means that next.js generates html for each page in advance, instead of having it all done by client-side javascript. pre-rendering can result in better performance and seo.
each generated html is associated with minimal javascript code necessary for that page. when a page is loaded by the browser, its javascript code runs and makes the page fully interactive. (this process is called hydration.)
next.js has two forms of pre-rendering: static generation and server-side rendering. the difference is in when it generates the html for a page.
: the html is generated at build time and will be reused on each request.
: the html is generated on each request.
importantly, next.js lets you choose which pre-rendering form you'd like to use for each page. you can create a "hybrid" next.js app by using static generation for most pages and using server-side rendering for others.
we recommend using static generation over server-side rendering for performance reasons. statically generated pages can be cached by cdn with no extra configuration to boost performance. however, in some cases, server-side rendering might be the only option.
you can also use client-side data fetching along with static generation or server-side rendering. that means some parts of a page can be rendered entirely by client side javascript. to learn more, take a look at the  documentation.
if a page uses static generation, the page html is generated at build time. that means in production, the page html is generated when you run  . this html will then be reused on each request. it can be cached by a cdn.
in next.js, you can statically generate pages with or without data. let's take a look at each case.
by default, next.js pre-renders pages using static generation without fetching data. here's an example:
note that this page does not need to fetch any external data to be pre-rendered. in cases like this, next.js generates a single html file per page during build time.
some pages require fetching external data for pre-rendering. there are two scenarios, and one or both might apply. in each case, you can use these functions that next.js provides:
your page content depends on external data: use .
your page paths depend on external data: use  (usually in addition to ).
example: your blog page might need to fetch the list of blog posts from a cms (content management system).
to fetch this data on pre-render, next.js allows you to  an  function called  from the same file. this function gets called at build time and lets you pass fetched data to the page's  on pre-render.
to learn more about how  works, check out the .
next.js allows you to create pages with dynamic routes. for example, you can create a file called  to show a single blog post based on . this will allow you to show a blog post with  when you access .
however, which  you want to pre-render at build time might depend on external data.
example: suppose that you've only added one blog post (with ) to the database. in this case, you'd only want to pre-render  at build time.
later, you might add the second post with . then you'd want to pre-render  as well.
so your page paths that are pre-rendered depend on external data. to handle this, next.js lets you  an  function called  from a dynamic page ( in this case). this function gets called at build time and lets you specify which paths you want to pre-render.
also in , you need to export  so that you can fetch the data about the post with this  and use it to pre-render the page:
we recommend using static generation (with and without data) whenever possible because your page can be built once and served by cdn, which makes it much faster than having a server render the page on every request.
you can use static generation for many types of pages, including:
marketing pages
blog posts and portfolios
e-commerce product listings
help and documentation
you should ask yourself: "can i pre-render this page ahead of a user's request?" if the answer is yes, then you should choose static generation.
on the other hand, static generation is not a good idea if you cannot pre-render a page ahead of a user's request. maybe your page shows frequently updated data, and the page content changes on every request.
in cases like this, you can do one of the following:
use static generation with client-side data fetching: you can skip pre-rendering some parts of a page and then use client-side javascript to populate them. to learn more about this approach, check out the .
use server-side rendering: next.js pre-renders a page on each request. it will be slower because the page cannot be cached by a cdn, but the pre-rendered page will always be up-to-date. we'll talk about this approach below.
also referred to as "ssr" or "dynamic rendering".
if a page uses server-side rendering, the page html is generated on each request.
to use server-side rendering for a page, you need to  an  function called . this function will be called by the server on every request.
for example, suppose that your page needs to pre-render frequently updated data (fetched from an external api). you can write  which fetches this data and passes it to  like below:
as you can see,  is similar to , but the difference is that  is run on every request instead of on build time.
to learn more about how  works, check out our
we've discussed two forms of pre-rendering for next.js.
static generation (recommended): the html is generated at build time and will be reused on each request. to make a page use static generation, either export the page component, or export  (and  if necessary). it's great for pages that can be pre-rendered ahead of a user's request. you can also use it with client-side rendering to bring in additional data.
server-side rendering: the html is generated on each request. to make a page use server-side rendering, export . because server-side rendering results in slower performance than static generation, use this only if absolutely necessary.
we recommend you to read the following sections next:
data fetching overview
note: next.js 13 introduces the  directory (beta). this new directory has support for  at the component level, using the new react  hook and an extended  web api.
data fetching in next.js allows you to render your content in different ways, depending on your application's use case. these include pre-rendering with server-side rendering or static generation, and updating or creating content at runtime with incremental static regeneration.
getserversideprops
if you export a function called  (server-side rendering) from a page, next.js will pre-render this page on each request using the data returned by .
note that irrespective of rendering type, any  will be passed to the page component and can be viewed on the client-side in the initial html. this is to allow the page to be  correctly. make sure that you don't pass any sensitive information that shouldn't be available on the client in .
only runs on server-side and never runs on the browser. if a page uses , then:
when you request this page directly,  runs at request time, and this page will be pre-rendered with the returned props
when you request this page on client-side page transitions through  or , next.js sends an api request to the server, which runs
returns json which will be used to render the page. all this work will be handled automatically by next.js, so you don’t need to do anything extra as long as you have  defined.
you can use the  to verify what next.js eliminates from the client-side bundle.
can only be exported from a page. you can’t export it from non-page files.
note that you must export  as a standalone function — it will not work if you add  as a property of the page component.
the  covers all parameters and props that can be used with .
you should use  only if you need to render a page whose data must be fetched at request time. this could be due to the nature of the data or properties of the request (such as  headers or geo location). pages using  will be server side rendered at request time and only be cached if .
if you do not need to render the data during the request, then you should consider fetching data on the  or .
it can be tempting to reach for an  when you want to fetch data from the server, then call that api route from . this is an unnecessary and inefficient approach, as it will cause an extra request to be made due to both  and api routes running on the server.
take the following example. an api route is used to fetch some data from a cms. that api route is then called directly from . this produces an additional call, reducing performance. instead, directly import the logic used inside your api route into . this could mean calling a cms, database, or other api directly from inside .
if your page contains frequently updating data, and you don’t need to pre-render the data, you can fetch the data on the . an example of this is user-specific data:
first, immediately show the page without data. parts of the page can be pre-rendered using static generation. you can show loading states for missing data
then, fetch the data on the client side and display it when ready
this approach works well for user dashboard pages, for example. because a dashboard is a private, user-specific page, seo is not relevant and the page doesn’t need to be pre-rendered. the data is frequently updated, which requires request-time data fetching.
the following example shows how to fetch data at request time and pre-render the result.
you can use caching headers () inside  to cache dynamic responses. for example, using .
learn more about .
if an error is thrown inside , it will show the  file. check out the documentation for  to learn more on how to create it. during development this file will not be used and the dev overlay will be shown instead.
getstaticpaths
if a page has  and uses , it needs to define a list of paths to be statically generated.
when you export a function called  (static site generation) from a page that uses dynamic routes, next.js will statically pre-render all the paths specified by .
you should use  if you’re statically pre-rendering pages that use dynamic routes and:
the data comes from a headless cms
the data comes from a database
the data comes from the filesystem
the data can be publicly cached (not user-specific)
the page must be pre-rendered (for seo) and be very fast —  generates  and  files, both of which can be cached by a cdn for performance
will only run during build in production, it will not be called during runtime. you can validate code written inside  is removed from the client-side bundle .
runs during  for any  returned during build
runs in the background when using
is called before initial render when using
must be used with
you cannot use  with
you can export  from a  that also uses
you cannot export  from non-page file (e.g. your  folder)
you must export  as a standalone function, and not a property of the page component
in development (),  will be called on every request.
allows you to control which pages are generated during the build instead of on-demand with . generating more pages during a build will cause slower builds.
you can defer generating all pages on-demand by returning an empty array for . this can be especially helpful when deploying your next.js application to multiple environments. for example, you can have faster builds by generating all pages on-demand for previews (but not production builds). this is helpful for sites with hundreds/thousands of static pages.
getstaticprops
if you export a function called  (static site generation) from a page, next.js will pre-render this page at build time using the props returned by .
you should use  if:
the data required to render the page is available at build time ahead of a user’s request
the data can be publicly cached (not user-specific). this condition can be bypassed in certain specific situation by using a middleware to rewrite the path.
always runs on the server and never on the client. you can validate code written inside  is removed from the client-side bundle .
always runs during
runs on-demand in the background when using
when combined with ,  will run in the background while the stale page is being revalidated, and the fresh page served to the browser.
does not have access to the incoming request (such as query parameters or http headers) as it generates static html. if you need access to the request for your page, consider using  in addition to .
the following example shows how you can fetch a list of blog posts from a cms.
as  runs only on the server-side, it will never run on the client-side. it won’t even be included in the js bundle for the browser, so you can write direct database queries without them being sent to browsers.
this means that instead of fetching an api route from  (that itself fetches data from an external source), you can write the server-side code directly in .
take the following example. an api route is used to fetch some data from a cms. that api route is then called directly from . this produces an additional call, reducing performance. instead, the logic for fetching the data from the cms can be shared by using a  directory. then it can be shared with .
alternatively, if you are not using api routes to fetch data, then the  api can be used directly in  to fetch data.
to verify what next.js eliminates from the client-side bundle, you can use the .
when a page with  is pre-rendered at build time, in addition to the page html file, next.js generates a json file holding the result of running .
this json file will be used in client-side routing through  or . when you navigate to a page that’s pre-rendered using , next.js fetches this json file (pre-computed at build time) and uses it as the props for the page component. this means that client-side page transitions will not call  as only the exported json is used.
when using incremental static generation,  will be executed in the background to generate the json needed for client-side navigation. you may see this in the form of multiple requests being made for the same page, however, this is intended and has no impact on end-user performance.
can only be exported from a page. you cannot export it from non-page files, , , or .
one of the reasons for this restriction is that react needs to have all the required data before the page is rendered.
also, you must use export  as a standalone function — it will not work if you add  as a property of the page component.
note: if you have created a , ensure you are passing the  to the page component as shown in the linked document, otherwise the props will be empty.
you can temporarily bypass static generation and render the page at request time instead of build time using . for example, you might be using a headless cms and want to preview drafts before they're published.
incremental static regeneration
next.js allows you to create or update static pages after you’ve built your site. incremental static regeneration (isr) enables you to use static-generation on a per-page basis, without needing to rebuild the entire site. with isr, you can retain the benefits of static while scaling to millions of pages.
to use isr, add the  prop to :
when a request is made to a page that was pre-rendered at build time, it will initially show the cached page.
any requests to the page after the initial request and before 10 seconds are also cached and instantaneous.
after the 10-second window, the next request will still show the cached (stale) page
next.js triggers a regeneration of the page in the background.
once the page generates successfully, next.js will invalidate the cache and show the updated page. if the background regeneration fails, the old page would still be unaltered.
when a request is made to a path that hasn’t been generated, next.js will server-render the page on the first request. future requests will serve the static file from the cache. isr on vercel .
note: check if your upstream data provider has caching enabled by default. you might need to disable (e.g. ), otherwise a revalidation won't be able to pull fresh data to update the isr cache. caching can occur at a cdn (for an endpoint being requested) when it returns the  header.
if you set a  time of , all visitors will see the same generated version of your site for one minute. the only way to invalidate the cache is from someone visiting that page after the minute has passed.
starting with , next.js supports on-demand incremental static regeneration to manually purge the next.js cache for a specific page. this makes it easier to update your site when:
content from your headless cms is created or updated
ecommerce metadata changes (price, description, category, reviews, etc.)
inside , you do not need to specify  to use on-demand revalidation. if  is omitted, next.js will use the default value of  (no revalidation) and only revalidate the page on-demand when  is called.
note:  won't be executed for on-demand isr requests. instead, call  on the exact path that you want revalidated. for example, if you have  and a rewrite from  -> , you would need to call .
first, create a secret token only known by your next.js app. this secret will be used to prevent unauthorized access to the revalidation api route. you can access the route (either manually or with a webhook) with the following url structure:
next, add the secret as an  to your application. finally, create the revalidation api route:
to see on-demand revalidation in action and provide feedback.
when running locally with ,  is invoked on every request. to verify your on-demand isr configuration is correct, you will need to create a  and start the :
then, you can confirm that static pages have successfully revalidated.
if there is an error inside  when handling background regeneration, or you manually throw an error, the last successfully generated page will continue to show. on the next subsequent request, next.js will retry calling .
incremental static regeneration (isr) works on  out of the box when you use .
you can use this approach when deploying to container orchestrators such as  or . by default, generated assets will be stored in-memory on each pod. this means that each pod will have its own copy of the static files. stale data may be shown until that specific pod is hit by a request.
to ensure consistency across all pods, you can disable in-memory caching. this will inform the next.js server to only leverage assets generated by isr in the file system.
you can use a shared network mount in your kubernetes pods (or similar setup) to reuse the same file-system cache between different containers. by sharing the same mount, the  folder which contains the  cache will also be shared and re-used.
to disable in-memory caching, set  to  in your  file:
note: you might need to consider a race condition between multiple pods trying to update the cache at the same time, depending on how your shared mount is configured.
client-side data fetching
client-side data fetching is useful when your page doesn't require seo indexing, when you don't need to pre-render your data, or when the content of your pages needs to update frequently. unlike the server-side rendering apis, you can use client-side data fetching at the component level.
if done at the page level, the data is fetched at runtime, and the content of the page is updated as the data changes. when used at the component level, the data is fetched at the time of the component mount, and the content of the component is updated as the data changes.
it's important to note that using client-side data fetching can affect the performance of your application and the load speed of your pages. this is because the data fetching is done at the time of the component or pages mount, and the data is not cached.
the following example shows how you can fetch data on the client side using the useeffect hook.
the team behind next.js has created a react hook library for data fetching called . it is highly recommended if you are fetching data on the client-side. it handles caching, revalidation, focus tracking, refetching on intervals, and more.
using the same example as above, we can now use swr to fetch the profile data. swr will automatically cache the data for us and will revalidate the data if it becomes stale.
for more information on using swr, check out the .
built-in css support
next.js allows you to import css files from a javascript file.
this is possible because next.js extends the concept of  beyond javascript.
to add a stylesheet to your application, import the css file within .
for example, consider the following stylesheet named :
create a  if not already present.
then,  the  file.
these styles () will apply to all pages and components in your application.
due to the global nature of stylesheets, and to avoid conflicts, you may only import them inside .
in development, expressing stylesheets this way allows your styles to be hot reloaded as you edit them—meaning you can keep application state.
in production, all css files will be automatically concatenated into a single minified  file.
since next.js 9.5.4, importing a css file from  is permitted anywhere in your application.
for global stylesheets, like  or , you should import the file inside .
for example:
for importing css required by a third party component, you can do so in your component. for example:
next.js supports  using the  file naming convention.
css modules locally scope css by automatically creating a unique class name.
this allows you to use the same css class name in different files without worrying about collisions.
this behavior makes css modules the ideal way to include component-level css.
css module files can be imported anywhere in your application.
for example, consider a reusable  component in the  folder:
first, create  with the following content:
then, create , importing and using the above css file:
css modules are an optional feature and are only enabled for files with the  extension.
regular  stylesheets and global css files are still supported.
in production, all css module files will be automatically concatenated into many minified and code-split  files.
these  files represent hot execution paths in your application, ensuring the minimal amount of css is loaded for your application to paint.
next.js allows you to import sass using both the  and  extensions.
you can use component-level sass via css modules and the  or  extension.
before you can use next.js' built-in sass support, be sure to install :
sass support has the same benefits and restrictions as the built-in css support detailed above.
note: sass supports , each with their own extension.
the  extension requires you use the ,
while the  extension requires you use the .
if you're not sure which to choose, start with the  extension which is a superset of css, and doesn't require you learn the
indented syntax ("sass").
if you want to configure the sass compiler you can do so by using  in .
for example to add :
next.js supports sass variables exported from css module files.
for example, using the exported  sass variable:
it's possible to use any existing css-in-js solution.
the simplest one is inline styles:
we bundle  to provide support for isolated scoped css.
the aim is to support "shadow css" similar to web components, which unfortunately .
see the above examples for other popular css-in-js solutions (like styled components).
a component using  looks like this:
please see the  for more examples.
yes, if you disable javascript the css will still be loaded in the production build (). during development, we require javascript to be enabled to provide the best developer experience with .
layouts
the react model allows us to deconstruct a  into a series of components. many of these components are often reused between pages. for example, you might have the same navigation bar and footer on every page.
if you only have one layout for your entire application, you can create a  and wrap your application with the layout. since the  component is re-used when changing pages, its component state will be preserved (e.g. input values).
if you need multiple layouts, you can add a property  to your page, allowing you to return a react component for the layout. this allows you to define the layout on a per-page basis. since we're returning a function, we can have complex nested layouts if desired.
when navigating between pages, we want to persist page state (input values, scroll position, etc.) for a single-page application (spa) experience.
this layout pattern enables state persistence because the react component tree is maintained between page transitions. with the component tree, react can understand which elements have changed to preserve state.
note: this process is called , which is how react understands which elements have changed.
when using typescript, you must first create a new type for your pages which includes a  function. then, you must create a new type for your  which overrides the  property to use the previously created type.
inside your layout, you can fetch data on the client-side using  or a library like . because this file is not a , you cannot use  or  currently.
image component and image optimization
the next.js image component, , is an extension of the html  element, evolved for the modern web. it includes a variety of built-in performance optimizations to help you achieve good . these scores are an important measurement of user experience on your website, and are .
some of the optimizations built into the image component include:
improved performance: always serve correctly sized image for each device, using modern image formats
visual stability: prevent  automatically
faster page loads: images are only loaded when they enter the viewport, with optional blur-up placeholders
asset flexibility: on-demand image resizing, even for images stored on remote servers
to add an image to your application, import the  component:
now, you can define the  for your image (either local or remote).
to use a local image,  your , , or  files:
dynamic  or  are not supported. the  must be static so it can be analyzed at build time.
next.js will automatically determine the  and  of your image based on the imported file. these values are used to prevent  while your image is loading.
to use a remote image, the  property should be a url string, which can be  or . because next.js does not have access to remote files during the build process, you'll need to provide the ,  and optional  props manually:
learn more about the  in .
sometimes you may want to optimize a remote image, but still use the built-in next.js image optimization api. to do this, leave the  at its default setting and enter an absolute url for the image  prop.
to protect your application from malicious users, you must define a list of remote hostnames you intend to use with the  component.
learn more about  configuration.
note that in the , a partial url () is provided for a remote image. this is possible because of the loader architecture.
a loader is a function that generates the urls for your image. it modifies the provided , and generates multiple urls to request the image at different sizes. these multiple urls are used in the automatic  generation, so that visitors to your site will be served an image that is the right size for their viewport.
the default loader for next.js applications uses the built-in image optimization api, which optimizes images from anywhere on the web, and then serves them directly from the next.js web server. if you would like to serve your images directly from a cdn or image server, you can write your own loader function with a few lines of javascript.
you can define a loader per-image with the , or at the application level with the .
you should add the  property to the image that will be the  for each page. doing so allows next.js to specially prioritize the image for loading (e.g. through preload tags or priority hints), leading to a meaningful boost in lcp.
the lcp element is typically the largest image or text block visible within the viewport of the page. when you run , you'll see a console warning if the lcp element is an  without the  property.
once you've identified the lcp image, you can add the property like this:
see more about priority in the .
one of the ways that images most commonly hurt performance is through layout shift, where the image pushes other elements around on the page as it loads in. this performance problem is so annoying to users that it has its own core web vital, called . the way to avoid image-based layout shifts is to . this allows the browser to reserve precisely enough space for the image before it loads.
because  is designed to guarantee good performance results, it cannot be used in a way that will contribute to layout shift, and must be sized in one of three ways:
automatically, using a
explicitly, by including a  and  property
implicitly, by using  which causes the image to expand to fill its parent element.
if you are accessing images from a source without knowledge of the images' sizes, there are several things you can do:
the  prop allows your image to be sized by its parent element. consider using css to give the image's parent element space on the page along  prop to match any media query break points. you can also use  with , , or , and  to define how the image should occupy that space.
normalize your images
if you're serving images from a source that you control, consider modifying your image pipeline to normalize the images to a specific size.
modify your api calls
if your application is retrieving image urls using an api call (such as to a cms), you may be able to modify the api call to return the image dimensions along with the url.
if none of the suggested methods works for sizing your images, the  component is designed to work well on a page alongside standard  elements.
styling the image component is similar to styling a normal  element, but there are a few guidelines to keep in mind:
use  or , not
in most cases, we recommend using the  prop. this can be an imported , a , etc.
you can also use the  prop to assign inline styles.
you cannot use  because it's scoped to the current component (unless you mark the style as ).
when using , the parent element must have
this is necessary for the proper rendering of the image element in that layout mode.
this is the default for  elements but should be specified otherwise.
for examples of the image component used with the various styles, see the .
the  component and next.js image optimization api can be configured in the . these configurations allow you to , ,  and more.
optimizing fonts
will automatically optimize your fonts (including custom fonts) and remove external network requests for improved privacy and performance.
includes built-in automatic self-hosting for any font file. this means you can optimally load web fonts with zero layout shift, thanks to the underlying css  property used.
this new font system also allows you to conveniently use all google fonts with performance and privacy in mind. css and font files are downloaded at build time and self-hosted with the rest of your static assets. no requests are sent to google by the browser.
to get started, install :
automatically self-host any google font. fonts are included in the deployment and served from the same domain as your deployment. no requests are sent to google by the browser.
import the font you would like to use from  as a function. we recommend using  for the best performance and flexibility.
to use the font in all your pages, add it to  under  as shown below:
if you can't use a variable font, you will need to specify a weight:
you can also use the font without a wrapper and  by injecting it inside the  as follows:
to use the font on a single page, add it to the specific page as shown below:
google fonts are automatically . this reduces the size of the font file and improves performance. you'll need to define which of these subsets you want to preload. failing to specify any subsets while  is true will result in a warning.
this can be done in 2 ways:
on a font per font basis by adding it to the function call
globally for all your fonts in your
if both are configured, the subset in the function call is used.
view the  for more information.
import  and specify the  of your local font file. we recommend using  for the best performance and flexibility.
can be used with tailwind css through a .
in the example below, we use the font  from  (you can use any font from google or local fonts). load your font with the  option to define your css variable name and assign it to . then, use  to add the css variable to your html document.
finally, add the css variable to your :
you can now use the  utility class to apply the font to your elements.
when a font function is called on a page of your site, it is not globally available and preloaded on all routes. rather, the font is only preloaded on the related route/s based on the type of file where it is used:
if it's a , it is preloaded on the unique route for that page
if it's in the , it is preloaded on all the routes of the site under
every time you call the  or google font function, that font is hosted as one instance in your application. therefore, if you load the same font function in multiple files, multiple instances of the same font are hosted. in this situation, it is recommended to do the following:
call the font loader function in one shared file
export it as a constant
import the constant in each file where you would like to use this font
static file serving
next.js can serve static files, like images, under a folder called  in the root directory. files inside  can then be referenced by your code starting from the base url ().
for example, if you add an image to , the following code will access the image:
note:  requires next.js 10 or later.
this folder is also useful for , , google site verification, and any other static files (including )!
note: don't name the  directory anything else. the name cannot be changed and is the only directory used to serve static assets.
note: be sure to not have a static file with the same name as a file in the  directory, as this will result in an error.
read more:
note: only assets that are in the  directory at  will be served by next.js. files added at runtime won't be available. we recommend using a third party service like  for persistent file storage.
fast refresh
fast refresh is a next.js feature that gives you instantaneous feedback on
edits made to your react components. fast refresh is enabled by default in all
next.js applications on 9.4 or newer. with next.js fast refresh enabled,
most edits should be visible within a second, without losing component
state.
if you edit a file that only exports react component(s), fast refresh will
update the code only for that file, and re-render your component. you can edit
anything in that file, including styles, rendering logic, event handlers, or
effects.
if you edit a file with exports that aren't react components, fast refresh
will re-run both that file, and the other files importing it. so if both
and  import , editing  will update
both components.
finally, if you edit a file that's imported by files outside of the
react tree, fast refresh will fall back to doing a full reload. you
might have a file which renders a react component but also exports a value
that is imported by a non-react component. for example, maybe your
component also exports a constant, and a non-react utility file imports it. in
that case, consider migrating the constant to a separate file and importing it
into both files. this will re-enable fast refresh to work. other cases can
usually be solved in a similar way.
if you make a syntax error during development, you can fix it and save the file
again. the error will disappear automatically, so you won't need to reload the
app. you will not lose component state.
if you make a mistake that leads to a runtime error inside your component,
you'll be greeted with a contextual overlay. fixing the error will automatically
dismiss the overlay, without reloading the app.
component state will be retained if the error did not occur during rendering. if
the error did occur during rendering, react will remount your application using
the updated code.
if you have
in your app (which is a good idea for graceful failures in production), they
will retry rendering on the next edit after a rendering error. this means having
an error boundary can prevent you from always getting reset to the root app
state. however, keep in mind that error boundaries shouldn't be too granular.
they are used by react in production, and should always be designed
intentionally.
fast refresh tries to preserve local react state in the component you're
editing, but only if it's safe to do so. here's a few reasons why you might see
local state being reset on every edit to a file:
local state is not preserved for class components (only function components
and hooks preserve state).
the file you're editing might have other exports in addition to a react
component.
sometimes, a file would export the result of calling a higher-order component
like . if the returned component is a
class, its state will be reset.
anonymous arrow functions like  cause fast refresh to not preserve local component state. for large codebases you can use our .
as more of your codebase moves to function components and hooks, you can expect
state to be preserved in more cases.
fast refresh preserves react local state in function components (and hooks) by
default.
sometimes you might want to force the state to be reset, and a component to
be remounted. for example, this can be handy if you're tweaking an animation
that only happens on mount. to do this, you can add
anywhere in the file you're editing. this directive is local to the file, and
instructs fast refresh to remount components defined in that file on every
edit.
you can put  or  into the components you edit during
development.
when possible, fast refresh attempts to preserve the state of your component
between edits. in particular,  and  preserve their previous
values as long as you don't change their arguments or the order of the hook
calls.
hooks with dependencies—such as , , and —will
always update during fast refresh. their list of dependencies will be ignored
while fast refresh is happening.
for example, when you edit  to
, it will re-run even though  (the dependency)
has not changed. if react didn't do that, your edit wouldn't reflect on the
screen!
sometimes, this can lead to unexpected results. for example, even a
with an empty array of dependencies would still re-run once during fast refresh.
however, writing code resilient to occasional re-running of  is a good practice even
without fast refresh. it will make it easier for you to introduce new dependencies to it later on
and it's enforced by ,
which we highly recommend enabling.
eslint
next.js provides an integrated  experience out of the box. add  as a script to :
then run  or :
if you don't already have eslint configured in your application, you will be guided through the installation and configuration process.
one of the following three options can be selected:
strict: includes next.js' base eslint configuration along with a stricter . this is the recommended configuration for developers setting up eslint for the first time.
base: includes next.js' base eslint configuration.
cancel: does not include any eslint configuration. only select this option if you plan on setting up your own custom eslint configuration.
if either of the two configuration options are selected, next.js will automatically install  and  as development dependencies in your application and create an  file in the root of your project that includes your selected configuration.
you can now run  every time you want to run eslint to catch errors. once eslint has been set up, it will also automatically run during every build (). errors will fail the build, while warnings will not.
if you do not want eslint to run during , refer to the documentation for .
we recommend using an appropriate  to view warnings and errors directly in your code editor during development.
the default configuration () includes everything you need to have an optimal out-of-the-box linting experience in next.js. if you do not have eslint already configured in your application, we recommend using  to set up eslint along with this configuration.
if you would like to use  along with other eslint configurations, refer to the  section to learn how to do so without causing any conflicts.
recommended rule-sets from the following eslint plugins are all used within :
this will take precedence over the configuration from .
next.js provides an eslint plugin, , already bundled within the base configuration that makes it possible to catch common issues and problems in a next.js application. the full set of rules is as follows:
✔: enabled in the recommended configuration
if you already have eslint configured in your application, we recommend extending from this plugin directly instead of including  unless a few conditions are met. refer to the  to learn more.
if you're using  in a project where next.js isn't installed in your root directory (such as a monorepo), you can tell  where to find your next.js application using the  property in your :
can be a path (relative or absolute), a glob (i.e. ), or an array of paths and/or globs.
by default, next.js will run eslint for all files in the , , , and  directories. however, you can specify which directories using the  option in the  config in  for production builds:
similarly, the  and  flags can be used for  to lint specific directories and files:
to improve performance, information of files processed by eslint are cached by default. this is stored in  or in your defined . if you include any eslint rules that depend on more than the contents of a single source file and need to disable the cache, use the  flag with .
if you would like to modify or disable any rules provided by the supported plugins (, , ), you can directly change them using the  property in your :
the  rule set is enabled when  is run for the first time and the strict option is selected.
updates  to error on a number of rules that are warnings by default if they affect .
the  entry point is automatically included for new applications built with .
eslint also contains code formatting rules, which can conflict with your existing  setup. we recommend including  in your eslint config to make eslint and prettier work together.
first, install the dependency:
then, add  to your existing eslint config:
if you would like to use  with  to run the linter on staged git files, you'll have to add the following to the  file in the root of your project in order to specify usage of the  flag.
if you already have eslint configured in your application and any of the following conditions are true:
you have one or more of the following plugins already installed (either separately or through a different config such as  or ):
you've defined specific  that are different from how babel is configured within next.js (this is not recommended unless you have )
you have  installed with node.js and/or typescript  defined to handle imports
then we recommend either removing these settings if you prefer how these properties have been configured within  or extending directly from the next.js eslint plugin instead:
the plugin can be installed normally in your project without needing to run :
this eliminates the risk of collisions or errors that can occur due to importing the same plugin or parser across multiple configurations.
if you already use a separate eslint configuration and want to include , ensure that it is extended last after other configurations. for example:
the  configuration already handles setting default values for the ,  and  properties. there is no need to manually re-declare any of these properties unless you need a different configuration for your use case. if you include any other shareable configurations, you will need to make sure that these properties are not overwritten or modified. otherwise, we recommend removing any configurations that share behavior with the  configuration or extending directly from the next.js eslint plugin as mentioned above.
typescript
next.js provides an integrated  experience, including zero-configuration set up and built-in types for pages, apis, and more.
you can create a typescript project with  using the  flag like so:
to get started in an existing project, create an empty  file in
the root folder:
next.js will automatically configure this file with default values. providing your own  with custom  is also supported.
you can also provide a relative path to a tsconfig.json file by setting  prop inside your  file.
starting in , next.js uses  by default to compile typescript and tsx for faster builds.
next.js will use babel to handle typescript if  is present. this has some  and some .
then, run  (normally  or ) and next.js will guide you through the installation of the required packages to finish the setup:
you're now ready to start converting files from  to  and leveraging the benefits of typescript!
a file named  will be created at the root of your project. this file ensures next.js types are picked up by the typescript compiler. you should not remove it or edit it as it can change at any time. this file should not be committed and should be ignored by version control (e.g. inside your  file).
typescript  mode is turned off by default. when you feel comfortable with typescript, it's recommended to turn it on in your .
instead of editing , you can include additional types by adding a new file e.g.  and then referencing it in the  array in your .
by default, next.js will do type checking as part of . we recommend using code editor type checking during development.
if you want to silence the error reports, refer to the documentation for .
for , , and , you can use the , , and  types respectively:
if you're using , you can .
the following is an example of how to use the built-in types for api routes:
you can also type the response data:
if you have a , you can use the built-in type  and change file name to  like so:
next.js automatically supports the   and  options.
you can learn more about this feature on the .
the  file must be a javascript file as it does not get parsed by babel or typescript, however you can add some type checking in your ide using jsdoc as below:
since  next.js supports  when enabled in your , this can help speed up type checking in larger applications.
it is highly recommended to be on at least  of typescript to experience the  when leveraging this feature.
next.js fails your production build () when typescript errors are present in your project.
if you'd like next.js to dangerously produce production code even when your application has errors, you can disable the built-in type checking step.
if disabled, be sure you are running type checks as part of your build or deploy process, otherwise this can be very dangerous.
open  and enable the  option in the  config:
environment variables
next.js comes with built-in support for environment variables, which allows you to do the following:
next.js has built-in support for loading environment variables from  into .
an example :
this loads , , and  into the node.js environment automatically allowing you to use them in  and .
for example, using :
note: in order to keep server-only secrets safe, environment variables are evaluated at build time, so only environment variables actually used will be included. this means that  is not a standard javascript object, so you’re not able to
use .
environment variables must be referenced as e.g. , not .
note: next.js will automatically expand variables () inside of your  files.
this allows you to reference other secrets, like so:
if you are trying to use a variable with a  in the actual value, it needs to be escaped like so: .
note: if you are using a  folder, please note that next.js will load the .env files only from the parent folder and not from the  folder.
by default environment variables are only available in the node.js environment, meaning they won't be exposed to the browser.
in order to expose a variable to the browser you have to prefix the variable with . for example:
this loads  into the node.js environment automatically, allowing you to use it anywhere in your code. the value will be inlined into javascript sent to the browser because of the  prefix. this inlining occurs at build time, so your various  envs need to be set when the project is built.
note that dynamic lookups will not be inlined, such as:
in general only one  file is needed. however, sometimes you might want to add some defaults for the  () or  () environment.
next.js allows you to set defaults in  (all environments),  (development environment), and  (production environment).
always overrides the defaults set.
note: , , and  files should be included in your repository as they define defaults.  should be added to , as those files are intended to be ignored.  is where secrets can be stored.
when deploying your next.js application to , environment variables can be configured .
all types of environment variables should be configured there. even environment variables used in development – which can be  afterwards.
if you've configured  you can pull them into a  for usage on your local machine using the following command:
apart from  and  environments, there is a 3rd option available: . in the same way you can set defaults for development or production environments, you can do the same with a  file for the  environment (though this one is not as common as the previous two). next.js will not load environment variables from  or  in the  environment.
this one is useful when running tests with tools like  or  where you need to set specific environment vars only for testing purposes. test default values will be loaded if  is set to , though you usually don't need to do this manually as testing tools will address it for you.
there is a small difference between  environment, and both  and  that you need to bear in mind:  won't be loaded, as you expect tests to produce the same results for everyone. this way every test execution will use the same env defaults across different executions by ignoring your  (which is intended to override the default set).
note: similar to default environment variables,  file should be included in your repository, but  shouldn't, as  are intended to be ignored through .
while running unit tests you can make sure to load your environment variables the same way next.js does by leveraging the  function from the  package.
environment variables are looked up in the following places, in order, stopping once the variable is found.
(not checked when  is .)
for example, if  is  and you define a variable in both  and , the value in  will be used.
note: the allowed values for  are ,  and .
supported browsers and features
next.js supports modern browsers with zero configuration.
chrome 64+
edge 79+
firefox 67+
opera 51+
safari 12+
if you would like to target specific browsers or features, next.js supports  configuration.
we inject , including:
— replacing:  and .
— replacing: the .
— replacing: , , and .
if any of your dependencies includes these polyfills, they’ll be eliminated automatically from the production build to avoid duplication.
in addition, to reduce bundle size, next.js will only load these polyfills for browsers that require them. the majority of the web traffic globally will not download these polyfills.
if your own code or any external npm dependencies require features not supported by your target browsers (such as ie 11), you need to add polyfills yourself.
in this case, you should add a top-level import for the specific polyfill you need in your  or the individual component.
next.js allows you to use the latest javascript features out of the box. in addition to , next.js also supports:
(es2017)
(es2018)
(es2020)
and  (part of stage 3 proposal)
and more!
in addition to  on the client-side, next.js polyfills  in the node.js environment. you can use  in your server code (such as /) without using polyfills such as  or .
next.js has built-in typescript support. .
you can customize babel configuration. .
optimizing scripts
the script component, , allows you to optimally load third-party scripts anywhere in your next.js application. it is an extension of the html  element and enables you to choose between multiple loading strategies to fit your use case.
websites often use third-party scripts to add functionality like analytics, ads, customer support widgets, and consent management. however, this can introduce problems that impact both user and developer experience:
some third-party scripts decrease loading performance and can degrade the user experience, especially if they are blocking the page content from being displayed.
developers are often unsure where and how to load third-party scripts in an application without impacting page performance.
browsers load and execute  elements based on the order of placement in html and the usage of  and  attributes. however, using the native  element creates some challenges:
as your application grows in size and complexity, it becomes increasingly difficult to manage the loading order of third-party scripts.
improve page performance by rendering and hydrating new content as soon as possible, but  attributes (like ) are incompatible without additional work.
the script component solves these problems by providing a declarative api for loading third-party scripts. it provides a set of built-in loading strategies that can be used to optimize the loading sequence of scripts with support for streaming. each of the strategies provided by the script component uses the best possible combination of react and web apis to ensure that scripts are loaded with minimal impact to page performance.
to get started, import the  component:
to load a third-party script in a single route, import  and include the script directly in your page component:
the script will only be fetched and executed when this specific page is loaded on the browser.
to load a third-party script for all routes, import  and include the script directly in :
this script will load and execute when any route in your application is accessed. next.js will ensure the script will only load once, even if a user navigates between multiple pages.
note: you should rarely need to load a third-party script for every page of your application. we recommend only including third-party scripts in specific pages in order to minimize any unnecessary impact to performance.
although the default behavior of  allows you load third-party scripts in any page, you can fine-tune its loading behavior by using the  property:
: load the script before any next.js code and before any page hydration occurs.
: (default) load the script early but after some hydration on the page occurs.
: load the script later during browser idle time.
: (experimental) load the script in a web worker.
refer to the  api reference documentation to learn more about each strategy and their use cases.
note: once a  component has been loaded by the browser, it will stay in the dom and client-side navigations won't re-execute the script.
note: the  strategy is not yet stable and does not yet work with the  directory. use with caution.
scripts that use the  strategy are offloaded and executed in a web worker with . this can improve the performance of your site by dedicating the main thread to the rest of your application code.
this strategy is still experimental and can only be used if the  flag is enabled in :
once setup is complete, defining  will automatically instantiate partytown in your application and offload the script to a web worker.
there are a number of trade-offs that need to be considered when loading a third-party script in a web worker. please see partytown's  documentation for more information.
inline scripts, or scripts not loaded from an external file, are also supported by the script component. they can be written by placing the javascript within curly braces:
or by using the  property:
note: an  property must be assigned for inline scripts in order for next.js to track and optimize the script.
event handlers can be used with the script component to execute additional code after a certain event occurs:
: execute code after the script has finished loading.
: execute code after the script has finished loading and every time the component is mounted.
: execute code if the script fails to load.
refer to the  api reference to learn more about each event handler and view examples.
there are many dom attributes that can be assigned to a  element that are not used by the script component, like  or . including any additional attributes will automatically forward it to the final, optimized  element that is included in the html.
routing
next.js has a file-system based router built on the .
when a file is added to the  directory, it's automatically available as a route.
the files inside the  directory can be used to define most common patterns.
the router will automatically route files named  to the root of the directory.
the router supports nested files. if you create a nested folder structure, files will automatically be routed in the same way still.
to match a dynamic segment, you can use the bracket syntax. this allows you to match named parameters.
→  ()
check out the  to learn more about how they work.
the next.js router allows you to do client-side route transitions between pages, similar to a single-page application.
a react component called  is provided to do this client-side route transition.
the example above uses multiple links. each one maps a path () to a known page:
any  in the viewport (initially or through scroll) will be prefetched by default (including the corresponding data) for pages using . the corresponding data for  routes is fetched only when the  is clicked.
you can also use interpolation to create the path, which comes in handy for . for example, to show a list of posts which have been passed to the component as a prop:
is used in the example to keep the path utf-8 compatible.
alternatively, using a url object:
now, instead of using interpolation to create the path, we use a url object in  where:
is the name of the page in the  directory.  in this case.
is an object with the dynamic segment.  in this case.
to access the  in a react component you can use  or .
in general we recommend using .
the router is divided in multiple parts:
dynamic routes
defining routes by using predefined paths is not always enough for complex applications. in next.js you can add brackets to a page () to create a dynamic route (a.k.a. url slugs, pretty urls, and others).
consider the following page :
any route like , , etc. will be matched by . the matched path parameter will be sent as a query parameter to the page, and it will be merged with the other query parameters.
for example, the route  will have the following  object:
similarly, the route  will have the following  object:
however, route parameters will override query parameters with the same name. for example, the route  will have the following  object:
multiple dynamic route segments work the same way. the page  will match the route  and its  object will be:
client-side navigations to dynamic routes are handled with . if we wanted to have links to the routes used above it will look like this:
read our docs for  to learn more.
dynamic routes can be extended to catch all paths by adding three dots () inside the brackets. for example:
matches , but also ,  and so on.
note: you can use names other than , such as:
matched parameters will be sent as a query parameter ( in the example) to the page, and it will always be an array, so, the path  will have the following  object:
and in the case of , and any other matching path, new parameters will be added to the array, like so:
catch all routes can be made optional by including the parameter in double brackets ().
for example,  will match , , , and so on.
the main difference between catch all and optional catch all routes is that with optional, the route without the parameter is also matched ( in the example above).
the  objects are as follows:
predefined routes take precedence over dynamic routes, and dynamic routes over catch all routes. take a look at the following examples:
- will match
- will match , , etc. but not
- will match , , etc. but not ,
pages that are statically optimized by  will be hydrated without their route parameters provided, i.e  will be an empty object ().
after hydration, next.js will trigger an update to your application to provide the route parameters in the  object.
imperatively
should be able to cover most of your routing needs, but you can also do client-side navigations without it, take a look at the .
the following example shows how to do basic page navigations with :
shallow routing
shallow routing allows you to change the url without running data fetching methods again, that includes , , and .
you'll receive the updated  and the  via the  (added by  or ), without losing state.
to enable shallow routing, set the  option to . consider the following example:
the url will get updated to . and the page won't get replaced, only the state of the route is changed.
you can also watch for url changes via  as shown below:
shallow routing only works for url changes in the current page. for example, let's assume we have another page called , and you run this:
since that's a new page, it'll unload the current page, load the new one and wait for data fetching even though we asked to do shallow routing.
when shallow routing is used with middleware it will not ensure the new page matches the current page like previously done without middleware. this is due to middleware being able to rewrite dynamically and can't be verified client-side without a data fetch which is skipped with shallow, so a shallow route change must always be treated as shallow.
api routes
api routes provide a solution to build your api with next.js.
any file inside the folder  is mapped to  and will be treated as an api endpoint instead of a . they are server-side only bundles and won't increase your client-side bundle size.
for example, the following api route  returns a  response with a status code of :
note: api routes will be affected by  in .
for an api route to work, you need to export a function as default (a.k.a request handler), which then receives the following parameters:
: an instance of , plus some
to handle different http methods in an api route, you can use  in your request handler, like so:
to fetch api endpoints, take a look into any of the examples at the start of this section.
for new projects, you can build your entire api with api routes. if you have an existing api, you do not need to forward calls to the api through an api route. some other use cases for api routes are:
masking the url of an external service (e.g.  instead of )
using  on the server to securely access external services.
api routes , meaning they are same-origin only by default. you can customize such behavior by wrapping the request handler with the .
api routes can't be used with
dynamic api routes
api routes support , and follow the same file naming rules used for .
for example, the api route  has the following code:
now, a request to  will respond with the text: .
a very common restful pattern is to set up routes like this:
- gets a list of posts, probably paginated
- gets post id 12345
we can model this in two ways:
option 1:
option 2:
both are equivalent. a third option of only using  is not valid because dynamic routes (including catch-all routes - see below) do not have an  state and  will not match  under any circumstances.
api routes can be extended to catch all paths by adding three dots () inside the brackets. for example:
an api route for  could look like this:
predefined api routes take precedence over dynamic api routes, and dynamic api routes over catch all api routes. take a look at the following examples:
api routes request helpers
api routes provide built-in request helpers which parse the incoming request ():
- an object containing the cookies sent by the request. defaults to
- an object containing the . defaults to
- an object containing the body parsed by , or  if no body was sent
every api route can export a  object to change the default configuration, which is the following:
the  object includes all config options available for api routes.
is automatically enabled. if you want to consume the body as a  or with , you can set this to .
one use case for disabling the automatic  is to allow you to verify the raw body of a webhook request, for example .
is the maximum size allowed for the parsed body, in any format supported by , like so:
is an explicit flag that tells the server that this route is being handled by an external resolver like express or connect. enabling this option disables warnings for unresolved requests.
is automatically enabled, warning when an api routes' response body is over 4mb.
if you are not using next.js in a serverless environment, and understand the performance implications of not using a cdn or dedicated media host, you can set this limit to .
can also take the number of bytes or any string format supported by , for example ,  or .
this value will be the maximum response size before a warning is displayed. default is 4mb. (see above)
for better type-safety, it is not recommended to extend the  and  objects. instead, use functions to work with them:
if you can't avoid these objects from being extended, you have to create your own type to include the extra properties:
keep in mind this is not safe since the code will still compile even if you remove  from the export.
api routes response helpers
the , (often abbreviated as ) includes a set of express.js-like helper methods to improve the developer experience and increase the speed of creating new api endpoints.
the included helpers are:
- a function to set the status code.  must be a valid
- sends a json response.  must be a
- sends the http response.  can be a , an  or a
- redirects to a specified path or url.  must be a valid . if not specified,  defaults to "307" "temporary redirect".
-  using .  must be a .
when sending a response back to the client, you can set the status code of the response.
the following example sets the status code of the response to  () and returns a  property with the value of  as a json response:
when sending a response back to the client you can send a json response, this must be a .
in a real world application you might want to let the client know the status of the request depending on the result of the requested endpoint.
the following example sends a json response with the status code  () and the result of the async operation. it's contained in a try catch block to handle any errors that may occur, with the appropriate status code and error message caught and sent back to the client:
sending an http response works the same way as when sending a json response. the only difference is that the response body can be a , an  or a .
the following example sends a http response with the status code  () and the result of the async operation.
taking a form as an example, you may want to redirect your client to a specified path or url once they have submitted the form.
the following example redirects the client to the  path if the form is successfully submitted:
you can make your response handlers more type-safe by importing the  and  types from , in addition to those, you can also type your response data:
note: the body of  is  because the client may include any payload. you should validate the type/shape of the body at runtime before using it.
to view more examples using types, check out the .
if you prefer to view your examples within a real projects structure you can checkout our examples repository:
edge api routes (beta)
edge api routes enable you to build high performance apis with next.js. using the , they are often faster than node.js-based api routes. this performance improvement does come with , like not having access to native node.js apis. instead, edge api routes are built on standard web apis.
any file inside the folder  is mapped to  and will be treated as an api endpoint instead of a page. they are server-side only bundles and won't increase your client-side bundle size.
edge api routes use the , whereas api routes use the .
edge api routes can  from the server and run after cached files (e.g. html, css, javascript) have been accessed. server-side streaming can help improve performance with faster .
view the  and  for the edge runtime.
going to production
before taking your next.js application to production, here are some recommendations to ensure the best user experience.
use  wherever possible.
ensure your database and backend are deployed in the same region.
aim to ship the least amount of javascript possible.
defer loading heavy javascript bundles until needed.
ensure  is set up.
configure the  (not found) and  (error) pages.
ensure you are .
run  to check for performance, best practices, accessibility, and seo. for best results, use a production build of next.js and use incognito in your browser so results aren't affected by extensions.
review .
improve performance using:
improve
caching improves response times and reduces the number of requests to external services. next.js automatically adds caching headers to immutable assets served from  including javascript, css, static images, and other media.
headers set in  will be overwritten in production to ensure that static assets can be cached effectively. if you need to revalidate the cache of a page that has been , you can do so by setting  in the page's  function. if you're using , there are also  for the default image optimization loader.
note: when running your application locally with , your headers are overwritten to prevent caching locally.
you can also use caching headers inside  and api routes for dynamic responses. for example, using .
by default,  headers will be set differently depending on how your page fetches data.
if the page uses  or , it will use the default  header set by  in order to prevent accidental caching of responses that cannot be cached. if you want a different cache behavior while using , use  inside of the function as shown above.
if the page is using , it will have a  header of , or if  is not used,  to cache for the maximum age possible.
note: your deployment provider must support caching for dynamic responses. if you are self-hosting, you will need to add this logic yourself using a key/value store like redis. if you are using vercel, .
to reduce the amount of javascript sent to the browser, you can use the following tools to understand what is included inside each javascript bundle:
– display the size of the imported package inside vscode.
– find the cost of adding a new dev dependency to your project.
- analyze how much a dependency can increase bundle sizes.
– visualize the size of webpack output files with an interactive, zoomable treemap.
- an online tool to quickly bundle & minify your projects, while viewing the compressed gzip/brotli bundle size, all running locally on your browser.
each file inside your  directory will automatically be code split into its own javascript bundle during . you can also use  to lazy-load components and libraries. for example, you might want to defer loading your modal code until a user clicks the open button.
since next.js runs on both the client and server, there are multiple forms of logging supported:
in the browser
on the server
if you want a structured logging package, we recommend . if you're using vercel, there are  compatible with next.js.
when an unhandled exception occurs, you can control the experience for your users with the . we recommend customizing this to your brand instead of the default next.js theme.
you can also log and track exceptions with a tool like sentry.  shows how to catch & report errors on both the client and server-side, using the sentry sdk for next.js. there's also a .
to improve loading performance, you first need to determine what to measure and how to measure it.  is a good industry standard that is measured using your own web browser. if you are not familiar with the metrics of core web vitals, review this  and determine which specific metric/s will be your drivers for loading performance. ideally, you would want to measure the loading performance in the following environments:
in the lab, using your own computer or a simulator.
in the field, using real-world data from actual visitors.
local, using a test that runs on your device.
remote, using a test that runs in the cloud.
once you are able to measure the loading performance, use the following strategies to improve it iteratively so that you apply one strategy, measure the new performance and continue tweaking until you do not see much improvement. then, you can move on to the next strategy.
use caching regions that are close to the regions where your database or api is deployed.
as described in the  section, use a  value that will not overload your backend.
use  to reduce the number of requests to your backend.
remove unused javascript. review this  to understand what core web vitals metrics bundle size affects and what strategies you can use to reduce it, such as:
setting up your code editor to view import costs and sizes
finding alternative smaller packages
dynamically loading components and dependencies
for more in-depth information, review this  and this .
deployment
congratulations, you are ready to deploy your next.js application to production. this document will show how to deploy either managed or self-hosted using the .
generates an optimized version of your application for production. this standard output includes:
html files for pages using  or
css files for global styles or for individually scoped styles
javascript for pre-rendering dynamic content from the next.js server
javascript for interactivity on the client-side through react
this output is generated inside the  folder:
– each javascript file inside this folder relates to the route with the same name. for example,  would be the javascript file loaded when viewing the  route in your application
– statically imported images from  are hashed and copied here
– global css files for all pages in your application
– the html and javascript entry points prerendered from the server. the  files are created when  is enabled and contain all the file paths that depend on a given page.
– shared javascript chunks used in multiple places throughout your application
– output for the build cache and cached images, responses, and pages from the next.js server. using a cache helps decrease build times and improve performance of loading images
all javascript code inside  has been compiled and browser bundles have been minified to help achieve the best performance and support .
is the fastest way to deploy your next.js application with zero configuration.
when deploying to vercel, the platform , runs , and optimizes the build output for you, including:
persisting cached assets across deployments if unchanged
with a unique url for every commit
are automatically statically optimized, if possible
assets (javascript, css, images, fonts) are compressed and served from a
are automatically optimized as isolated  that can scale infinitely
are automatically optimized as  that have zero cold starts and boot instantly
in addition, vercel provides features like:
automatic performance monitoring with
automatic https and ssl certificates
automatic ci/cd (through github, gitlab, bitbucket, etc.)
support for
support for  with
instant global deployments via
for free to try it out.
you can self-host next.js with support for all features using node.js or docker. you can also do a static html export, which .
next.js can be deployed to any hosting provider that supports node.js. for example,  or a .
first, ensure your  has the  and  scripts:
then, run  to build your application. finally, run  to start the node.js server. this server supports all features of next.js.
if you are using , consider adding  for more performant  in your production environment by running  in your project directory. on linux platforms,  may require  to prevent excessive memory usage.
next.js can be deployed to any hosting provider that supports  containers. you can use this approach when deploying to container orchestrators such as  or , or when running inside a single node in any cloud provider.
on your machine
clone the  example
build your container:
run your container:
if you need to use different environment variables across multiple environments, check out our  example.
if you’d like to do a static html export of your next.js app, follow the directions on our .
the following services support next.js . below, you’ll find examples or guides to deploy next.js to each service.
note: there are also managed platforms that allow you to use a dockerfile as shown in the .
the following services support deploying next.js using .
you can also manually deploy the  output to any static hosting provider, often through your ci/cd pipeline like github actions, jenkins, aws codebuild, circle ci, azure pipelines, and more.
note: not all serverless providers implement the  from . please check with the provider to see what features are supported.
when you deploy your next.js application, you want to see the latest version without needing to reload.
next.js will automatically load the latest version of your application in the background when routing. for client-side navigations,  will temporarily function as a normal  tag.
note: if a new page (with an old version) has already been prefetched by , next.js will use the old version. navigating to a page that has not been prefetched (and is not cached at the cdn level) will load the latest version.
sometimes you might want to run some cleanup code on process signals like  or .
you can do that by setting the env variable  to  and then register a handler for that signal inside your  file.
authentication
authentication verifies who a user is, while authorization controls what a user can access. next.js supports multiple authentication patterns, each designed for different use cases. this page will go through each case so that you can choose based on your constraints.
the first step to identifying which authentication pattern you need is understanding the  you want. we can then determine which authentication providers support this strategy. there are two main patterns:
use  to server-render a loading state, followed by fetching user data client-side.
fetch user data  to eliminate a flash of unauthenticated content.
next.js automatically determines that a page is static if there are no blocking data requirements. this means the absence of  and  in the page. instead, your page can render a loading state from the server, followed by fetching the user client-side.
one advantage of this pattern is it allows pages to be served from a global cdn and preloaded using . in practice, this results in a faster tti ().
let's look at an example for a profile page. this will initially render a loading skeleton. once the request for a user has finished, it will show the user's name:
you can view this . check out the  example to see how it works.
if you export an  function called  from a page, next.js will pre-render this page on each request using the data returned by .
let's transform the profile example to use . if there's a session, return  as a prop to the  component in the page. notice there is not a loading skeleton in .
an advantage of this pattern is preventing a flash of unauthenticated content before redirecting. it's important to note fetching user data in  will block rendering until the request to your authentication provider resolves. to prevent creating a bottleneck and increasing your ttfb (), you should ensure your authentication lookup is fast. otherwise, consider .
now that we've discussed authentication patterns, let's look at specific providers and explore how they're used with next.js.
if you have an existing database with user data, you'll likely want to utilize an open-source solution that's provider agnostic.
if you want a low-level, encrypted, and stateless session utility use .
if you want a full-featured authentication system with built-in providers (google, facebook, github…), jwt, jwe, email/password, magic links and more… use .
both of these libraries support either authentication pattern. if you're interested in , we also have examples for it using secure and encrypted cookies:
to see examples with other authentication providers, check out the .
testing
learn how to set up next.js with commonly used testing tools: , , and .
cypress is a test runner used for end-to-end (e2e) and integration testing.
you can use  with the  to quickly get started.
to get started with cypress, install the  package:
add cypress to the  scripts field:
run cypress for the first time to generate examples that use their recommended folder structure:
you can look through the generated examples and the  section of the cypress documentation to help you get familiar with cypress.
assuming the following two next.js pages:
add a test to check your navigation is working correctly:
you can use  instead of  if you add  to the  configuration file.
since cypress is testing a real next.js application, it requires the next.js server to be running prior to starting cypress. we recommend running your tests against your production code to more closely resemble how your application will behave.
run  and , then run  in another terminal window to start cypress.
note: alternatively, you can install the  package and add it to the  scripts field:  to start the next.js production server in conjunction with cypress. remember to rebuild your application after new changes.
you will have noticed that running cypress so far has opened an interactive browser which is not ideal for ci environments. you can also run cypress headlessly using the  command:
you can learn more about cypress and continuous integration from these resources:
playwright is a testing framework that lets you automate chromium, firefox, and webkit with a single api. you can use it to write end-to-end (e2e) and integration tests across all platforms.
the fastest way to get started is to use  with the . this will create a next.js project complete with playwright all set up.
you can also use  to add playwright to an existing  project.
to manually get started with playwright, install the  package:
add playwright to the  scripts field:
add a test to verify that your navigation is working correctly:
you can use  instead of , if you add  to the  configuration file.
since playwright is testing a real next.js application, it requires the next.js server to be running prior to starting playwright. it is recommended to run your tests against your production code to more closely resemble how your application will behave.
run  and , then run  in another terminal window to run the playwright tests.
note: alternatively, you can use the  feature to let playwright start the development server and wait until it's fully available.
playwright will by default run your tests in the . to install all the playwright dependencies, run .
you can learn more about playwright and continuous integration from these resources:
jest and react testing library are frequently used together for unit testing. there are three ways you can start using jest within your next.js application:
using one of our
with the
with
the following sections will go through how you can set up jest with each of these options:
you can use  with the  example to quickly get started with jest and react testing library:
since the release of , next.js now has built-in configuration for jest.
to set up jest, install , , , :
create a  file in your project's root directory and add the following:
under the hood,  is automatically configuring jest for you, including:
setting up  using
auto mocking stylesheets (, , and their scss variants), image imports and
loading  (and all variants) into
ignoring  from test resolving and transforms
ignoring  from test resolving
loading  for flags that enable swc transforms
note: to test environment variables directly, load them manually in a separate setup script or in your  file. for more information, please see .
if you opt out of the , you will need to manually configure jest and install  and  in addition to the packages above.
here are the recommended options to configure jest for next.js:
you can learn more about each configuration option in the .
handling stylesheets and image imports
stylesheets and images aren't used in the tests but importing them may cause errors, so they will need to be mocked. create the mock files referenced in the configuration above -  and  - inside a  directory:
for more information on handling static assets, please refer to the .
optional: extend jest with custom matchers
includes a set of convenient  such as  making it easier to write tests. you can import the custom matchers for every test by adding the following option to the jest configuration file:
then, inside , add the following import:
if you need to add more setup options before each test, it's common to add them to the  file above.
optional: absolute imports and module path aliases
if your project is using , you will need to configure jest to resolve the imports by matching the paths option in the  file with the  option in the  file. for example:
add a test script to package.json
add the jest executable in watch mode to the  scripts:
will re-run tests when a file is changed. for more jest cli options, please refer to the .
create your first tests
your project is now ready to run tests. follow jest's convention by adding tests to the  folder in your project's root directory.
for example, we can add a test to check if the  component successfully renders a heading:
optionally, add a  to keep track of any unexpected changes to your  component:
note: test files should not be included inside the pages directory because any files inside the pages directory are considered routes.
running your test suite
run  to run your test suite. after your tests pass or fail, you will notice a list of interactive jest commands that will be helpful as you add more tests.
for further reading, you may find these resources helpful:
- use good testing practices to match elements.
the next.js community has created packages and articles you may find helpful:
for storybook.
by gleb bahmutov.
for more information on what to read next, we recommend:
accessibility
the next.js team is committed to making next.js accessible to all developers (and their end-users). by adding accessibility features to next.js by default, we aim to make the web more inclusive for everyone.
when transitioning between pages rendered on the server (e.g. using the  tag) screen readers and other assistive technology announce the page title when the page loads so that users understand that the page has changed.
in addition to traditional page navigations, next.js also supports client-side transitions for improved performance (using ). to ensure that client-side transitions are also announced to assistive technology, next.js includes a route announcer by default.
the next.js route announcer looks for the page name to announce by first inspecting , then the  element, and finally the url pathname. for the most accessible user experience, ensure that each page in your application has a unique and descriptive title.
next.js provides an  out of the box, including custom rules for next.js. by default, next.js includes  to help catch accessibility issues early, including warning on:
for example, this plugin helps ensure you add alt text to  tags, use correct  attributes, use correct  attributes, and more.
by default, next.js prerenders pages to static html files. this means that javascript is not required to view the html markup from the server and is instead used to add interactivity on the client side.
if your application requires javascript to be disabled, and only html to be used, you can remove all javascript from your application using an experimental flag:
check  between foreground and background elements
use  when working with animations
building forms with next.js
a web form has a client-server relationship. they are used to send data handled by a web server for processing and storage. the form itself is the client, and the server is any storage mechanism that can be used to store, retrieve and send data when needed.
this guide will teach you how to create a web form with next.js.
html forms are built using the  tag. it takes a set of attributes and fields to structure the form for features like text fields, checkboxes, dropdown menus, buttons, radio buttons, etc.
here's the syntax of a html form:
the front-end looks like this:
the html  tag acts as a container for different  elements like  field and submit . let's study each of these elements:
: an attribute that specifies where the form data is sent when the form is submitted. it's generally a url (an absolute url or a relative url).
: specifies the , i.e.,  or  used to send data while submitting the form.
: an element that defines the label for other form elements. labels aid accessibility, especially for screen readers.
: the form element that is widely used to structure the form fields. it depends significantly on the value of the  attribute. input types can be , , , , and more.
: represents a clickable button that's used to submit the form data.
a process that checks if the information provided by a user is correct or not. form validation also ensures that the provided information is in the correct format (e.g. there's an @ in the email field). these are of two types:
client-side: validation is done in the browser
server-side: validation is done on the server
though both of these types are equally important, this guide will focus on client-side validation only.
client-side validation is further categorized as:
built-in: uses html-based attributes like , , , , , etc.
javascript-based: validation that's coded with javascript.
: specifies which fields must be filled before submitting the form.
: specifies the data's type (i.e a number, email address, string, etc).
: specifies minimum length for the text data string.
: specifies maximum length for the text data string.
so, a form using this attributes may look like:
with these validation checks in place, when a user tries to submit an empty field for name, it gives an error that pops right in the form field. similarly, a roll number can only be entered if it's 10-20 characters long.
form validation is important to ensure that a user has submitted the correct data, in a correct format. javascript offers an additional level of validation along with html native form attributes on the client side. developers generally prefer validating form data through javascript because its data processing is faster when compared to server-side validation, however front-end validation may be less secure in some scenarios as a malicious user could always send malformed data to your server.
the following example shows using javascript to validate a form:
the html  tag is used to embed any client-side javascript. it can either contain inline scripting statements (as shown in the example above) or point to an external script file via the  attribute.
this example validates the name and roll number of a user. the  function does not allow an empty name field, and the roll number must be at least three digits long. the validation is performed when you hit the submit button. you are not redirected to the next page until the given values are correct.
javascript validation with regular expressions uses the  html attribute. a regular expression (commonly known as regex) is an object that describes a pattern of characters. you can only apply the  attribute to the  element. this way, you can validate the input value using regular expressions (regex) by defining your own rules. once again, if the value does not match the defined pattern, the input will give an error.
the below example shows using the  attribute on an  element:
the password form field must only contain digits (0 to 9), lowercase alphabets (a to z) and it must be no more than 15 characters in length. no other characters (#,$,&, etc.) are allowed. the rule in regex is written as .
to learn more about html forms, check out the .
in the following section you will be creating forms in react using next.js.
create a new next.js app. you can use the  for a quick start. in your command line terminal, run the following:
answer the questions to create your project, and give it a name, this example uses . next  into this directory, and run  or  command to start the development server.
open the url printed in the terminal to ensure that your app is running successfully.
both the client and the server will be built using next.js. for the server part, create an api endpoint where you will send the form data.
next.js offers a file-based system for routing that's built on the . any file inside the folder  is mapped to  and will be treated as an api endpoint instead of a page. this  is going to be server-side only.
go to , create a file called  and paste this code written in node.js:
this form  function will receive the request  from the client (i.e. submitted form data). and in return, it'll send a response  as json that will have both the first and the last name. you can access this api endpoint at  or replace the localhost url with an actual vercel deployment when you deploy.
moreover, you can also attach this api to a database like mongodb or google sheets. this way, your submitted form data will be securely stored for later use. for this guide, no database is used. instead, the same data is returned to the user to demo how it's done.
you can now use  relative endpoint inside the  attribute of the form. you are sending form data to the server when the form is submitted via  http method (which is used to send data).
if you submit this form, it will submit the data to the forms api endpoint . the server then responds, generally handling the data and loading the url defined by the action attribute, causing a new page load. so in this case you'll be redirected to  with the following response from the server.
you have created a next.js api route for form submission. now it's time to configure the client (the form itself) inside next.js using react. the first step will be extending your knowledge of html forms and converting it into react (using ).
here's the same form in a  written using .
here's what changed:
the  attribute is changed to . (since  is a keyword associated with the "for" loop in javascript, react elements use  instead.)
the  attribute now has a relative url which is the form api endpoint.
this completes the basic structure of your next.js-based form.
you can view the entire source code of  example repo that we're creating here as a working example. feel free to clone it and start right away. this demo is built with create-next-app, and you can preview the basic form css styles inside  file.
javascript brings interactivity to our web applications, but sometimes you need to control the javascript bundle from being too large, or your sites visitors might have javascript disabled.
there are several reasons why users disable javascript:
addressing bandwidth constraints
increasing device (phone or laptop) battery life
for privacy so they won’t be tracked with analytical scripts
regardless of the reason, disabling javascript will impact site functionality partially, if not completely.
next open the  directory. inside the  directory, create a file .
quick tip: in next.js, a page is a react component exported from a , , , or  file in the pages directory. each page is associated with a route based on its file name.
example: if you create , it will be accessible at .
let's use the same code from above:
with javascript disabled, when you hit the submit button, an event is triggered, which collects the form data and sends it to our forms api endpoint as defined in the  attribute and using  http . you'll be redirected to the  endpoint since that's how form  works.
the form data will be submitted on the server as a request  to the form handler function written above. it will process the data and return a json string as a response  with your submitted name included.
to improve the experience here, as a response you can redirect the user to a page and thank them for submitting the form.
inside , you'll create another file called . this will create a  page on your next.js app.
now, as soon as the form is submitted, we prevent the form's default behavior of reloading the page. we'll take the form data, convert it to json string, and send it to our server, the api endpoint. finally, our server will respond with the name submitted. all of this with a basic javascript  function.
here's what this function looks like. it's well documented for you to understand each step:
it's a next.js page with a react function component called  with a  element written in jsx. there's no action on the  element. instead, we use the  event handler to send data to our  function.
the  function processes your form data through a series of steps:
the  stops the  element from refreshing the entire page.
we created a javascript object called  with the  and  values from the form.
json is a language-agnostic data transfer format. so we use  to convert the data to json.
we then use  to send the data to our  endpoint using json and http  method.
server sends back a response with the name submitted. woohoo! 🥳
this guide has covered the following:
the basic html  element
understanding forms with react.js
validating forms data with and without javascript
using next.js api routes to handle  and  from the client and server
for more details go through .
next.js compiler
the next.js compiler, written in rust using , allows next.js to transform and minify your javascript code for production. this replaces babel for individual files and terser for minifying output bundles.
compilation using the next.js compiler is 17x faster than babel and enabled by default since next.js version 12. if you have an existing babel configuration or are using , your application will opt-out of the next.js compiler and continue using babel.
is an extensible rust-based platform for the next generation of fast developer tools.
swc can be used for compilation, minification, bundling, and more – and is designed to be extended. it's something you can call to perform code transformations (either built-in or custom). running those transformations happens through higher-level tools like next.js.
we chose to build on swc for a few reasons:
extensibility: swc can be used as a crate inside next.js, without having to fork the library or workaround design constraints.
performance: we were able to achieve ~3x faster fast refresh and ~5x faster builds in next.js by switching to swc, with more room for optimization still in progress.
webassembly: rust's support for wasm is essential for supporting all possible platforms and taking next.js development everywhere.
community: the rust community and ecosystem are amazing and still growing.
we're working to port  to the next.js compiler.
first, update to the latest version of next.js: . then, update your  file:
,  and  are not yet implemented. you can follow the progress .  and  transforms are the main requirement for using  in next.js.
jest support not only includes the transformation previously provided by babel, but also simplifies configuring jest together with next.js including:
auto mocking of ,  (and their  variants), and image imports
automatically sets up  using swc
ignores  from test resolving and transforms
loads  for flags that enable experimental swc transforms
to enable  support:
note: in next.js all javascript files in  directory are considered routes. so, for  you'll need to specify  configuration settings outside of the , otherwise  will generate files next to the source file in the  directory, and this file will be considered a route, which will break production builds.
allows to remove jsx properties. this is often used for testing. similar to .
to remove properties matching the default regex :
to remove custom properties:
this transform allows for removing all  calls in application code (not ). similar to .
remove all  calls:
remove  output except :
next.js will automatically detect  in  or . legacy decorators are commonly used with older versions of libraries like .
this flag is only supported for compatibility with existing applications. we do not recommend using legacy decorators in new applications.
first, update to the latest version of next.js: . then, update your  or  file:
next.js will automatically detect  in  or  and apply that. this is commonly used with libraries like .
next.js' swc compiler is used for minification by default since v13. this is 7x faster than terser.
if terser is still needed for any reason this can be configured.
while the minifier is experimental, we are making the following options available for debugging purposes. they will not be available once the minifier is made stable.
if your app works with the options above, it means  is the problematic option.
see  for detailed options.
allows to modularize imports, similar to .
transforms member style imports:
...into default style imports:
config for the above transform:
advanced transformations:
using regular expressions
similar to , but the transform is templated with  and regular expressions are in rust  crate's syntax.
the config:
cause this code:
to become:
handlebars templating
this transform uses  to template the replacement import path in the  field. these variables and helper functions are available:
: has type . all groups matched by the regular expression.  is the full match.
: has type . the name of the member import.
, , , : helper functions to convert a string to lower, upper, camel or kebab cases.
you can generate swc's internal transform traces as chromium's .
once enabled, swc will generate trace named as  under . chromium's trace viewer (chrome://tracing/, ), or compatible flamegraph viewer () can load & visualize generated traces.
you can configure swc's transform to use swc's experimental plugin support written in wasm to customize transformation behavior.
accepts an array of tuples for configuring plugins. a tuple for the plugin contains the path to the plugin and an object for plugin configuration. the path to the plugin can be an npm module package name or an absolute path to the  binary itself.
when your application has a  file, next.js will automatically fall back to using babel for transforming individual files. this ensures backwards compatibility with existing applications that leverage custom babel plugins.
if you're using a custom babel setup, . we're working to port as many commonly used babel transformations as possible, as well as supporting plugins in the future.
turbopack (alpha)
is an incremental bundler optimized for javascript and typescript, written in rust, and built into next.js 13.
on large applications turbopack updates 10x faster than vite and 700x faster than webpack. for the biggest applications, the difference is even larger with updates up to 20x faster than vite.
turbopack can be used in next.js 13 in both the  and  directories:
create a next.js 13 project with turbopack
start the next.js development server (with turbopack)
to learn more about the currently supported features for turbopack, view the .
preview mode
this document is for next.js versions 9.3 and up. if you’re using older versions of next.js, refer to our .
in the  and the , we talked about how to pre-render a page at build time (static generation) using  and .
static generation is useful when your pages fetch data from a headless cms. however, it’s not ideal when you’re writing a draft on your headless cms and want to preview the draft immediately on your page. you’d want next.js to render these pages at request time instead of build time and fetch the draft content instead of the published content. you’d want next.js to bypass static generation only for this specific case.
next.js has a feature called preview mode which solves this problem. here are instructions on how to use it.
take a look at the  first if you’re not familiar with next.js api routes.
first, create a preview api route. it can have any name - e.g.  (or  if using typescript).
in this api route, you need to call  on the response object. the argument for  should be an object, and this can be used by  (more on this later). for now, we’ll use .
sets some cookies on the browser which turns on the preview mode. any requests to next.js containing these cookies will be considered as the preview mode, and the behavior for statically generated pages will change (more on this later).
you can test this manually by creating an api route like below and accessing it from your browser manually:
if you use your browser’s developer tools, you’ll notice that the  and  cookies will be set on this request.
in practice, you’d want to call this api route securely from your headless cms. the specific steps will vary depending on which headless cms you’re using, but here are some common steps you could take.
these steps assume that the headless cms you’re using supports setting custom preview urls. if it doesn’t, you can still use this method to secure your preview urls, but you’ll need to construct and access the preview url manually.
first, you should create a secret token string using a token generator of your choice. this secret will only be known by your next.js app and your headless cms. this secret prevents people who don’t have access to your cms from accessing preview urls.
second, if your headless cms supports setting custom preview urls, specify the following as the preview url. (this assumes that your preview api route is located at .)
should be your deployment domain.
should be replaced with the secret token you generated.
should be the path for the page that you want to preview. if you want to preview , then you should use .
your headless cms might allow you to include a variable in the preview url so that  can be set dynamically based on the cms’s data like so:
finally, in the preview api route:
check that the secret matches and that the  parameter exists (if not, the request should fail).
call .
then redirect the browser to the path specified by . (the following example uses a ).
if it succeeds, then the browser will be redirected to the path you want to preview with the preview mode cookies being set.
the next step is to update  to support the preview mode.
if you request a page which has  with the preview mode cookies set (via ), then  will be called at request time (instead of at build time).
furthermore, it will be called with a  object where:
will be .
will be the same as the argument used for .
we used  in the preview api route, so  will be . you can use this to pass session information from the preview api route to  if necessary.
if you’re also using , then  will also be available.
you can update  to fetch different data based on  and/or .
for example, your headless cms might have a different api endpoint for draft posts. if so, you can use  to modify the api endpoint url like below:
that’s it! if you access the preview api route (with  and ) from your headless cms or manually, you should now be able to see the preview content. and if you update your draft without publishing, you should be able to preview the draft.
note: during rendering  exposes an  flag, see the  for more info.
by default, no expiration date is set for preview mode cookies, so the preview session ends when the browser is closed.
to clear the preview mode cookies manually, create an api route that calls :
then, send a request to  to invoke the api route. if calling this route using , you must pass  to prevent calling  during link prefetching.
takes an optional second parameter which should be an options object. it accepts the following keys:
: specifies the number (in seconds) for the preview session to last for.
: specifies the path the cookie should be applied under. defaults to  enabling preview mode for all paths.
you can pass an object to  and have it be available in . however, because the data will be stored in a cookie, there’s a size limitation. currently, preview data is limited to 2kb.
the preview mode works on  as well. it will also be available on the  object containing  and .
api routes will have access to  and  under the request object. for example:
both the bypass cookie value and the private key for encrypting the  change when  is completed.
this ensures that the bypass cookie can’t be guessed.
note: to test preview mode locally over http your browser will need to allow third-party cookies and local storage access.
the following pages might also be useful.
dynamic import
next.js supports lazy loading external libraries with  and react components with . deferred loading helps improve the initial loading performance by decreasing the amount of javascript necessary to render the page. components or libraries are only imported and included in the javascript bundle when they're used.
is an extension of . when used in combination with , components can delay hydration until the suspense boundary is resolved.
by using , the header component will not be included in the page's initial javascript bundle. the page will render the suspense  first, followed by the  component when the  boundary is resolved.
note: in , the path must be explicitly written. it can't be a template string nor a variable. furthermore the  has to be inside the  call for next.js to be able to match webpack bundles / module ids to the specific  call and preload them before rendering.  can't be used inside of react rendering as it needs to be marked in the top level of the module for preloading to work, similar to .
to dynamically import a named export, you can return it from the  returned by :
to dynamically load a component on the client side, you can use the  option to disable server-rendering. this is useful if an external dependency or component relies on browser apis like .
this example uses the external library  for fuzzy search. the module is only loaded in the browser after the user types in the search input.
automatic static optimization
next.js automatically determines that a page is static (can be prerendered) if it has no blocking data requirements. this determination is made by the absence of  and  in the page.
this feature allows next.js to emit hybrid applications that contain both server-rendered and statically generated pages.
statically generated pages are still reactive: next.js will hydrate your application client-side to give it full interactivity.
one of the main benefits of this feature is that optimized pages require no server-side computation, and can be instantly streamed to the end-user from multiple cdn locations. the result is an ultra fast loading experience for your users.
if  or  is present in a page, next.js will switch to render the page on-demand, per-request (meaning ).
if the above is not the case, next.js will statically optimize your page automatically by prerendering the page to static html.
during prerendering, the router's  object will be empty since we do not have  information to provide during this phase. after hydration, next.js will trigger an update to your application to provide the route parameters in the  object.
the cases where the query will be updated after hydration triggering another render are:
the page is a .
the page has query values in the url.
are configured in your  since these can have parameters that may need to be parsed and provided in the .
to be able to distinguish if the query is fully updated and ready for use, you can leverage the  field on .
note: parameters added with  to a page that's using  will always be available inside the  object.
will emit  files for statically optimized pages. for example, the result for the page  would be:
and if you add  to the page, it will then be javascript, like so:
if you have a  with  then this optimization will be turned off in pages without .
if you have a  with  be sure you check if  is defined before assuming the page is server-side rendered.  will be  for pages that are prerendered.
avoid using the  value on  in the rendering tree until the router's  field is . statically optimized pages only know  on the client and not the server, so using it as a prop may lead to mismatch errors. the  demonstrates one way to use  as a prop.
static html export
allows you to export your next.js application to static html, which can be run standalone without the need of a node.js server. it is recommended to only use  if you don't need any of the  requiring a server.
if you're looking to build a hybrid site where only some pages are prerendered to static html, next.js already does that automatically. learn more about  and .
update your build script in  to use :
running  will generate an  directory.
builds an html version of your app. during ,  and  will generate an html file for each page in your  directory (or more for ). then,  will copy the already exported files into the correct directory.  will generate the html files during  instead of .
for more advanced scenarios, you can define a parameter called  in your  file to configure exactly which pages will be generated.
warning: using  for defining routes with any  powered page is now ignored and gets overridden. we recommend not to use them together.
the majority of core next.js features needed to build a static site are supported, including:
prefetching with
preloading javascript
any styling options (e.g. css modules, styled-jsx)
using a
features that require a node.js server, or dynamic logic that cannot be computed during the build process, are not supported:
(default loader)
it's possible to use the  api instead of , but it comes with a few caveats:
cannot be used alongside  or  on any given page. if you have dynamic routes, instead of using  you'll need to configure the  parameter in your  file to let the exporter know which html files it should output.
when  is called during export, the  and  fields of its  parameter will be empty objects, since during export there is no server running.
will be called on every client-side navigation, if you'd like to only fetch data at build-time, switch to .
should fetch from an api and cannot use node.js-specific libraries or the file system like  can.
we recommend migrating towards  over  whenever possible.
absolute imports and module path aliases
next.js automatically supports the  and   and  options since .
note:  can be used when you don't use typescript
note: you need to restart dev server to reflect modifications done in  /
these options allow you to configure module aliases, for example a common pattern is aliasing certain directories to use absolute paths.
one useful feature of these options is that they integrate automatically into certain editors, for example vscode.
the  configuration option allows you to import directly from the root of the project.
an example of this configuration:
while  is useful you might want to add other aliases that don't match 1 on 1. for this typescript has the  option.
using  allows you to configure module aliases. for example  to .
using mdx with next.js
mdx is a superset of markdown that lets you write jsx directly in your markdown files. it is a powerful way to add dynamic interactivity, and embed components within your content, helping you to bring your pages to life.
next.js supports mdx through a number of different means, this page will outline some of the ways you can begin integrating mdx into your next.js project.
authoring in markdown is an intuitive way to write content, its terse syntax, once adopted, can enable you to write content that is both readable and maintainable. because you can use  elements in your markdown, you can also get creative when styling your markdown pages.
however, because markdown is essentially static content, you can't create dynamic content based on user interactivity. where mdx shines is in its ability to let you create and use your react components directly in the markup. this opens up a wide range of possibilities when composing your sites pages with interactivity in mind.
internally mdx uses remark and rehype. remark is a markdown processor powered by a plugins ecosystem. this plugin ecosystem lets you parse code, transform  elements, change syntax, extract frontmatter, and more. using  is a popular option.
rehype is an  processor, also powered by a plugin ecosystem. similar to remark, these plugins let you manipulate, sanitize, compile and configure all types of data, elements and content.
to use a plugin from either remark or rehype, you will need to add it to the mdx packages config.
the  package is configured in the  file at your projects root. it sources data from local files, allowing you to create pages with a  extension, directly in your  directory.
the following steps outline how to setup  in your next.js project:
install the required packages:
require the package and configure to support top level  pages. the following adds the  object key allowing you to pass in any plugins:
create a new mdx page within the  directory:
you can now import a react component directly inside your mdx page:
frontmatter is a yaml like key/value pairing that can be used to store data about a page.  does not support frontmatter by default, though there are many solutions for adding frontmatter to your mdx content, such as .
to access page metadata with , you can export a meta object from within the  file:
to add a layout to your mdx page, create a new component and import it into the mdx page. then you can wrap the mdx page with your layout component:
one of the pleasant aspects of using markdown, is that it maps to native  elements, making writing fast, and intuitive:
the above generates the following :
when you want to style your own elements to give a custom feel to your website or application, you can pass in shortcodes. these are your own custom components that map to  elements. to do this you use the  and pass a components object as a prop. each object key in the components object maps to a  element name.
to enable you need to specify  in .
then setup the provider in your page
if you use it across the site you may want to add the provider to  so all mdx pages pick up the custom element config.
next.js supports a new mdx compiler written in rust. this compiler is still experimental and is not recommended for production use. to use the new compiler, you need to configure  when you pass it to :
amp support
with next.js you can turn any react page into an amp page, with minimal config, and without leaving react.
you can read more about amp in the official  site.
to enable amp support for a page, and to learn more about the different amp configs, read the .
only css-in-js is supported.  aren't supported by amp pages at the moment. you can .
adding amp components
the amp community provides  to make amp pages more interactive. next.js will automatically import all components used on a page and there is no need to manually import amp component scripts:
the above example uses the  component.
by default, the latest version of a component is always imported. if you want to customize the version, you can use , as in the following example:
amp validation
amp pages are automatically validated with  during development. errors and warnings will appear in the terminal where you started next.js.
pages are also validated during  and any warnings / errors will be printed to the terminal. any amp errors will cause the export to exit with status code  because the export is not valid amp.
you can set up custom amp validator in  as shown below:
to turn off amp validation add the following code to
amp in static html export
when using  to do  statically prerender pages, next.js will detect if the page supports amp and change the exporting behavior based on that.
for example, the hybrid amp page  would output:
- html page with client-side react runtime
- amp page
and if  is an amp-only page, then it would output:
- optimized amp page
next.js will automatically insert a link to the amp version of your page in the html version, so you don't have to, like so:
and the amp version of your page will include a link to the html page:
when  is enabled the exported pages for  would be:
- html page
amp currently doesn't have built-in types for typescript, but it's in their roadmap ().
as a workaround you can manually create a file called  inside your project and add these .
customizing babel config
next.js includes the  preset to your app, which includes everything needed to compile react applications and server-side code. but if you want to extend the default babel configs, it's also possible.
to start, you only need to define a  file (or ) at the top of your app. if such a file is found, it will be considered as the source of truth, and therefore it needs to define what next.js needs as well, which is the  preset.
here's an example  file:
you can  to learn about the presets included by .
to add presets/plugins without configuring them, you can do it this way:
to add presets/plugins with custom configuration, do it on the  preset like so:
to learn more about the available options for each config, visit their documentation site.
next.js uses the current node.js version for server-side compilations.
the  option on  should be kept to , otherwise webpack code splitting is turned off.
customizing postcss config
next.js compiles css for its  using postcss.
out of the box, with no configuration, next.js compiles css with the following transformations:
automatically adds vendor prefixes to css rules (back to ie11).
are corrected to behave like .
new css features are automatically compiled for internet explorer 11 compatibility:
by default,  and  (css variables) are not compiled for ie11 support.
to compile  for ie11, you can place the following comment at the top of your css file:
you can also enable ie11 support for
in your entire project by configuring autoprefixer with the configuration shown below (collapsed).
see  below for more information.
css variables are not compiled because it is .
if you must use variables, consider using something like  which are compiled away by .
next.js allows you to configure the target browsers (for  and compiled css features) through .
to customize browserslist, create a  key in your  like so:
you can use the  tool to visualize what browsers you are targeting.
no configuration is needed to support css modules. to enable css modules for a file, rename the file to have the extension .
you can learn more about .
warning: when you define a custom postcss configuration file, next.js completely disables the .
be sure to manually configure all the features you need compiled, including .
you also need to install any plugins included in your custom configuration manually, i.e. .
to customize the postcss configuration, create a  file in the root of your project.
this is the default configuration used by next.js:
note: next.js also allows the file to be named , or, to be read from the  key in .
it is also possible to configure postcss with a  file, which is useful when you want to conditionally include plugins based on environment:
note: next.js also allows the file to be named .
do not use  to import the postcss plugins. plugins must be provided as strings.
note: if your  needs to support other non-next.js tools in the same project, you must use the interoperable object-based format instead:
custom server
by default, next.js includes its own server with . if you have an existing backend, you can still use it with next.js (this is not a custom server). a custom next.js server allows you to start a server 100% programmatically in order to use custom server patterns. most of the time, you will not need this – but it's available for complete customization.
note: a custom server cannot be deployed on .
before deciding to use a custom server, please keep in mind that it should only be used when the integrated router of next.js can't meet your app requirements. a custom server will remove important performance optimizations, like serverless functions and .
take a look at the following example of a custom server:
doesn't go through babel or webpack. make sure the syntax and sources this file requires are compatible with the current node version you are running.
to run the custom server you'll need to update the  in  like so:
the custom server uses the following import to connect the server with the next.js application:
the above  import is a function that receives an object with the following options:
:  - whether or not to launch next.js in dev mode. defaults to
:  - location of the next.js project. defaults to
:  - hide error messages containing server information. defaults to
:  - the same object you would use in . defaults to
the returned  can then be used to let next.js handle requests as required.
by default,  will serve each file in the  folder under a pathname matching the filename. if your project uses a custom server, this behavior may result in the same content being served from multiple paths, which can present problems with seo and ux.
to disable this behavior and prevent routing based on files in , open  and disable the  config:
note that  disables filename routes from ssr; client-side routing may still access those paths. when using this option, you should guard against navigation to routes you do not want programmatically.
you may also wish to configure the client-side router to disallow client-side redirects to filename routes; for that refer to .
custom
next.js uses the  component to initialize pages. you can override it and control the page initialization and:
persist layouts between page changes
keeping state when navigating pages
custom error handling using
inject additional data into pages
to override the default , create the file  as shown below:
the  prop is the active , so whenever you navigate between routes,  will change to the new . therefore, any props you send to  will be received by the .
is an object with the initial props that were preloaded for your page by one of our , otherwise it's an empty object.
the  receives a single argument called . it's an object with the same set of properties as the  in .
if your app is running and you added a custom , you'll need to restart the development server. only required if  didn't exist before.
adding a custom  in your  will disable  in pages without .
when you add  in your custom app, you must , call  inside  and merge the returned object into the return value.
does not support next.js  like  or . if you need global data fetching, consider .
if you’re using typescript, take a look at .
note: next.js 13 introduces the  directory (beta). this new directory has support for layouts, nested routes, and uses server components by default. inside , you can modify the initial  and  tags using a root layout.
a custom  can update the  and  tags used to render a . this file is only rendered on the server, so event handlers like  cannot be used in .
the code above is the default  added by next.js. custom attributes are allowed as props. for example, we might want to add  to the  tag:
or add a  to the  tag:
, ,  and  are required for the page to be properly rendered.
the  component used in  is not the same as . the  component used here should only be used for any  code that is common for all pages. for all other cases, such as  tags, we recommend using  in your pages or components.
react components outside of  will not be initialized by the browser. do not add application logic here or custom css (like ). if you need shared components in all your pages (like a menu or a toolbar), read  instead.
currently does not support next.js  like  or .
note: this is advanced and only needed for libraries like css-in-js to support server-side rendering. this is not needed for built-in  support.
for  support, we recommend avoiding customizing  and , if possible.
the  object shown below is equivalent to the one received in , with the addition of .
note:  in  is not called during client-side transitions.
you can use the built-in  type and change the file name to  like so:
custom error page
a 404 page may be accessed very often. server-rendering an error page for every visit increases the load of the next.js server. this can result in increased costs and slow experiences.
to avoid the above pitfalls, next.js provides a static 404 page by default without having to add any additional files.
to create a custom 404 page you can create a  file. this file is statically generated at build time.
note: you can use  inside this page if you need to fetch data at build time.
server-rendering an error page for every visit adds complexity to responding to errors. to help users get responses to errors as fast as possible, next.js provides a static 500 page by default without having to add any additional files.
to customize the 500 page you can create a  file. this file is statically generated at build time.
500 errors are handled both client-side and server-side by the  component. if you wish to override it, define the file  and add the following code:
is only used in production. in development you’ll get an error with the call stack to know where the error originated from.
if you want to render the built-in error page you can by importing the  component:
the  component also takes  as a property if you want to pass in a text message along with a .
if you have a custom  component be sure to import that one instead.  exports the default component used by next.js.
does not currently support next.js  like  or .
, like , is a reserved pathname.  is used to define the customized layouts and behaviors of the error pages.  will render 404 when accessed directly via  or rendering in a .
directory
pages can also be added under  as an alternative to the root  directory.
the  directory is very common in many apps and next.js supports it by default.
will be ignored if  is present in the root directory
config files like  and , as well as environment variables, should be inside the root directory, moving them to  won't work. same goes for the
continuous integration (ci) build caching
to improve build performance, next.js saves a cache to  that is shared between builds.
to take advantage of this cache in continuous integration (ci) environments, your ci workflow will need to be configured to correctly persist the cache between builds.
if your ci is not configured to persist  between builds, you may see a  error.
here are some example cache configurations for common ci providers:
next.js caching is automatically configured for you. there's no action required on your part.
edit your  step in  to include :
if you do not have a  key, please follow circleci's .
add or merge the following into your :
use  with .
add (or merge in) the following to your :
using github's , add the following step in your workflow file:
add or merge the following into your  at the top level (same level as ):
then reference it in the  section of your pipeline's :
using heroku's , add a  array in your top-level package.json:
using azure pipelines' , add the following task to your pipeline yaml file somewhere prior to the task that executes :
multi zones
a zone is a single deployment of a next.js app. you can have multiple zones and merge them as a single app.
for example, let's say you have the following apps:
an app for serving
another app for serving all other pages
with multi zones support, you can merge both these apps into a single one allowing your customers to browse it using a single url, but you can develop and deploy both apps independently.
there are no zone related apis. you only need to do the following:
make sure to keep only the pages you need in your app, meaning that an app can't have pages from another app, if app  has  then app  shouldn't have it too.
make sure to configure a  to avoid conflicts with pages and static files.
you can merge zones using  in one of the apps or any http proxy.
for  applications, you can use a  to deploy both apps with a single .
measuring performance
allows you to analyze and measure the performance of
pages using different metrics.
you can start collecting your  with zero-configuration on . there's also support for analytics if you're .
the rest of this documentation describes the built-in relayer next.js analytics uses.
first, you will need to create a  component and define a  function:
this function is fired when the final values for any of the metrics have finished calculating on
the page. you can use to log any of the results to the console or send to a particular endpoint.
the  object returned to the function consists of a number of properties:
: unique identifier for the metric in the context of the current page load
: metric name
: first recorded timestamp of the performance entry in  (if applicable)
: value, or duration in , of the performance entry
: type of metric ( or )
there are two types of metrics that are tracked:
web vitals
custom metrics
are a set of useful metrics that aim to capture the user
experience of a web page. the following web vitals are all included:
(ttfb)
(fcp)
(lcp)
(fid)
(cls)
(inp) (experimental)
you can handle all the results of these metrics using the  label:
there's also the option of handling each of the metrics separately:
a third-party library, , is used to measure
these metrics. browser compatibility depends on the particular metric, so refer to the  section to find out which
browsers are supported.
in addition to the core metrics listed above, there are some additional custom metrics that
measure the time it takes for the page to hydrate and render:
: length of time it takes for the page to start and finish hydrating (in ms)
: length of time it takes for a page to start rendering after a
route change (in ms)
: length of time it takes for a page to finish render after a route change (in ms)
these metrics work in all browsers that support the .
with the relay function, you can send any of results to an analytics endpoint to measure and track
real user performance on your site. for example:
note: if you use , using the
value can allow you to construct metric distributions manually (to calculate percentiles,
etc.)
read more about .
when debugging issues related to web vitals, it is often helpful if we can pinpoint the source of the problem.
for example, in the case of cumulative layout shift (cls), we might want to know the first element that shifted when the single largest layout shift occurred.
or, in the case of largest contentful paint (lcp), we might want to identify the element corresponding to the lcp for the page.
if the lcp element is an image, knowing the url of the image resource can help us locate the asset we need to optimize.
pinpointing the biggest contributor to the web vitals score, aka ,
allows us to obtain more in-depth information like entries for ,  and .
attribution is disabled by default in next.js but can be enabled per metric by specifying the following in .
valid attribution values are all  metrics specified in the  type.
if you are using typescript, you can use the built-in type :
middleware
middleware allows you to run code before a request is completed, then based on the incoming request, you can modify the response by rewriting, redirecting, modifying the request or response headers, or responding directly.
middleware runs before cached content, so you can personalize static files and pages. common examples of middleware would be authentication, a/b testing, localized pages, bot protection, and more. regarding localized pages, you can start with  and implement middleware for more advanced use cases.
note: if you were using middleware prior to , please see the .
to begin using middleware, follow the steps below:
install the latest version of next.js:
create a  (or ) file at the root or in the  directory (same level as your )
export a middleware function from the  file:
middleware will be invoked for every route in your project. the following is the execution order:
from
middleware (, , etc.)
() from
filesystem routes (, , pages, etc.)
dynamic routes ()
there are two ways to define which paths middleware will run on:
custom matcher config
conditional statements
allows you to filter middleware to run on specific paths.
you can match a single path or multiple paths with an array syntax:
the  config allows full regex so matching like negative lookaheads or character matching is supported. an example of a negative lookahead to match all except specific paths can be seen here:
note: the  values need to be constants so they can be statically analyzed at build-time. dynamic values such as variables will be ignored.
configured matchers:
must start with
can include named parameters:  matches  and  but not
can have modifiers on named parameters (starting with ):  matches  because  is zero or more.  is zero or one and  one or more
can use regular expression enclosed in parenthesis:  is the same as
read more details on  documentation.
note: for backward compatibility, next.js always considers  as . therefore, a matcher of  will match.
the  api allows you to:
the incoming request to a different url
the response by displaying a given url
set request headers for api routes, , and  destinations
set response cookies
set response headers
to produce a response from middleware, you should  to a route ( or ) that produces a response.
cookies are regular headers. on a , they are stored in the  header. on a  they are in the  header. next.js provides a convenient way to access and manipulate these cookies through the  extension on  and .
for incoming requests,  comes with the following methods: , , , and  cookies. you can check for the existence of a cookie with  or remove all cookies with .
for outgoing responses,  have the following methods , , , and .
you can set request and response headers using the  api (setting request headers is available since next.js v13.0.0).
note: avoid setting large headers as it might cause  error depending on your backend web server configuration.
you can respond to middleware directly by returning a  (responding from middleware is available since next.js v13.0.0).
to enable middleware responses, update :
once enabled, you can provide a response from middleware using the  or  api:
debugging
this documentation explains how you can debug your next.js frontend and backend code with full source maps support using either the  or .
any debugger that can attach to node.js can also be used to debug a next.js application. you can find more details in the node.js .
create a file named  at the root of your project with the following content:
can be replaced with  if you're using yarn. if you're  your application starts on, replace the  in  with the port you're using instead.
now go to the debug panel (ctrl+shift+d on windows/linux, ⇧+⌘+d on macos), select a launch configuration, then press f5 or select debug: start debugging from the command palette to start your debugging session.
click the drop down menu listing the runtime configuration, and click . create a  debug configuration with  as the url. customize to your liking (e.g. browser for debugging, store as project file), and click . run this debug configuration, and the selected browser should automatically open. at this point, you should have 2 applications in debug mode: the nextjs node application, and the client/ browser application.
start your development server as usual by running , , or . once the server starts, open  (or your alternate url) in chrome. next, open chrome's developer tools (ctrl+shift+j on windows/linux, ⌥+⌘+i on macos), then go to the sources tab.
now, any time your client-side code reaches a  statement, code execution will pause and that file will appear in the debug area. you can also press ctrl+p on windows/linux or ⌘+p on macos to search for a file and set breakpoints manually. note that when searching here, your source files will have paths starting with .
to debug server-side next.js code with chrome devtools, you need to pass the  flag to the underlying node.js process:
if you're using  or  (see ) then you should update the  script on your :
launching the next.js dev server with the  flag will look something like this:
be aware that running  or  won't work. this would try to start multiple debuggers on the same port: one for the npm/yarn process and one for next.js. you would then get an error like  in your console.
once the server starts, open a new tab in chrome and visit , where you should see your next.js application inside the remote target section. click inspect under your application to open a separate devtools window, then go to the sources tab.
debugging server-side code here works much like debugging client-side code with chrome devtools, except that when you search for files here with ctrl+p or ⌘+p, your source files will have paths starting with  (where  will be replaced with the name of your application according to your  file).
windows users may run into an issue when using  as that syntax is not supported on windows platforms. to get around this, install the  package as a development dependency ( with  and ) and replace the  script with the following.
will set the  environment variable regardless of which platform you are on (including mac, linux, and windows) and allow you to debug consistently across devices and operating systems.
to learn more about how to use a javascript debugger, take a look at the following documentation:
error handling
this documentation explains how you can handle development, server-side, and client-side errors.
when there is a runtime error during the development phase of your next.js application, you will encounter an overlay. it is a modal that covers the webpage. it is only visible when the development server runs using , , or  and not in production. fixing the error will automatically dismiss the overlay.
here is an example of an overlay:
next.js provides a static 500 page by default to handle server-side errors that occur in your application. you can also  by creating a  file.
having a 500 page in your application does not show specific errors to the app user.
you can also use  to handle specific runtime error like .
react  is a graceful way to handle a javascript error on the client so that the other parts of the application continue working. in addition to preventing the page from crashing, it allows you to provide a custom fallback component and even log error information.
to use error boundaries for your next.js application, you must create a class component  and wrap the  prop in the  file. this component will be responsible to:
render a fallback ui after an error is thrown
provide a way to reset the application's state
log error information
you can create an  class component by extending . for example:
the  component keeps track of an  state. the value of this state variable is a boolean. when the value of  is , then the  component will render a fallback ui. otherwise, it will render the children components.
after creating an  component, import it in the  file to wrap the  prop in your next.js application.
you can learn more about  in react's documentation.
to monitor client errors, use a service like , bugsnag or datadog.
source maps
source maps are enabled by default during development. during production builds, they are disabled to prevent you leaking your source on the client, unless you specifically opt in with the configuration flag.
next.js provides a configuration flag you can use to enable browser source map generation during the production build:
when the  option is enabled, the source maps will be output in the same directory as the javascript files. next.js will automatically serve these files when requested.
adding source maps can increase  time
increases memory usage during
next.js codemods
next.js provides codemod transformations to help upgrade your next.js codebase when a feature is deprecated.
codemods are transformations that run on your codebase programmatically. this allows for a large amount of changes to be applied without having to manually go through every file.
- name of transform, see available transforms below.
- files or directory to transform
do a dry-run, no code will be edited
prints the changed output for comparison
safely removes  from  or adds  prop.
transforms into:
this codemod safely migrates existing next.js 10, 11, 12 applications importing  to the renamed  import in next.js 13.
this codemod dangerously migrates from  to the new  by adding inline styles and removing unused props. please note this codemod is experimental and only covers static usage (such as ) but not dynamic usage (such as ).
removes  prop and adds
removes  prop
changes next.config.js  to "custom", removes , and sets  to a new file.
migrates a create react app project to next.js; creating a pages directory and necessary config to match behavior. client-side only rendering is leveraged initially to prevent breaking compatibility due to  usage during ssr and can be enabled seamlessly to allow gradual adoption of next.js specific features.
please share any feedback related to this transform .
transforms files that do not import  to include the import in order for the new  to work.
transforms anonymous components into named components to make sure they work with .
the component will have a camel cased name based on the name of the file, and it also works with arrow functions.
go to your project
run the codemod:
transforms the  hoc into next.js 9 page configuration.
transforms the deprecated automatically injected  property on top level pages to using  and the  property it injects. read more here:
this is one case. all the cases that are transformed (and tested) can be found in the .
internationalized routing
next.js has built-in support for internationalized () routing since . you can provide a list of locales, the default locale, and domain-specific locales and next.js will automatically handle the routing.
the i18n routing support is currently meant to complement existing i18n library solutions like , , , ,  and others by streamlining the routes and locale parsing.
to get started, add the  config to your  file.
locales are , a standardized format for defining locales.
generally a locale identifier is made up of a language, region, and script separated by a dash: . the region and script are optional. an example:
- english as spoken in the united states
- dutch as spoken in the netherlands
- dutch, no specific region
if user locale is  and it is not listed in your configuration, they will be redirected to  if available, or to the default locale otherwise.
if you don't plan to support all regions of a country, it is therefore a good practice to include country locales that will act as fallbacks.
there are two locale handling strategies: sub-path routing and domain routing.
sub-path routing puts the locale in the url path.
with the above configuration , , and  will be available to be routed to, and  is the default locale. if you have a  the following urls would be available:
the default locale does not have a prefix.
by using domain routing you can configure locales to be served from different domains:
for example if you have  the following urls will be available:
when a user visits the application root (generally ), next.js will try to automatically detect which locale the user prefers based on the  header and the current domain.
if a locale other than the default locale is detected, the user will be redirected to either:
when using sub-path routing: the locale prefixed path
when using domain routing: the domain with that locale specified as the default
when using domain routing, if a user with the  header  visits , they will be redirected to  since that domain handles the  locale by default.
when using sub-path routing, the user would be redirected to .
with next.js 12 and , we can add a prefix to the default locale with a .
for example, here's a  file with support for a few languages. note the  locale has been added intentionally.
next, we can use  to add custom routing rules:
this  skips adding the default prefix to  and  files like fonts or images. if a request is made to the default locale, we redirect to our prefix .
the automatic locale detection can be disabled with:
when  is set to  next.js will no longer automatically redirect based on the user's preferred locale and will only provide locale information detected from either the locale based domain or locale path as described above.
you can access the locale information via the next.js router. for example, using the  hook the following properties are available:
contains the currently active locale.
contains all configured locales.
contains the configured default locale.
when  pages with  or , the locale information is provided in  provided to the function.
when leveraging , the configured locales are provided in the context parameter of the function under  and the configured defaultlocale under .
you can use  or  to transition between locales.
for , a  prop can be provided to transition to a different locale from the currently active one. if no  prop is provided, the currently active  is used during client-transitions. for example:
when using the  methods directly, you can specify the  that should be used via the transition options. for example:
note that to handle switching only the  while preserving all routing information such as  query values or hidden href query values, you can provide the  parameter as an object:
see  for more information on the object structure for .
if you have a  that already includes the locale you can opt-out of automatically handling the locale prefixing:
next.js supports overriding the accept-language header with a  cookie. this cookie can be set using a language switcher and then when a user comes back to the site it will leverage the locale specified in the cookie when redirecting from  to the correct locale location.
for example, if a user prefers the locale  in their accept-language header but a  cookie is set the  locale when visiting  the user will be redirected to the  locale location until the cookie is removed or expired.
since next.js knows what language the user is visiting it will automatically add the  attribute to the  tag.
next.js doesn't know about variants of a page so it's up to you to add the  meta tags using . you can learn more about  in the .
note that internationalized routing does not integrate with  as  does not leverage the next.js routing layer. hybrid next.js applications that do not use  are fully supported.
for pages using  with , all locale variants of the page desired to be prerendered need to be returned from . along with the  object returned for , you can also return a  field specifying which locale you want to render. for example:
for  and non-dynamic  pages, a version of the page will be generated for each locale. this is important to consider because it can increase build times depending on how many locales are configured inside .
for example, if you have 50 locales configured with 10 non-dynamic pages using , this means  will be called 500 times. 50 versions of the 10 pages will be generated during each build.
to decrease the build time of dynamic pages with , use a . this allows you to return only the most popular paths and locales from  for prerendering during the build. then, next.js will build the remaining pages at runtime as they are requested.
for pages that are , a version of the page will be generated for each locale.
for non-dynamic  pages, a version is generated for each locale like above.  is called with each  that is being rendered. if you would like to opt-out of a certain locale from being pre-rendered, you can return  from  and this variant of the page will not be generated.
: 100 total locales
: 100 total locale domain items
note: these limits have been added initially to prevent potential . you can workaround these limits with custom routing using  in next.js 12.
output file tracing
during a build, next.js will automatically trace each page and its dependencies to determine all of the files that are needed for deploying a production version of your application.
this feature helps reduce the size of deployments drastically. previously, when deploying with docker you would need to have all files from your package's  installed to run . starting with next.js 12, you can leverage output file tracing in the  directory to only include the necessary files.
furthermore, this removes the need for the deprecated  target which can cause various issues and also creates unnecessary duplication.
during , next.js will use  to statically analyze , , and  usage to determine all files that a page might load.
next.js' production server is also traced for its needed files and output at  which can be leveraged in production.
to leverage the  files emitted to the  output directory, you can read the list of files in each trace that are relative to the  file and then copy them to your deployment location.
next.js can automatically create a  folder that copies only the necessary files for a production deployment including select files in .
to leverage this automatic copying you can enable it in your :
this will create a folder at  which can then be deployed on its own without installing .
additionally, a minimal  file is also output which can be used instead of . this minimal server does not copy the  or  folders by default as these should ideally be handled by a cdn instead, although these folders can be copied to the  and  folders manually, after which  file will serve these automatically.
note:  is read during  and serialized into the  output file. if the legacy  are being used, the values will be specific to values at build time.
if your project uses  with the default , you must install  as a dependency:
while tracing in monorepo setups, the project directory is used for tracing by default. for ,  would be the tracing root and any files outside of that folder will not be included. to include files outside of this folder you can set  in your .
there are some cases in which next.js might fail to include required files, or might incorrectly include unused files. in those cases, you can export page configs props  and  respectively. each prop accepts an array of  relative to the project's root to either include or exclude in the trace.
currently, next.js does not do anything with the emitted  files. the files must be read by your deployment platform, for example , to create a minimal deployment. in a future release, a new command is planned to utilize these  files.
tracing dependencies can be slow because it requires very complex computations and analysis. we created  in rust as a faster and smarter alternative to the javascript implementation.
to enable it, you can add the following configuration to your :
security headers
to improve the security of your application, you can use  in  to apply http response headers to all routes in your application.
this header controls dns prefetching, allowing browsers to proactively perform domain name resolution on external links, images, css, javascript, and more. this prefetching is performed in the background, so the  is more likely to be resolved by the time the referenced items are needed. this reduces latency when the user clicks a link.
this header informs browsers it should only be accessed using https, instead of using http. using the configuration below, all present and future subdomains will use https for a  of 2 years. this blocks access to pages or subdomains that can only be served over http.
if you're deploying to , this header is not necessary as it's automatically added to all deployments unless you declare  in your .
this header stops pages from loading when they detect reflected cross-site scripting (xss) attacks. although this protection is not necessary when sites implement a strong  disabling the use of inline javascript (), it can still provide protection for older web browsers that don't support csp.
this header indicates whether the site should be allowed to be displayed within an . this can prevent against clickjacking attacks. this header has been superseded by csp's  option, which has better support in modern browsers.
this header allows you to control which features and apis can be used in the browser. it was previously named . you can view the full list of permission options .
this header prevents the browser from attempting to guess the type of content if the  header is not explicitly set. this can prevent xss exploits for websites that allow users to upload and share files. for example, a user trying to download an image, but having it treated as a different  like an executable, which could be malicious. this header also applies to downloading browser extensions. the only valid value for this header is .
this header controls how much information the browser includes when navigating from the current website (origin) to another. you can read about the different options .
this header helps prevent cross-site scripting (xss), clickjacking and other code injection attacks. content security policy (csp) can specify allowed origins for content including scripts, stylesheets, images, fonts, objects, media (audio, video), iframes, and more.
you can read about the many different csp options .
you can add content security policy directives using a template string.
when a directive uses a keyword such as , wrap it in single quotes .
in the header's value, replace the new line with a space.
for more information, we recommend the following sections:
react 18
next.js 13 requires using react 18, unlocking:
new apis like  and more.
in next.js 13, you can start using the  directory (beta) to take advantage of streaming server-rendering. learn more by reading the  directory (beta) documentation:
to try streaming ssr.
in next.js 13, you can start using the  directory (beta) which use server components by default. learn more by reading the  directory (beta) documentation:
to try server components.
next.js has two server runtimes where you can render parts of your application code: the node.js runtime and the . depending on your deployment infrastructure, both runtimes support streaming.
by default, next.js uses the node.js runtime.  and  use the edge runtime.
streaming ssr
streaming allows you to incrementally render parts of your ui to the client.
react server components
react server components allow developers to build applications that span the server and client, combining the rich interactivity of client-side apps with the improved performance of traditional server rendering.
edge and node.js runtimes
to configure the runtime for your whole application, you can set the experimental option  in your  file:
you can detect which runtime you're using by looking at the  environment variable during runtime, and examining the  variable during compilation.
on each page, you can optionally export a  config set to either  or :
when both the per-page runtime and global runtime are set, the per-page runtime overrides the global runtime. if the per-page runtime is not set, the global runtime option will be used.
next.js' default runtime configuration is good for most use cases, but there’re still many reasons to change to one runtime over the other one.
for example, for api routes that rely on native node.js apis, they need to run with the node.js runtime. however, if an api only uses something like cookie-based authentication, using middleware and the edge runtime will be a better choice due to its lower latency as well as better scalability.
enable you to build high performance apis with next.js using the edge runtime.
upgrade guide
to update to next.js version 13, run the following command using your preferred package manager:
the  have been changed to drop internet explorer and target modern browsers.
the minimum node.js version has been bumped from 12.22.0 to 14.0.0, since 12.x has reached end-of-life.
the minimum react version has been bumped from 17.0.2 to 18.2.0.
the  configuration property was changed from  to . see  for more info.
the  import was renamed to . the  import was renamed to . a  to safely and automatically rename your imports.
the  child can no longer be . add the  prop to use the legacy behavior or remove the  to upgrade. a  to automatically upgrade your code.
the  configuration property has been removed and superseded by .
next.js 13 introduces a new  with new features and conventions. however, upgrading to next.js 13 does not require using the new .
you can continue using  with new features that work in both directories, such as the updated , , , and .
next.js 12 introduced many improvements to the image component with a temporary import: . these improvements included less client-side javascript, easier ways to extend and style images, better accessibility, and native browser lazy loading.
starting in next.js 13, this new behavior is now the default for .
there are two codemods to help you migrate to the new image component:
: this codemod will safely and automatically rename  imports to  to maintain the same behavior as next.js 12. we recommend running this codemod to quickly update to next.js 13 automatically.
: after running the previous codemod, you can optionally run this experimental codemod to upgrade  to the new , which will remove unused props and add inline styles. please note this codemod is experimental and only covers static usage (such as ) but not dynamic usage (such as ).
alternatively, you can manually update by following the  and also see the .
the  no longer requires manually adding an  tag as a child. this behavior was added as an experimental option in  and is now the default. in next.js 13,  always renders  and allows you to forward props to the underlying tag.
to upgrade your links to next.js 13, you can use the .
the behavior of  has been updated to support both  and . if incrementally adopting , read the .
previously, next.js helped you optimize fonts by inlining font css. version 13 introduces the new  module which gives you the ability to customize your font loading experience while still ensuring great performance and privacy.
see  to learn how to use .
if you were using middleware prior to , please see the  for more information.
the minimum node.js version has been bumped from 12.0.0 to 12.22.0 which is the first version of node.js with native es modules support.
the minimum required react version is . to upgrade you can run the following command in the terminal:
or using :
to upgrade you can run the following command in the terminal:
next.js now uses a rust-based compiler, , to compile javascript/typescript. this new compiler is up to 17x faster than babel when compiling individual files and allows for up to 5x faster fast refresh.
next.js provides full backwards compatibility with applications that have . all transformations that next.js handles by default like styled-jsx and tree-shaking of  /  /  have been ported to rust.
when an application has a custom babel configuration, next.js will automatically opt-out of using swc for compiling javascript/typescript and will fall back to using babel in the same way that it was used in next.js 11.
many of the integrations with external libraries that currently require custom babel transformations will be ported to rust-based swc transforms in the near future. these include but are not limited to:
styled components
emotion
relay
in order to prioritize transforms that will help you adopt swc, please provide your  on .
you can opt-in to replacing terser with swc for minifying javascript up to 7x faster using a flag in :
minification using swc is an opt-in flag to ensure it can be tested against more real-world next.js applications before it becomes the default in next.js 12.1. if you have feedback about minification, please leave it on .
on top of the rust-based compiler, we've implemented a new css parser based on the css parser that was used for the styled-jsx babel transform. this new parser has improved handling of css and now errors when invalid css is used that would previously slip through and cause unexpected behavior.
because of this change, invalid css will throw an error during development and . this change only affects styled-jsx usage.
now renders the  inside a  instead of .
if your application has specific css targeting span, for example, , upgrading to next.js 12 might incorrectly match the wrapping element inside the  component. you can avoid this by restricting the selector to a specific class such as  and updating the relevant component with that classname, such as .
if your application has specific css targeting the   tag, for example , it may not match anymore. you can update the selector , or preferably, add a new  wrapping the  component and target that instead such as .
the  prop is unchanged and will still be passed to the underlying  element.
see the  for more info.
previously, next.js used a  connection to receive hmr events. next.js 12 now uses a websocket connection.
in some cases when proxying requests to the next.js dev server, you will need to ensure the upgrade request is handled correctly. for example, in  you would need to add the following configuration:
for custom servers, such as , you may need to use  to ensure the request is passed correctly, for example:
if you are already using webpack 5 you can skip this section.
next.js has adopted webpack 5 as the default for compilation in next.js 11. as communicated in the  next.js 12 removes support for webpack 4.
if your application is still using webpack 4 using the opt-out flag you will now see an error linking to the .
if you do not have  in  you can skip this section.
the target option has been deprecated in favor of built-in support for tracing what dependencies are needed to run a page.
during , next.js will automatically trace each page and its dependencies to determine all of the files that are needed for deploying a production version of your application.
if you are currently using the  option set to  please read the .
most applications already use the latest version of react, with next.js 11 the minimum react version has been updated to 17.0.2.
to upgrade you can run the following command:
webpack 5 is now the default for all next.js applications. if you did not have custom webpack configuration your application is already using webpack 5. if you do have custom webpack configuration you can refer to the  for upgrading guidance.
the build output directory (defaults to ) is now cleared by default except for the next.js caches. you can refer to  for more information.
if your application was relying on this behavior previously you can disable the new default behavior by adding the  flag in .
next.js 11 supports the  environment variable to set the port the application has to run on. using / is still recommended but if you were prohibited from using  in any way you can now use  as an alternative:
example:
next.js 11 supports static image imports with . this new feature relies on being able to process image imports. if you previously added the  or  packages you can either move to the new built-in support using  or disable the feature:
the  component's  has been deprecated since next.js 9 as it's no longer needed and has since been a no-op, in next.js 11 it has been removed.
if your  has a custom  method you can remove  as it is no longer needed.
this export has been deprecated since next.js 9 as it's no longer needed and has since been a no-op with a warning during development. in next.js 11 it has been removed.
if your  imports  from  you can remove  as it has been removed. learn more in .
this property has been deprecated since next.js 4 and has since shown a warning during development. with the introduction of  /  these methods already disallowed usage of . in next.js 11 it has been removed completely.
you can learn more in .
the  property on  was deprecated in next.js 10.0.1. you can use  instead. in next.js 11  was removed.
the  and  option for  have been deprecated since next.js 9.5 showing a warning that it has been deprecated. this was done in order to make  close to  in api surface. in next.js 11 the  and  options have been removed.
this option hasn't been mentioned in the documentation since next.js 8 so it's less likely that your application is using it.
if your application does use  and  you can refer to .
has been a no-op since next.js 9.5, in next.js 11 it was removed. you can safely remove your usage of .
moment.js includes translations for a lot of locales by default. next.js now automatically excludes these locales by default to optimize bundle size for applications using moment.js.
to load a specific locale use this snippet:
you can opt-out of this new default by adding  to  if you do not want the new behavior, do note it's highly recommended to not disable this new optimization as it significantly reduces the size of moment.js.
in case you're accessing  during rendering, in next.js 11  is no longer provided during pre-rendering. ensure you're accessing  in :
if your application uses  which was an internal property that was not public please make sure to use  as well.
react 17 introduced a new  that brings a long-time next.js feature to the wider react ecosystem: not having to  when using jsx. when using react 17 next.js will automatically use the new transform. this transform does not make the  variable global, which was an unintended side-effect of the previous next.js implementation. a  to automatically fix cases where you accidentally used  without importing it.
there were no breaking changes between version 9 and 10.
to upgrade run the following command:
if you previously configured  in your  file for dynamic routes, these rules can be removed when leveraging next.js 9's new .
next.js 9's dynamic routes are automatically configured on  and do not require any  customization.
you can read more about .
if you previously copied the  example, you may be able to remove your .
removing  from  (when possible) is important to leverage new next.js features!
the following  does nothing and may be removed:
next.js will now ignore usage  and warn you to remove it. please remove this plugin from your .
remove references to  from your custom  (if present).
usage of  should also be removed from your .
typescript definitions are published with the  package, so you need to uninstall  as they would conflict.
the following types are different:
this list was created by the community to help you upgrade, if you find other differences please send a pull request to this list to help other users.
from:
you may no longer export a custom variable named  from a page (i.e.  / ).
this exported variable is now used to specify page-level next.js configuration like opt-in amp and api route features.
you must rename a non-next.js-purposed  export to something different.
dynamic components will not render anything by default while loading. you can still customize this behavior by setting the  property:
next.js now has the concept of page-level configuration, so the  higher-order component has been removed for consistency.
this change can be automatically migrated by running the following commands at the root of your next.js project:
to perform this migration by hand, or view what the codemod will produce, see below:
before
after
previously, exporting  would result in . this behavior has been changed to result in .
you can revert to the previous behavior by creating a  with the following content:
pages in  are now considered .
pages in this directory will no longer contain a client-side bundle.
the ability to load multiple modules at once has been deprecated in  to be closer to react's implementation ( and ).
updating code that relies on this behavior is relatively straightforward! we've provided an example of a before/after to help you migrate your application:
incrementally adopting next.js
next.js has been designed for gradual adoption. with next.js, you can continue using your existing code and add as much (or as little) react as you need. by starting small and incrementally adding more pages, you can prevent derailing feature work by avoiding a complete rewrite.
the first strategy is to configure your server or proxy such that, everything under a specific subpath points to a next.js app. for example, your existing website might be at , and you might configure your proxy such that  serves a next.js e-commerce store.
using , you can configure your next.js application's assets and links to automatically work with your new subpath . since each page in next.js is its own , pages like  will route to  in your application.
to learn more about , take a look at our .
the second strategy is to create a new next.js app that points to the root url of your domain. then, you can use  inside  to have some subpaths to be proxied to your existing app.
for example, let's say you created a next.js app to be served from  with the following . now, requests for the pages you’ve added to this next.js app (e.g.  if you’ve added ) will be handled by next.js, and requests for any other route (e.g. ) will be proxied to .
note: if you use  in , the catch-all fallback  defined in  will not be run. they are instead caught by the  fallback.
to learn more about rewrites, take a look at our .
note: if you are incrementally migrating to a dynamic route (e.g. ) and using  or  along with a fallback , ensure you consider the case where pages are not found. when next.js matches the dynamic route it stops checking any further routes. using  in  will return the 404 page without applying the fallback . if this is not desired, you can use  with  cache-control headers when returning your props. then, you can manually proxy to your existing backend using something like  instead of returning .
next.js and  make it straightforward to adopt micro frontends and deploy as a . this allows you to use  to adopt new applications incrementally. some benefits of micro-frontends:
smaller, more cohesive and maintainable codebases.
more scalable organizations with decoupled, autonomous teams.
the ability to upgrade, update, or even rewrite parts of the frontend in a more incremental fashion.
once your monorepo is set up, push changes to your git repository as usual and you'll see the commits deployed to the vercel projects you've connected.
to learn more, read about  and  or .
migrating from gatsby
this guide will help you understand how to transition from an existing gatsby project to next.js. migrating to next.js will allow you to:
choose which  strategy you want on a per-page basis.
use  to update existing pages by re-rendering them in the background as traffic comes in.
and more! let’s walk through a series of steps to complete the migration.
the first step towards migrating to next.js is to update  and dependencies. you should:
remove all gatsby-related packages (but keep  and ).
install .
add next.js related commands to . one is , which runs a development server at . you should also add  and  for creating and starting a production build.
here's an example  ():
gatsby uses the  directory for the compiled output, whereas next.js uses it for static assets. here are the steps for migration ():
remove  and  from  and delete both directories.
rename gatsby’s  directory as .
add  to .
both gatsby and next support a  directory, which uses . gatsby's directory is , which is also .
gatsby creates dynamic routes using the  api inside of . with next, we can use  inside of  to achieve the same effect. rather than having a  directory, you can use the react component inside your dynamic route file. for example:
gatsby:  api inside  for each blog post, then have a template file at .
next: create  which contains the blog post template. the value of  is accessible through a . for example, the route  would forward the query object  to  ().
with gatsby, global css imports are included in . with next, you should create a  for global css. when migrating, you can copy over your css imports directly and update the relative file path, if necessary. next.js has .
the gatsby  and next.js  component have a slightly different api.
update any import statements, switch  to .
the largest difference between gatsby and next.js is how data fetching is implemented. gatsby is opinionated with graphql being the default strategy for retrieving data across your application. with next.js, you get to choose which strategy you want (graphql is one supported option).
gatsby uses the  tag to query data in the pages of your site. this may include local data, remote data, or information about your site configuration. gatsby only allows the creation of static pages. with next.js, you can choose on a  which  you want. for example,  allows you to do server-side rendering. if you wanted to generate a static page, you'd export  /  inside the page, rather than using . for example:
you'll commonly see gatsby plugins used for reading the file system (), handling markdown files (), and so on. for example, the popular starter blog example has . next takes a different approach. it includes common features directly inside the framework, and gives the user full control over integrations with external packages. for example, rather than abstracting reading from the file system to a plugin, you can use the native node.js  package inside  /  to read from the file system.
next.js has a built-in .
the next.js image component, , is an extension of the html  element, evolved for the modern web.
the automatic image optimization allows for resizing, optimizing, and serving images in modern formats like  when the browser supports it. this avoids shipping large images to devices with a smaller viewport. it also allows next.js to automatically adopt future image formats and serve them to browsers that support those formats.
instead of optimizing images at build time, next.js optimizes images on-demand, as users request them. unlike static site generators and static-only solutions, your build times aren't increased, whether shipping 10 images or 10 million images.
this means you can remove common gatsby plugins like:
instead, use the built-in  component and .
the  component's default loader is not supported when using . however, other loader options will work.
with gatsby, your site's metadata (name, description, etc.) is located inside . this is then exposed through the graphql api and consumed through a  or a static query inside a component.
with next.js, we recommend creating a config file similar to below. you can then import this file anywhere without having to use graphql to access your site's metadata.
most gatsby examples use  to assist with adding  tags for proper seo. with next.js, we use  to add  tags to your  element. for example, here's an seo component with gatsby:
and here's the same example using next.js, including reading from a site config file.
take a look at  for more details on how an app can be migrated from gatsby to next.js. if you have questions or if this guide didn't work for you, feel free to reach out to our community on .
migrating from create react app
this guide will help you understand how to transition from an existing non-ejected create react app project to next.js. migrating to next.js will allow you to:
remove  (but keep  and ). if you're using react router, you can also remove .
here's an example :
create react app uses the  directory for the  as well as static assets, but next.js only uses it for static assets. when migrating from create react app, the location of the  directory remains the same.
move any images, fonts, or other static assets to .
convert  (the entry point of your application) to next.js. any  code should be moved to a . any shared layout between all pages should be moved to a .
see  for css/sass files.
with create react app, you're likely using react router. instead of using a third-party library, next.js includes its own .
create a  directory at the root of your project.
then, move the  file to . this file is the  of your next.js application. populate this file with code that is used to display the index route in your create react app.
convert all other  components to new files in the  directory.
for routes that require dynamic content (e.g. ), you can use  with next.js (e.g. ). the value of  is accessible through a . for example, the route  would forward the query object  to  ().
for more information, see .
next.js has built-in support for ,  and .
with create react app, you can import  files directly inside react components. next.js allows you to do the same, but requires these files to be . for global styles, you'll need a  to add a .
with client-side rendered applications (like create react app), you can access , , , and other  out of the box.
since next.js uses , you'll need to safely access those web apis only when you're on the client-side. for example, the following code snippet will allow access to  only on the client-side.
a recommended way of accessing web apis safely is by using the  hook, which only executes client-side:
since version 10.0.0, next.js has a built-in .
instead of optimizing images at build time, next.js optimizes images on-demand, as users request them. your build times aren't increased, whether shipping 10 images or 10 million images.
next.js has support for   similar to create react app. the main difference is the prefix used to expose environment variables on the client-side.
change all environment variables with the  prefix to .
server-side environment variables will be available at build-time and in .
most create react app examples use  to assist with adding  tags for proper seo. with next.js, we use  to add  tags to your  element. for example, here's an seo component with create react app:
and here's the same example using next.js.
if you want to move your existing create react app to next.js and keep a single-page app, you can move your old application's entry point to an  named .
if you've ejected create react app, here are some things to consider:
if you have custom file loaders set up for css, sass, or other assets, this is all built-in with next.js.
if you've manually added  (e.g. optional chaining) or , check to see what's included by default with next.js.
if you have a custom code splitting setup, you can remove that. next.js has automatic code splitting on a .
you can  with next.js without ejecting from the framework.
you should reference the default  and  of next.js to see what's included by default.
you can learn more about next.js by completing our . if you have questions or if this guide didn't work for you, feel free to reach out to our community on .
migrating from react router
this guide will help you understand how to transition from  to  routes with next.js. using  and  will allow you to:
decrease bundle size by removing react router as a dependency.
define your application routes through the file system.
utilize the latest improvements to the next.js framework.
first, uninstall react router. you'll be migrating to the built-in routing with next.js.
the  component for performing client-side route transitions is slightly different from react router.
most react applications that use react router have a top-level navigation file, containing a list of routes. for example:
with next.js, you can express the same application structure in the file system. when a file is added to the  directory it's automatically available as a route.
in the example below, routes like  would render the  component. if a slug was not provided, it would render the list of all blog posts.
rather than using the  syntax inside your  component, next.js uses the  syntax in the file name for . we can transform this to next.js by creating two new files,  (showing all pages) and  (showing an individual post).
next.js has built-in support for . this means you can remove any instances of  in your code.
next.js has built-in support for . this means you can remove any instances of:
, , and
modifications to your  for
each file inside your  directory will be code split into its own javascript bundle during the build process. next.js  es2020 dynamic  for javascript. with it you can import javascript modules dynamically and work with them. they also work with ssr.
for more information, read about .
next.js has built-in support for . this means you can remove any custom  components you have defined.
the default behavior of  and  is to scroll to the top of the page. you can also  if you prefer.
frequently asked questions
yes! next.js is used by many of the top websites in the world. see the
for more info.
next.js provides a variety of methods depending on your use case. you can use:
client-side data fetching: fetch data with  or  inside your react components
server-side rendering with
static-site generation with
incremental static regeneration by
to learn more about data fetching, visit our .
next.js includes a built-in router for a few reasons:
it uses a file-system based router which reduces configuration
it supports shallow routing which allows you to change the url without running data fetching methods
routes are always lazy-loadable
if you're migrating from react router, see the .
yes! we have hundreds of examples in our .
yes! here's an  and an .
yes! here's our .
yes! when you deploy your next.js application to , your static assets are automatically detected and served by the edge network. if you self-host next.js, you can learn how to manually configure the asset prefix .
in most cases, no manual webpack configuration is necessary since next.js automatically configures webpack. for advanced cases where more control is needed, refer to the .
many of the goals we set out to accomplish were the ones listed in the  by guillermo rauch.
the ease-of-use of php is a great inspiration. we feel next.js is a suitable replacement for many scenarios where you would otherwise use php to output html.
unlike php, we benefit from the es6 module system and every page exports a component or function that can be easily imported for lazy evaluation or testing.
as we were researching options for server-rendering react that didn’t involve a large number of steps, we came across  (now deprecated), a similar approach to next.js by the creator of react jordan walke.
next.js cli
the next.js cli allows you to start, build, and export your application.
to get a list of the available cli commands, run the following command inside your project directory:
( comes with npm 5.2+ and higher)
the output should look like this:
you can pass any  to  commands:
note: running  without a command is the same as running
creates an optimized production build of your application. the output displays information about each route.
size – the number of assets downloaded when navigating to the page client-side. the size for each route only includes its dependencies.
first load js – the number of assets downloaded when visiting the page from the server. the amount of js shared by all is shown as a separate metric.
both of these values are compressed with gzip. the first load is indicated by green, yellow, or red. aim for green for performant applications.
you can enable production profiling for react with the  flag in . this requires :
after that, you can use the profiler in the same way as you would in development.
you can enable more verbose build output with the  flag in . this requires next.js 9.5.3:
with this flag enabled additional build output like rewrites, redirects, and headers will be shown.
starts the application in development mode with hot-code reloading, error reporting, and more:
the application will start at  by default. the default port can be changed with , like so:
or using the  environment variable:
note:  can not be set in  as booting up the http server happens before any other code is initialized.
you can also set the hostname to be different from the default of , this can be useful for making the application available for other devices on the network. the default hostname can be changed with , like so:
starts the application in production mode. the application should be compiled with  first.
when deploying next.js behind a downstream proxy (e.g. a load-balancer like aws elb/alb) it's important to configure next's underlying http server with  that are larger than the downstream proxy's timeouts. otherwise, once a keep-alive timeout is reached for a given tcp connection, node.js will immediately terminate that connection without notifying the downstream proxy. this results in a proxy error whenever it attempts to reuse a connection that node.js has already terminated.
to configure the timeout values for the production next.js server, pass  (in milliseconds) to , like so:
runs eslint for all files in the , , and  directories. it also
provides a guided setup to install any required dependencies if eslint is not already configured in
your application.
if you have other directories that you would like to lint, you can specify them using the
flag:
next.js collects completely anonymous telemetry data about general usage.
participation in this anonymous program is optional, and you may opt-out if you'd not like to share any information.
to learn more about telemetry, .
prints relevant details about the current system which can be used to report next.js bugs.
this information includes operating system platform/arch/version, binaries (node.js, npm, yarn, pnpm) and npm package versions (, , ).
running the following in your project's root directory:
will give you information like this example:
this information should then be pasted into github issues.
create next app
the easiest way to get started with next.js is by using . this cli tool enables you to quickly start building a new next.js application, with everything set up for you. you can create a new app using the default next.js template, or by using one of the . to get started, use the following command:
you can create a new project interactively by running:
you will be asked for the name of your project, and then whether you want to
create a typescript project:
select yes to install the necessary types/dependencies and create a new ts project.
you can also pass command line arguments to set up a new project
non-interactively. see :
allows you to create a new next.js app within seconds. it is officially maintained by the creators of next.js, and includes a number of benefits:
interactive experience: running  (with no arguments) launches an interactive experience that guides you through setting up a project.
zero dependencies: initializing a project is as quick as one second. create next app has zero dependencies.
offline support: create next app will automatically detect if you're offline and bootstrap your project using your local package cache.
support for examples: create next app can bootstrap your application using an example from the next.js examples collection (e.g. ).
tested: the package is part of the next.js monorepo and tested using the same integration test suite as next.js itself, ensuring it works as expected with every release.
next/router
before moving forward, we recommend you to read  first.
if you want to access the  inside any function component in your app, you can use the  hook, take a look at the following example:
is a , meaning it cannot be used with classes. you can either use  or wrap your class in a function component.
the following is the definition of the  object returned by both  and :
:  - the path for current route file that comes after . therefore, ,  and trailing slash () are not included.
:  - the query string parsed to an object, including  parameters. it will be an empty object during prerendering if the page doesn't use . defaults to
:  - the path as shown in the browser including the search params and respecting the  configuration.  and  are not included.
:  - whether the current page is in .
:  - the active  (if enabled).
:  - the active locale (if enabled).
:  - all supported locales (if enabled).
:  - the current default locale (if enabled).
:  - any configured domain locales.
:  - whether the router fields are updated client-side and ready for use. should only be used inside of  methods and not for conditionally rendering on the server. see related docs for use case with
:  - whether the application is currently in .
using the  field may lead to a mismatch between client and server if the page is rendered using server-side rendering or . avoid using  until the  field is .
the following methods are included inside :
handles client-side transitions, this method is useful for cases where  is not enough.
:  - the url to navigate to (see  for  properties).
:  - optional decorator for the path that will be shown in the browser url bar. before next.js 9.5.3 this was used for dynamic routes, check our  to see how it worked. note: when this path differs from the one provided in  the previous / behavior is used as shown in the
- optional object with the following configuration options:
- optional boolean, controls scrolling to the top of the page after navigation. defaults to
: update the path of the current page without rerunning ,  or . defaults to
- optional string, indicates locale of the new page
you don't need to use  for external urls.  is better suited for those cases.
navigating to , which is a predefined route:
navigating , which is a dynamic route:
redirecting the user to , useful for pages behind :
when navigating to the same page in next.js, the page's state will not be reset by default as react does not unmount unless the parent component has changed.
in the above example, navigating between  and  will not reset the count . the  is maintained between renders because the top-level react component, , is the same.
if you do not want this behavior, you have a couple of options:
manually ensure each state is updated using . in the above example, that could look like:
use a react  to . to do this for all pages, you can use a custom app:
you can use a url object in the same way you can use it for . works for both the  and  parameters:
similar to the  prop in ,  will prevent adding a new url entry into the  stack.
the api for  is exactly the same as the api for .
take a look at the following example:
prefetch pages for faster client-side transitions. this method is only useful for navigations without , as  takes care of prefetching pages automatically.
this is a production only feature. next.js doesn't prefetch pages in development.
- the url to prefetch, including explicit routes (e.g. ) and dynamic routes (e.g. )
- optional decorator for . before next.js 9.5.3 this was used to prefetch dynamic routes, check our  to see how it worked
- optional object with the following allowed fields:
- allows providing a different locale from the active one. if ,  has to include the locale as the active locale won't be used.
let's say you have a login page, and after a login, you redirect the user to the dashboard. for that case, we can prefetch the dashboard to make a faster transition, like in the following example:
in some cases (for example, if using a ), you may wish to listen to  and do something before the router acts on it.
- the function to run on incoming  events. the function receives the state of the event as an object with the following props:
:  - the route for the new state. this is usually the name of a
:  - the url that will be shown in the browser
:  - additional options sent by
if  returns , the next.js router will not handle , and you'll be responsible for handling it in that case. see .
you could use  to manipulate the request, or force a ssr refresh, as in the following example:
navigate back in history. equivalent to clicking the browser’s back button. it executes .
reload the current url. equivalent to clicking the browser’s refresh button. it executes .
you can listen to different events happening inside the next.js router. here's a list of supported events:
- fires when a route starts to change
- fires when a route changed completely
- fires when there's an error when changing routes, or a route load is cancelled
- indicates if the navigation was cancelled
- fires before changing the browser's history
- fires when the hash will change but not the page
- fires when the hash has changed but not the page
note: here  is the url shown in the browser, including the .
for example, to listen to the router event , open or create  and subscribe to the event, like so:
we use a  () for this example to subscribe to the event because it's not unmounted on page navigations, but you can subscribe to router events on any component in your application.
router events should be registered when a component mounts ( or  / ) or imperatively when an event happens.
if a route load is cancelled (for example, by clicking two links rapidly in succession),  will fire. and the passed  will contain a  property set to , as in the following example:
certain methods accessible on the  object return a promise. if you have the eslint rule,  enabled, consider disabling it either globally, or for the affected line.
if your application needs this rule, you should either  the promise – or use an  function,  the promise, then void the function call. this is not applicable when the method is called from inside an  handler.
the affected methods are:
if  is not the best fit for you,  can also add the same  to any component.
to use class components with , the component needs to accept a router prop:
next/link
client-side transitions between routes can be enabled via the  component exported by .
for an example, consider a  directory with the following files:
we can have a link to each of these pages like so:
accepts the following props:
- the path or url to navigate to. this is the only required prop. it can also be an object, see
- optional decorator for the path that will be shown in the browser url bar. before next.js 9.5.3 this was used for dynamic routes, check our  to see how it worked. note: when this path differs from the one provided in  the previous / behavior is used as shown in the .
- changes behavior so that child must be . defaults to .
- forces  to send the  property to its child. defaults to
- prefetch the page in the background. defaults to . any  that is in the viewport (initially or through scroll) will be preloaded. prefetch can be disabled by passing . when  is set to , prefetching will still occur on hover. pages using  will preload  files with the data for faster page transitions. prefetching is only enabled in production.
- replace the current  state instead of adding a new url into the stack. defaults to
- scroll to the top of the page after a navigation. defaults to
- update the path of the current page without rerunning ,  or . defaults to
- the active locale is automatically prepended.  allows for providing a different locale. when   has to include the locale as the default behavior is disabled.
there is nothing to do when linking to a , including , since next.js 9.5.3 (for older versions check our ). however, it can become quite common and handy to use  or an  to generate the link.
for example, the dynamic route  will match the following link:
if the child of  is a custom component that wraps an  tag, you must add  to . this is necessary if you’re using libraries like . without this, the  tag will not have the  attribute, which hurts your site's accessibility and might affect seo. if you're using , there is a built-in rule  to ensure correct usage of .
if you’re using ’s jsx pragma feature (), you must use  even if you use an  tag directly.
the component should support  property to trigger navigation correctly
if the child of  is a functional component, in addition to using  and , you must wrap the component in :
can also receive a url object and it will automatically format it to create the url string. here's how to do it:
the above example has a link to:
a predefined route:
you can use every property as defined in the .
the default behavior of the  component is to  a new url into the  stack. you can use the  prop to prevent adding a new entry, as in the following example:
the default behavior of  is to scroll to the top of the page. when there is a hash defined it will scroll to the specific id, like a normal  tag. to prevent scrolling to the top / hash  can be added to :
it's common to use  for authentication or other purposes that involve rewriting the user to a different page. in order for the  component to properly prefetch links with rewrites via middleware, you need to tell next.js both the url to display and the url to prefetch. this is required to avoid un-necessary fetches to middleware to know the correct route to prefetch.
for example, if you have want to serve a  route that has authenticated and visitor views, you may add something similar to the following in your middleware to redirect the user to the correct page:
in this case, you would want to use the following code in your  component (inside ):
note: if you're using , you'll need to adapt your  and  props. for example, if you have a dynamic route like  that you want to present differently via middleware, you would write: .
next/image
note: this page is the api reference for the  component. for a feature overview and usage information, please see the  documentation.
note: if you are using a version of next.js prior to 13, you'll want to use the  documentation since the component was renamed.
this  component uses browser native , which may fallback to eager loading for older browsers before safari 15.4. when using the blur-up placeholder, older browsers before safari 12 will fallback to empty placeholder. when using styles with / of , it is possible to cause  on older browsers before safari 15 that don't . for more details, see .
displays a gray border while loading. possible solutions:
use css
use  if the image is above the fold
displays a white background while loading. possible solutions:
enable
the  component requires the following properties.
must be one of the following:
a  image file, or
a path string. this can be either an absolute external url,
or an internal path depending on the  prop.
when using an external url, you must add it to  in .
the  property represents the rendered width in pixels, so it will affect how large the image appears.
required, except for  or images with the .
the  property represents the rendered height in pixels, so it will affect how large the image appears.
the  property is used to describe the image for screen readers and search engines. it is also the fallback text if images have been disabled or an error occurs while loading the image.
it should contain text that could replace the image . it is not meant to supplement the image and should not repeat information that is already provided in the captions above or below the image.
if the image is  or , the  property should be an empty string ().
the  component accepts a number of additional properties beyond those which are required. this section describes the most commonly-used properties of the image component. find details about more rarely-used properties in the  section.
a custom function used to resolve image urls.
a  is a function returning a url string for the image, given the following parameters:
here is an example of using a custom loader:
alternatively, you can use the  configuration in next.config.js to configure every instance of  in your application, without passing a prop.
a boolean that causes the image to fill the parent element instead of setting  and .
the parent element must assign , , or  style.
by default, the img element will automatically be assigned the  style.
the default image fit behavior will stretch the image to fit the container. you may prefer to set  for an image which is letterboxed to fit the container and preserve aspect ratio.
alternatively,  will cause the image to fill the entire container and be cropped to preserve aspect ratio. for this to look correct, the  style should be assigned to the parent element.
see also:
a string that provides information about how wide the image will be at different breakpoints. the value of  will greatly affect performance for images using  or which are styled to have a responsive size.
the  property serves two important purposes related to image performance:
first, the value of  is used by the browser to determine which size of the image to download, from 's automatically-generated source set. when the browser chooses, it does not yet know the size of the image on the page, so it selects an image that is the same size or larger than the viewport. the  property allows you to tell the browser that the image will actually be smaller than full screen. if you don't specify a  value in an image with the  property, a default value of  (full screen width) is used.
second, the  property configures how  automatically generates an image source set. if no  value is present, a small source set is generated, suitable for a fixed-size image. if  is defined, a large source set is generated, suitable for a responsive image. if the  property includes sizes such as , which represent a percentage of the viewport width, then the source set is trimmed to not include any values which are too small to ever be necessary.
for example, if you know your styling will cause an image to be full-width on mobile devices, in a 2-column layout on tablets, and a 3-column layout on desktop displays, you should include a sizes property such as the following:
this example  could have a dramatic effect on performance metrics. without the  sizes, the image selected from the server would be 3 times as wide as it needs to be. because file size is proportional to the square of the width, without  the user would download an image that's 9 times larger than necessary.
learn more about  and :
the quality of the optimized image, an integer between  and , where  is the best quality and therefore largest file size. defaults to .
when true, the image will be considered high priority and
. lazy loading is automatically disabled for images using .
you should use the  property on any image detected as the  element. it may be appropriate to have multiple priority images, as different images may be the lcp element for different viewport sizes.
should only be used when the image is visible above the fold. defaults to .
a placeholder to use while the image is loading. possible values are  or . defaults to .
when , the  property will be used as the placeholder. if  is an object from a  and the imported image is , , , or , then  will be automatically populated.
for dynamic images, you must provide the  property. solutions such as  can help with  generation.
when , there will be no placeholder while the image is loading, only empty space.
try it out:
in some cases, you may need more advanced usage. the  component optionally accepts the following advanced properties.
allows  to the underlying image element.
also keep in mind that the required  and  props can interact with your styling. if you use styling to modify an image's , you must set the  style as well, or your image will be distorted.
a callback function that is invoked once the image is completely loaded and the  has been removed.
the callback function will be called with one argument, a reference to the underlying  element.
a callback function that is invoked when the image is loaded.
note that the load event might occur before the placeholder is removed and the image is fully decoded.
instead, use .
a callback function that is invoked if the image fails to load.
attention: this property is only meant for advanced usage. switching an
image to load with  will normally hurt performance.
we recommend using the  property instead, which
properly loads the image eagerly for nearly all use cases.
the loading behavior of the image. defaults to .
when , defer loading the image until it reaches a calculated distance from
the viewport.
when , load the image immediately.
a  to
be used as a placeholder image before the  image successfully loads. only takes effect when combined
with .
must be a base64-encoded image. it will be enlarged and blurred, so a very small image (10px or
less) is recommended. including larger images as placeholders may harm your application performance.
you can also  to match the image.
when true, the source image will be served as-is instead of changing quality,
size, or format. defaults to .
this prop can be assigned to all images by updating  with the following configuration:
other properties on the  component will be passed to the underlying
element with the exception of the following:
. use  instead.
. it is always .
to protect your application from malicious users, configuration is required in order to use external images. this ensures that only external images from your account can be served from the next.js image optimization api. these external images can be configured with the  property in your  file, as shown below:
note: the example above will ensure the  property of  must start with . any other protocol, hostname, port, or unmatched path will respond with 400 bad request.
below is another example of the  property in the  file:
note: the example above will ensure the  property of  must start with  or  or any number of subdomains. any other protocol or unmatched hostname will respond with 400 bad request.
wildcard patterns can be used for both  and  and have the following syntax:
match a single path segment or subdomain
match any number of path segments at the end or subdomains at the beginning
the  syntax does not work in the middle of the pattern.
note: we recommend using  instead so you can restrict protocol and pathname.
similar to , the  configuration can be used to provide a list of allowed hostnames for external images.
however, the  configuration does not support wildcard pattern matching and it cannot restrict protocol, port, or pathname.
below is an example of the  property in the  file:
if you want to use a cloud provider to optimize images instead of using the next.js built-in image optimization api, you can configure the  in your  like the following:
this must point to a file relative to the root of your next.js application. the file must export a default function that returns a string, for example:
alternatively, you can use the  to configure each instance of .
the following configuration is for advanced use cases and is usually not necessary. if you choose to configure the properties below, you will override any changes to the next.js defaults in future updates.
if you know the expected device widths of your users, you can specify a list of device width breakpoints using the  property in . these widths are used when the  component uses  prop to ensure the correct image is served for user's device.
if no configuration is provided, the default below is used.
you can specify a list of image widths using the  property in your  file. these widths are concatenated with the array of  to form the full array of sizes used to generate image s.
the reason there are two separate lists is that imagesizes is only used for images which provide a  prop, which indicates that the image is less than the full width of the screen. therefore, the sizes in imagesizes should all be smaller than the smallest size in devicesizes.
the default  will automatically detect the browser's supported image formats via the request's  header.
if the  head matches more than one of the configured formats, the first match in the array is used. therefore, the array order matters. if there is no match (or the source image is ), the image optimization api will fallback to the original image's format.
you can enable avif support with the following configuration.
note: avif generally takes 20% longer to encode but it compresses 20% smaller compared to webp. this means that the first time an image is requested, it will typically be slower and then subsequent requests that are cached will be faster.
note: if you self-host with a proxy/cdn in front of next.js, you must configure the proxy to forward the  header.
the following describes the caching algorithm for the default . for all other loaders, please refer to your cloud provider's documentation.
images are optimized dynamically upon request and stored in the  directory. the optimized image file will be served for subsequent requests until the expiration is reached. when a request is made that matches a cached but expired file, the expired image is served stale immediately. then the image is optimized again in the background (also called revalidation) and saved to the cache with the new expiration date.
the cache status of an image can be determined by reading the value of the  response header. the possible values are the following:
- the path is not in the cache (occurs at most once, on the first visit)
- the path is in the cache but exceeded the revalidate time so it will be updated in the background
- the path is in the cache and has not exceeded the revalidate time
the expiration (or rather max age) is defined by either the  configuration or the upstream image  header, whichever is larger. specifically, the  value of the  header is used. if both  and  are found, then  is preferred. the  is also passed-through to any downstream clients including cdns and browsers.
you can configure  to increase the cache duration when the upstream image does not include  header or the value is very low.
you can configure  and  to reduce the total number of possible generated images.
you can configure  to disable multiple formats in favor of a single image format.
you can configure the time to live (ttl) in seconds for cached optimized images. in many cases, it's better to use a  which will automatically hash the file contents and cache the image forever with a  header of .
the expiration (or rather max age) of the optimized image is defined by either the  or the upstream image  header, whichever is larger.
if you need to change the caching behavior per image, you can configure  to set the  header on the upstream image (e.g. , not  itself).
there is no mechanism to invalidate the cache at this time, so its best to keep  low. otherwise you may need to manually change the  prop or delete .
the default behavior allows you to import static files such as  and then pass that to the  property.
in some cases, you may wish to disable this feature if it conflicts with other plugins that expect the import to behave differently.
you can disable static image imports inside your :
the default  does not optimize svg images for a few reasons. first, svg is a vector format meaning it can be resized losslessly. second, svg has many of the same features as html/css, which can lead to vulnerabilities without proper .
if you need to serve svg images with the default image optimization api, you can set  and  inside your :
the default  will automatically bypass image optimization for animated images and serve the image as-is.
auto-detection for animated files is best-effort and supports gif, apng, and webp. if you want to explicitly bypass image optimization for a given animated image, use the  prop.
for an overview of the image component features and usage guidelines, see:
next/script
this api reference will help you understand how to use  available for the script component. for features and usage, please see the  page.
here's a summary of the props available for the script component:
a path string specifying the url of an external script. this can be either an absolute external url or an internal path.
note: the  property is required unless an inline script is used.
the  component accepts a number of additional properties beyond those which are required.
the loading strategy of the script. there are four different strategies that can be used:
: load before any next.js code and before any page hydration occurs.
: (default) load early but after some hydration on the page occurs.
: load during browser idle time.
: (experimental) load in a web worker.
scripts that load with the  strategy are injected into the initial html from the server, downloaded before any next.js module, and executed in the order they are placed before any hydration occurs on the page.
scripts denoted with this strategy are preloaded and fetched before any first-party code, but their execution does not block page hydration from occuring.
scripts must be placed inside  and are designed to load scripts that are needed by the entire site (i.e. the script will load when any page in the application has been loaded server-side).
this strategy should only be used for critical scripts that need to be fetched before any part of the page becomes interactive.
note: scripts with  will always be injected inside the  of the html document regardless of where it's placed in the component.
some examples of scripts that should be loaded as soon as possible with  include:
bot detectors
cookie consent managers
scripts that use the  strategy are injected into the html client-side and will load after some (or all) hydration occurs on the page. this is the default strategy of the script component and should be used for any script that needs to load as soon as possible but not before any first-party next.js code.
scripts can be placed inside of any page or layout and will only load and execute when that page (or group of pages) is opened in the browser.
some examples of scripts that are good candidates for  include:
tag managers
analytics
scripts that use the  strategy are injected into the html client-side during browser idle time and will load after all resources on the page have been fetched. this strategy should be used for any background or low priority scripts that do not need to load early.
examples of scripts that do not need to load immediately and can be fetched with  include:
chat support plugins
social media widgets
scripts that use the  strategy are off-loaded to a web worker in order to free up the main thread and ensure that only critical, first-party resources are processed on it. while this strategy can be used for any script, it is an advanced use case that is not guaranteed to support all third-party scripts.
to use  as a strategy, the  flag must be enabled in :
scripts can only currently be used in the  directory:
note:  can't be used with  – consider using  instead. learn more about usage of  in the .
some third-party scripts require users to run javascript code once after the script has finished loading in order to instantiate content or call a function. if you are loading a script with either afterinteractive or lazyonload as a loading strategy, you can execute code after it has loaded using the onload property.
here's an example of executing a lodash method only after the library has been loaded.
note: learn more about usage of  in the .
some third-party scripts require users to run javascript code after the script has finished loading and every time the component is mounted (after a route navigation for example). you can execute code after the script's load event when it first loads and then after every subsequent component re-mount using the onready property.
here's an example of how to re-instantiate a google maps js embed everytime the component is mounted:
note:  cannot be used with the  loading strategy. learn more about usage of  in the .
sometimes it is helpful to catch when a script fails to load. these errors can be handled with the onerror property:
next/head
we expose a built-in component for appending elements to the  of the page:
to avoid duplicate tags in your  you can use the  property, which will make sure the tag is only rendered once, as in the following example:
in this case only the second  is rendered.  tags with duplicate  attributes are automatically handled.
the contents of  get cleared upon unmounting the component, so make sure each page completely defines what it needs in , without making assumptions about what other pages added.
,  or any other elements (e.g. ) need to be contained as direct children of the  element,
or wrapped into maximum one level of  or arrays—otherwise the tags won't be correctly picked up on client-side navigations.
we recommend using  in your component instead of manually creating a  in .
next/amp
amp support is one of our advanced features, you can .
to enable amp, add the following config to your page:
the  config accepts the following values:
- the page will be amp-only
- the page will have two versions, one with amp and another one with html
to learn more about the  config, read the sections below.
the page above is an amp-only page, which means:
the page has no next.js or react client-side runtime
the page is automatically optimized with , an optimizer that applies the same transformations as amp caches (improves performance by up to 42%)
the page has a user-accessible (optimized) version of the page and a search-engine indexable (unoptimized) version of the page
the page above is a hybrid amp page, which means:
the page is rendered as traditional html (default) and amp html (by adding  to the url)
the amp version of the page only has valid optimizations applied with amp optimizer so that it is indexable by search-engines
the page uses  to differentiate between modes, it's a  that returns  if the page is using amp, and  otherwise.
next/server
provides server-only helpers for use in  and .
the  object is an extension of the native  interface, with the following added methods and properties:
- a  instance with cookies from the . it reads/mutates the  header of the request. see also .
- a method that takes a cookie  and returns an object with  and . if a cookie with  isn't found, it returns . if multiple cookies match, it will only return the first match.
- a method that is similar to , but returns a list of all the cookies with a matching . if  is unspecified, it returns all the available cookies.
- a method that takes an object with properties of  as defined in the  spec.
- a method that takes either a cookie  or a list of names. and removes the cookies matching the name(s). returns  for deleted and  for undeleted cookies.
- a method that takes a cookie  and returns a  based on if the cookie exists () or not ().
- a method that takes no argument and will effectively remove the  header.
: includes an extended, parsed, url object that gives you access to next.js specific properties such as , ,  and . includes the following properties:
: ()
: () - has the ip address of the . this information is provided by your hosting platform.
- has the geographic location from the . this information is provided by your hosting platform. includes the following properties:
you can use the  object as a direct replacement for the native  interface, giving you more control over how you manipulate the request.
can be imported from :
the  object extends the native  object, and includes the  method.
the  method can be used to prolong the execution of the function if you have other background work to make.
the  object can be imported from :
the  class extends the native  interface, with the following:
public methods are available on an instance of the  class. depending on your use case, you can create an instance and assign to a variable, then access the following public methods:
- a  instance with the cookies from the . it a
a  instance with cookies from the . it reads/mutates the  header of the response. see also .
the following static methods are available on the  class directly:
- returns a  with a redirect set
- returns a  with a rewrite set
- returns a  that will continue the middleware chain
to use the methods above, you must return the  object returned.  can be imported from :
the  helper allows you to interact with the user agent object from the request. it is abstracted from the native  object, and is an opt in feature. it has the following properties:
: () whether the request comes from a known bot
: () the name of the browser
: () the version of the browser, determined dynamically
: () the model of the device, determined dynamically
: () the type of the browser, can be one of the following values: , , , , , , or
: () the vendor of the device, determined dynamically
: () the name of the browser engine, could be one of the following values: , , , , , , , , , , , , , , , ,  or
: () the version of the browser engine, determined dynamically, or
: () the name of the os, could be
: () the version of the os, determined dynamically, or
: () the architecture of the cpu, could be one of the following values: , , , , , , , , , , , , , , ,  or
when using  you may notice that the status codes used are  for a temporary redirect, and  for a permanent redirect. while traditionally a  was used for a temporary redirect, and a  for a permanent redirect, many browsers changed the request method of the redirect, from a  to  request when using a , regardless of the origins request method.
taking the following example of a redirect from  to , if you make a  request to  to create a new user, and are conforming to a  temporary redirect, the request method will be changed from a  to a  request. this doesn't make sense, as to create a new user, you should be making a  request to , and not a  request.
the introduction of the  status code means that the request method is preserved as .
- temporary redirect, will change the request method from  to
- temporary redirect, will preserve the request method as
the  method uses a  by default, instead of a  temporary redirect, meaning your requests will always be preserved as  requests.
if you want to cause a  response to a  request, use .
about http redirects.
can be used to access  from edge middleware. they are evaluated during :
@next/font
this api reference will help you understand how to use  and . for features and usage, please see the  page.
for usage, review  and .
the path of the font file as a string or an array of objects (with type ) relative to the directory where the font loader function is called.
used in
required
examples:
where  is placed in a directory named  inside the  directory
if the font loader function is called in  using , then  is placed in  at the root of the project
the font  with the following possibilities:
a string with possible values of the weights available for the specific font or a range of values if it's a  font
an array of weight values if the font is not a . it applies to  only.
used in  and
required if the font being used is not
: a string for a single weight value - for the font , the possible values are , , , , , , , ,  or  where  is the default)
: a string for the range between  and  for a variable font
: an array of 3 possible values for a non variable font
a string  with default value of
an array of style values if the font is not a . it applies to  only.
optional
: a string - it can be  or  for
: a string - it can take any value for  but is expected to come from
: an array of 2 values for  - the values are from  and
the font  defined by an array of string values with the names of each subset you would like to be . fonts specified via  will have a link preload tag injected into the head when the  option is true, which is the default.
: an array with the subset
some variable fonts have extra  that can be included. by default, only the font weight is included to keep the file size down. the possible values of  depend on the specific font.
: an array with value  for the  variable font which has  as additional  as shown . you can find the possible  values for your font by using the filter on the  and looking for axes other than
the font  with possible string  of , , ,  or  with default value of .
: a string assigned to the  value
a boolean value that specifies whether the font should be  or not. the default is .
the fallback font to use if the font cannot be loaded. an array of strings of fallback fonts with no default.
: an array setting the fallback fonts to  or
for : a boolean value that sets whether an automatic fallback font should be used to reduce . the default is .
for : a string or boolean  value that sets whether an automatic fallback font should be used to reduce . the possible values are ,  or . the default is .
: for `
: for
a string value to define the css variable name to be used if the style is applied with the .
: the css variable  is declared
an array of font face  key-value pairs that define the generated  further.
you can apply the font styles in three ways:
returns a read-only css  for the loaded font to be passed to an html element.
returns a read-only css  object for the loaded font to be passed to an html element, including  to access the font family name and fallback fonts.
if you would like to set your styles in an external style sheet and specify additional options there, use the css variable method.
in addition to importing the font, also import the css file where the css variable is defined and set the variable option of the font loader object as follows:
to use the font, set the  of the parent container of the text you would like to style to the font loader's  value and the  of the text to the  property from the external css file.
define the  selector class in the  css file as follows:
in the example above, the text  is styled using the  font and the generated font fallback with  and .
next/legacy/image
starting with next.js 13, the  component was rewritten to improves both the performance and developer experience. in order to provide a backwards compatible upgrade solution, the old  was renamed to .
compared to , the new  component has the following changes:
removes  wrapper around  in favor of
adds support for canonical  prop
removes  prop in favor of  or
removes  implementation in favor of
removes  prop since there is no native equivalent
removes  config in favor of  prop
changed  prop from optional to required
changed  callback to receive reference to  element
or an internal path depending on the  prop or .
when using an external url, you must add it to
the  property can represent either the rendered width or original width in pixels, depending on the  and  properties.
when using  or  the  property represents the rendered width in pixels, so it will affect how large the image appears.
when using , , the  property represents the original width in pixels, so it will only affect the aspect ratio.
the  property is required, except for , or those with .
the  property can represent either the rendered height or original height in pixels, depending on the  and  properties.
when using  or  the  property represents the rendered height in pixels, so it will affect how large the image appears.
when using , , the  property represents the original height in pixels, so it will only affect the aspect ratio.
the layout behavior of the image as the viewport changes size.
when , the image will scale the dimensions down for smaller viewports, but maintain the original dimensions for larger viewports.
when , the image dimensions will not change as the viewport changes (no responsiveness) similar to the native  element.
when , the image will scale the dimensions down for smaller viewports and scale up for larger viewports.
ensure the parent element uses  in their stylesheet.
when , the image will stretch both width and height to the dimensions of the parent element, provided the parent element is relative.
this is usually paired with the  property.
ensure the parent element has  in their stylesheet.
a custom function used to resolve urls. setting the loader as a prop on the image component overrides the default loader defined in the .
a string that provides information about how wide the image will be at different breakpoints. the value of  will greatly affect performance for images using  or . it will be ignored for images using  or .
first, the value of  is used by the browser to determine which size of the image to download, from 's automatically-generated source set. when the browser chooses, it does not yet know the size of the image on the page, so it selects an image that is the same size or larger than the viewport. the  property allows you to tell the browser that the image will actually be smaller than full screen. if you don't specify a  value, a default value of  (full screen width) is used.
second, the  value is parsed and used to trim the values in the automatically-created source set. if the  property includes sizes such as , which represent a percentage of the viewport width, then the source set is trimmed to not include any values which are too small to ever be necessary.
the quality of the optimized image, an integer between  and  where  is the best quality. defaults to .
note that all  modes apply their own styles to the image element, and these automatic styles take precedence over the  prop.
defines how the image will fit into its parent container when using .
this value is passed to the  for the  image.
defines how the image is positioned within its parent element when using .
this value is passed to the  applied to the image.
the  function accepts one parameter, an object with the following properties:
a string (with similar syntax to the margin property) that acts as the bounding box used to detect the intersection of the viewport with the image and trigger lazy . defaults to .
if the image is nested in a scrollable parent element other than the root document, you will also need to assign the  prop.
a react  pointing to the scrollable parent element. defaults to  (the document viewport).
the ref must point to a dom element or a react component that  to the underlying dom element.
example pointing to a dom element
example pointing to a react component
. use
instead.
if you want to use a cloud provider to optimize images instead of using the next.js built-in image optimization api, you can configure the  and  prefix in your  file. this allows you to use relative urls for the image  and automatically generate the correct absolute url for your provider.
the following image optimization cloud providers are included:
default: works automatically with , , or a custom server
: works automatically when you deploy on vercel, no configuration necessary.
custom:  use a custom cloud provider by implementing the  prop on the  component
if you need a different provider, you can use the  prop with .
images can not be optimized at build time using , only on-demand. to use  with , you will need to use a different loader than the default.
the  component's default loader uses  because it is quick to install and suitable for a development environment. when using  in your production environment, it is strongly recommended that you install  by running  in your project directory. this is not necessary for vercel deployments, as  is installed automatically.
if you know the expected device widths of your users, you can specify a list of device width breakpoints using the  property in . these widths are used when the  component uses  or  to ensure the correct image is served for user's device.
the cache status of an image can be determined by reading the value of the  ( when deployed on vercel) response header. the possible values are the following:
edge runtime
the next.js edge runtime is based on standard web apis, which is used by  and .
you can use  to access  for both  and .
running  on  will not show all your environment variables. you have to access the variables directly as shown below:
the edge runtime has some restrictions including:
native node.js apis are not supported. for example, you can't read or write to the filesystem
can be used, as long as they implement es modules and do not use native node.js apis
calling  directly is not allowed. use es modules instead
the following javascript language features are disabled, and will not work:
: evaluates javascript code represented as a string
: creates a new function with the code provided as an argument
in rare cases, your code could contain (or import) some dynamic code evaluation statements which can not be reached at runtime and which can not be removed by treeshaking.
you can relax the check to allow specific files with your middleware or edge api route exported configuration:
is a , or an array of globs, ignoring dynamic code evaluation for specific files. the globs are relative to your application root folder.
be warned that if these statements are executed on the edge, they will throw and cause a runtime error.
getinitialprops
enables  in a page and allows you to do initial data population, it means sending the  with the data already populated from the server. this is especially useful for .
will disable .
is an  function that can be added to any page as a . take a look at the following example:
or using a class component:
is used to asynchronously fetch some data, which then populates .
data returned from  is serialized when server rendering, similar to what  does. make sure the returned object from  is a plain  and not using ,  or .
for the initial page load,  will run on the server only.  will then run on the client when navigating to a different route via the  component or by using . however, if  is used in a custom , and the page being navigated to implements , then  will run on the server.
receives a single argument called , it's an object with the following properties:
- current route. that is the path of the page in
- query string section of url parsed as an object
-  of the actual path (including the query) shown in the browser
-  (server only)
- error object if any error is encountered during the rendering
can not be used in children components, only in the default export of every page
if you are using server-side only modules inside , make sure to , otherwise it'll slow down your app
if you're using typescript, you can use the  type for function components:
and for , you can use :
when exporting a function called  (server-side rendering) from a page, next.js will pre-render this page on each request using the data returned by . this is useful if you want to fetch data that changes often, and have the page update to show the most current data.
you can import modules in top-level scope for use in . imports used will not be bundled for the client-side. this means you can write server-side code directly in , including fetching data from your database.
the  parameter is an object containing the following keys:
: if this page uses a ,  contains the route parameters. if the page name is  , then  will look like .
: , with an additional  prop, which is an object with string keys mapping to string values of cookies.
: an object representing the query string, including dynamic route parameters.
:  is  if the page is in the  and  otherwise.
: the  data set by .
: a normalized version of the request  that strips the  prefix for client transitions and includes original query values.
contains the active locale (if enabled).
contains all supported locales (if enabled).
contains the configured default locale (if enabled).
the  function should return an object with any one of the following properties:
the  object is a key-value pair, where each value is received by the page component. it should be a  so that any props passed, could be serialized with .
the  boolean allows the page to return a  status and . with , the page will return a  even if there was a successfully generated page before. this is meant to support use cases like user-generated content getting removed by its author.
the  object allows redirecting to internal and external resources. it should match the shape of . in some rare cases, you might need to assign a custom status code for older  clients to properly redirect. in these cases, you can use the  property instead of the  property, but not both.
the type of  can be specified using  from :
if you want to get inferred typings for your props, you can use :
implicit typing for  will also work properly:
when exporting a function called  from a page that uses , next.js will statically pre-render all the paths specified by .
the  function should return an object with the following required properties:
the  key determines which paths will be pre-rendered. for example, suppose that you have a page that uses  named . if you export  from this page and return the following for :
then, next.js will statically generate  and  during  using the page component in .
the value for each  object must match the parameters used in the page name:
if the page name is , then  should contain  and .
if the page name uses  like , then  should contain  (which is an array). if this array is , then next.js will statically generate the page at .
if the page uses an , use , ,  or  to render the root-most route. for example, if you supply  for , next.js will statically generate the page .
the  strings are case-sensitive and ideally should be normalized to ensure the paths are generated correctly. for example, if  is returned for a param it will only match if  is the actual path visited, not  or .
separate of the  object a  field can be returned when , which configures the locale for the path being generated.
if  is , then any paths not returned by  will result in a 404 page.
when  is run, next.js will check if  returned , it will then build only the paths returned by . this option is useful if you have a small number of paths to create, or new page data is not added often. if you find that you need to add more paths, and you have , you will need to run  again so that the new paths can be generated.
the following example pre-renders one blog post per page called . the list of blog posts will be fetched from a cms and returned by . then, for each page, it fetches the post data from a cms using .
if  is , then the behavior of  changes in the following ways:
the paths returned from  will be rendered to  at build time by .
the paths that have not been generated at build time will not result in a 404 page. instead, next.js will serve a  version of the page on the first request to such a path. web crawlers, such as google, won't be served a fallback and instead the path will behave as in .
when a page with  is navigated to through  or  (client-side) next.js will not serve a fallback and instead the page will behave as .
in the background, next.js will statically generate the requested path  and . this includes running .
when complete, the browser receives the  for the generated path. this will be used to automatically render the page with the required props. from the user’s perspective, the page will be swapped from the fallback page to the full page.
at the same time, next.js adds this path to the list of pre-rendered pages. subsequent requests to the same path will serve the generated page, like other pages pre-rendered at build time.
note:  is not supported when using .
is useful if your app has a very large number of static pages that depend on data (such as a very large e-commerce site). if you want to pre-render all product pages, the builds would take a very long time.
instead, you may statically generate a small subset of pages and use  for the rest. when someone requests a page that is not generated yet, the user will see the page with a loading indicator or skeleton component.
shortly after,  finishes and the page will be rendered with the requested data. from now on, everyone who requests the same page will get the statically pre-rendered page.
this ensures that users always have a fast experience while preserving fast builds and the benefits of static generation.
will not update generated pages, for that take a look at .
if  is , new paths not returned by  will wait for the  to be generated, identical to ssr (hence why blocking), and then be cached for future requests so it only happens once per path.
will behave as follows:
the paths that have not been generated at build time will not result in a 404 page. instead, next.js will ssr on the first request and return the generated .
when complete, the browser receives the  for the generated path. from the user’s perspective, it will transition from "the browser is requesting the page" to "the full page is loaded". there is no flash of loading/fallback state.
will not update generated pages by default. to update generated pages, use  in conjunction with .
in the “fallback” version of a page:
the page’s props will be empty.
using the , you can detect if the fallback is being rendered,  will be .
the following example showcases using :
for typescript, you can use the  type from :
exporting a function called  will pre-render a page at build time using the props returned from the function:
contains the route parameters for pages using . for example, if the page name is  , then  will look like . you should use this together with , which we’ll explain later.
is  if the page is in the  and  otherwise.
contains the  data set by .
the  function should return an object containing either , , or  followed by an optional  property.
the  property is the amount in seconds after which a page re-generation can occur (defaults to  or no revalidation).
the cache status of a page leveraging isr can be determined by reading the value of the  response header. the possible values are the following:
the  boolean allows the page to return a  status and . with , the page will return a  even if there was a successfully generated page before. this is meant to support use cases like user-generated content getting removed by its author. note,  follows the same  behavior .
note:  is not needed for  mode as only paths returned from  will be pre-rendered.
the  object allows redirecting to internal or external resources. it should match the shape of .
in some rare cases, you might need to assign a custom status code for older  clients to properly redirect. in these cases, you can use the  property instead of the  property, but not both. you can also set  similar to redirects in .
if the redirects are known at build-time, they should be added in  instead.
files can be read directly from the filesystem in .
in order to do so you have to get the full path to a file.
since next.js compiles your code into a separate directory you can't use  as the path it returns will be different from the pages directory.
instead you can use  which gives you the directory where next.js is being executed.
static optimization indicator
note: this indicator was removed in next.js version 10.0.1. we recommend upgrading to the latest version of next.js.
when a page qualifies for  we show an indicator to let you know.
this is helpful since automatic static optimization can be very beneficial and knowing immediately in development if the page qualifies can be useful.
in some cases this indicator might not be useful, like when working on electron applications. to remove it open  and disable the  config in :
next.config.js
for custom advanced configuration of next.js, you can create a  or  file in the root of your project directory (next to ).
is a regular node.js module, not a json file. it gets used by the next.js server and build phases, and it's not included in the browser build.
take a look at the following  example:
if you need , you can use :
you can also use a function:
since next.js 12.1.0, you can use an async function:
is the current context in which the configuration is loaded. you can see the . phases can be imported from :
the commented lines are the place where you can put the configs allowed by , which are .
however, none of the configs are required, and it's not necessary to understand what each config does. instead, search for the features you need to enable or modify in this section and they will show you what to do.
avoid using new javascript features not available in your target node.js version.  will not be parsed by webpack, babel or typescript.
since the release of  we now have a more intuitive and ergonomic experience for . give it a try!
note: environment variables specified in this way will always be included in the javascript bundle, prefixing the environment variable name with  only has an effect when specifying them .
to add environment variables to the javascript bundle, open  and add the  config:
now you can access  in your code. for example:
next.js will replace  with  at build time. trying to destructure  variables won't work due to the nature of webpack .
for example, the following line:
will end up being:
base path
to deploy a next.js application under a sub-path of a domain you can use the  config option.
allows you to set a path prefix for the application. for example, to use  instead of  (the default), open  and add the  config:
note: this value must be set at build time and can not be changed without re-building as the value is inlined in the client-side bundles.
when linking to other pages using  and  the  will be automatically applied.
for example, using  will automatically become  when  is set to .
output html:
this makes sure that you don't have to change all links in your application when changing the  value.
when using the  component, you will need to add the  in front of .
for example, using  will properly serve your image when  is set to .
rewrites
rewrites allow you to map an incoming request path to a different destination path.
rewrites act as a url proxy and mask the destination path, making it appear the user hasn't changed their location on the site. in contrast,  will reroute to a new page and show the url changes.
to use rewrites you can use the  key in :
rewrites are applied to client-side routing, a  will have the rewrite applied in the above example.
is an async function that expects to return either an array or an object of arrays (see below) holding objects with  and  properties:
:  - is the incoming request path pattern.
:  is the path you want to route to.
:  or  - if false the basepath won't be included when matching, can be used for external rewrites only.
:  or  - whether the locale should not be included when matching.
is an array of  with the ,  and  properties.
when the  function returns an array, rewrites are applied after checking the filesystem (pages and  files) and before dynamic routes. when the  function returns an object of arrays with a specific shape, this behavior can be changed and more finely controlled, as of  of next.js:
note: rewrites in  do not check the filesystem/dynamic routes immediately after matching a source, they continue until all  have been checked.
the order next.js routes are checked is:
are checked/applied
rewrites are checked/applied
static files from the ,  files, and non-dynamic pages are checked/served
rewrites are checked/applied, if one of these rewrites is matched we check dynamic routes/static files after each match
rewrites are checked/applied, these are applied before rendering the 404 page and after dynamic routes/all static assets have been checked. if you use  in , the fallback  defined in your  will not be run.
when using parameters in a rewrite the parameters will be passed in the query by default when none of the parameters are used in the .
if a parameter is used in the destination none of the parameters will be automatically passed in the query.
you can still pass the parameters manually in the query if one is already used in the destination by specifying the query in the .
note: for static pages from the  or  params from rewrites will be parsed on the client after hydration and provided in the query.
path matches are allowed, for example  will match  (no nested paths):
to match a wildcard path you can use  after a parameter, for example  will match :
to match a regex path you can wrap the regex in parenthesis after a parameter, for example  will match  but not :
the following characters , , , , , , ,  are used for regex path matching, so when used in the  as non-special values they must be escaped by adding  before them:
to only match a rewrite when header, cookie, or query values also match the  field can be used. both the  and all  items must match for the rewrite to be applied.
items have the following fields:
:  - must be either , , , or .
:  - the key from the selected type to match against.
:  or  - the value to check for, if undefined any value will match. a regex like string can be used to capture a specific part of the value, e.g. if the value  is used for  then  will be usable in the destination with .
rewrites allow you to rewrite to an external url. this is especially useful for incrementally adopting next.js. the following is an example rewrite for redirecting the  route of your main app to an external site.
if you're using , you also need to insert a trailing slash in the  parameter. if the destination server is also expecting a trailing slash it should be included in the  parameter as well.
you can also have next.js fall back to proxying to an existing website after checking all next.js routes.
this way you don't have to change the rewrites configuration when migrating more pages to next.js
see additional information on incremental adoption .
when leveraging  with rewrites each  and  is automatically prefixed with the  unless you add  to the rewrite:
when leveraging  with rewrites each  and  is automatically prefixed to handle the configured  unless you add  to the rewrite. if  is used you must prefix the  and  with a locale for it to be matched correctly.
redirects
redirects allow you to redirect an incoming request path to a different destination path.
to use redirects you can use the  key in :
is an async function that expects an array to be returned holding objects with , , and  properties:
is the incoming request path pattern.
is the path you want to route to.
or  - if  will use the 308 status code which instructs clients/search engines to cache the redirect forever, if  will use the 307 status code which is temporary and is not cached.
why does next.js use 307 and 308? traditionally a 302 was used for a temporary redirect, and a 301 for a permanent redirect, but many browsers changed the request method of the redirect to , regardless of the original method. for example, if the browser made a request to  which returned status code  with location , the subsequent request might be  instead of the expected . next.js uses the 307 temporary redirect, and 308 permanent redirect status codes to explicitly preserve the request method used.
:  or  - if false the  won't be included when matching, can be used for external redirects only.
redirects are checked before the filesystem which includes pages and  files.
redirects are not applied to client-side routing (, ), unless  is present and matches the path.
when a redirect is applied, any query values provided in the request will be passed through to the redirect destination. for example, see the following redirect configuration:
when  is requested, the client will be redirected to .
to match a regex path you can wrap the regex in parentheses after a parameter, for example  will match  but not :
to only match a redirect when header, cookie, or query values also match the  field can be used. both the  and all  items must match for the redirect to be applied.
when leveraging  with redirects each  and  is automatically prefixed with the  unless you add  to the redirect:
when leveraging  with redirects each  and  is automatically prefixed to handle the configured  unless you add  to the redirect. if  is used you must prefix the  and  with a locale for it to be matched correctly.
in some rare cases, you might need to assign a custom status code for older http clients to properly redirect. in these cases, you can use the  property instead of the  property, but not both. note: to ensure ie11 compatibility a  header is automatically added for the 308 status code.
inside , you can use .
inside  and , you can redirect specific pages at request-time.
headers
headers allow you to set custom http headers on the response to an incoming request on a given path.
to set custom http headers you can use the  key in :
is an async function that expects an array to be returned holding objects with  and  properties:
is an array of response header objects, with  and  properties.
headers are checked before the filesystem which includes pages and  files.
if two headers match the same path and set the same header key, the last header key will override the first. using the below headers, the path  will result in the header  being  due to the last header value set being .
to only apply a header when either header, cookie, or query values also match the  field can be used. both the  and all  items must match for the header to be applied.
when leveraging  with headers each  is automatically prefixed with the  unless you add  to the header:
when leveraging  with headers each  is automatically prefixed to handle the configured  unless you add  to the header. if  is used you must prefix the  with a locale for it to be matched correctly.
you can set the  header in your  by using the  method:
you cannot set  headers in  file as these will be overwritten in production to ensure that api routes and static assets are cached effectively.
if you need to revalidate the cache of a page that has been , you can do so by setting the  prop in the page's  function.
custom page extensions
you can extend the default page extensions (, , , ) used by next.js. inside , add the  config:
changing these values affects all next.js pages, including the following:
for example, if you reconfigure  page extensions to , you would need to rename pages like .
you can colocate test files or other files used by components in the  directory. inside , add the  config:
then, rename your pages to have a file extension that includes  (e.g. rename  to ). ensure you rename all next.js pages, including the files mentioned above.
cdn support with asset prefix
attention:  automatically configures a global cdn for your next.js project.
you do not need to manually setup an asset prefix.
note: next.js 9.5+ added support for a customizable , which is better
suited for hosting your application on a sub-path like .
we do not suggest you use a custom asset prefix for this use case.
to set up a , you can set up an asset prefix and configure your cdn's origin to resolve to the domain that next.js is hosted on.
open  and add the  config:
next.js will automatically use your asset prefix for the javascript and css files it loads from the  path ( folder). for example, with the above configuration, the following request for a js chunk:
would instead become:
the exact configuration for uploading your files to a given cdn will depend on your cdn of choice. the only folder you need to host on your cdn is the contents of , which should be uploaded as  as the above url request indicates. do not upload the rest of your  folder, as you should not expose your server code and other configuration to the public.
while  covers requests to , it does not influence the following paths:
files in the  folder; if you want to serve those assets over a cdn, you'll have to introduce the prefix yourself
requests for  pages. these requests will always be made against the main domain since they're not static.
requests for  pages. these requests will always be made against the main domain to support , even if you're not using it (for consistency).
custom webpack config
note: changes to webpack config are not covered by semver so proceed at your own risk
before continuing to add custom webpack configuration to your application make sure next.js doesn't already support your use-case:
some commonly asked for features are available as plugins:
in order to extend our usage of , you can define a function that extends its config inside , like so:
the  function is executed twice, once for the server and once for the client. this allows you to distinguish between client and server configuration using the  property.
the second argument to the  function is an object with the following properties:
:  - the build id, used as a unique identifier between builds
:  - indicates if the compilation will be done in development
:  - it's  for server-side compilation, and  for client-side compilation
:  - the target runtime for server-side compilation; either  or , it's  for client-side compilation.
:  - default loaders used internally by next.js:
:  - default  configuration
example usage of :
notice that  is  when  is  or , nextruntime "" is currently for middleware and server components in edge runtime only.
compression
next.js provides  compression to compress rendered content and static files. in general you will want to enable compression on a http proxy like , to offload load from the  process.
to disable compression, open  and disable the  config:
runtime configuration
generally you'll want to use  to provide your configuration. the reason for this is that runtime configuration adds rendering / initialization overhead and is incompatible with .
to add runtime configuration to your app open  and add the  and  configs:
place any server-only runtime config under .
anything accessible to both client and server-side code should be under .
a page that relies on  must use  or  or your application must have a  with  to opt-out of . runtime configuration won't be available to any page (or component in a page) without being server-side rendered.
to get access to the runtime configs in your app use , like so:
disabling x-powered-by
by default next.js will add the  header. to opt-out of it, open  and disable the  config:
disabling etag generation
next.js will generate  for every page by default. you may want to disable etag generation for html pages depending on your cache strategy.
open  and disable the  option:
disabling http keep-alive
next.js automatically polyfills  and enables  by default. you may want to disable http keep-alive for certain  calls or globally.
for a single  call, you can add the agent option:
to override all  calls globally, you can use :
setting a custom build directory
you can specify a name to use for a custom build directory to use instead of .
now if you run  next.js will use  instead of the default  folder.
should not leave your project directory. for example,  is an invalid directory.
configuring the build id
next.js uses a constant id generated at build time to identify which version of your application is being served. this can cause problems in multi-server deployments when  is run on every server. in order to keep a consistent build id between builds you can provide your own build id.
open  and add the  function:
configuring ondemandentries
next.js exposes some options that give you some control over how the server will dispose or keep in memory built pages in development.
to change the defaults, open  and add the  config:
ignoring eslint
when eslint is detected in your project, next.js fails your production build () when errors are present.
if you'd like next.js to produce production code even when your application has eslint errors, you can disable the built-in linting step completely. this is not recommended unless you already have eslint configured to run in a separate part of your workflow (for example, in ci or a pre-commit hook).
ignoring typescript errors
exportpathmap
this feature is exclusive to . please refer to  if you want to learn more about it.
allows you to specify a mapping of request paths to page destinations, to be used during export. paths defined in  will also be available when using .
let's start with an example, to create a custom  for an app with the following pages:
open  and add the following  config:
note: the  field in  cannot be used with  or  as they are rendered to html files at build-time and additional query information cannot be provided during .
the pages will then be exported as html files, for example,  will become .
is an  function that receives 2 arguments: the first one is , which is the default map used by next.js. the second argument is an object with:
-  when  is being called in development.  when running . in development  is used to define routes.
- absolute path to the project directory
- absolute path to the  directory (). when  is  the value of  will be .
- absolute path to the  directory (configurable with the  config)
- the generated build id
the returned object is a map of pages where the  is the  and the  is an object that accepts the following fields:
:  - the page inside the  directory to render
:  - the  object passed to  when prerendering. defaults to
the exported  can also be a filename (for example, ), but you may need to set the  header to  when serving its content if it is different than .
it is possible to configure next.js to export pages as  files and require trailing slashes,  becomes  and is routable via . this was the default behavior prior to next.js 9.
to switch back and add a trailing slash, open  and enable the  config:
will use  as the default output directory, you can customize this using the  argument, like so:
trailing slash
by default next.js will redirect urls with trailing slashes to their counterpart without a trailing slash. for example  will redirect to . you can configure this behavior to act the opposite way, where urls without trailing slashes are redirected to their counterparts with trailing slashes.
with this option set, urls like  will redirect to .
react strict mode
suggested: we strongly suggest you enable strict mode in your next.js application to better prepare your application for the future of react.
react's  is a development mode only feature for highlighting potential problems in an application. it helps to identify unsafe lifecycles, legacy api usage, and a number of other features.
the next.js runtime is strict mode-compliant. to opt-in to strict mode, configure the following option in your :
if you or your team are not ready to use strict mode in your entire application, that's ok! you can incrementally migrate on a page-by-page basis using .
url imports
url imports are an experimental feature that allows you to import modules directly from external servers (instead of from the local disk).
warning: this feature is experimental. only use domains that you trust to download and execute on your machine. please exercise
discretion, and caution until the feature is flagged as stable.
to opt-in, add the allowed url prefixes inside :
then, you can import modules directly from urls:
url imports can be used everywhere normal package imports can be used.
this feature is being designed with security as the top priority. to start, we added an experimental flag forcing you to explicitly allow the domains you accept url imports from. we're working to take this further by limiting url imports to execute in the browser sandbox using the .
when using url imports, next.js will create a lockfile in the  directory.
this directory is intended to be committed to git and should not be included in your  file.
when running , next.js will download and add all newly discovered url imports to your lockfile
when running , next.js will use only the lockfile to build the application for production
typically, no network requests are needed and any outdated lockfile will cause the build to fail.
one exception is resources that respond with .
these resources will have a  entry in the lockfile and will always be fetched from the network on each build.
build indicator
when you edit your code, and next.js is compiling the application, a compilation indicator appears in the bottom right corner of the page.
note: this indicator is only present in development mode and will not appear when building and running the app in production mode.
in some cases this indicator can be misplaced on your page, for example, when conflicting with a chat launcher. to change its position, open  and set the  in the  object to  (default), ,  or :
in some cases this indicator might not be useful for you. to remove it, open  and disable the  config in  object:
this page is an overview of the react documentation and related resources.
react is a javascript library for building user interfaces. learn what react is all about on  or .
try react
react has been designed from the start for gradual adoption, and you can use as little or as much react as you need. whether you want to get a taste of react, add some interactivity to a simple html page, or start a complex react-powered app, the links in this section will help you get started.
online playgrounds
if you’re interested in playing around with react, you can use an online code playground. try a hello world template on , , or .
if you prefer to use your own text editor, you can also , edit it, and open it from the local filesystem in your browser. it does a slow runtime code transformation, so we’d only recommend using this for simple demos.
add react to a website
you can . you can then either gradually expand its presence, or keep it contained to a few dynamic widgets.
create a new react app
when starting a react project, a  might still be the best option. it only takes a minute to set up!
as your application grows, you might want to consider a more integrated setup. there are several javascript toolchains we recommend for larger applications. each of them can work with little to no configuration and lets you take full advantage of the rich react ecosystem.
learn react
people come to react from different backgrounds and with different learning styles. whether you prefer a more theoretical or a practical approach, we hope you’ll find this section helpful.
if you prefer to learn by doing, start with our .
if you prefer to learn concepts step by step, start with our .
like any unfamiliar technology, react does have a learning curve. with practice and some patience, you will get the hang of it.
first examples
the  contains a few small react examples with a live editor. even if you don’t know anything about react yet, try changing their code and see how it affects the result.
react for beginners
if you feel that the react documentation goes at a faster pace than you’re comfortable with, check out . it introduces the most important react concepts in a detailed, beginner-friendly way. once you’re done, give the documentation another try!
react for designers
if you’re coming from a design background,  are a great place to get started.
javascript resources
the react documentation assumes some familiarity with programming in the javascript language. you don’t have to be an expert, but it’s harder to learn both react and javascript at the same time.
we recommend going through  to check your knowledge level. it will take you between 30 minutes and an hour but you will feel more confident learning react.
whenever you get confused by something in javascript,  and  are great websites to check. there are also  where you can ask for help.
practical tutorial
if you prefer to learn by doing, check out our . in this tutorial, we build a tic-tac-toe game in react. you might be tempted to skip it because you’re not into building games — but give it a chance. the techniques you’ll learn in the tutorial are fundamental to building any react apps, and mastering it will give you a much deeper understanding.
step-by-step guide
if you prefer to learn concepts step by step, our  is the best place to start. every next chapter in it builds on the knowledge introduced in the previous chapters so you won’t miss anything as you go along.
thinking in react
many react users credit reading  as the moment react finally “clicked” for them. it’s probably the oldest react walkthrough but it’s still just as relevant.
recommended courses
sometimes people find third-party books and video courses more helpful than the official documentation. we maintain , some of which are free.
advanced concepts
once you’re comfortable with the  and played with react a little bit, you might be interested in more advanced topics. this section will introduce you to the powerful, but less commonly used react features like  and .
api reference
this documentation section is useful when you want to learn more details about a particular react api. for example,  can provide you with details on how  works, and what different lifecycle methods are useful for.
glossary and faq
the  contains an overview of the most common terms you’ll see in the react documentation. there is also a faq section dedicated to short questions and answers about common topics, including , , and .
staying informed
the  is the official source for the updates from the react team. anything important, including release notes or deprecation notices, will be posted there first.
you can also follow the  on twitter, but you won’t miss anything essential if you only read the blog.
not every react release deserves its own blog post, but you can find a detailed changelog for every release in the , as well as on the  page.
versioned documentation
this documentation always reflects the latest stable version of react. since react 16, you can find older versions of the documentation on a . note that documentation for past versions is snapshotted at the time of the release, and isn’t being continuously updated.
something missing?
if something is missing in the documentation or if you found some part confusing, please  with your suggestions for improvement, or tweet at the . we love hearing from you!is this page useful?
use as little or as much react as you need.
react has been designed from the start for gradual adoption, and you can use as little or as much react as you need. perhaps you only want to add some “sprinkles of interactivity” to an existing page. react components are a great way to do that.
the majority of websites aren’t, and don’t need to be, single-page apps. with a few lines of code and no build tooling, try react in a small part of your website. you can then either gradually expand its presence, or keep it contained to a few dynamic widgets.
(no bundler necessary!)
add react in one minute
in this section, we will show how to add a react component to an existing html page. you can follow along with your own website, or create an empty html file to practice.
there will be no complicated tools or install requirements — to complete this section, you only need an internet connection, and a minute of your time.
optional:
step 1: add a dom container to the html
first, open the html page you want to edit. add an empty  tag to mark the spot where you want to display something with react. for example:
we gave this  a unique  html attribute. this will allow us to find it from the javascript code later and display a react component inside of it.
you can place a “container”  like this anywhere inside the  tag. you may have as many independent dom containers on one page as you need. they are usually empty — react will replace any existing content inside dom containers.
step 2: add the script tags
next, add three  tags to the html page right before the closing  tag:
the first two tags load react. the third one will load your component code.
step 3: create a react component
create a file called  next to your html page.
open  and paste it into the file you created.
this code defines a react component called . don’t worry if you don’t understand it yet — we’ll cover the building blocks of react later in our  and . for now, let’s just get it showing on the screen!
after , add three lines to the bottom of :
these three lines of code find the  we added to our html in the first step, create a react app with it, and then display our “like” button react component inside of it.
that’s it!
there is no step four. you have just added the first react component to your website.
check out the next sections for more tips on integrating react.
tip: reuse a component
commonly, you might want to display react components in multiple places on the html page. here is an example that displays the “like” button three times and passes some data to it:
note
this strategy is mostly useful while react-powered parts of the page are isolated from each other. inside react code, it’s easier to use  instead.
tip: minify javascript for production
before deploying your website to production, be mindful that unminified javascript can significantly slow down the page for your users.
if you already minify the application scripts, your site will be production-ready if you ensure that the deployed html loads the versions of react ending in :
if you don’t have a minification step for your scripts, .
optional: try react with jsx
in the examples above, we only relied on features that are natively supported by browsers. this is why we used a javascript function call to tell react what to display:
however, react also offers an option to use  instead:
these two code snippets are equivalent. while jsx is , many people find it helpful for writing ui code — both with react and with other libraries.
you can play with jsx using .
quickly try jsx
the quickest way to try jsx in your project is to add this  tag to your page:
now you can use jsx in any  tag by adding  attribute to it. here is  that you can download and play with.
this approach is fine for learning and creating simple demos. however, it makes your website slow and isn’t suitable for production. when you’re ready to move forward, remove this new  tag and the  attributes you’ve added. instead, in the next section you will set up a jsx preprocessor to convert all your  tags automatically.
add jsx to a project
adding jsx to a project doesn’t require complicated tools like a bundler or a development server. essentially, adding jsx is a lot like adding a css preprocessor. the only requirement is to have  installed on your computer.
go to your project folder in the terminal, and paste these two commands:
step 1: run  (if it fails, )
step 2: run
we’re using npm here only to install the jsx preprocessor; you won’t need it for anything else. both react and the application code can stay as  tags with no changes.
congratulations! you just added a production-ready jsx setup to your project.
run jsx preprocessor
create a folder called  and run this terminal command:
is not a typo — it’s a .
if you see an error message saying “you have mistakenly installed the  package”, you might have missed . perform it in the same folder, and then try again.
don’t wait for it to finish — this command starts an automated watcher for jsx.
if you now create a file called  with this , the watcher will create a preprocessed  with the plain javascript code suitable for the browser. when you edit the source file with jsx, the transform will re-run automatically.
as a bonus, this also lets you use modern javascript syntax features like classes without worrying about breaking older browsers. the tool we just used is called babel, and you can learn more about it from .
if you notice that you’re getting comfortable with build tools and want them to do more for you,  describes some of the most popular and approachable toolchains. if not — those script tags will do just fine!is this page useful?
use an integrated toolchain for the best user and developer experience.
this page describes a few popular react toolchains which help with tasks like:
scaling to many files and components.
using third-party libraries from npm.
detecting common mistakes early.
live-editing css and js in development.
optimizing the output for production.
the toolchains recommended on this page don’t require configuration to get started.
you might not need a toolchain
if you don’t experience the problems described above or don’t feel comfortable using javascript tools yet, consider , optionally .
this is also the easiest way to integrate react into an existing website. you can always add a larger toolchain if you find it helpful!
recommended toolchains
the react team primarily recommends these solutions:
if you’re learning react or creating a new  app, use .
if you’re building a server-rendered website with node.js, try .
if you’re building a static content-oriented website, try .
if you’re building a component library or integrating with an existing codebase, try .
create react app
is a comfortable environment for learning react, and is the best way to start building a new  application in react.
it sets up your development environment so that you can use the latest javascript features, provides a nice developer experience, and optimizes your app for production. you’ll need to have  on your machine. to create a project, run:
on the first line is not a typo — it’s a .
create react app doesn’t handle backend logic or databases; it just creates a frontend build pipeline, so you can use it with any backend you want. under the hood, it uses  and , but you don’t need to know anything about them.
when you’re ready to deploy to production, running  will create an optimized build of your app in the  folder. you can learn more about create react app  and the .
next.js
is a popular and lightweight framework for static and server‑rendered applications built with react. it includes styling and routing solutions out of the box, and assumes that you’re using  as the server environment.
learn next.js from .
gatsby
is the best way to create static websites with react. it lets you use react components, but outputs pre-rendered html and css to guarantee the fastest load time.
learn gatsby from  and a .
more flexible toolchains
the following toolchains offer more flexibility and choice. we recommend them to more experienced users:
combines the power of  with the simplicity of presets, and includes a preset for  and .
is a toolkit for full-stack monorepo development, with built-in support for react, next.js, , and more.
is a fast, zero configuration web application bundler that .
is a server-rendering framework that doesn’t require any configuration, but offers more flexibility than next.js.
creating a toolchain from scratch
a javascript build toolchain typically consists of:
a package manager, such as  or . it lets you take advantage of a vast ecosystem of third-party packages, and easily install or update them.
a bundler, such as  or . it lets you write modular code and bundle it together into small packages to optimize load time.
a compiler such as . it lets you write modern javascript code that still works in older browsers.
if you prefer to set up your own javascript toolchain from scratch,  that re-creates some of the create react app functionality.
don’t forget to ensure your custom toolchain .is this page useful?
cdn links
both react and reactdom are available over a cdn.
the versions above are only meant for development, and are not suitable for production. minified and optimized production versions of react are available at:
to load a specific version of  and , replace  with the version number.
why the  attribute?
if you serve react from a cdn, we recommend to keep the  attribute set:
we also recommend to verify that the cdn you are using sets the  http header:
this enables a better  in react 16 and later.is this page useful?
release channels
react relies on a thriving open source community to file bug reports, open pull requests, and . to encourage feedback we sometimes share special builds of react that include unreleased features.
this document will be most relevant to developers who work on frameworks, libraries, or developer tooling. developers who use react primarily to build user-facing applications should not need to worry about our prerelease channels.
each of react’s release channels is designed for a distinct use case:
is for stable, semver react releases. it’s what you get when you install react from npm. this is the channel you’re already using today. use this for all user-facing react applications.
tracks the main branch of the react source code repository. think of these as release candidates for the next minor semver release. use this for integration testing between react and third party projects.
includes experimental apis and features that aren’t available in the stable releases. these also track the main branch, but with additional feature flags turned on. use this to try out upcoming features before they are released.
all releases are published to npm, but only latest uses . prereleases (those in the next and experimental channels) have versions generated from a hash of their contents and the commit date, e.g.  for next and  for experimental.
the only officially supported release channel for user-facing applications is latest. next and experimental releases are provided for testing purposes only, and we provide no guarantees that behavior won’t change between releases. they do not follow the semver protocol that we use for releases from latest.
by publishing prereleases to the same registry that we use for stable releases, we are able to take advantage of the many tools that support the npm workflow, like  and .
latest channel
latest is the channel used for stable react releases. it corresponds to the  tag on npm. it is the recommended channel for all react apps that are shipped to real users.
if you’re not sure which channel you should use, it’s latest. if you’re a react developer, this is what you’re already using.
you can expect updates to latest to be extremely stable. versions follow the semantic versioning scheme. learn more about our commitment to stability and incremental migration in our .
next channel
the next channel is a prerelease channel that tracks the main branch of the react repository. we use prereleases in the next channel as release candidates for the latest channel. you can think of next as a superset of latest that is updated more frequently.
the degree of change between the most recent next release and the most recent latest release is approximately the same as you would find between two minor semver releases. however, the next channel does not conform to semantic versioning. you should expect occasional breaking changes between successive releases in the next channel.
do not use prereleases in user-facing applications.
releases in next are published with the  tag on npm. versions are generated from a hash of the build’s contents and the commit date, e.g. .
using the next channel for integration testing
the next channel is designed to support integration testing between react and other projects.
all changes to react go through extensive internal testing before they are released to the public. however, there are a myriad of environments and configurations used throughout the react ecosystem, and it’s not possible for us to test against every single one.
if you’re the author of a third party react framework, library, developer tool, or similar infrastructure-type project, you can help us keep react stable for your users and the entire react community by periodically running your test suite against the most recent changes. if you’re interested, follow these steps:
set up a cron job using your preferred continuous integration platform. cron jobs are supported by both  and .
in the cron job, update your react packages to the most recent react release in the next channel, using  tag on npm. using the npm cli:
or yarn:
run your test suite against the updated packages.
if everything passes, great! you can expect that your project will work with the next minor react release.
if something breaks unexpectedly, please let us know by .
a project that uses this workflow is next.js. (no pun intended! seriously!) you can refer to their  as an example.
experimental channel
like next, the experimental channel is a prerelease channel that tracks the main branch of the react repository. unlike next, experimental releases include additional features and apis that are not ready for wider release.
usually, an update to next is accompanied by a corresponding update to experimental. they are based on the same source revision, but are built using a different set of feature flags.
experimental releases may be significantly different than releases to next and latest. do not use experimental releases in user-facing applications. you should expect frequent breaking changes between releases in the experimental channel.
releases in experimental are published with the  tag on npm. versions are generated from a hash of the build’s contents and the commit date, e.g. .
what goes into an experimental release?
experimental features are ones that are not ready to be released to the wider public, and may change drastically before they are finalized. some experiments may never be finalized — the reason we have experiments is to test the viability of proposed changes.
for example, if the experimental channel had existed when we announced hooks, we would have released hooks to the experimental channel weeks before they were available in latest.
you may find it valuable to run integration tests against experimental. this is up to you. however, be advised that experimental is even less stable than next. we do not guarantee any stability between experimental releases.
how can i learn more about experimental features?
experimental features may or may not be documented. usually, experiments aren’t documented until they are close to shipping in next or latest.
if a feature is not documented, they may be accompanied by an .
we will post to the  when we’re ready to announce new experiments, but that doesn’t mean we will publicize every experiment.
you can always refer to our public github repository’s  for a comprehensive list of changes.is this page useful?
hello world
the smallest react example looks like this:
it displays a heading saying “hello, world!” on the page.
click the link above to open an online editor. feel free to make some changes, and see how they affect the output. most pages in this guide will have editable examples like this one.
how to read this guide
in this guide, we will examine the building blocks of react apps: elements and components. once you master them, you can create complex apps from small reusable pieces.
this guide is designed for people who prefer learning concepts step by step. if you prefer to learn by doing, check out our . you might find this guide and the tutorial complementary to each other.
this is the first chapter in a step-by-step guide about main react concepts. you can find a list of all its chapters in the navigation sidebar. if you’re reading this from a mobile device, you can access the navigation by pressing the button in the bottom right corner of your screen.
every chapter in this guide builds on the knowledge introduced in earlier chapters. you can learn most of react by reading the “main concepts” guide chapters in the order they appear in the sidebar. for example,  is the next chapter after this one.
knowledge level assumptions
react is a javascript library, and so we’ll assume you have a basic understanding of the javascript language. if you don’t feel very confident, we recommend  to check your knowledge level and enable you to follow along this guide without getting lost. it might take you between 30 minutes and an hour, but as a result you won’t have to feel like you’re learning both react and javascript at the same time.
this guide occasionally uses some newer javascript syntax in the examples. if you haven’t worked with javascript in the last few years,  should get you most of the way.
let’s get started!
keep scrolling down, and you’ll find the link to the  right before the website footer.is this page useful?
introducing jsx
consider this variable declaration:
this funny tag syntax is neither a string nor html.
it is called jsx, and it is a syntax extension to javascript. we recommend using it with react to describe what the ui should look like. jsx may remind you of a template language, but it comes with the full power of javascript.
jsx produces react “elements”. we will explore rendering them to the dom in the . below, you can find the basics of jsx necessary to get you started.
why jsx?
react embraces the fact that rendering logic is inherently coupled with other ui logic: how events are handled, how the state changes over time, and how the data is prepared for display.
instead of artificially separating technologies by putting markup and logic in separate files, react  with loosely coupled units called “components” that contain both. we will come back to components in a , but if you’re not yet comfortable putting markup in js,  might convince you otherwise.
react  using jsx, but most people find it helpful as a visual aid when working with ui inside the javascript code. it also allows react to show more useful error and warning messages.
with that out of the way, let’s get started!
embedding expressions in jsx
in the example below, we declare a variable called  and then use it inside jsx by wrapping it in curly braces:
you can put any valid  inside the curly braces in jsx. for example, , , or  are all valid javascript expressions.
in the example below, we embed the result of calling a javascript function, , into an  element.
we split jsx over multiple lines for readability. while it isn’t required, when doing this, we also recommend wrapping it in parentheses to avoid the pitfalls of .
jsx is an expression too
after compilation, jsx expressions become regular javascript function calls and evaluate to javascript objects.
this means that you can use jsx inside of  statements and  loops, assign it to variables, accept it as arguments, and return it from functions:
specifying attributes with jsx
you may use quotes to specify string literals as attributes:
you may also use curly braces to embed a javascript expression in an attribute:
don’t put quotes around curly braces when embedding a javascript expression in an attribute. you should either use quotes (for string values) or curly braces (for expressions), but not both in the same attribute.
warning:
since jsx is closer to javascript than to html, react dom uses  property naming convention instead of html attribute names.
for example,  becomes  in jsx, and  becomes .
specifying children with jsx
if a tag is empty, you may close it immediately with , like xml:
jsx tags may contain children:
jsx prevents injection attacks
it is safe to embed user input in jsx:
by default, react dom  any values embedded in jsx before rendering them. thus it ensures that you can never inject anything that’s not explicitly written in your application. everything is converted to a string before being rendered. this helps prevent  attacks.
jsx represents objects
babel compiles jsx down to  calls.
these two examples are identical:
performs a few checks to help you write bug-free code but essentially it creates an object like this:
these objects are called “react elements”. you can think of them as descriptions of what you want to see on the screen. react reads these objects and uses them to construct the dom and keep it up to date.
we will explore rendering react elements to the dom in the .
tip:
we recommend using the  for your editor of choice so that both es6 and jsx code is properly highlighted.
is this page useful?
rendering elements
elements are the smallest building blocks of react apps.
an element describes what you want to see on the screen:
unlike browser dom elements, react elements are plain objects, and are cheap to create. react dom takes care of updating the dom to match the react elements.
note:
one might confuse elements with a more widely known concept of “components”. we will introduce components in the . elements are what components are “made of”, and we encourage you to read this section before jumping ahead.
rendering an element into the dom
let’s say there is a  somewhere in your html file:
we call this a “root” dom node because everything inside it will be managed by react dom.
applications built with just react usually have a single root dom node. if you are integrating react into an existing app, you may have as many isolated root dom nodes as you like.
to render a react element, first pass the dom element to , then pass the react element to :
it displays “hello, world” on the page.
updating the rendered element
react elements are . once you create an element, you can’t change its children or attributes. an element is like a single frame in a movie: it represents the ui at a certain point in time.
with our knowledge so far, the only way to update the ui is to create a new element, and pass it to .
consider this ticking clock example:
it calls  every second from a  callback.
in practice, most react apps only call  once. in the next sections we will learn how such code gets encapsulated into .
we recommend that you don’t skip topics because they build on each other.
react only updates what’s necessary
react dom compares the element and its children to the previous one, and only applies the dom updates necessary to bring the dom to the desired state.
you can verify by inspecting the  with the browser tools:
even though we create an element describing the whole ui tree on every tick, only the text node whose contents have changed gets updated by react dom.
in our experience, thinking about how the ui should look at any given moment, rather than how to change it over time, eliminates a whole class of bugs.is this page useful?
components and props
components let you split the ui into independent, reusable pieces, and think about each piece in isolation. this page provides an introduction to the idea of components. you can find a .
conceptually, components are like javascript functions. they accept arbitrary inputs (called “props”) and return react elements describing what should appear on the screen.
function and class components
the simplest way to define a component is to write a javascript function:
this function is a valid react component because it accepts a single “props” (which stands for properties) object argument with data and returns a react element. we call such components “function components” because they are literally javascript functions.
you can also use an  to define a component:
the above two components are equivalent from react’s point of view.
function and class components both have some additional features that we will discuss in the .
rendering a component
previously, we only encountered react elements that represent dom tags:
however, elements can also represent user-defined components:
when react sees an element representing a user-defined component, it passes jsx attributes and children to this component as a single object. we call this object “props”.
for example, this code renders “hello, sara” on the page:
let’s recap what happens in this example:
we call  with the  element.
react calls the  component with  as the props.
our  component returns a  element as the result.
react dom efficiently updates the dom to match .
note: always start component names with a capital letter.
react treats components starting with lowercase letters as dom tags. for example,  represents an html div tag, but  represents a component and requires  to be in scope.
to learn more about the reasoning behind this convention, please read .
composing components
components can refer to other components in their output. this lets us use the same component abstraction for any level of detail. a button, a form, a dialog, a screen: in react apps, all those are commonly expressed as components.
for example, we can create an  component that renders  many times:
typically, new react apps have a single  component at the very top. however, if you integrate react into an existing app, you might start bottom-up with a small component like  and gradually work your way to the top of the view hierarchy.
extracting components
don’t be afraid to split components into smaller components.
for example, consider this  component:
it accepts  (an object),  (a string), and  (a date) as props, and describes a comment on a social media website.
this component can be tricky to change because of all the nesting, and it is also hard to reuse individual parts of it. let’s extract a few components from it.
first, we will extract :
the  doesn’t need to know that it is being rendered inside a . this is why we have given its prop a more generic name:  rather than .
we recommend naming props from the component’s own point of view rather than the context in which it is being used.
we can now simplify  a tiny bit:
next, we will extract a  component that renders an  next to the user’s name:
this lets us simplify  even further:
extracting components might seem like grunt work at first, but having a palette of reusable components pays off in larger apps. a good rule of thumb is that if a part of your ui is used several times (, , ), or is complex enough on its own (, , ), it is a good candidate to be extracted to a separate component.
props are read-only
whether you declare a component , it must never modify its own props. consider this  function:
such functions are called  because they do not attempt to change their inputs, and always return the same result for the same inputs.
in contrast, this function is impure because it changes its own input:
react is pretty flexible but it has a single strict rule:
all react components must act like pure functions with respect to their props.
of course, application uis are dynamic and change over time. in the , we will introduce a new concept of “state”. state allows react components to change their output over time in response to user actions, network responses, and anything else, without violating this rule.is this page useful?
state and lifecycle
this page introduces the concept of state and lifecycle in a react component. you can find a .
consider the ticking clock example from . in , we have only learned one way to update the ui. we call  to change the rendered output:
in this section, we will learn how to make the  component truly reusable and encapsulated. it will set up its own timer and update itself every second.
we can start by encapsulating how the clock looks:
however, it misses a crucial requirement: the fact that the  sets up a timer and updates the ui every second should be an implementation detail of the .
ideally we want to write this once and have the  update itself:
to implement this, we need to add “state” to the  component.
state is similar to props, but it is private and fully controlled by the component.
converting a function to a class
you can convert a function component like  to a class in five steps:
create an , with the same name, that extends .
add a single empty method to it called .
move the body of the function into the  method.
replace  with  in the  body.
delete the remaining empty function declaration.
is now defined as a class rather than a function.
the  method will be called each time an update happens, but as long as we render  into the same dom node, only a single instance of the  class will be used. this lets us use additional features such as local state and lifecycle methods.
adding local state to a class
we will move the  from props to state in three steps:
replace  with  in the  method:
add a  that assigns the initial :
note how we pass  to the base constructor:
class components should always call the base constructor with .
remove the  prop from the  element:
we will later add the timer code back to the component itself.
the result looks like this:
next, we’ll make the  set up its own timer and update itself every second.
adding lifecycle methods to a class
in applications with many components, it’s very important to free up resources taken by the components when they are destroyed.
we want to  whenever the  is rendered to the dom for the first time. this is called “mounting” in react.
we also want to  whenever the dom produced by the  is removed. this is called “unmounting” in react.
we can declare special methods on the component class to run some code when a component mounts and unmounts:
these methods are called “lifecycle methods”.
the  method runs after the component output has been rendered to the dom. this is a good place to set up a timer:
note how we save the timer id right on  ().
while  is set up by react itself and  has a special meaning, you are free to add additional fields to the class manually if you need to store something that doesn’t participate in the data flow (like a timer id).
we will tear down the timer in the  lifecycle method:
finally, we will implement a method called  that the  component will run every second.
it will use  to schedule updates to the component local state:
now the clock ticks every second.
let’s quickly recap what’s going on and the order in which the methods are called:
when  is passed to , react calls the constructor of the  component. since  needs to display the current time, it initializes  with an object including the current time. we will later update this state.
react then calls the  component’s  method. this is how react learns what should be displayed on the screen. react then updates the dom to match the ’s render output.
when the  output is inserted in the dom, react calls the  lifecycle method. inside it, the  component asks the browser to set up a timer to call the component’s  method once a second.
every second the browser calls the  method. inside it, the  component schedules a ui update by calling  with an object containing the current time. thanks to the  call, react knows the state has changed, and calls the  method again to learn what should be on the screen. this time,  in the  method will be different, and so the render output will include the updated time. react updates the dom accordingly.
if the  component is ever removed from the dom, react calls the  lifecycle method so the timer is stopped.
using state correctly
there are three things you should know about .
do not modify state directly
for example, this will not re-render a component:
instead, use :
the only place where you can assign  is the constructor.
state updates may be asynchronous
react may batch multiple  calls into a single update for performance.
because  and  may be updated asynchronously, you should not rely on their values for calculating the next state.
for example, this code may fail to update the counter:
to fix it, use a second form of  that accepts a function rather than an object. that function will receive the previous state as the first argument, and the props at the time the update is applied as the second argument:
we used an  above, but it also works with regular functions:
state updates are merged
when you call , react merges the object you provide into the current state.
for example, your state may contain several independent variables:
then you can update them independently with separate  calls:
the merging is shallow, so  leaves  intact, but completely replaces .
the data flows down
neither parent nor child components can know if a certain component is stateful or stateless, and they shouldn’t care whether it is defined as a function or a class.
this is why state is often called local or encapsulated. it is not accessible to any component other than the one that owns and sets it.
a component may choose to pass its state down as props to its child components:
the  component would receive the  in its props and wouldn’t know whether it came from the ’s state, from the ’s props, or was typed by hand:
this is commonly called a “top-down” or “unidirectional” data flow. any state is always owned by some specific component, and any data or ui derived from that state can only affect components “below” them in the tree.
if you imagine a component tree as a waterfall of props, each component’s state is like an additional water source that joins it at an arbitrary point but also flows down.
to show that all components are truly isolated, we can create an  component that renders three s:
each  sets up its own timer and updates independently.
in react apps, whether a component is stateful or stateless is considered an implementation detail of the component that may change over time. you can use stateless components inside stateful components, and vice versa.is this page useful?
handling events
handling events with react elements is very similar to handling events on dom elements. there are some syntax differences:
react events are named using camelcase, rather than lowercase.
with jsx you pass a function as the event handler, rather than a string.
for example, the html:
is slightly different in react:
another difference is that you cannot return  to prevent default behavior in react. you must call  explicitly. for example, with plain html, to prevent the default form behavior of submitting, you can write:
in react, this could instead be:
here,  is a synthetic event. react defines these synthetic events according to the , so you don’t need to worry about cross-browser compatibility. react events do not work exactly the same as native events. see the  reference guide to learn more.
when using react, you generally don’t need to call  to add listeners to a dom element after it is created. instead, just provide a listener when the element is initially rendered.
when you define a component using an , a common pattern is for an event handler to be a method on the class. for example, this  component renders a button that lets the user toggle between “on” and “off” states:
you have to be careful about the meaning of  in jsx callbacks. in javascript, class methods are not  by default. if you forget to bind  and pass it to ,  will be  when the function is actually called.
this is not react-specific behavior; it is a part of . generally, if you refer to a method without  after it, such as , you should bind that method.
if calling  annoys you, there are two ways you can get around this. you can use  to correctly bind callbacks:
this syntax is enabled by default in .
if you aren’t using class fields syntax, you can use an  in the callback:
the problem with this syntax is that a different callback is created each time the  renders. in most cases, this is fine. however, if this callback is passed as a prop to lower components, those components might do an extra re-rendering. we generally recommend binding in the constructor or using the class fields syntax, to avoid this sort of performance problem.
passing arguments to event handlers
inside a loop, it is common to want to pass an extra parameter to an event handler. for example, if  is the row id, either of the following would work:
the above two lines are equivalent, and use  and  respectively.
in both cases, the  argument representing the react event will be passed as a second argument after the id. with an arrow function, we have to pass it explicitly, but with  any further arguments are automatically forwarded.is this page useful?
conditional rendering
in react, you can create distinct components that encapsulate behavior you need. then, you can render only some of them, depending on the state of your application.
conditional rendering in react works the same way conditions work in javascript. use javascript operators like  or the  to create elements representing the current state, and let react update the ui to match them.
consider these two components:
we’ll create a  component that displays either of these components depending on whether a user is logged in:
this example renders a different greeting depending on the value of  prop.
element variables
you can use variables to store elements. this can help you conditionally render a part of the component while the rest of the output doesn’t change.
consider these two new components representing logout and login buttons:
in the example below, we will create a  called .
it will render either  or  depending on its current state. it will also render a  from the previous example:
while declaring a variable and using an  statement is a fine way to conditionally render a component, sometimes you might want to use a shorter syntax. there are a few ways to inline conditions in jsx, explained below.
inline if with logical && operator
you may  by wrapping them in curly braces. this includes the javascript logical  operator. it can be handy for conditionally including an element:
it works because in javascript,  always evaluates to , and  always evaluates to .
therefore, if the condition is , the element right after  will appear in the output. if it is , react will ignore and skip it.
note that returning a falsy expression will still cause the element after  to be skipped but will return the falsy expression. in the example below,  will be returned by the render method.
inline if-else with conditional operator
another method for conditionally rendering elements inline is to use the javascript conditional operator .
in the example below, we use it to conditionally render a small block of text.
it can also be used for larger expressions although it is less obvious what’s going on:
just like in javascript, it is up to you to choose an appropriate style based on what you and your team consider more readable. also remember that whenever conditions become too complex, it might be a good time to .
preventing component from rendering
in rare cases you might want a component to hide itself even though it was rendered by another component. to do this return  instead of its render output.
in the example below, the  is rendered depending on the value of the prop called . if the value of the prop is , then the component does not render:
returning  from a component’s  method does not affect the firing of the component’s lifecycle methods. for instance  will still be called.is this page useful?
lists and keys
first, let’s review how you transform lists in javascript.
given the code below, we use the  function to take an array of  and double their values. we assign the new array returned by  to the variable  and log it:
this code logs  to the console.
in react, transforming arrays into lists of  is nearly identical.
rendering multiple components
you can build collections of elements and  using curly braces .
below, we loop through the  array using the javascript  function. we return a  element for each item. finally, we assign the resulting array of elements to :
then, we can include the entire  array inside a  element:
this code displays a bullet list of numbers between 1 and 5.
basic list component
usually you would render lists inside a .
we can refactor the previous example into a component that accepts an array of  and outputs a list of elements.
when you run this code, you’ll be given a warning that a key should be provided for list items. a “key” is a special string attribute you need to include when creating lists of elements. we’ll discuss why it’s important in the next section.
let’s assign a  to our list items inside  and fix the missing key issue.
keys
keys help react identify which items have changed, are added, or are removed. keys should be given to the elements inside the array to give the elements a stable identity:
the best way to pick a key is to use a string that uniquely identifies a list item among its siblings. most often you would use ids from your data as keys:
when you don’t have stable ids for rendered items, you may use the item index as a key as a last resort:
we don’t recommend using indexes for keys if the order of items may change. this can negatively impact performance and may cause issues with component state. check out robin pokorny’s article for an . if you choose not to assign an explicit key to list items then react will default to using indexes as keys.
here is an  if you’re interested in learning more.
extracting components with keys
keys only make sense in the context of the surrounding array.
for example, if you  a  component, you should keep the key on the  elements in the array rather than on the  element in the  itself.
example: incorrect key usage
example: correct key usage
a good rule of thumb is that elements inside the  call need keys.
keys must only be unique among siblings
keys used within arrays should be unique among their siblings. however, they don’t need to be globally unique. we can use the same keys when we produce two different arrays:
keys serve as a hint to react but they don’t get passed to your components. if you need the same value in your component, pass it explicitly as a prop with a different name:
with the example above, the  component can read , but not .
embedding map() in jsx
in the examples above we declared a separate  variable and included it in jsx:
jsx allows  in curly braces so we could inline the  result:
sometimes this results in clearer code, but this style can also be abused. like in javascript, it is up to you to decide whether it is worth extracting a variable for readability. keep in mind that if the  body is too nested, it might be a good time to .is this page useful?
forms
html form elements work a bit differently from other dom elements in react, because form elements naturally keep some internal state. for example, this form in plain html accepts a single name:
this form has the default html form behavior of browsing to a new page when the user submits the form. if you want this behavior in react, it just works. but in most cases, it’s convenient to have a javascript function that handles the submission of the form and has access to the data that the user entered into the form. the standard way to achieve this is with a technique called “controlled components”.
controlled components
in html, form elements such as , , and  typically maintain their own state and update it based on user input. in react, mutable state is typically kept in the state property of components, and only updated with .
we can combine the two by making the react state be the “single source of truth”. then the react component that renders a form also controls what happens in that form on subsequent user input. an input form element whose value is controlled by react in this way is called a “controlled component”.
for example, if we want to make the previous example log the name when it is submitted, we can write the form as a controlled component:
since the  attribute is set on our form element, the displayed value will always be , making the react state the source of truth. since  runs on every keystroke to update the react state, the displayed value will update as the user types.
with a controlled component, the input’s value is always driven by the react state. while this means you have to type a bit more code, you can now pass the value to other ui elements too, or reset it from other event handlers.
the textarea tag
in html, a  element defines its text by its children:
in react, a  uses a  attribute instead. this way, a form using a  can be written very similarly to a form that uses a single-line input:
notice that  is initialized in the constructor, so that the text area starts off with some text in it.
the select tag
in html,  creates a drop-down list. for example, this html creates a drop-down list of flavors:
note that the coconut option is initially selected, because of the  attribute. react, instead of using this  attribute, uses a  attribute on the root  tag. this is more convenient in a controlled component because you only need to update it in one place. for example:
overall, this makes it so that , , and  all work very similarly - they all accept a  attribute that you can use to implement a controlled component.
you can pass an array into the  attribute, allowing you to select multiple options in a  tag:
the file input tag
in html, an  lets the user choose one or more files from their device storage to be uploaded to a server or manipulated by javascript via the .
because its value is read-only, it is an uncontrolled component in react. it is discussed together with other uncontrolled components .
handling multiple inputs
when you need to handle multiple controlled  elements, you can add a  attribute to each element and let the handler function choose what to do based on the value of .
note how we used the es6  syntax to update the state key corresponding to the given input name:
it is equivalent to this es5 code:
also, since  automatically , we only needed to call it with the changed parts.
controlled input null value
specifying the  prop on a  prevents the user from changing the input unless you desire so. if you’ve specified a  but the input is still editable, you may have accidentally set  to  or .
the following code demonstrates this. (the input is locked at first but becomes editable after a short delay.)
alternatives to controlled components
it can sometimes be tedious to use controlled components, because you need to write an event handler for every way your data can change and pipe all of the input state through a react component. this can become particularly annoying when you are converting a preexisting codebase to react, or integrating a react application with a non-react library. in these situations, you might want to check out , an alternative technique for implementing input forms.
fully-fledged solutions
if you’re looking for a complete solution including validation, keeping track of the visited fields, and handling form submission,  is one of the popular choices. however, it is built on the same principles of controlled components and managing state — so don’t neglect to learn them.is this page useful?
lifting state up
often, several components need to reflect the same changing data. we recommend lifting the shared state up to their closest common ancestor. let’s see how this works in action.
in this section, we will create a temperature calculator that calculates whether the water would boil at a given temperature.
we will start with a component called . it accepts the  temperature as a prop, and prints whether it is enough to boil the water:
next, we will create a component called . it renders an  that lets you enter the temperature, and keeps its value in .
additionally, it renders the  for the current input value.
adding a second input
our new requirement is that, in addition to a celsius input, we provide a fahrenheit input, and they are kept in sync.
we can start by extracting a  component from . we will add a new  prop to it that can either be  or :
we can now change the  to render two separate temperature inputs:
we have two inputs now, but when you enter the temperature in one of them, the other doesn’t update. this contradicts our requirement: we want to keep them in sync.
we also can’t display the  from . the  doesn’t know the current temperature because it is hidden inside the .
writing conversion functions
first, we will write two functions to convert from celsius to fahrenheit and back:
these two functions convert numbers. we will write another function that takes a string  and a converter function as arguments and returns a string. we will use it to calculate the value of one input based on the other input.
it returns an empty string on an invalid , and it keeps the output rounded to the third decimal place:
for example,  returns an empty string, and  returns .
currently, both  components independently keep their values in the local state:
however, we want these two inputs to be in sync with each other. when we update the celsius input, the fahrenheit input should reflect the converted temperature, and vice versa.
in react, sharing state is accomplished by moving it up to the closest common ancestor of the components that need it. this is called “lifting state up”. we will remove the local state from the  and move it into the  instead.
if the  owns the shared state, it becomes the “source of truth” for the current temperature in both inputs. it can instruct them both to have values that are consistent with each other. since the props of both  components are coming from the same parent  component, the two inputs will always be in sync.
let’s see how this works step by step.
first, we will replace  with  in the  component. for now, let’s pretend  already exists, although we will need to pass it from the  in the future:
we know that . when the  was in the local state, the  could just call  to change it. however, now that the  is coming from the parent as a prop, the  has no control over it.
in react, this is usually solved by making a component “controlled”. just like the dom  accepts both a  and an  prop, so can the custom  accept both  and  props from its parent .
now, when the  wants to update its temperature, it calls :
there is no special meaning to either  or  prop names in custom components. we could have called them anything else, like name them  and  which is a common convention.
the  prop will be provided together with the  prop by the parent  component. it will handle the change by modifying its own local state, thus re-rendering both inputs with the new values. we will look at the new  implementation very soon.
before diving into the changes in the , let’s recap our changes to the  component. we have removed the local state from it, and instead of reading , we now read . instead of calling  when we want to make a change, we now call , which will be provided by the :
now let’s turn to the  component.
we will store the current input’s  and  in its local state. this is the state we “lifted up” from the inputs, and it will serve as the “source of truth” for both of them. it is the minimal representation of all the data we need to know in order to render both inputs.
for example, if we enter 37 into the celsius input, the state of the  component will be:
if we later edit the fahrenheit field to be 212, the state of the  will be:
we could have stored the value of both inputs but it turns out to be unnecessary. it is enough to store the value of the most recently changed input, and the scale that it represents. we can then infer the value of the other input based on the current  and  alone.
the inputs stay in sync because their values are computed from the same state:
now, no matter which input you edit,  and  in the  get updated. one of the inputs gets the value as is, so any user input is preserved, and the other input value is always recalculated based on it.
let’s recap what happens when you edit an input:
react calls the function specified as  on the dom . in our case, this is the  method in the  component.
the  method in the  component calls  with the new desired value. its props, including , were provided by its parent component, the .
when it previously rendered, the  had specified that  of the celsius  is the ’s  method, and  of the fahrenheit  is the ’s  method. so either of these two  methods gets called depending on which input we edited.
inside these methods, the  component asks react to re-render itself by calling  with the new input value and the current scale of the input we just edited.
react calls the  component’s  method to learn what the ui should look like. the values of both inputs are recomputed based on the current temperature and the active scale. the temperature conversion is performed here.
react calls the  methods of the individual  components with their new props specified by the . it learns what their ui should look like.
react calls the  method of the  component, passing the temperature in celsius as its props.
react dom updates the dom with the boiling verdict and to match the desired input values. the input we just edited receives its current value, and the other input is updated to the temperature after conversion.
every update goes through the same steps so the inputs stay in sync.
lessons learned
there should be a single “source of truth” for any data that changes in a react application. usually, the state is first added to the component that needs it for rendering. then, if other components also need it, you can lift it up to their closest common ancestor. instead of trying to sync the state between different components, you should rely on the .
lifting state involves writing more “boilerplate” code than two-way binding approaches, but as a benefit, it takes less work to find and isolate bugs. since any state “lives” in some component and that component alone can change it, the surface area for bugs is greatly reduced. additionally, you can implement any custom logic to reject or transform user input.
if something can be derived from either props or state, it probably shouldn’t be in the state. for example, instead of storing both  and , we store just the last edited  and its . the value of the other input can always be calculated from them in the  method. this lets us clear or apply rounding to the other field without losing any precision in the user input.
when you see something wrong in the ui, you can use  to inspect the props and move up the tree until you find the component responsible for updating the state. this lets you trace the bugs to their source:
composition vs inheritance
react has a powerful composition model, and we recommend using composition instead of inheritance to reuse code between components.
in this section, we will consider a few problems where developers new to react often reach for inheritance, and show how we can solve them with composition.
containment
some components don’t know their children ahead of time. this is especially common for components like  or  that represent generic “boxes”.
we recommend that such components use the special  prop to pass children elements directly into their output:
this lets other components pass arbitrary children to them by nesting the jsx:
anything inside the  jsx tag gets passed into the  component as a  prop. since  renders  inside a , the passed elements appear in the final output.
while this is less common, sometimes you might need multiple “holes” in a component. in such cases you may come up with your own convention instead of using :
react elements like  and  are just objects, so you can pass them as props like any other data. this approach may remind you of “slots” in other libraries but there are no limitations on what you can pass as props in react.
specialization
sometimes we think about components as being “special cases” of other components. for example, we might say that a  is a special case of .
in react, this is also achieved by composition, where a more “specific” component renders a more “generic” one and configures it with props:
composition works equally well for components defined as classes:
so what about inheritance?
at facebook, we use react in thousands of components, and we haven’t found any use cases where we would recommend creating component inheritance hierarchies.
props and composition give you all the flexibility you need to customize a component’s look and behavior in an explicit and safe way. remember that components may accept arbitrary props, including primitive values, react elements, or functions.
if you want to reuse non-ui functionality between components, we suggest extracting it into a separate javascript module. the components may import it and use that function, object, or class, without extending it.is this page useful?
react is, in our opinion, the premier way to build big, fast web apps with javascript. it has scaled very well for us at facebook and instagram.
one of the many great parts of react is how it makes you think about apps as you build them. in this document, we’ll walk you through the thought process of building a searchable product data table using react.
start with a mock
imagine that we already have a json api and a mock from our designer. the mock looks like this:
our json api returns some data that looks like this:
step 1: break the ui into a component hierarchy
the first thing you’ll want to do is to draw boxes around every component (and subcomponent) in the mock and give them all names. if you’re working with a designer, they may have already done this, so go talk to them! their photoshop layer names may end up being the names of your react components!
but how do you know what should be its own component? use the same techniques for deciding if you should create a new function or object. one such technique is the , that is, a component should ideally only do one thing. if it ends up growing, it should be decomposed into smaller subcomponents.
since you’re often displaying a json data model to a user, you’ll find that if your model was built correctly, your ui (and therefore your component structure) will map nicely. that’s because ui and data models tend to adhere to the same information architecture. separate your ui into components, where each component matches one piece of your data model.
you’ll see here that we have five components in our app. we’ve italicized the data each component represents. the numbers in the image correspond to the numbers below.
(orange): contains the entirety of the example
(blue): receives all user input
(green): displays and filters the data collection based on user input
(turquoise): displays a heading for each category
(red): displays a row for each product
if you look at , you’ll see that the table header (containing the “name” and “price” labels) isn’t its own component. this is a matter of preference, and there’s an argument to be made either way. for this example, we left it as part of  because it is part of rendering the data collection which is ’s responsibility. however, if this header grows to be complex (e.g., if we were to add affordances for sorting), it would certainly make sense to make this its own  component.
now that we’ve identified the components in our mock, let’s arrange them into a hierarchy. components that appear within another component in the mock should appear as a child in the hierarchy:
step 2: build a static version in react
see the pen  on .
now that you have your component hierarchy, it’s time to implement your app. the easiest way is to build a version that takes your data model and renders the ui but has no interactivity. it’s best to decouple these processes because building a static version requires a lot of typing and no thinking, and adding interactivity requires a lot of thinking and not a lot of typing. we’ll see why.
to build a static version of your app that renders your data model, you’ll want to build components that reuse other components and pass data using props. props are a way of passing data from parent to child. if you’re familiar with the concept of state, don’t use state at all to build this static version. state is reserved only for interactivity, that is, data that changes over time. since this is a static version of the app, you don’t need it.
you can build top-down or bottom-up. that is, you can either start with building the components higher up in the hierarchy (i.e. starting with ) or with the ones lower in it (). in simpler examples, it’s usually easier to go top-down, and on larger projects, it’s easier to go bottom-up and write tests as you build.
at the end of this step, you’ll have a library of reusable components that render your data model. the components will only have  methods since this is a static version of your app. the component at the top of the hierarchy () will take your data model as a prop. if you make a change to your underlying data model and call  again, the ui will be updated. you can see how your ui is updated and where to make changes. react’s one-way data flow (also called one-way binding) keeps everything modular and fast.
refer to the  if you need help executing this step.
a brief interlude: props vs state
there are two types of “model” data in react: props and state. it’s important to understand the distinction between the two; skim  if you aren’t sure what the difference is. see also
step 3: identify the minimal (but complete) representation of ui state
to make your ui interactive, you need to be able to trigger changes to your underlying data model. react achieves this with state.
to build your app correctly, you first need to think of the minimal set of mutable state that your app needs. the key here is . figure out the absolute minimal representation of the state your application needs and compute everything else you need on-demand. for example, if you’re building a todo list, keep an array of the todo items around; don’t keep a separate state variable for the count. instead, when you want to render the todo count, take the length of the todo items array.
think of all the pieces of data in our example application. we have:
the original list of products
the search text the user has entered
the value of the checkbox
the filtered list of products
let’s go through each one and figure out which one is state. ask three questions about each piece of data:
is it passed in from a parent via props? if so, it probably isn’t state.
does it remain unchanged over time? if so, it probably isn’t state.
can you compute it based on any other state or props in your component? if so, it isn’t state.
the original list of products is passed in as props, so that’s not state. the search text and the checkbox seem to be state since they change over time and can’t be computed from anything. and finally, the filtered list of products isn’t state because it can be computed by combining the original list of products with the search text and value of the checkbox.
so finally, our state is:
step 4: identify where your state should live
ok, so we’ve identified what the minimal set of app state is. next, we need to identify which component mutates, or owns, this state.
remember: react is all about one-way data flow down the component hierarchy. it may not be immediately clear which component should own what state. this is often the most challenging part for newcomers to understand, so follow these steps to figure it out:
for each piece of state in your application:
identify every component that renders something based on that state.
find a common owner component (a single component above all the components that need the state in the hierarchy).
either the common owner or another component higher up in the hierarchy should own the state.
if you can’t find a component where it makes sense to own the state, create a new component solely for holding the state and add it somewhere in the hierarchy above the common owner component.
let’s run through this strategy for our application:
needs to filter the product list based on state and  needs to display the search text and checked state.
the common owner component is .
it conceptually makes sense for the filter text and checked value to live in
cool, so we’ve decided that our state lives in . first, add an instance property  to ’s  to reflect the initial state of your application. then, pass  and  to  and  as a prop. finally, use these props to filter the rows in  and set the values of the form fields in .
you can start seeing how your application will behave: set  to  and refresh your app. you’ll see that the data table is updated correctly.
step 5: add inverse data flow
so far, we’ve built an app that renders correctly as a function of props and state flowing down the hierarchy. now it’s time to support data flowing the other way: the form components deep in the hierarchy need to update the state in .
react makes this data flow explicit to help you understand how your program works, but it does require a little more typing than traditional two-way data binding.
if you try to type or check the box in the previous version of the example (step 4), you’ll see that react ignores your input. this is intentional, as we’ve set the  prop of the  to always be equal to the  passed in from .
let’s think about what we want to happen. we want to make sure that whenever the user changes the form, we update the state to reflect the user input. since components should only update their own state,  will pass callbacks to  that will fire whenever the state should be updated. we can use the  event on the inputs to be notified of it. the callbacks passed by  will call , and the app will be updated.
and that’s it
hopefully, this gives you an idea of how to think about building components and applications with react. while it may be a little more typing than you’re used to, remember that code is read far more often than it’s written, and it’s less difficult to read this modular, explicit code. as you start to build large libraries of components, you’ll appreciate this explicitness and modularity, and with code reuse, your lines of code will start to shrink. :)is this page useful?
why accessibility?
web accessibility (also referred to as ) is the design and creation of websites that can be used by everyone. accessibility support is necessary to allow assistive technology to interpret web pages.
react fully supports building accessible websites, often by using standard html techniques.
standards and guidelines
wcag
the  provides guidelines for creating accessible web sites.
the following wcag checklists provide an overview:
wai-aria
the  document contains techniques for building fully accessible javascript widgets.
note that all  html attributes are fully supported in jsx. whereas most dom properties and attributes in react are camelcased, these attributes should be hyphen-cased (also known as kebab-case, lisp-case, etc) as they are in plain html:
semantic html
semantic html is the foundation of accessibility in a web application. using the various html elements to reinforce the meaning of information
in our websites will often give us accessibility for free.
sometimes we break html semantics when we add  elements to our jsx to make our react code work, especially when working with lists (,  and ) and the html .
in these cases we should rather use  to group together multiple elements.
for example,
you can map a collection of items to an array of fragments as you would any other type of element as well:
when you don’t need any props on the fragment tag you can use the , if your tooling supports it:
for more info, see .
accessible forms
labeling
every html form control, such as  and , needs to be labeled accessibly. we need to provide descriptive labels that are also exposed to screen readers.
the following resources show us how to do this:
although these standard html practices can be directly used in react, note that the  attribute is written as  in jsx:
notifying the user of errors
error situations need to be understood by all users. the following link shows us how to expose error texts to screen readers as well:
focus control
ensure that your web application can be fully operated with the keyboard only:
keyboard focus and focus outline
keyboard focus refers to the current element in the dom that is selected to accept input from the keyboard. we see it everywhere as a focus outline similar to that shown in the following image:
only ever use css that removes this outline, for example by setting , if you are replacing it with another focus outline implementation.
mechanisms to skip to desired content
provide a mechanism to allow users to skip past navigation sections in your application as this assists and speeds up keyboard navigation.
skiplinks or skip navigation links are hidden navigation links that only become visible when keyboard users interact with the page. they are very easy to implement with internal page anchors and some styling:
also use landmark elements and roles, such as  and , to demarcate page regions as assistive technology allow the user to quickly navigate to these sections.
read more about the use of these elements to enhance accessibility here:
programmatically managing focus
our react applications continuously modify the html dom during runtime, sometimes leading to keyboard focus being lost or set to an unexpected element. in order to repair this, we need to programmatically nudge the keyboard focus in the right direction. for example, by resetting keyboard focus to a button that opened a modal window after that modal window is closed.
mdn web docs takes a look at this and describes how we can build .
to set focus in react, we can use .
using this, we first create a ref to an element in the jsx of a component class:
then we can focus it elsewhere in our component when needed:
sometimes a parent component needs to set focus to an element in a child component. we can do this by  through a special prop on the child component that forwards the parent’s ref to the child’s dom node.
when using a  to extend components, it is recommended to  to the wrapped component using the  function of react. if a third party hoc does not implement ref forwarding, the above pattern can still be used as a fallback.
a great focus management example is the . this is a relatively rare example of a fully accessible modal window. not only does it set initial focus on
the cancel button (preventing the keyboard user from accidentally activating the success action) and trap keyboard focus inside the modal, it also resets focus back to the element that initially triggered the modal.
while this is a very important accessibility feature, it is also a technique that should be used judiciously. use it to repair the keyboard focus flow when it is disturbed, not to try and anticipate how
users want to use applications.
mouse and pointer events
ensure that all functionality exposed through a mouse or pointer event can also be accessed using the keyboard alone. depending only on the pointer device will lead to many cases where keyboard users cannot use your application.
to illustrate this, let’s look at a prolific example of broken accessibility caused by click events. this is the outside click pattern, where a user can disable an opened popover by clicking outside the element.
this is typically implemented by attaching a  event to the  object that closes the popover:
this may work fine for users with pointer devices, such as a mouse, but operating this with the keyboard alone leads to broken functionality when tabbing to the next element as the  object never receives a  event. this can lead to obscured functionality which blocks users from using your application.
the same functionality can be achieved by using appropriate event handlers instead, such as  and :
this code exposes the functionality to both pointer device and keyboard users. also note the added  props to support screen-reader users. for simplicity’s sake the keyboard events to enable  interaction of the popover options have not been implemented.
this is one example of many cases where depending on only pointer and mouse events will break functionality for keyboard users. always testing with the keyboard will immediately highlight the problem areas which can then be fixed by using keyboard aware event handlers.
more complex widgets
a more complex user experience should not mean a less accessible one. whereas accessibility is most easily achieved by coding as close to html as possible, even the most complex widget can be coded accessibly.
here we require knowledge of  as well as .
these are toolboxes filled with html attributes that are fully supported in jsx and enable us to construct fully accessible, highly functional react components.
each type of widget has a specific design pattern and is expected to function in a certain way by users and user agents alike:
other points for consideration
setting the language
indicate the human language of page texts as screen reader software uses this to select the correct voice settings:
setting the document title
set the document  to correctly describe the current page content as this ensures that the user remains aware of the current page context:
we can set this in react using the .
color contrast
ensure that all readable text on your website has sufficient color contrast to remain maximally readable by users with low vision:
it can be tedious to manually calculate the proper color combinations for all cases in your website so instead, you can .
both the axe and wave tools mentioned below also include color contrast tests and will report on contrast errors.
if you want to extend your contrast testing abilities you can use these tools:
development and testing tools
there are a number of tools we can use to assist in the creation of accessible web applications.
the keyboard
by far the easiest and also one of the most important checks is to test if your entire website can be reached and used with the keyboard alone. do this by:
disconnecting your mouse.
using  and  to browse.
using  to activate elements.
where required, using your keyboard arrow keys to interact with some elements, such as menus and dropdowns.
development assistance
we can check some accessibility features directly in our jsx code. often intellisense checks are already provided in jsx aware ide’s for the aria roles, states and properties. we also have access to the following tool:
eslint-plugin-jsx-a11y
the  plugin for eslint provides ast linting feedback regarding accessibility issues in your jsx. many ide’s allow you to integrate these findings directly into code analysis and source code windows.
has this plugin with a subset of rules activated. if you want to enable even more accessibility rules, you can create an  file in the root of your project with this content:
testing accessibility in the browser
a number of tools exist that can run accessibility audits on web pages in your browser. please use them in combination with other accessibility checks mentioned here as they can only
test the technical accessibility of your html.
axe, axe-core and react-axe
deque systems offers  for automated and end-to-end accessibility tests of your applications. this module includes integrations for selenium.
or axe, is an accessibility inspector browser extension built on .
you can also use the  module to report these accessibility findings directly to the console while developing and debugging.
webaim wave
the  is another accessibility browser extension.
accessibility inspectors and the accessibility tree
is a subset of the dom tree that contains accessible objects for every dom element that should be exposed
to assistive technology, such as screen readers.
in some browsers we can easily view the accessibility information for each element in the accessibility tree:
screen readers
testing with a screen reader should form part of your accessibility tests.
please note that browser / screen reader combinations matter. it is recommended that you test your application in the browser best suited to your screen reader of choice.
commonly used screen readers
nvda in firefox
or nvda is an open source windows screen reader that is widely used.
refer to the following guides on how to best use nvda:
voiceover in safari
voiceover is an integrated screen reader on apple devices.
refer to the following guides on how to activate and use voiceover:
jaws in internet explorer
or jaws, is a prolifically used screen reader on windows.
refer to the following guides on how to best use jaws:
other screen readers
chromevox in google chrome
is an integrated screen reader on chromebooks and is available  for google chrome.
refer to the following guides on how best to use chromevox:
code-splitting
bundling
most react apps will have their files “bundled” using tools like ,  or . bundling is the process of following imported files and merging them into a single file: a “bundle”. this bundle can then be included on a webpage to load an entire app at once.
example
app:
bundle:
your bundles will end up looking a lot different than this.
if you’re using , , , or a similar tool, you will have a webpack setup out of the box to bundle your app.
if you aren’t, you’ll need to set up bundling yourself. for example, see the  and  guides on the webpack docs.
code splitting
bundling is great, but as your app grows, your bundle will grow too. especially if you are including large third-party libraries. you need to keep an eye on the code you are including in your bundle so that you don’t accidentally make it so large that your app takes a long time to load.
to avoid winding up with a large bundle, it’s good to get ahead of the problem and start “splitting” your bundle. code-splitting is a feature
supported by bundlers like ,  and browserify (via ) which can create multiple bundles that can be dynamically loaded at runtime.
code-splitting your app can help you “lazy-load” just the things that are currently needed by the user, which can dramatically improve the performance of your app. while you haven’t reduced the overall amount of code in your app, you’ve avoided loading code that the user may never need, and reduced the amount of code needed during the initial load.
the best way to introduce code-splitting into your app is through the dynamic  syntax.
before:
after:
when webpack comes across this syntax, it automatically starts code-splitting your app. if you’re using create react app, this is already configured for you and you can  immediately. it’s also supported out of the box in .
if you’re setting up webpack yourself, you’ll probably want to read webpack’s . your webpack config should look vaguely .
when using , you’ll need to make sure that babel can parse the dynamic import syntax but is not transforming it. for that you will need .
the  function lets you render a dynamic import as a regular component.
this will automatically load the bundle containing the  when this component is first rendered.
takes a function that must call a dynamic . this must return a  which resolves to a module with a  export containing a react component.
the lazy component should then be rendered inside a  component, which allows us to show some fallback content (such as a loading indicator) while we’re waiting for the lazy component to load.
the  prop accepts any react elements that you want to render while waiting for the component to load. you can place the  component anywhere above the lazy component. you can even wrap multiple lazy components with a single  component.
avoiding fallbacks
any component may suspend as a result of rendering, even components that were already shown to the user. in order for screen content to always be consistent, if an already shown component suspends, react has to hide its tree up to the closest  boundary. however, from the user’s perspective, this can be disorienting.
consider this tab switcher:
in this example, if tab gets changed from  to , but  suspends, the user will see a glimmer. this makes sense because the user no longer wants to see , the  component is not ready to render anything, and react needs to keep the user experience consistent, so it has no choice but to show the  above.
however, sometimes this user experience is not desirable. in particular, it is sometimes better to show the “old” ui while the new ui is being prepared. you can use the new  api to make react do this:
here, you tell react that setting tab to  is not an urgent update, but is a  that may take some time. react will then keep the old ui in place and interactive, and will switch to showing  when it is ready. see  for more info.
error boundaries
if the other module fails to load (for example, due to network failure), it will trigger an error. you can handle these errors to show a nice user experience and manage recovery with . once you’ve created your error boundary, you can use it anywhere above your lazy components to display an error state when there’s a network error.
route-based code splitting
deciding where in your app to introduce code splitting can be a bit tricky. you want to make sure you choose places that will split bundles evenly, but won’t disrupt the user experience.
a good place to start is with routes. most people on the web are used to page transitions taking some amount of time to load. you also tend to be re-rendering the entire page at once so your users are unlikely to be interacting with other elements on the page at the same time.
here’s an example of how to setup route-based code splitting into your app using libraries like  with .
named exports
currently only supports default exports. if the module you want to import uses named exports, you can create an intermediate module that reexports it as the default. this ensures that tree shaking keeps working and that you don’t pull in unused components.
context
context provides a way to pass data through the component tree without having to pass props down manually at every level.
in a typical react application, data is passed top-down (parent to child) via props, but such usage can be cumbersome for certain types of props (e.g. locale preference, ui theme) that are required by many components within an application. context provides a way to share values like these between components without having to explicitly pass a prop through every level of the tree.
when to use context
context is designed to share data that can be considered “global” for a tree of react components, such as the current authenticated user, theme, or preferred language. for example, in the code below we manually thread through a “theme” prop in order to style the button component:
using context, we can avoid passing props through intermediate elements:
before you use context
context is primarily used when some data needs to be accessible by many components at different nesting levels. apply it sparingly because it makes component reuse more difficult.
if you only want to avoid passing some props through many levels,  is often a simpler solution than context.
for example, consider a  component that passes a  and  prop several levels down so that deeply nested  and  components can read it:
it might feel redundant to pass down the  and  props through many levels if in the end only the  component really needs it. it’s also annoying that whenever the  component needs more props from the top, you have to add them at all the intermediate levels too.
one way to solve this issue without context is to  so that the intermediate components don’t need to know about the  or  props:
with this change, only the top-most page component needs to know about the  and  components’ use of  and .
this inversion of control can make your code cleaner in many cases by reducing the amount of props you need to pass through your application and giving more control to the root components. such inversion, however, isn’t the right choice in every case; moving more complexity higher in the tree makes those higher-level components more complicated and forces the lower-level components to be more flexible than you may want.
you’re not limited to a single child for a component. you may pass multiple children, or even have multiple separate “slots” for children, :
this pattern is sufficient for many cases when you need to decouple a child from its immediate parents. you can take it even further with  if the child needs to communicate with the parent before rendering.
however, sometimes the same data needs to be accessible by many components in the tree, and at different nesting levels. context lets you “broadcast” such data, and changes to it, to all components below. common examples where using context might be simpler than the alternatives include managing the current locale, theme, or a data cache.
creates a context object. when react renders a component that subscribes to this context object it will read the current context value from the closest matching  above it in the tree.
the  argument is only used when a component does not have a matching provider above it in the tree. this default value can be helpful for testing components in isolation without wrapping them. note: passing  as a provider value does not cause consuming components to use .
every context object comes with a provider react component that allows consuming components to subscribe to context changes.
the provider component accepts a  prop to be passed to consuming components that are descendants of this provider. one provider can be connected to many consumers. providers can be nested to override values deeper within the tree.
all consumers that are descendants of a provider will re-render whenever the provider’s  prop changes. the propagation from provider to its descendant consumers (including  and ) is not subject to the  method, so the consumer is updated even when an ancestor component skips an update.
changes are determined by comparing the new and old values using the same algorithm as .
the way changes are determined can cause some issues when passing objects as : see .
the  property on a class can be assigned a context object created by . using this property lets you consume the nearest current value of that context type using . you can reference this in any of the lifecycle methods including the render function.
you can only subscribe to a single context using this api. if you need to read more than one see .
if you are using the experimental , you can use a static class field to initialize your .
a react component that subscribes to context changes. using this component lets you subscribe to a context within a .
requires a . the function receives the current context value and returns a react node. the  argument passed to the function will be equal to the  prop of the closest provider for this context above in the tree. if there is no provider for this context above, the  argument will be equal to the  that was passed to .
for more information about the ‘function as a child’ pattern, see .
context object accepts a  string property. react devtools uses this string to determine what to display for the context.
for example, the following component will appear as mydisplayname in the devtools:
examples
dynamic context
a more complex example with dynamic values for the theme:
theme-context.js
themed-button.js
app.js
updating context from a nested component
it is often necessary to update the context from a component that is nested somewhere deeply in the component tree. in this case you can pass a function down through the context to allow consumers to update the context:
theme-toggler-button.js
consuming multiple contexts
to keep context re-rendering fast, react needs to make each context consumer a separate node in the tree.
if two or more context values are often used together, you might want to consider creating your own render prop component that provides both.
caveats
because context uses reference identity to determine when to re-render, there are some gotchas that could trigger unintentional renders in consumers when a provider’s parent re-renders. for example, the code below will re-render all consumers every time the provider re-renders because a new object is always created for :
to get around this, lift the value into the parent’s state:
legacy api
react previously shipped with an experimental context api. the old api will be supported in all 16.x releases, but applications using it should migrate to the new version. the legacy api will be removed in a future major react version. read the .
in the past, javascript errors inside components used to corrupt react’s internal state and cause it to    on next renders. these errors were always caused by an earlier error in the application code, but react did not provide a way to handle them gracefully in components, and could not recover from them.
introducing error boundaries
a javascript error in a part of the ui shouldn’t break the whole app. to solve this problem for react users, react 16 introduces a new concept of an “error boundary”.
error boundaries are react components that catch javascript errors anywhere in their child component tree, log those errors, and display a fallback ui instead of the component tree that crashed. error boundaries catch errors during rendering, in lifecycle methods, and in constructors of the whole tree below them.
error boundaries do not catch errors for:
event handlers ()
asynchronous code (e.g.  or  callbacks)
server side rendering
errors thrown in the error boundary itself (rather than its children)
a class component becomes an error boundary if it defines either (or both) of the lifecycle methods  or . use  to render a fallback ui after an error has been thrown. use  to log error information.
then you can use it as a regular component:
error boundaries work like a javascript  block, but for components. only class components can be error boundaries. in practice, most of the time you’ll want to declare an error boundary component once and use it throughout your application.
note that error boundaries only catch errors in the components below them in the tree. an error boundary can’t catch an error within itself. if an error boundary fails trying to render the error message, the error will propagate to the closest error boundary above it. this, too, is similar to how the  block works in javascript.
live demo
check out .
where to place error boundaries
the granularity of error boundaries is up to you. you may wrap top-level route components to display a “something went wrong” message to the user, just like how server-side frameworks often handle crashes. you may also wrap individual widgets in an error boundary to protect them from crashing the rest of the application.
new behavior for uncaught errors
this change has an important implication. as of react 16, errors that were not caught by any error boundary will result in unmounting of the whole react component tree.
we debated this decision, but in our experience it is worse to leave corrupted ui in place than to completely remove it. for example, in a product like messenger leaving the broken ui visible could lead to somebody sending a message to the wrong person. similarly, it is worse for a payments app to display a wrong amount than to render nothing.
this change means that as you migrate to react 16, you will likely uncover existing crashes in your application that have been unnoticed before. adding error boundaries lets you provide better user experience when something goes wrong.
for example, facebook messenger wraps content of the sidebar, the info panel, the conversation log, and the message input into separate error boundaries. if some component in one of these ui areas crashes, the rest of them remain interactive.
we also encourage you to use js error reporting services (or build your own) so that you can learn about unhandled exceptions as they happen in production, and fix them.
component stack traces
react 16 prints all errors that occurred during rendering to the console in development, even if the application accidentally swallows them. in addition to the error message and the javascript stack, it also provides component stack traces. now you can see where exactly in the component tree the failure has happened:
you can also see the filenames and line numbers in the component stack trace. this works by default in  projects:
if you don’t use create react app, you can add  manually to your babel configuration. note that it’s intended only for development and must be disabled in production.
component names displayed in the stack traces depend on the  property. if you support older browsers and devices which may not yet provide this natively (e.g. ie 11), consider including a  polyfill in your bundled application, such as . alternatively, you may explicitly set the  property on all your components.
how about try/catch?
/  is great but it only works for imperative code:
however, react components are declarative and specify what should be rendered:
error boundaries preserve the declarative nature of react, and behave as you would expect. for example, even if an error occurs in a  method caused by a  somewhere deep in the tree, it will still correctly propagate to the closest error boundary.
how about event handlers?
error boundaries do not catch errors inside event handlers.
react doesn’t need error boundaries to recover from errors in event handlers. unlike the render method and lifecycle methods, the event handlers don’t happen during rendering. so if they throw, react still knows what to display on the screen.
if you need to catch an error inside an event handler, use the regular javascript  /  statement:
note that the above example is demonstrating regular javascript behavior and doesn’t use error boundaries.
naming changes from react 15
react 15 included a very limited support for error boundaries under a different method name: . this method no longer works, and you will need to change it to  in your code starting from the first 16 beta release.
for this change, we’ve provided a  to automatically migrate your code.is this page useful?
forwarding refs
ref forwarding is a technique for automatically passing a  through a component to one of its children. this is typically not necessary for most components in the application. however, it can be useful for some kinds of components, especially in reusable component libraries. the most common scenarios are described below.
forwarding refs to dom components
consider a  component that renders the native  dom element:
react components hide their implementation details, including their rendered output. other components using  usually will not need to  to the inner  dom element. this is good because it prevents components from relying on each other’s dom structure too much.
although such encapsulation is desirable for application-level components like  or , it can be inconvenient for highly reusable “leaf” components like  or . these components tend to be used throughout the application in a similar manner as a regular dom  and , and accessing their dom nodes may be unavoidable for managing focus, selection, or animations.
ref forwarding is an opt-in feature that lets some components take a  they receive, and pass it further down (in other words, “forward” it) to a child.
in the example below,  uses  to obtain the  passed to it, and then forward it to the dom  that it renders:
this way, components using  can get a ref to the underlying  dom node and access it if necessary—just like if they used a dom  directly.
here is a step-by-step explanation of what happens in the above example:
we create a  by calling  and assign it to a  variable.
we pass our  down to  by specifying it as a jsx attribute.
react passes the  to the  function inside  as a second argument.
we forward this  argument down to  by specifying it as a jsx attribute.
when the ref is attached,  will point to the  dom node.
the second  argument only exists when you define a component with  call. regular function or class components don’t receive the  argument, and ref is not available in props either.
ref forwarding is not limited to dom components. you can forward refs to class component instances, too.
note for component library maintainers
when you start using  in a component library, you should treat it as a breaking change and release a new major version of your library. this is because your library likely has an observably different behavior (such as what refs get assigned to, and what types are exported), and this can break apps and other libraries that depend on the old behavior.
conditionally applying  when it exists is also not recommended for the same reasons: it changes how your library behaves and can break your users’ apps when they upgrade react itself.
forwarding refs in higher-order components
this technique can also be particularly useful with  (also known as hocs). let’s start with an example hoc that logs component props to the console:
the “logprops” hoc passes all  through to the component it wraps, so the rendered output will be the same. for example, we can use this hoc to log all props that get passed to our “fancy button” component:
there is one caveat to the above example: refs will not get passed through. that’s because  is not a prop. like , it’s handled differently by react. if you add a ref to a hoc, the ref will refer to the outermost container component, not the wrapped component.
this means that refs intended for our  component will actually be attached to the  component:
fortunately, we can explicitly forward refs to the inner  component using the  api.  accepts a render function that receives  and  parameters and returns a react node. for example:
displaying a custom name in devtools
accepts a render function. react devtools uses this function to determine what to display for the ref forwarding component.
for example, the following component will appear as ”forwardref” in the devtools:
if you name the render function, devtools will also include its name (e.g. ”forwardref(myfunction)”):
you can even set the function’s  property to include the component you’re wrapping:
fragments
a common pattern in react is for a component to return multiple elements. fragments let you group a list of children without adding extra nodes to the dom.
there is also a new  for declaring them.
motivation
a common pattern is for a component to return a list of children. take this example react snippet:
would need to return multiple  elements in order for the rendered html to be valid. if a parent div was used inside the  of , then the resulting html will be invalid.
results in a  output of:
fragments solve this problem.
usage
which results in a correct  output of:
short syntax
there is a new, shorter syntax you can use for declaring fragments. it looks like empty tags:
you can use  the same way you’d use any other element except that it doesn’t support keys or attributes.
keyed fragments
fragments declared with the explicit  syntax may have keys. a use case for this is mapping a collection to an array of fragments — for example, to create a description list:
is the only attribute that can be passed to . in the future, we may add support for additional attributes, such as event handlers.
you can try out the new jsx fragment syntax with this .is this page useful?
higher-order components
a higher-order component (hoc) is an advanced technique in react for reusing component logic. hocs are not part of the react api, per se. they are a pattern that emerges from react’s compositional nature.
concretely, a higher-order component is a function that takes a component and returns a new component.
whereas a component transforms props into ui, a higher-order component transforms a component into another component.
hocs are common in third-party react libraries, such as redux’s  and relay’s .
in this document, we’ll discuss why higher-order components are useful, and how to write your own.
use hocs for cross-cutting concerns
we previously recommended mixins as a way to handle cross-cutting concerns. we’ve since realized that mixins create more trouble than they are worth.  about why we’ve moved away from mixins and how you can transition your existing components.
components are the primary unit of code reuse in react. however, you’ll find that some patterns aren’t a straightforward fit for traditional components.
for example, say you have a  component that subscribes to an external data source to render a list of comments:
later, you write a component for subscribing to a single blog post, which follows a similar pattern:
and  aren’t identical — they call different methods on , and they render different output. but much of their implementation is the same:
on mount, add a change listener to .
inside the listener, call  whenever the data source changes.
on unmount, remove the change listener.
you can imagine that in a large app, this same pattern of subscribing to  and calling  will occur over and over again. we want an abstraction that allows us to define this logic in a single place and share it across many components. this is where higher-order components excel.
we can write a function that creates components, like  and , that subscribe to . the function will accept as one of its arguments a child component that receives the subscribed data as a prop. let’s call the function :
the first parameter is the wrapped component. the second parameter retrieves the data we’re interested in, given a  and the current props.
when  and  are rendered,  and  will be passed a  prop with the most current data retrieved from :
note that a hoc doesn’t modify the input component, nor does it use inheritance to copy its behavior. rather, a hoc composes the original component by wrapping it in a container component. a hoc is a pure function with zero side-effects.
and that’s it! the wrapped component receives all the props of the container, along with a new prop, , which it uses to render its output. the hoc isn’t concerned with how or why the data is used, and the wrapped component isn’t concerned with where the data came from.
because  is a normal function, you can add as many or as few arguments as you like. for example, you may want to make the name of the  prop configurable, to further isolate the hoc from the wrapped component. or you could accept an argument that configures , or one that configures the data source. these are all possible because the hoc has full control over how the component is defined.
like components, the contract between  and the wrapped component is entirely props-based. this makes it easy to swap one hoc for a different one, as long as they provide the same props to the wrapped component. this may be useful if you change data-fetching libraries, for example.
don’t mutate the original component. use composition.
resist the temptation to modify a component’s prototype (or otherwise mutate it) inside a hoc.
there are a few problems with this. one is that the input component cannot be reused separately from the enhanced component. more crucially, if you apply another hoc to  that also mutates , the first hoc’s functionality will be overridden! this hoc also won’t work with function components, which do not have lifecycle methods.
mutating hocs are a leaky abstraction—the consumer must know how they are implemented in order to avoid conflicts with other hocs.
instead of mutation, hocs should use composition, by wrapping the input component in a container component:
this hoc has the same functionality as the mutating version while avoiding the potential for clashes. it works equally well with class and function components. and because it’s a pure function, it’s composable with other hocs, or even with itself.
you may have noticed similarities between hocs and a pattern called container components. container components are part of a strategy of separating responsibility between high-level and low-level concerns. containers manage things like subscriptions and state, and pass props to components that handle things like rendering ui. hocs use containers as part of their implementation. you can think of hocs as parameterized container component definitions.
convention: pass unrelated props through to the wrapped component
hocs add features to a component. they shouldn’t drastically alter its contract. it’s expected that the component returned from a hoc has a similar interface to the wrapped component.
hocs should pass through props that are unrelated to its specific concern. most hocs contain a render method that looks something like this:
this convention helps ensure that hocs are as flexible and reusable as possible.
convention: maximizing composability
not all hocs look the same. sometimes they accept only a single argument, the wrapped component:
usually, hocs accept additional arguments. in this example from relay, a config object is used to specify a component’s data dependencies:
the most common signature for hocs looks like this:
what?! if you break it apart, it’s easier to see what’s going on.
in other words,  is a higher-order function that returns a higher-order component!
this form may seem confusing or unnecessary, but it has a useful property. single-argument hocs like the one returned by the  function have the signature . functions whose output type is the same as its input type are really easy to compose together.
(this same property also allows  and other enhancer-style hocs to be used as decorators, an experimental javascript proposal.)
the  utility function is provided by many third-party libraries including lodash (as ), , and .
convention: wrap the display name for easy debugging
the container components created by hocs show up in the  like any other component. to ease debugging, choose a display name that communicates that it’s the result of a hoc.
the most common technique is to wrap the display name of the wrapped component. so if your higher-order component is named , and the wrapped component’s display name is , use the display name :
higher-order components come with a few caveats that aren’t immediately obvious if you’re new to react.
don’t use hocs inside the render method
react’s diffing algorithm (called ) uses component identity to determine whether it should update the existing subtree or throw it away and mount a new one. if the component returned from  is identical () to the component from the previous render, react recursively updates the subtree by diffing it with the new one. if they’re not equal, the previous subtree is unmounted completely.
normally, you shouldn’t need to think about this. but it matters for hocs because it means you can’t apply a hoc to a component within the render method of a component:
the problem here isn’t just about performance — remounting a component causes the state of that component and all of its children to be lost.
instead, apply hocs outside the component definition so that the resulting component is created only once. then, its identity will be consistent across renders. this is usually what you want, anyway.
in those rare cases where you need to apply a hoc dynamically, you can also do it inside a component’s lifecycle methods or its constructor.
static methods must be copied over
sometimes it’s useful to define a static method on a react component. for example, relay containers expose a static method  to facilitate the composition of graphql fragments.
when you apply a hoc to a component, though, the original component is wrapped with a container component. that means the new component does not have any of the static methods of the original component.
to solve this, you could copy the methods onto the container before returning it:
however, this requires you to know exactly which methods need to be copied. you can use  to automatically copy all non-react static methods:
another possible solution is to export the static method separately from the component itself.
refs aren’t passed through
while the convention for higher-order components is to pass through all props to the wrapped component, this does not work for refs. that’s because  is not really a prop — like , it’s handled specially by react. if you add a ref to an element whose component is the result of a hoc, the ref refers to an instance of the outermost container component, not the wrapped component.
the solution for this problem is to use the  api (introduced with react 16.3). .is this page useful?
integrating with other libraries
react can be used in any web application. it can be embedded in other applications and, with a little care, other applications can be embedded in react. this guide will examine some of the more common use cases, focusing on integration with  and , but the same ideas can be applied to integrating components with any existing code.
integrating with dom manipulation plugins
react is unaware of changes made to the dom outside of react. it determines updates based on its own internal representation, and if the same dom nodes are manipulated by another library, react gets confused and has no way to recover.
this does not mean it is impossible or even necessarily difficult to combine react with other ways of affecting the dom, you just have to be mindful of what each is doing.
the easiest way to avoid conflicts is to prevent the react component from updating. you can do this by rendering elements that react has no reason to update, like an empty .
how to approach the problem
to demonstrate this, let’s sketch out a wrapper for a generic jquery plugin.
we will attach a  to the root dom element. inside , we will get a reference to it so we can pass it to the jquery plugin.
to prevent react from touching the dom after mounting, we will return an empty  from the  method. the  element has no properties or children, so react has no reason to update it, leaving the jquery plugin free to manage that part of the dom:
note that we defined both  and  . many jquery plugins attach event listeners to the dom so it’s important to detach them in . if the plugin does not provide a method for cleanup, you will probably have to provide your own, remembering to remove any event listeners the plugin registered to prevent memory leaks.
integrating with jquery chosen plugin
for a more concrete example of these concepts, let’s write a minimal wrapper for the plugin , which augments  inputs.
just because it’s possible, doesn’t mean that it’s the best approach for react apps. we encourage you to use react components when you can. react components are easier to reuse in react applications, and often provide more control over their behavior and appearance.
first, let’s look at what chosen does to the dom.
if you call it on a  dom node, it reads the attributes off of the original dom node, hides it with an inline style, and then appends a separate dom node with its own visual representation right after the . then it fires jquery events to notify us about the changes.
let’s say that this is the api we’re striving for with our  wrapper react component:
we will implement it as an  for simplicity.
first, we will create an empty component with a  method where we return  wrapped in a :
notice how we wrapped  in an extra . this is necessary because chosen will append another dom element right after the  node we passed to it. however, as far as react is concerned,  always only has a single child. this is how we ensure that react updates won’t conflict with the extra dom node appended by chosen. it is important that if you modify the dom outside of react flow, you must ensure react doesn’t have a reason to touch those dom nodes.
next, we will implement the lifecycle methods. we need to initialize chosen with the ref to the  node in , and tear it down in :
note that react assigns no special meaning to the  field. it only works because we have previously assigned this field from a  in the  method:
this is enough to get our component to render, but we also want to be notified about the value changes. to do this, we will subscribe to the jquery  event on the  managed by chosen.
we won’t pass  directly to chosen because component’s props might change over time, and that includes event handlers. instead, we will declare a  method that calls , and subscribe it to the jquery  event:
finally, there is one more thing left to do. in react, props can change over time. for example, the  component can get different children if parent component’s state changes. this means that at integration points it is important that we manually update the dom in response to prop updates, since we no longer let react manage the dom for us.
chosen’s documentation suggests that we can use jquery  api to notify it about changes to the original dom element. we will let react take care of updating  inside , but we will also add a  lifecycle method that notifies chosen about changes in the children list:
this way, chosen will know to update its dom element when the  children managed by react change.
the complete implementation of the  component looks like this:
integrating with other view libraries
react can be embedded into other applications thanks to the flexibility of .
although react is commonly used at startup to load a single root react component into the dom,  can also be called multiple times for independent parts of the ui which can be as small as a button, or as large as an app.
in fact, this is exactly how react is used at facebook. this lets us write applications in react piece by piece, and combine them with our existing server-generated templates and other client-side code.
replacing string-based rendering with react
a common pattern in older web applications is to describe chunks of the dom as a string and insert it into the dom like so: . these points in a codebase are perfect for introducing react. just rewrite the string based rendering as a react component.
so the following jquery implementation…
…could be rewritten using a react component:
from here you could start moving more logic into the component and begin adopting more common react practices. for example, in components it is best not to rely on ids because the same component can be rendered multiple times. instead, we will use the  and register the click handler directly on the react  element:
you can have as many such isolated components as you like, and use  to render them to different dom containers. gradually, as you convert more of your app to react, you will be able to combine them into larger components, and move some of the  calls up the hierarchy.
embedding react in a backbone view
views typically use html strings, or string-producing template functions, to create the content for their dom elements. this process, too, can be replaced with rendering a react component.
below, we will create a backbone view called . it will override backbone’s  function to render a react  component into the dom element provided by backbone (). here, too, we are using :
it is important that we also call  in the  method so that react unregisters event handlers and other resources associated with the component tree when it is detached.
when a component is removed from within a react tree, the cleanup is performed automatically, but because we are removing the entire tree by hand, we must call this method.
integrating with model layers
while it is generally recommended to use unidirectional data flow such as , , or , react components can use a model layer from other frameworks and libraries.
using backbone models in react components
the simplest way to consume  models and collections from a react component is to listen to the various change events and manually force an update.
components responsible for rendering models would listen to  events, while components responsible for rendering collections would listen for  and  events. in both cases, call  to rerender the component with the new data.
in the example below, the  component renders a backbone collection, using the  component to render individual items.
extracting data from backbone models
the approach above requires your react components to be aware of the backbone models and collections. if you later plan to migrate to another data management solution, you might want to concentrate the knowledge about backbone in as few parts of the code as possible.
one solution to this is to extract the model’s attributes as plain data whenever it changes, and keep this logic in a single place. the following is  that extracts all attributes of a backbone model into state, passing the data to the wrapped component.
this way, only the higher-order component needs to know about backbone model internals, and most components in the app can stay agnostic of backbone.
in the example below, we will make a copy of the model’s attributes to form the initial state. we subscribe to the  event (and unsubscribe on unmounting), and when it happens, we update the state with the model’s current attributes. finally, we make sure that if the  prop itself changes, we don’t forget to unsubscribe from the old model, and subscribe to the new one.
note that this example is not meant to be exhaustive with regards to working with backbone, but it should give you an idea for how to approach this in a generic way:
to demonstrate how to use it, we will connect a  react component to a backbone model, and update its  attribute every time the input changes:
this technique is not limited to backbone. you can use react with any model library by subscribing to its changes in the lifecycle methods and, optionally, copying the data into the local react state.is this page useful?
jsx in depth
fundamentally, jsx just provides syntactic sugar for the  function. the jsx code:
compiles into:
you can also use the self-closing form of the tag if there are no children. so:
if you want to test out how some specific jsx is converted into javascript, you can try out .
specifying the react element type
the first part of a jsx tag determines the type of the react element.
capitalized types indicate that the jsx tag is referring to a react component. these tags get compiled into a direct reference to the named variable, so if you use the jsx  expression,  must be in scope.
react must be in scope
since jsx compiles into calls to , the  library must also always be in scope from your jsx code.
for example, both of the imports are necessary in this code, even though  and  are not directly referenced from javascript:
if you don’t use a javascript bundler and loaded react from a  tag, it is already in scope as the  global.
using dot notation for jsx type
you can also refer to a react component using dot-notation from within jsx. this is convenient if you have a single module that exports many react components. for example, if  is a component, you can use it directly from jsx with:
user-defined components must be capitalized
when an element type starts with a lowercase letter, it refers to a built-in component like  or  and results in a string  or  passed to . types that start with a capital letter like  compile to  and correspond to a component defined or imported in your javascript file.
we recommend naming components with a capital letter. if you do have a component that starts with a lowercase letter, assign it to a capitalized variable before using it in jsx.
for example, this code will not run as expected:
to fix this, we will rename  to  and use  when referring to it:
choosing the type at runtime
you cannot use a general expression as the react element type. if you do want to use a general expression to indicate the type of the element, just assign it to a capitalized variable first. this often comes up when you want to render a different component based on a prop:
to fix this, we will assign the type to a capitalized variable first:
props in jsx
there are several different ways to specify props in jsx.
javascript expressions as props
you can pass any javascript expression as a prop, by surrounding it with . for example, in this jsx:
for , the value of  will be  because the expression  gets evaluated.
statements and  loops are not expressions in javascript, so they can’t be used in jsx directly. instead, you can put these in the surrounding code. for example:
you can learn more about  and  in the corresponding sections.
string literals
you can pass a string literal as a prop. these two jsx expressions are equivalent:
when you pass a string literal, its value is html-unescaped. so these two jsx expressions are equivalent:
this behavior is usually not relevant. it’s only mentioned here for completeness.
props default to “true”
if you pass no value for a prop, it defaults to . these two jsx expressions are equivalent:
in general, we don’t recommend not passing a value for a prop, because it can be confused with the   which is short for  rather than . this behavior is just there so that it matches the behavior of html.
spread attributes
if you already have  as an object, and you want to pass it in jsx, you can use  as a “spread” syntax to pass the whole props object. these two components are equivalent:
you can also pick specific props that your component will consume while passing all other props using the spread syntax.
in the example above, the  prop is safely consumed and is not passed on to the  element in the dom.
all other props are passed via the  object making this component really flexible. you can see that it passes an  and  props.
spread attributes can be useful but they also make it easy to pass unnecessary props to components that don’t care about them or to pass invalid html attributes to the dom. we recommend using this syntax sparingly.
children in jsx
in jsx expressions that contain both an opening tag and a closing tag, the content between those tags is passed as a special prop: . there are several different ways to pass children:
you can put a string between the opening and closing tags and  will just be that string. this is useful for many of the built-in html elements. for example:
this is valid jsx, and  in  will simply be the string . html is unescaped, so you can generally write jsx just like you would write html in this way:
jsx removes whitespace at the beginning and ending of a line. it also removes blank lines. new lines adjacent to tags are removed; new lines that occur in the middle of string literals are condensed into a single space. so these all render to the same thing:
jsx children
you can provide more jsx elements as the children. this is useful for displaying nested components:
you can mix together different types of children, so you can use string literals together with jsx children. this is another way in which jsx is like html, so that this is both valid jsx and valid html:
a react component can also return an array of elements:
javascript expressions as children
you can pass any javascript expression as children, by enclosing it within . for example, these expressions are equivalent:
this is often useful for rendering a list of jsx expressions of arbitrary length. for example, this renders an html list:
javascript expressions can be mixed with other types of children. this is often useful in lieu of string templates:
functions as children
normally, javascript expressions inserted in jsx will evaluate to a string, a react element, or a list of those things. however,  works just like any other prop in that it can pass any sort of data, not just the sorts that react knows how to render. for example, if you have a custom component, you could have it take a callback as :
children passed to a custom component can be anything, as long as that component transforms them into something react can understand before rendering. this usage is not common, but it works if you want to stretch what jsx is capable of.
booleans, null, and undefined are ignored
, , , and  are valid children. they simply don’t render. these jsx expressions will all render to the same thing:
this can be useful to conditionally render react elements. this jsx renders the  component only if  is :
one caveat is that some , such as the  number, are still rendered by react. for example, this code will not behave as you might expect because  will be printed when  is an empty array:
to fix this, make sure that the expression before  is always boolean:
conversely, if you want a value like , , , or  to appear in the output, you have to  first:
optimizing performance
internally, react uses several clever techniques to minimize the number of costly dom operations required to update the ui. for many applications, using react will lead to a fast user interface without doing much work to specifically optimize for performance. nevertheless, there are several ways you can speed up your react application.
use the production build
if you’re benchmarking or experiencing performance problems in your react apps, make sure you’re testing with the minified production build.
by default, react includes many helpful warnings. these warnings are very useful in development. however, they make react larger and slower so you should make sure to use the production version when you deploy the app.
if you aren’t sure whether your build process is set up correctly, you can check it by installing . if you visit a site with react in production mode, the icon will have a dark background:
if you visit a site with react in development mode, the icon will have a red background:
it is expected that you use the development mode when working on your app, and the production mode when deploying your app to the users.
you can find instructions for building your app for production below.
if your project is built with , run:
this will create a production build of your app in the  folder of your project.
remember that this is only necessary before deploying to production. for normal development, use .
single-file builds
we offer production-ready versions of react and react dom as single files:
remember that only react files ending with  are suitable for production.
brunch
for the most efficient brunch production build, install the  plugin:
then, to create a production build, add the  flag to the  command:
remember that you only need to do this for production builds. you shouldn’t pass the  flag or apply this plugin in development, because it will hide useful react warnings and make the builds much slower.
browserify
for the most efficient browserify production build, install a few plugins:
to create a production build, make sure that you add these transforms (the order matters):
the  transform ensures the right build environment is set. make it global ().
the  transform removes development imports. make it global too ().
finally, the resulting bundle is piped to  for mangling ().
remember that you only need to do this for production builds. you shouldn’t apply these plugins in development because they will hide useful react warnings, and make the builds much slower.
rollup
for the most efficient rollup production build, install a few plugins:
to create a production build, make sure that you add these plugins (the order matters):
the  plugin ensures the right build environment is set.
the  plugin provides support for commonjs in rollup.
the  plugin compresses and mangles the final bundle.
for a complete setup example .
remember that you only need to do this for production builds. you shouldn’t apply the  plugin or the  plugin with  value in development because they will hide useful react warnings, and make the builds much slower.
webpack
if you’re using create react app, please follow .
this section is only relevant if you configure webpack directly.
webpack v4+ will minify your code by default in production mode.
you can learn more about this in .
remember that you only need to do this for production builds. you shouldn’t apply  in development because it will hide useful react warnings, and make the builds much slower.
profiling components with the devtools profiler
16.5+ and  0.57+ provide enhanced profiling capabilities in dev mode with the react devtools profiler.
an overview of the profiler can be found in the blog post .
a video walkthrough of the profiler is also .
if you haven’t yet installed the react devtools, you can find them here:
a production profiling bundle of  is also available as .
read more about how to use this bundle at
before react 17, we use the standard  to profile components with the chrome performance tab.
for a more detailed walkthrough, check out .
virtualize long lists
if your application renders long lists of data (hundreds or thousands of rows), we recommend using a technique known as “windowing”. this technique only renders a small subset of your rows at any given time, and can dramatically reduce the time it takes to re-render the components as well as the number of dom nodes created.
and  are popular windowing libraries. they provide several reusable components for displaying lists, grids, and tabular data. you can also create your own windowing component, like , if you want something more tailored to your application’s specific use case.
avoid reconciliation
react builds and maintains an internal representation of the rendered ui. it includes the react elements you return from your components. this representation lets react avoid creating dom nodes and accessing existing ones beyond necessity, as that can be slower than operations on javascript objects. sometimes it is referred to as a “virtual dom”, but it works the same way on react native.
when a component’s props or state change, react decides whether an actual dom update is necessary by comparing the newly returned element with the previously rendered one. when they are not equal, react will update the dom.
even though react only updates the changed dom nodes, re-rendering still takes some time. in many cases it’s not a problem, but if the slowdown is noticeable, you can speed all of this up by overriding the lifecycle function , which is triggered before the re-rendering process starts. the default implementation of this function returns , leaving react to perform the update:
if you know that in some situations your component doesn’t need to update, you can return  from  instead, to skip the whole rendering process, including calling  on this component and below.
in most cases, instead of writing  by hand, you can inherit from . it is equivalent to implementing  with a shallow comparison of current and previous props and state.
shouldcomponentupdate in action
here’s a subtree of components. for each one,  indicates what  returned, and  indicates whether the rendered react elements were equivalent. finally, the circle’s color indicates whether the component had to be reconciled or not.
since  returned  for the subtree rooted at c2, react did not attempt to render c2, and thus didn’t even have to invoke  on c4 and c5.
for c1 and c3,  returned , so react had to go down to the leaves and check them. for c6  returned , and since the rendered elements weren’t equivalent react had to update the dom.
the last interesting case is c8. react had to render this component, but since the react elements it returned were equal to the previously rendered ones, it didn’t have to update the dom.
note that react only had to do dom mutations for c6, which was inevitable. for c8, it bailed out by comparing the rendered react elements, and for c2’s subtree and c7, it didn’t even have to compare the elements as we bailed out on , and  was not called.
if the only way your component ever changes is when the  or the  variable changes, you could have  check that:
in this code,  is just checking if there is any change in  or . if those values don’t change, the component doesn’t update. if your component got more complex, you could use a similar pattern of doing a “shallow comparison” between all the fields of  and  to determine if the component should update. this pattern is common enough that react provides a helper to use this logic - just inherit from . so this code is a simpler way to achieve the same thing:
most of the time, you can use  instead of writing your own . it only does a shallow comparison, so you can’t use it if the props or state may have been mutated in a way that a shallow comparison would miss.
this can be a problem with more complex data structures. for example, let’s say you want a  component to render a comma-separated list of words, with a parent  component that lets you click a button to add a word to the list. this code does not work correctly:
the problem is that  will do a simple comparison between the old and new values of . since this code mutates the  array in the  method of , the old and new values of  will compare as equal, even though the actual words in the array have changed. the  will thus not update even though it has new words that should be rendered.
the power of not mutating data
the simplest way to avoid this problem is to avoid mutating values that you are using as props or state. for example, the  method above could be rewritten using  as:
es6 supports a  for arrays which can make this easier. if you’re using create react app, this syntax is available by default.
you can also rewrite code that mutates objects to avoid mutation, in a similar way. for example, let’s say we have an object named  and we want to write a function that changes  to be . we could write:
to write this without mutating the original object, we can use  method:
now returns a new object, rather than mutating the old one.  is in es6 and requires a polyfill.
makes it easier to update objects without mutation as well:
this feature was added to javascript in es2018.
if you’re using create react app, both  and the object spread syntax are available by default.
when you deal with deeply nested objects, updating them in an immutable way can feel convoluted. if you run into this problem, check out  or . these libraries let you write highly readable code without losing the benefits of immutability.is this page useful?
portals
portals provide a first-class way to render children into a dom node that exists outside the dom hierarchy of the parent component.
the first argument () is any , such as an element, string, or fragment. the second argument () is a dom element.
normally, when you return an element from a component’s render method, it’s mounted into the dom as a child of the nearest parent node:
however, sometimes it’s useful to insert a child into a different location in the dom:
a typical use case for portals is when a parent component has an  or  style, but you need the child to visually “break out” of its container. for example, dialogs, hovercards, and tooltips.
when working with portals, remember that  becomes very important.
for modal dialogs, ensure that everyone can interact with them by following the .
event bubbling through portals
even though a portal can be anywhere in the dom tree, it behaves like a normal react child in every other way. features like context work exactly the same regardless of whether the child is a portal, as the portal still exists in the react tree regardless of position in the dom tree.
this includes event bubbling. an event fired from inside a portal will propagate to ancestors in the containing react tree, even if those elements are not ancestors in the dom tree. assuming the following html structure:
a  component in  would be able to catch an uncaught, bubbling event from the sibling node .
catching an event bubbling up from a portal in a parent component allows the development of more flexible abstractions that are not inherently reliant on portals. for example, if you render a  component, the parent can capture its events regardless of whether it’s implemented using portals.is this page useful?
profiler api
the  measures how often a react application renders and what the “cost” of rendering is.
its purpose is to help identify parts of an application that are slow and may benefit from .
profiling adds some additional overhead, so it is disabled in .
to opt into production profiling, react provides a special production build with profiling enabled.
read more about how to use this build at
a  can be added anywhere in a react tree to measure the cost of rendering that part of the tree.
it requires two props: an  (string) and an  callback (function) which react calls any time a component within the tree “commits” an update.
for example, to profile a  component and its descendants:
multiple  components can be used to measure different parts of an application:
components can also be nested to measure different components within the same subtree:
although  is a light-weight component, it should be used only when necessary; each use adds some cpu and memory overhead to an application.
callback
the  requires an  function as a prop.
react calls this function any time a component within the profiled tree “commits” an update.
it receives parameters describing what was rendered and how long it took.
let’s take a closer look at each of the props:
the  prop of the  tree that has just committed.
this can be used to identify which part of the tree was committed if you are using multiple profilers.
identifies whether the tree has just been mounted for the first time or re-rendered due to a change in props, state, or hooks.
time spent rendering the  and its descendants for the current update.
this indicates how well the subtree makes use of memoization (e.g. , , ).
ideally this value should decrease significantly after the initial mount as many of the descendants will only need to re-render if their specific props change.
duration of the most recent  time for each individual component within the  tree.
this value estimates a worst-case cost of rendering (e.g. the initial mount or a tree with no memoization).
timestamp when react began rendering the current update.
timestamp when react committed the current update.
this value is shared between all profilers in a commit, enabling them to be grouped if desirable.
set of  that were being traced when the update was scheduled (e.g. when  or  were called).
interactions can be used to identify the cause of an update, although the api for tracing them is still experimental.
learn more about it at
react without es6
normally you would define a react component as a plain javascript class:
if you don’t use es6 yet, you may use the  module instead:
the api of es6 classes is similar to  with a few exceptions.
declaring default props
with functions and es6 classes  is defined as a property on the component itself:
with , you need to define  as a function on the passed object:
setting the initial state
in es6 classes, you can define the initial state by assigning  in the constructor:
with , you have to provide a separate  method that returns the initial state:
autobinding
in react components declared as es6 classes, methods follow the same semantics as regular es6 classes. this means that they don’t automatically bind  to the instance. you’ll have to explicitly use  in the constructor:
with , this is not necessary because it binds all methods:
this means writing es6 classes comes with a little more boilerplate code for event handlers, but the upside is slightly better performance in large applications.
if the boilerplate code is too unattractive to you, you may use  syntax:
you also have a few other options:
bind methods in the constructor.
use arrow functions, e.g. .
keep using .
mixins
es6 launched without any mixin support. therefore, there is no support for mixins when you use react with es6 classes.
we also found numerous issues in codebases using mixins, .
this section exists only for the reference.
sometimes very different components may share some common functionality. these are sometimes called .  lets you use a legacy  system for that.
one common use case is a component wanting to update itself on a time interval. it’s easy to use , but it’s important to cancel your interval when you don’t need it anymore to save memory. react provides  that let you know when a component is about to be created or destroyed. let’s create a simple mixin that uses these methods to provide an easy  function that will automatically get cleaned up when your component is destroyed.
if a component is using multiple mixins and several mixins define the same lifecycle method (i.e. several mixins want to do some cleanup when the component is destroyed), all of the lifecycle methods are guaranteed to be called. methods defined on mixins run in the order mixins were listed, followed by a method call on the component.is this page useful?
react without jsx
jsx is not a requirement for using react. using react without jsx is especially convenient when you don’t want to set up compilation in your build environment.
each jsx element is just syntactic sugar for calling . so, anything you can do with jsx can also be done with just plain javascript.
for example, this code written with jsx:
can be compiled to this code that does not use jsx:
if you’re curious to see more examples of how jsx is converted to javascript, you can try out .
the component can either be provided as a string, as a subclass of , or a plain function.
if you get tired of typing  so much, one common pattern is to assign a shorthand:
if you use this shorthand form for , it can be almost as convenient to use react without jsx.
alternatively, you can refer to community projects such as  and  which offer a terser syntax.is this page useful?
reconciliation
react provides a declarative api so that you don’t have to worry about exactly what changes on every update. this makes writing applications a lot easier, but it might not be obvious how this is implemented within react. this article explains the choices we made in react’s “diffing” algorithm so that component updates are predictable while being fast enough for high-performance apps.
when you use react, at a single point in time you can think of the  function as creating a tree of react elements. on the next state or props update, that  function will return a different tree of react elements. react then needs to figure out how to efficiently update the ui to match the most recent tree.
there are some generic solutions to this algorithmic problem of generating the minimum number of operations to transform one tree into another. however, the  have a complexity in the order of o(n3) where n is the number of elements in the tree.
if we used this in react, displaying 1000 elements would require in the order of one billion comparisons. this is far too expensive. instead, react implements a heuristic o(n) algorithm based on two assumptions:
two elements of different types will produce different trees.
the developer can hint at which child elements may be stable across different renders with a  prop.
in practice, these assumptions are valid for almost all practical use cases.
the diffing algorithm
when diffing two trees, react first compares the two root elements. the behavior is different depending on the types of the root elements.
elements of different types
whenever the root elements have different types, react will tear down the old tree and build the new tree from scratch. going from  to , or from  to , or from  to  - any of those will lead to a full rebuild.
when tearing down a tree, old dom nodes are destroyed. component instances receive . when building up a new tree, new dom nodes are inserted into the dom. component instances receive  and then . any state associated with the old tree is lost.
any components below the root will also get unmounted and have their state destroyed. for example, when diffing:
this will destroy the old  and remount a new one.
this method is considered legacy and you should  in new code:
dom elements of the same type
when comparing two react dom elements of the same type, react looks at the attributes of both, keeps the same underlying dom node, and only updates the changed attributes. for example:
by comparing these two elements, react knows to only modify the  on the underlying dom node.
when updating , react also knows to update only the properties that changed. for example:
when converting between these two elements, react knows to only modify the  style, not the .
after handling the dom node, react then recurses on the children.
component elements of the same type
when a component updates, the instance stays the same, so that state is maintained across renders. react updates the props of the underlying component instance to match the new element, and calls ,  and  on the underlying instance.
next, the  method is called and the diff algorithm recurses on the previous result and the new result.
these methods are considered legacy and you should  in new code:
recursing on children
by default, when recursing on the children of a dom node, react just iterates over both lists of children at the same time and generates a mutation whenever there’s a difference.
for example, when adding an element at the end of the children, converting between these two trees works well:
react will match the two  trees, match the two  trees, and then insert the  tree.
if you implement it naively, inserting an element at the beginning has worse performance. for example, converting between these two trees works poorly:
react will mutate every child instead of realizing it can keep the  and  subtrees intact. this inefficiency can be a problem.
in order to solve this issue, react supports a  attribute. when children have keys, react uses the key to match children in the original tree with children in the subsequent tree. for example, adding a  to our inefficient example above can make the tree conversion efficient:
now react knows that the element with key  is the new one, and the elements with the keys  and  have just moved.
in practice, finding a key is usually not hard. the element you are going to display may already have a unique id, so the key can just come from your data:
when that’s not the case, you can add a new id property to your model or hash some parts of the content to generate a key. the key only has to be unique among its siblings, not globally unique.
as a last resort, you can pass an item’s index in the array as a key. this can work well if the items are never reordered, but reorders will be slow.
reorders can also cause issues with component state when indexes are used as keys. component instances are updated and reused based on their key. if the key is an index, moving an item changes it. as a result, component state for things like uncontrolled inputs can get mixed up and updated in unexpected ways.
here is  on codepen, and here is .
tradeoffs
it is important to remember that the reconciliation algorithm is an implementation detail. react could rerender the whole app on every action; the end result would be the same. just to be clear, rerender in this context means calling  for all components, it doesn’t mean react will unmount and remount them. it will only apply the differences following the rules stated in the previous sections.
we are regularly refining the heuristics in order to make common use cases faster. in the current implementation, you can express the fact that a subtree has been moved amongst its siblings, but you cannot tell that it has moved somewhere else. the algorithm will rerender that full subtree.
because react relies on heuristics, if the assumptions behind them are not met, performance will suffer.
the algorithm will not try to match subtrees of different component types. if you see yourself alternating between two component types with very similar output, you may want to make it the same type. in practice, we haven’t found this to be an issue.
keys should be stable, predictable, and unique. unstable keys (like those produced by ) will cause many component instances and dom nodes to be unnecessarily recreated, which can cause performance degradation and lost state in child components.
refs and the dom
refs provide a way to access dom nodes or react elements created in the render method.
in the typical react dataflow,  are the only way that parent components interact with their children. to modify a child, you re-render it with new props. however, there are a few cases where you need to imperatively modify a child outside of the typical dataflow. the child to be modified could be an instance of a react component, or it could be a dom element. for both of these cases, react provides an escape hatch.
when to use refs
there are a few good use cases for refs:
managing focus, text selection, or media playback.
triggering imperative animations.
integrating with third-party dom libraries.
avoid using refs for anything that can be done declaratively.
for example, instead of exposing  and  methods on a  component, pass an  prop to it.
don’t overuse refs
your first inclination may be to use refs to “make things happen” in your app. if this is the case, take a moment and think more critically about where state should be owned in the component hierarchy. often, it becomes clear that the proper place to “own” that state is at a higher level in the hierarchy. see the  guide for examples of this.
the examples below have been updated to use the  api introduced in react 16.3. if you are using an earlier release of react, we recommend using  instead.
creating refs
refs are created using  and attached to react elements via the  attribute. refs are commonly assigned to an instance property when a component is constructed so they can be referenced throughout the component.
accessing refs
when a ref is passed to an element in , a reference to the node becomes accessible at the  attribute of the ref.
the value of the ref differs depending on the type of the node:
when the  attribute is used on an html element, the  created in the constructor with  receives the underlying dom element as its  property.
when the  attribute is used on a custom class component, the  object receives the mounted instance of the component as its .
you may not use the  attribute on function components because they don’t have instances.
the examples below demonstrate the differences.
adding a ref to a dom element
this code uses a  to store a reference to a dom node:
react will assign the  property with the dom element when the component mounts, and assign it back to  when it unmounts.  updates happen before  or  lifecycle methods.
adding a ref to a class component
if we wanted to wrap the  above to simulate it being clicked immediately after mounting, we could use a ref to get access to the custom input and call its  method manually:
note that this only works if  is declared as a class:
refs and function components
by default, you may not use the  attribute on function components because they don’t have instances:
if you want to allow people to take a  to your function component, you can use  (possibly in conjunction with ), or you can convert the component to a class.
you can, however, use the  attribute inside a function component as long as you refer to a dom element or a class component:
exposing dom refs to parent components
in rare cases, you might want to have access to a child’s dom node from a parent component. this is generally not recommended because it breaks component encapsulation, but it can occasionally be useful for triggering focus or measuring the size or position of a child dom node.
while you could , this is not an ideal solution, as you would only get a component instance rather than a dom node. additionally, this wouldn’t work with function components.
if you use react 16.3 or higher, we recommend to use  for these cases. ref forwarding lets components opt into exposing any child component’s ref as their own. you can find a detailed example of how to expose a child’s dom node to a parent component .
if you use react 16.2 or lower, or if you need more flexibility than provided by ref forwarding, you can use  and explicitly pass a ref as a differently named prop.
when possible, we advise against exposing dom nodes, but it can be a useful escape hatch. note that this approach requires you to add some code to the child component. if you have absolutely no control over the child component implementation, your last option is to use , but it is discouraged and deprecated in .
callback refs
react also supports another way to set refs called “callback refs”, which gives more fine-grain control over when refs are set and unset.
instead of passing a  attribute created by , you pass a function. the function receives the react component instance or html dom element as its argument, which can be stored and accessed elsewhere.
the example below implements a common pattern: using the  callback to store a reference to a dom node in an instance property.
react will call the  callback with the dom element when the component mounts, and call it with  when it unmounts. refs are guaranteed to be up-to-date before  or  fires.
you can pass callback refs between components like you can with object refs that were created with .
in the example above,  passes its ref callback as an  prop to the , and the  passes the same function as a special  attribute to the . as a result,  in  will be set to the dom node corresponding to the  element in the .
legacy api: string refs
if you worked with react before, you might be familiar with an older api where the  attribute is a string, like , and the dom node is accessed as . we advise against it because string refs have , are considered legacy, and are likely to be removed in one of the future releases.
if you’re currently using  to access refs, we recommend using either the  or the  instead.
caveats with callback refs
if the  callback is defined as an inline function, it will get called twice during updates, first with  and then again with the dom element. this is because a new instance of the function is created with each render, so react needs to clear the old ref and set up the new one. you can avoid this by defining the  callback as a bound method on the class, but note that it shouldn’t matter in most cases.is this page useful?
render props
the term  refers to a technique for sharing code between react components using a prop whose value is a function.
a component with a render prop takes a function that returns a react element and calls it instead of implementing its own render logic.
libraries that use render props include ,  and .
in this document, we’ll discuss why render props are useful, and how to write your own.
use render props for cross-cutting concerns
components are the primary unit of code reuse in react, but it’s not always obvious how to share the state or behavior that one component encapsulates to other components that need that same state.
for example, the following component tracks the mouse position in a web app:
as the cursor moves around the screen, the component displays its (x, y) coordinates in a .
now the question is: how can we reuse this behavior in another component? in other words, if another component needs to know about the cursor position, can we encapsulate that behavior so that we can easily share it with that component?
since components are the basic unit of code reuse in react, let’s try refactoring the code a bit to use a  component that encapsulates the behavior we need to reuse elsewhere.
now the  component encapsulates all behavior associated with listening for  events and storing the (x, y) position of the cursor, but it’s not yet truly reusable.
for example, let’s say we have a  component that renders the image of a cat chasing the mouse around the screen. we might use a  prop to tell the component the coordinates of the mouse so it knows where to position the image on the screen.
as a first pass, you might try rendering the  inside ’s  method, like this:
this approach will work for our specific use case, but we haven’t achieved the objective of truly encapsulating the behavior in a reusable way. now, every time we want the mouse position for a different use case, we have to create a new component (i.e. essentially another ) that renders something specifically for that use case.
here’s where the render prop comes in: instead of hard-coding a  inside a  component, and effectively changing its rendered output, we can provide  with a function prop that it uses to dynamically determine what to render–a render prop.
now, instead of effectively cloning the  component and hard-coding something else in its  method to solve for a specific use case, we provide a  prop that  can use to dynamically determine what it renders.
more concretely, a render prop is a function prop that a component uses to know what to render.
this technique makes the behavior that we need to share extremely portable. to get that behavior, render a  with a  prop that tells it what to render with the current (x, y) of the cursor.
one interesting thing to note about render props is that you can implement most  (hoc) using a regular component with a render prop. for example, if you would prefer to have a  hoc instead of a  component, you could easily create one using a regular  with a render prop:
so using a render prop makes it possible to use either pattern.
using props other than
it’s important to remember that just because the pattern is called “render props” you don’t have to use a prop named  to use this pattern. in fact, .
although the examples above use , we could just as easily use the  prop!
and remember, the  prop doesn’t actually need to be named in the list of “attributes” in your jsx element. instead, you can put it directly inside the element!
you’ll see this technique used in the  api.
since this technique is a little unusual, you’ll probably want to explicitly state that  should be a function in your  when designing an api like this.
be careful when using render props with react.purecomponent
using a render prop can negate the advantage that comes from using  if you create the function inside a  method. this is because the shallow prop comparison will always return  for new props, and each  in this case will generate a new value for the render prop.
for example, continuing with our  component from above, if  were to extend  instead of , our example would look like this:
in this example, each time  renders, it generates a new function as the value of the  prop, thus negating the effect of  extending  in the first place!
to get around this problem, you can sometimes define the prop as an instance method, like so:
in cases where you cannot define the prop statically (e.g. because you need to close over the component’s props and/or state)  should extend  instead.is this page useful?
static type checking
static type checkers like  and  identify certain types of problems before you even run your code. they can also improve developer workflow by adding features like auto-completion. for this reason, we recommend using flow or typescript instead of  for larger code bases.
flow
is a static type checker for your javascript code. it is developed at facebook and is often used with react. it lets you annotate the variables, functions, and react components with a special type syntax, and catch mistakes early. you can read an  to learn its basics.
to use flow, you need to:
add flow to your project as a dependency.
ensure that flow syntax is stripped from the compiled code.
add type annotations and run flow to check them.
we will explain these steps below in detail.
adding flow to a project
first, navigate to your project directory in the terminal. you will need to run the following command:
if you use , run:
this command installs the latest version of flow into your project.
now, add  to the  section of your  to be able to use this from the terminal:
finally, run one of the following commands:
this command will create a flow configuration file that you will need to commit.
stripping flow syntax from the compiled code
flow extends the javascript language with a special syntax for type annotations. however, browsers aren’t aware of this syntax, so we need to make sure it doesn’t end up in the compiled javascript bundle that is sent to the browser.
the exact way to do this depends on the tools you use to compile javascript.
if your project was set up using , congratulations! the flow annotations are already being stripped by default so you don’t need to do anything else in this step.
babel
these instructions are not for create react app users. even though create react app uses babel under the hood, it is already configured to understand flow. only follow this step if you don’t use create react app.
if you manually configured babel for your project, you will need to install a special preset for flow.
if you use yarn, run:
if you use npm, run:
then add the  preset to your . for example, if you configure babel through  file, it could look like this:
this will let you use the flow syntax in your code.
flow does not require the  preset, but they are often used together. flow itself understands jsx syntax out of the box.
other build setups
if you don’t use either create react app or babel, you can use  to strip the type annotations.
running flow
if you followed the instructions above, you should be able to run flow for the first time.
you should see a message like:
adding flow type annotations
by default, flow only checks the files that include this annotation:
typically it is placed at the top of a file. try adding it to some files in your project and run  or  to see if flow already found any issues.
there is also  to force flow to check all files regardless of the annotation. this can be too noisy for existing projects, but is reasonable for a new project if you want to fully type it with flow.
now you’re all set! we recommend to check out the following resources to learn more about flow:
is a programming language developed by microsoft. it is a typed superset of javascript, and includes its own compiler. being a typed language, typescript can catch errors and bugs at build time, long before your app goes live. you can learn more about using typescript with react .
to use typescript, you need to:
add typescript as a dependency to your project
configure the typescript compiler options
use the right file extensions
add definitions for libraries you use
let’s go over these in detail.
using typescript with create react app
create react app supports typescript out of the box.
to create a new project with typescript support, run:
you can also add it to an existing create react app project, .
if you use create react app, you can skip the rest of this page. it describes the manual setup which doesn’t apply to create react app users.
adding typescript to a project
it all begins with running one command in your terminal.
congrats! you’ve installed the latest version of typescript into your project. installing typescript gives us access to the  command. before configuration, let’s add  to the “scripts” section in our :
configuring the typescript compiler
the compiler is of no help to us until we tell it what to do. in typescript, these rules are defined in a special file called . to generate this file:
looking at the now generated , you can see that there are many options you can use to configure the compiler. for a detailed description of all the options, check .
of the many options, we’ll look at  and . in its true fashion, the compiler will take in typescript files and generate javascript files. however we don’t want to get confused with our source files and the generated output.
we’ll address this in two steps:
firstly, let’s arrange our project structure like this. we’ll place all our source code in the  directory.
next, we’ll tell the compiler where our source code is and where the output should go.
great! now when we run our build script the compiler will output the generated javascript to the  folder. the  provides a  with a good set of rules to get you started.
generally, you don’t want to keep the generated javascript in your source control, so be sure to add the build folder to your .
file extensions
in react, you most likely write your components in a  file. in typescript we have 2 file extensions:
is the default file extension while  is a special extension used for files which contain .
running typescript
if you followed the instructions above, you should be able to run typescript for the first time.
if you see no output, it means that it completed successfully.
type definitions
to be able to show errors and hints from other packages, the compiler relies on declaration files. a declaration file provides all the type information about a library. this enables us to use javascript libraries like those on npm in our project.
there are two main ways to get declarations for a library:
bundled - the library bundles its own declaration file. this is great for us, since all we need to do is install the library, and we can use it right away. to check if a library has bundled types, look for an  file in the project. some libraries will have it specified in their  under the  or  field.
- definitelytyped is a huge repository of declarations for libraries that don’t bundle a declaration file. the declarations are crowd-sourced and managed by microsoft and open source contributors. react for example doesn’t bundle its own declaration file. instead we can get it from definitelytyped. to do so enter this command in your terminal.
local declarations
sometimes the package that you want to use doesn’t bundle declarations nor is it available on definitelytyped. in that case, we can have a local declaration file. to do this, create a  file in the root of your source directory. a simple declaration could look like this:
you are now ready to code! we recommend to check out the following resources to learn more about typescript:
rescript
is a typed language that compiles to javascript. some of its core features are  guaranteed 100% type coverage, first-class jsx support and  to allow integration in existing js / ts react codebases.
you can find more infos on integrating rescript in your existing js / react codebase .
kotlin
is a statically typed language developed by jetbrains. its target platforms include the jvm, android, llvm, and .
jetbrains develops and maintains several tools specifically for the react community:  as well as . the latter helps you start building react apps with kotlin with no build configuration.
other languages
note there are other statically typed languages that compile to javascript and are thus react compatible. for example,  with . check out their respective sites for more information, and feel free to add more statically typed languages that work with react to this page!is this page useful?
strict mode
is a tool for highlighting potential problems in an application. like ,  does not render any visible ui. it activates additional checks and warnings for its descendants.
strict mode checks are run in development mode only; they do not impact the production build.
you can enable strict mode for any part of your application. for example:
in the above example, strict mode checks will not be run against the  and  components. however,  and , as well as all of their descendants, will have the checks.
currently helps with:
additional functionality will be added with future releases of react.
identifying unsafe lifecycles
as explained , certain legacy lifecycle methods are unsafe for use in async react applications. however, if your application uses third party libraries, it can be difficult to ensure that these lifecycles aren’t being used. fortunately, strict mode can help with this!
when strict mode is enabled, react compiles a list of all class components using the unsafe lifecycles, and logs a warning message with information about these components, like so:
addressing the issues identified by strict mode now will make it easier for you to take advantage of concurrent rendering in future releases of react.
warning about legacy string ref api usage
previously, react provided two ways for managing refs: the legacy string ref api and the callback api. although the string ref api was the more convenient of the two, it had  and so our official recommendation was to .
react 16.3 added a third option that offers the convenience of a string ref without any of the downsides:
since object refs were largely added as a replacement for string refs, strict mode now warns about usage of string refs.
callback refs will continue to be supported in addition to the new  api.
you don’t need to replace callback refs in your components. they are slightly more flexible, so they will remain as an advanced feature.
warning about deprecated finddomnode usage
react used to support  to search the tree for a dom node given a class instance. normally you don’t need this because you can .
can also be used on class components but this was breaking abstraction levels by allowing a parent to demand that certain children were rendered. it creates a refactoring hazard where you can’t change the implementation details of a component because a parent might be reaching into its dom node.  only returns the first child, but with the use of fragments, it is possible for a component to render multiple dom nodes.  is a one time read api. it only gave you an answer when you asked for it. if a child component renders a different node, there is no way to handle this change. therefore  only worked if components always return a single dom node that never changes.
you can instead make this explicit by passing a ref to your custom component and pass that along to the dom using .
you can also add a wrapper dom node in your component and attach a ref directly to it.
in css, the  attribute can be used if you don’t want the node to be part of the layout.
detecting unexpected side effects
conceptually, react does work in two phases:
the render phase determines what changes need to be made to e.g. the dom. during this phase, react calls  and then compares the result to the previous render.
the commit phase is when react applies any changes. (in the case of react dom, this is when react inserts, updates, and removes dom nodes.) react also calls lifecycles like  and  during this phase.
the commit phase is usually very fast, but rendering can be slow. for this reason, the upcoming concurrent mode (which is not enabled by default yet) breaks the rendering work into pieces, pausing and resuming the work to avoid blocking the browser. this means that react may invoke render phase lifecycles more than once before committing, or it may invoke them without committing at all (because of an error or a higher priority interruption).
render phase lifecycles include the following class component methods:
(or )
updater functions (the first argument)
because the above methods might be called more than once, it’s important that they do not contain side-effects. ignoring this rule can lead to a variety of problems, including memory leaks and invalid application state. unfortunately, it can be difficult to detect these problems as they can often be .
strict mode can’t automatically detect side effects for you, but it can help you spot them by making them a little more deterministic. this is done by intentionally double-invoking the following functions:
class component , , and  methods
class component static  method
function component bodies
state updater functions (the first argument to )
functions passed to , , or
this only applies to development mode. lifecycles will not be double-invoked in production mode.
for example, consider the following code:
at first glance, this code might not seem problematic. but if  is not , then instantiating this component multiple times could lead to invalid application state. this sort of subtle bug might not manifest during development, or it might do so inconsistently and so be overlooked.
by intentionally double-invoking methods like the component constructor, strict mode makes patterns like this easier to spot.
in react 17, react automatically modifies the console methods like  to silence the logs in the second call to lifecycle functions. however, it may cause undesired behavior in certain cases where .
starting from react 18, react does not suppress any logs. however, if you have react devtools installed, the logs from the second call will appear slightly dimmed. react devtools also offers a setting (off by default) to suppress them completely.
detecting legacy context api
the legacy context api is error-prone, and will be removed in a future major version. it still works for all 16.x releases but will show this warning message in strict mode:
read the  to help migrate to the new version.
ensuring reusable state
in the future, we’d like to add a feature that allows react to add and remove sections of the ui while preserving state. for example, when a user tabs away from a screen and back, react should be able to immediately show the previous screen. to do this, react will support remounting trees using the same component state used before unmounting.
this feature will give react better performance out-of-the-box, but requires components to be resilient to effects being mounted and destroyed multiple times. most effects will work without any changes, but some effects do not properly clean up subscriptions in the destroy callback, or implicitly assume they are only mounted or destroyed once.
to help surface these issues, react 18 introduces a new development-only check to strict mode. this new check will automatically unmount and remount every component, whenever a component mounts for the first time, restoring the previous state on the second mount.
to demonstrate the development behavior you’ll see in strict mode with this feature, consider what happens when react mounts a new component. without this change, when a component mounts, react creates the effects:
with strict mode starting in react 18, whenever a component mounts in development, react will simulate immediately unmounting and remounting the component:
on the second mount, react will restore the state from the first mount. this feature simulates user behavior such as a user tabbing away from a screen and back, ensuring that code will properly handle state restoration.
when the component unmounts, effects are destroyed as normal:
unmounting and remounting includes:
this only applies to development mode, production behavior is unchanged.
for help supporting common issues, see:
typechecking with proptypes
has moved into a different package since react v15.5. please use .
we provide  to automate the conversion.
as your app grows, you can catch a lot of bugs with typechecking. for some applications, you can use javascript extensions like  or  to typecheck your whole application. but even if you don’t use those, react has some built-in typechecking abilities. to run typechecking on the props for a component, you can assign the special  property:
in this example, we are using a class component, but the same functionality could also be applied to function components, or components created by  or .
exports a range of validators that can be used to make sure the data you receive is valid. in this example, we’re using . when an invalid value is provided for a prop, a warning will be shown in the javascript console. for performance reasons,  is only checked in development mode.
proptypes
here is an example documenting the different validators provided:
requiring single child
with  you can specify that only a single child can be passed to a component as children.
default prop values
you can define default values for your  by assigning to the special  property:
since es2022 you can also declare  as static property within a react component class. for more information, see the . this modern syntax will require a compilation step to work within older browsers.
the  will be used to ensure that  will have a value if it was not specified by the parent component. the  typechecking happens after  are resolved, so typechecking will also apply to the .
function components
if you are using function components in your regular development, you may want to make some small changes to allow proptypes to be properly applied.
let’s say you have a component like this:
to add proptypes, you may want to declare the component in a separate function before exporting, like this:
then, you can add proptypes directly to the :
uncontrolled components
in most cases, we recommend using  to implement forms. in a controlled component, form data is handled by a react component. the alternative is uncontrolled components, where form data is handled by the dom itself.
to write an uncontrolled component, instead of writing an event handler for every state update, you can  to get form values from the dom.
for example, this code accepts a single name in an uncontrolled component:
since an uncontrolled component keeps the source of truth in the dom, it is sometimes easier to integrate react and non-react code when using uncontrolled components. it can also be slightly less code if you want to be quick and dirty. otherwise, you should usually use controlled components.
if it’s still not clear which type of component you should use for a particular situation, you might find  to be helpful.
default values
in the react rendering lifecycle, the  attribute on form elements will override the value in the dom. with an uncontrolled component, you often want react to specify the initial value, but leave subsequent updates uncontrolled. to handle this case, you can specify a  attribute instead of . changing the value of  attribute after a component has mounted will not cause any update of the value in the dom.
likewise,  and  support , and  and  supports .
in react, an  is always an uncontrolled component because its value can only be set by a user, and not programmatically.
you should use the file api to interact with the files. the following example shows how to create a  to access file(s) in a submit handler:
web components
react and  are built to solve different problems.  web components provide strong encapsulation for reusable components, while react provides a declarative library that keeps the dom in sync with your data. the two goals are complementary. as a developer, you are free to use react in your web components, or to use web components in react, or both.
most people who use react don’t use web components, but you may want to, especially if you are using third-party ui components that are written using web components.
using web components in react
web components often expose an imperative api. for instance, a  web component might expose  and  functions. to access the imperative apis of a web component, you will need to use a ref to interact with the dom node directly. if you are using third-party web components, the best solution is to write a react component that behaves as a wrapper for your web component.
events emitted by a web component may not properly propagate through a react render tree.
you will need to manually attach event handlers to handle these events within your react components.
one common confusion is that web components use “class” instead of “classname”.
using react in your web components
this code will not work if you transform classes with babel. see  for the discussion.
include the  before you load your web components to fix this issue.
react top-level api
is the entry point to the react library. if you load react from a  tag, these top-level apis are available on the  global. if you use es6 with npm, you can write . if you use es5 with npm, you can write .
overview
components
react components let you split the ui into independent, reusable pieces, and think about each piece in isolation. react components can be defined by subclassing  or .
if you don’t use es6 classes, you may use the  module instead. see  for more information.
react components can also be defined as functions which can be wrapped:
creating react elements
we recommend  to describe what your ui should look like. each jsx element is just syntactic sugar for calling . you will not typically invoke the following methods directly if you are using jsx.
see  for more information.
transforming elements
provides several apis for manipulating elements:
also provides a component for rendering multiple elements without a wrapper.
refs
suspense
suspense lets components “wait” for something before rendering. today, suspense only supports one use case: . in the future, it will support other use cases like data fetching.
transitions
transitions are a new concurrent feature introduced in react 18. they allow you to mark updates as transitions, which tells react that they can be interrupted and avoid going back to suspense fallbacks for already visible content.
hooks
hooks are a new addition in react 16.8. they let you use state and other react features without writing a class. hooks have a  and a separate api reference:
reference
is the base class for react components when they are defined using :
see the  for a list of methods and properties related to the base  class.
is similar to . the difference between them is that  doesn’t implement , but  implements it with a shallow prop and state comparison.
if your react component’s  function renders the same result given the same props and state, you can use  for a performance boost in some cases.
’s  only shallowly compares the objects. if these contain complex data structures, it may produce false-negatives for deeper differences. only extend  when you expect to have simple props and state, or use  when you know deep data structures have changed. or, consider using  to facilitate fast comparisons of nested data.
furthermore, ’s  skips prop updates for the whole component subtree. make sure all the children components are also “pure”.
is a .
if your component renders the same result given the same props, you can wrap it in a call to  for a performance boost in some cases by memoizing the result. this means that react will skip rendering the component, and reuse the last rendered result.
only checks for prop changes. if your function component wrapped in  has a ,  or  hook in its implementation, it will still rerender when state or context change.
by default it will only shallowly compare complex objects in the props object. if you want control over the comparison, you can also provide a custom comparison function as the second argument.
this method only exists as a . do not rely on it to “prevent” a render, as this can lead to bugs.
unlike the  method on class components, the  function returns  if the props are equal and  if the props are not equal. this is the inverse from .
create and return a new  of the given type. the type argument can be either a tag name string (such as  or ), a  type (a class or a function), or a  type.
code written with  will be converted to use . you will not typically invoke  directly if you are using jsx. see  to learn more.
clone and return a new react element using  as the starting point.  should contain all new props, , or . the resulting element will have the original element’s props with the new props merged in shallowly. new children will replace existing children.  and  from the original element will be preserved if no  and  present in the .
is almost equivalent to:
however, it also preserves s. this means that if you get a child with a  on it, you won’t accidentally steal it from your ancestor. you will get the same  attached to your new element. the new  or  will replace old ones if present.
this api was introduced as a replacement of the deprecated .
return a function that produces react elements of a given type. like , the type argument can be either a tag name string (such as  or ), a  type (a class or a function), or a  type.
this helper is considered legacy, and we encourage you to either use jsx or use  directly instead.
you will not typically invoke  directly if you are using jsx. see  to learn more.
verifies the object is a react element. returns  or .
provides utilities for dealing with the  opaque data structure.
invokes a function on every immediate child contained within  with  set to . if  is an array it will be traversed and the function will be called for each child in the array. if children is  or , this method will return  or  rather than an array.
if  is a  it will be treated as a single child and not traversed.
like  but does not return an array.
returns the total number of components in , equal to the number of times that a callback passed to  or  would be invoked.
verifies that  has only one child (a react element) and returns it. otherwise this method throws an error.
does not accept the return value of  because it is an array rather than a react element.
returns the  opaque data structure as a flat array with keys assigned to each child. useful if you want to manipulate collections of children in your render methods, especially if you want to reorder or slice  before passing it down.
changes keys to preserve the semantics of nested arrays when flattening lists of children. that is,  prefixes each key in the returned array so that each element’s key is scoped to the input array containing it.
the  component lets you return multiple elements in a  method without creating an additional dom element:
you can also use it with the shorthand  syntax. for more information, see .
creates a  that can be attached to react elements via the ref attribute.
creates a react component that forwards the  attribute it receives to another component below in the tree. this technique is not very common but is particularly useful in two scenarios:
accepts a rendering function as an argument. react will call this function with  and  as two arguments. this function should return a react node.
in the above example, react passes a  given to  element as a second argument to the rendering function inside the  call. this rendering function passes the  to the  element.
as a result, after react attaches the ref,  will point directly to the  dom element instance.
lets you define a component that is loaded dynamically. this helps reduce the bundle size to delay loading components that aren’t used during the initial render.
you can learn how to use it from our . you might also want to check out  explaining how to use it in more detail.
note that rendering  components requires that there’s a  component higher in the rendering tree. this is how you specify a loading indicator.
lets you specify the loading indicator in case some components in the tree below it are not yet ready to render. in the future we plan to let  handle more scenarios such as data fetching. you can read about this in .
today, lazy loading components is the only use case supported by :
it is documented in our . note that  components can be deep inside the  tree — it doesn’t have to wrap every one of them. the best practice is to place  where you want to see a loading indicator, but to use  wherever you want to do code splitting.
for content that is already shown to the user, switching back to a loading indicator can be disorienting. it is sometimes better to show the “old” ui while the new ui is being prepared. to do this, you can use the new transition apis  and  to mark updates as transitions and avoid unexpected fallbacks.
in server side rendering
during server side rendering suspense boundaries allow you to flush your application in smaller chunks by suspending.
when a component suspends we schedule a low priority task to render the closest suspense boundary’s fallback. if the component unsuspends before we flush the fallback then we send down the actual content and throw away the fallback.
during hydration
suspense boundaries depend on their parent boundaries being hydrated before they can hydrate, but they can hydrate independently from sibling boundaries. events on a boundary before its hydrated will cause the boundary to hydrate at
a higher priority than neighboring boundaries.
lets you mark updates inside the provided callback as transitions. this method is designed to be used when  is not available.
updates in a transition yield to more urgent updates such as clicks.
updates in a transition will not show a fallback for re-suspended content, allowing the user to continue interacting while rendering the update.
does not provide an  flag. to track the pending status of a transition see .
reactdom
the  package provides dom-specific methods that can be used at the top level of your app and as an escape hatch to get outside the react model if you need to.
if you use es5 with npm, you can write:
the  package also provides modules specific to client and server apps:
the  package exports these methods:
these  methods are also exported, but are considered legacy:
both  and  have been replaced with new  in react 18. these methods will warn that your app will behave as if it’s running react 17 (learn more ).
browser support
react supports all modern browsers, although  for older versions.
we do not support older browsers that don’t support es5 methods or microtasks such as internet explorer. you may find that your apps do work in older browsers if polyfills such as  are included in the page, but you’re on your own if you choose to take this path.
creates a portal. portals provide a way to .
force react to flush any updates inside the provided callback synchronously. this ensures that the dom is updated immediately.
can significantly hurt performance. use sparingly.
may force pending suspense boundaries to show their  state.
may also run pending effects and synchronously apply any updates they contain before returning.
may also flush updates outside the callback when necessary to flush the updates inside the callback. for example, if there are pending updates from a click, react may flush those before flushing the updates inside the callback.
legacy reference
has been replaced with  in react 18. see  for more info.
render a react element into the dom in the supplied  and return a  to the component (or returns  for ).
if the react element was previously rendered into , this will perform an update on it and only mutate the dom as necessary to reflect the latest react element.
if the optional callback is provided, it will be executed after the component is rendered or updated.
controls the contents of the container node you pass in. any existing dom elements inside are replaced when first called. later calls use react’s dom diffing algorithm for efficient updates.
does not modify the container node (only modifies the children of the container). it may be possible to insert a component to an existing dom node without overwriting the existing children.
currently returns a reference to the root  instance. however, using this return value is legacy
and should be avoided because future versions of react may render components asynchronously in some cases. if you need a reference to the root  instance, the preferred solution is to attach a
to the root element.
using  to hydrate a server-rendered container is deprecated. use  instead.
same as , but is used to hydrate a container whose html contents were rendered by . react will attempt to attach event listeners to the existing markup.
react expects that the rendered content is identical between the server and the client. it can patch up differences in text content, but you should treat mismatches as bugs and fix them. in development mode, react warns about mismatches during hydration. there are no guarantees that attribute differences will be patched up in case of mismatches. this is important for performance reasons because in most apps, mismatches are rare, and so validating all markup would be prohibitively expensive.
if a single element’s attribute or text content is unavoidably different between the server and the client (for example, a timestamp), you may silence the warning by adding  to the element. it only works one level deep, and is intended to be an escape hatch. don’t overuse it. unless it’s text content, react still won’t attempt to patch it up, so it may remain inconsistent until future updates.
if you intentionally need to render something different on the server and the client, you can do a two-pass rendering. components that render something different on the client can read a state variable like , which you can set to  in . this way the initial render pass will render the same content as the server, avoiding mismatches, but an additional pass will happen synchronously right after hydration. note that this approach will make your components slower because they have to render twice, so use it with caution.
remember to be mindful of user experience on slow connections. the javascript code may load significantly later than the initial html render, so if you render something different in the client-only pass, the transition can be jarring. however, if executed well, it may be beneficial to render a “shell” of the application on the server, and only show some of the extra widgets on the client. to learn how to do this without getting the markup mismatch issues, refer to the explanation in the previous paragraph.
remove a mounted react component from the dom and clean up its event handlers and state. if no component was mounted in the container, calling this function does nothing. returns  if a component was unmounted and  if there was no component to unmount.
is an escape hatch used to access the underlying dom node. in most cases, use of this escape hatch is discouraged because it pierces the component abstraction.
if this component has been mounted into the dom, this returns the corresponding native browser dom element. this method is useful for reading values out of the dom, such as form field values and performing dom measurements. in most cases, you can attach a ref to the dom node and avoid using  at all.
when a component renders to  or ,  returns . when a component renders to a string,  returns a text dom node containing that value. as of react 16, a component may return a fragment with multiple children, in which case  will return the dom node corresponding to the first non-empty child.
only works on mounted components (that is, components that have been placed in the dom). if you try to call this on a component that has not been mounted yet (like calling  in  on a component that has yet to be created) an exception will be thrown.
cannot be used on function components.
reactdomclient
the  package provides client-specific methods used for initializing an app on the client. most of your components should not need to use this module.
the following methods can be used in client environments:
create a react root for the supplied  and return the root. the root can be used to render a react element into the dom with :
accepts two options:
: optional callback called when react automatically recovers from errors.
: optional prefix react uses for ids generated by . useful to avoid conflicts when using multiple roots on the same page. must be the same prefix used on the server.
the root can also be unmounted with :
controls the contents of the container node you pass in. any existing dom elements inside are replaced when render is called. later calls use react’s dom diffing algorithm for efficient updates.
using  to hydrate a server-rendered container is not supported. use  instead.
reactdomserver
the  object enables you to render components to static markup. typically, it’s used on a node server:
these methods are only available in the environments with :
(deprecated)
these methods are only available in the environments with  (this includes browsers, deno, and some modern edge runtimes):
the following methods can be used in the environments that don’t support streams:
render a react element to its initial html. returns a stream with a  method to pipe the output and  to abort the request. fully supports suspense and streaming of html with “delayed” content blocks “popping in” via inline  tags later.
if you call  on a node that already has this server-rendered markup, react will preserve it and only attach event handlers, allowing you to have a very performant first-load experience.
see the .
this is a node.js-specific api. environments with , like deno and modern edge runtimes, should use  instead.
streams a react element to its initial html. returns a promise that resolves to a . fully supports suspense and streaming of html.
this api depends on . for node.js, use  instead.
render a react element to its initial html. returns a  that outputs an html string. the html output by this stream is exactly equal to what  would return. you can use this method to generate html on the server and send the markup down on the initial request for faster page loads and to allow search engines to crawl your pages for seo purposes.
server-only. this api is not available in the browser.
the stream returned from this method will return a byte stream encoded in utf-8. if you need a stream in another encoding, take a look at a project like , which provides transform streams for transcoding text.
similar to , except this doesn’t create extra dom attributes that react uses internally, such as . this is useful if you want to use react as a simple static page generator, as stripping away the extra attributes can save some bytes.
the html output by this stream is exactly equal to what  would return.
if you plan to use react on the client to make the markup interactive, do not use this method. instead, use  on the server and  on the client.
render a react element to its initial html. react will return an html string. you can use this method to generate html on the server and send the markup down on the initial request for faster page loads and to allow search engines to crawl your pages for seo purposes.
this api has limited suspense support and does not support streaming.
on the server, it is recommended to use either  (for node.js) or  (for web streams) instead.
if you plan to use react on the client to make the markup interactive, do not use this method. instead, use  on the server and  on the client.is this page useful?
dom elements
react implements a browser-independent dom system for performance and cross-browser compatibility. we took the opportunity to clean up a few rough edges in browser dom implementations.
in react, all dom properties and attributes (including event handlers) should be camelcased. for example, the html attribute  corresponds to the attribute  in react. the exception is  and  attributes, which should be lowercased. for example, you can keep  as .
differences in attributes
there are a number of attributes that work differently between react and html:
checked
the  attribute is supported by  components of type  or . you can use it to set whether the component is checked. this is useful for building controlled components.  is the uncontrolled equivalent, which sets whether the component is checked when it is first mounted.
classname
to specify a css class, use the  attribute. this applies to all regular dom and svg elements like , , and others.
if you use react with web components (which is uncommon), use the  attribute instead.
dangerouslysetinnerhtml
is react’s replacement for using  in the browser dom. in general, setting html from code is risky because it’s easy to inadvertently expose your users to a  attack. so, you can set html directly from react, but you have to type out  and pass an object with a  key, to remind yourself that it’s dangerous. for example:
htmlfor
since  is a reserved word in javascript, react elements use  instead.
onchange
the  event behaves as you would expect it to: whenever a form field is changed, this event is fired. we intentionally do not use the existing browser behavior because  is a misnomer for its behavior and react relies on this event to handle user input in real time.
selected
if you want to mark an  as selected, reference the value of that option in the  of its  instead.
check out  for detailed instructions.
style
some examples in the documentation use  for convenience, but using the  attribute as the primary means of styling elements is generally not recommended. in most cases,  should be used to reference classes defined in an external css stylesheet.  is most often used in react applications to add dynamically-computed styles at render time. see also .
the  attribute accepts a javascript object with camelcased properties rather than a css string. this is consistent with the dom  javascript property, is more efficient, and prevents xss security holes. for example:
note that styles are not autoprefixed. to support older browsers, you need to supply corresponding style properties:
style keys are camelcased in order to be consistent with accessing the properties on dom nodes from js (e.g. ). vendor prefixes  should begin with a capital letter. this is why  has an uppercase “w”.
react will automatically append a “px” suffix to certain numeric inline style properties. if you want to use units other than “px”, specify the value as a string with the desired unit. for example:
not all style properties are converted to pixel strings though. certain ones remain unitless (eg , , ). a complete list of unitless properties can be seen .
suppresscontenteditablewarning
normally, there is a warning when an element with children is also marked as , because it won’t work. this attribute suppresses that warning. don’t use this unless you are building a library like  that manages  manually.
suppresshydrationwarning
if you use server-side react rendering, normally there is a warning when the server and the client render different content. however, in some rare cases, it is very hard or impossible to guarantee an exact match. for example, timestamps are expected to differ on the server and on the client.
if you set  to , react will not warn you about mismatches in the attributes and the content of that element. it only works one level deep, and is intended to be used as an escape hatch. don’t overuse it. you can read more about hydration in the .
value
the  attribute is supported by ,  and  components. you can use it to set the value of the component. this is useful for building controlled components.  is the uncontrolled equivalent, which sets the value of the component when it is first mounted.
all supported html attributes
as of react 16, any standard  dom attributes are fully supported.
react has always provided a javascript-centric api to the dom. since react components often take both custom and dom-related props, react uses the  convention just like the dom apis:
these props work similarly to the corresponding html attributes, with the exception of the special cases documented above.
some of the dom attributes supported by react include:
similarly, all svg attributes are fully supported:
you may also use custom attributes as long as they’re fully lowercase.is this page useful?
syntheticevent
this reference guide documents the  wrapper that forms part of react’s event system. see the  guide to learn more.
your event handlers will be passed instances of , a cross-browser wrapper around the browser’s native event. it has the same interface as the browser’s native event, including  and , except the events work identically across all browsers.
if you find that you need the underlying browser event for some reason, simply use the  attribute to get it. the synthetic events are different from, and do not map directly to, the browser’s native events. for example in   will point to a  event. the specific mapping is not part of the public api and may change at any time. every  object has the following attributes:
as of v17,  doesn’t do anything because the  is no longer .
as of v0.14, returning  from an event handler will no longer stop event propagation. instead,  or  should be triggered manually, as appropriate.
supported events
react normalizes events so that they have consistent properties across different browsers.
the event handlers below are triggered by an event in the bubbling phase. to register an event handler for the capture phase, append  to the event name; for example, instead of using , you would use  to handle the click event in the capture phase.
clipboard events
event names:
properties:
composition events
keyboard events
the  property can take any of the values documented in the .
focus events
these focus events work on all elements in the react dom, not just form elements.
onfocus
the  event is called when the element (or some element inside of it) receives focus. for example, it’s called when the user clicks on a text input.
onblur
the  event handler is called when focus has left the element (or left some element inside of it). for example, it’s called when the user clicks outside of a focused text input.
detecting focus entering and leaving
you can use the  and  to differentiate if the focusing or blurring events originated from outside of the parent element. here is a demo you can copy and paste that shows how to detect focusing a child, focusing the element itself, and focus entering or leaving the whole subtree.
form events
for more information about the onchange event, see .
generic events
mouse events
the  and  events propagate from the element being left to the one being entered instead of ordinary bubbling and do not have a capture phase.
pointer events
as defined in the , pointer events extend  with the following properties:
a note on cross-browser support:
pointer events are not yet supported in every browser (at the time of writing this article, supported browsers include: chrome, firefox, edge, and internet explorer). react deliberately does not polyfill support for other browsers because a standard-conform polyfill would significantly increase the bundle size of .
if your application requires pointer events, we recommend adding a third party pointer event polyfill.
selection events
touch events
ui events
starting with react 17, the  event does not bubble in react. this matches the browser behavior and prevents the confusion when a nested scrollable element fires events on a distant parent.
wheel events
media events
image events
animation events
transition events
other events
test utilities
importing
makes it easy to test react components in the testing framework of your choice. at facebook we use  for painless javascript testing. learn how to get started with jest through the jest website’s .
we recommend using  which is designed to enable and encourage writing tests that use your components as the end users do.
for react versions <= 16, the  library makes it easy to assert, manipulate, and traverse your react components’ output.
to prepare a component for assertions, wrap the code rendering it and performing updates inside an  call. this makes your test run closer to how react works in the browser.
if you use , it also provides an  export that behaves the same way.
for example, let’s say we have this  component:
here is how we can test it:
don’t forget that dispatching dom events only works when the dom container is added to the . you can use a library like  to reduce the boilerplate code.
the  document contains more details on how  behaves, with examples and usage.
pass a mocked component module to this method to augment it with useful methods that allow it to be used as a dummy react component. instead of rendering as usual, the component will become a simple  (or other tag if  is provided) containing any provided children.
is a legacy api. we recommend using  instead.
returns  if  is any react element.
returns  if  is a react element whose type is of a react .
returns  if  is a dom component (such as a  or ).
returns  if  is a user-defined component, such as a class or a function.
returns  if  is a component whose type is of a react .
traverse all components in  and accumulate all components where  is . this is not that useful on its own, but it’s used as a primitive for other test utils.
finds all dom elements of components in the rendered tree that are dom components with the class name matching .
like  but expects there to be one result, and returns that one result, or throws exception if there is any other number of matches besides one.
finds all dom elements of components in the rendered tree that are dom components with the tag name matching .
finds all instances of components with type equal to .
same as  but expects there to be one result and returns that one result, or throws exception if there is any other number of matches besides one.
render a react element into a detached dom node in the document. this function requires a dom. it is effectively equivalent to:
you will need to have ,  and  globally available before you import . otherwise react will think it can’t access the dom and methods like  won’t work.
other utilities
simulate an event dispatch on a dom node with optional  event data.
has a method for .
clicking an element
changing the value of an input field and then pressing enter.
you will have to provide any event property that you’re using in your component (e.g. keycode, which, etc…) as react is not creating any of these for you.
test renderer
this package provides a react renderer that can be used to render react components to pure javascript objects, without depending on the dom or a native mobile environment.
essentially, this package makes it easy to grab a snapshot of the platform view hierarchy (similar to a dom tree) rendered by a react dom or react native component without using a browser or .
you can use jest’s snapshot testing feature to automatically save a copy of the json tree to a file and check in your tests that it hasn’t changed: .
you can also traverse the output to find specific nodes and make assertions about them.
testrenderer
testrenderer instance
testinstance
create a  instance with the passed react element. it doesn’t use the real dom, but it still fully renders the component tree into memory so you can make assertions about it. returns a .
similar to the ,  prepares a component for assertions. use this version of  to wrap calls to  and .
return an object representing the rendered tree. this tree only contains the platform-specific nodes like  or  and their props, but doesn’t contain any user-written components. this is handy for .
return an object representing the rendered tree. the representation is more detailed than the one provided by , and includes the user-written components. you probably don’t need this method unless you’re writing your own assertion library on top of the test renderer.
re-render the in-memory tree with a new root element. this simulates a react update at the root. if the new element has the same type and key as the previous element, the tree will be updated; otherwise, it will re-mount a new tree.
unmount the in-memory tree, triggering the appropriate lifecycle events.
return the instance corresponding to the root element, if available. this will not work if the root element is a function component because they don’t have instances.
returns the root “test instance” object that is useful for making assertions about specific nodes in the tree. you can use it to find other “test instances” deeper below.
find a single descendant test instance for which  returns . if  does not return  for exactly one test instance, it will throw an error.
find a single descendant test instance with the provided . if there is not exactly one test instance with the provided , it will throw an error.
find all descendant test instances for which  returns .
find all descendant test instances with the provided .
the component instance corresponding to this test instance. it is only available for class components, as function components don’t have instances. it matches the  value inside the given component.
the component type corresponding to this test instance. for example, a  component has a type of .
the props corresponding to this test instance. for example, a  component has  as props.
the parent test instance of this test instance.
the children test instances of this test instance.
ideas
you can pass  function to  as the option, which allows for custom mock refs.
accepts the current element and should return a mock ref object.
this is useful when you test a component that relies on refs.
javascript environment requirements
react 18 supports all modern browsers (edge, firefox, chrome, safari, etc).
if you support older browsers and devices such as internet explorer which do not provide modern browser features natively or have non-compliant implementations, consider including a global polyfill in your bundled application.
here is a list of the modern features react 18 uses:
the correct polyfill for these features depend on your environment. for many users, you can configure your  settings. for others, you may need to import polyfills like  directly.is this page useful?
glossary of react terms
single-page application
a single-page application is an application that loads a single html page and all the necessary assets (such as javascript and css) required for the application to run. any interactions with the page or subsequent pages do not require a round trip to the server which means the page is not reloaded.
though you may build a single-page application in react, it is not a requirement. react can also be used for enhancing small parts of existing websites with additional interactivity. code written in react can coexist peacefully with markup rendered on the server by something like php, or with other client-side libraries. in fact, this is exactly how react is being used at facebook.
es6, es2015, es2016, etc
these acronyms all refer to the most recent versions of the ecmascript language specification standard, which the javascript language is an implementation of. the es6 version (also known as es2015) includes many additions to the previous versions such as: arrow functions, classes, template literals,  and  statements. you can learn more about specific versions .
compilers
a javascript compiler takes javascript code, transforms it and returns javascript code in a different format. the most common use case is to take es6 syntax and transform it into syntax that older browsers are capable of interpreting.  is the compiler most commonly used with react.
bundlers
bundlers take javascript and css code written as separate modules (often hundreds of them), and combine them together into a few files better optimized for the browsers. some bundlers commonly used in react applications include  and .
package managers
package managers are tools that allow you to manage dependencies in your project.  and  are two package managers commonly used in react applications. both of them are clients for the same npm package registry.
cdn stands for content delivery network. cdns deliver cached, static content from a network of servers across the globe.
jsx is a syntax extension to javascript. it is similar to a template language, but it has full power of javascript. jsx gets compiled to  calls which return plain javascript objects called “react elements”. to get a basic introduction to jsx  and find a more in-depth tutorial on jsx .
react dom uses camelcase property naming convention instead of html attribute names. for example,  becomes  in jsx. the attribute  is also written as  since  is a reserved word in javascript:
react elements are the building blocks of react applications. one might confuse elements with a more widely known concept of “components”. an element describes what you want to see on the screen. react elements are immutable.
typically, elements are not used directly, but get returned from components.
react components are small, reusable pieces of code that return a react element to be rendered to the page. the simplest version of react component is a plain javascript function that returns a react element:
components can also be es6 classes:
components can be broken down into distinct pieces of functionality and used within other components. components can return other components, arrays, strings and numbers. a good rule of thumb is that if a part of your ui is used several times (button, panel, avatar), or is complex enough on its own (app, feedstory, comment), it is a good candidate to be a reusable component. component names should also always start with a capital letter ( not ). see  for more information on rendering components.
are inputs to a react component. they are data passed down from a parent component to a child component.
remember that  are readonly. they should not be modified in any way:
if you need to modify some value in response to user input or a network response, use  instead.
is available on every component. it contains the content between the opening and closing tags of a component. for example:
the string  is available in  in the  component:
for components defined as classes, use :
a component needs  when some data associated with it changes over time. for example, a  component might need  in its state, and a  component might want to keep track of  in its state.
the most important difference between  and  is that  are passed from a parent component, but  is managed by the component itself. a component cannot change its , but it can change its .
for each particular piece of changing data, there should be just one component that “owns” it in its state. don’t try to synchronize states of two different components. instead,  to their closest shared ancestor, and pass it down as props to both of them.
lifecycle methods are custom functionality that gets executed during the different phases of a component. there are methods available when the component gets created and inserted into the dom (), when the component updates, and when the component gets unmounted or removed from the dom.
react has two different approaches to dealing with form inputs.
an input form element whose value is controlled by react is called a controlled component. when a user enters data into a controlled component a change event handler is triggered and your code decides whether the input is valid (by re-rendering with the updated value). if you do not re-render then the form element will remain unchanged.
an uncontrolled component works like form elements do outside of react. when a user inputs data into a form field (an input box, dropdown, etc) the updated information is reflected without react needing to do anything. however, this also means that you can’t force the field to have a certain value.
in most cases you should use controlled components.
a “key” is a special string attribute you need to include when creating arrays of elements. keys help react identify which items have changed, are added, or are removed. keys should be given to the elements inside an array to give the elements a stable identity.
keys only need to be unique among sibling elements in the same array. they don’t need to be unique across the whole application or even a single component.
don’t pass something like  to keys. it is important that keys have a “stable identity” across re-renders so that react can determine when items are added, removed, or re-ordered. ideally, keys should correspond to unique and stable identifiers coming from your data, such as .
react supports a special attribute that you can attach to any component. the  attribute can be an object created by  or a callback function, or a string (in legacy api). when the  attribute is a callback function, the function receives the underlying dom element or class instance (depending on the type of element) as its argument. this allows you to have direct access to the dom element or component instance.
use refs sparingly. if you find yourself often using refs to “make things happen” in your app, consider getting more familiar with .
handling events with react elements has some syntactic differences:
react event handlers are named using camelcase, rather than lowercase.
when a component’s props or state change, react decides whether an actual dom update is necessary by comparing the newly returned element with the previously rendered one. when they are not equal, react will update the dom. this process is called “reconciliation”.is this page useful?
introducing hooks
hooks are a new addition in react 16.8. they let you use state and other react features without writing a class.
this new function  is the first “hook” we’ll learn about, but this example is just a teaser. don’t worry if it doesn’t make sense yet!
you can start learning hooks . on this page, we’ll continue by explaining why we’re adding hooks to react and how they can help you write great applications.
react 16.8.0 is the first release to support hooks. when upgrading, don’t forget to update all packages, including react dom.
react native has supported hooks since .
video introduction
at react conf 2018, sophie alpert and dan abramov introduced hooks, followed by ryan florence demonstrating how to refactor an application to use them. watch the video here:
no breaking changes
before we continue, note that hooks are:
completely opt-in. you can try hooks in a few components without rewriting any existing code. but you don’t have to learn or use hooks right now if you don’t want to.
100% backwards-compatible. hooks don’t contain any breaking changes.
available now. hooks are now available with the release of v16.8.0.
there are no plans to remove classes from react. you can read more about the gradual adoption strategy for hooks in the  of this page.
hooks don’t replace your knowledge of react concepts. instead, hooks provide a more direct api to the react concepts you already know: props, state, context, refs, and lifecycle. as we will show later, hooks also offer a new powerful way to combine them.
if you just want to start learning hooks, feel free to  you can also keep reading this page to learn more about why we’re adding hooks, and how we’re going to start using them without rewriting our applications.
hooks solve a wide variety of seemingly unconnected problems in react that we’ve encountered over five years of writing and maintaining tens of thousands of components. whether you’re learning react, use it daily, or even prefer a different library with a similar component model, you might recognize some of these problems.
it’s hard to reuse stateful logic between components
react doesn’t offer a way to “attach” reusable behavior to a component (for example, connecting it to a store). if you’ve worked with react for a while, you may be familiar with patterns like  and  that try to solve this. but these patterns require you to restructure your components when you use them, which can be cumbersome and make code harder to follow. if you look at a typical react application in react devtools, you will likely find a “wrapper hell” of components surrounded by layers of providers, consumers, higher-order components, render props, and other abstractions. while we could , this points to a deeper underlying problem: react needs a better primitive for sharing stateful logic.
with hooks, you can extract stateful logic from a component so it can be tested independently and reused. hooks allow you to reuse stateful logic without changing your component hierarchy. this makes it easy to share hooks among many components or with the community.
we’ll discuss this more in .
complex components become hard to understand
we’ve often had to maintain components that started out simple but grew into an unmanageable mess of stateful logic and side effects. each lifecycle method often contains a mix of unrelated logic. for example, components might perform some data fetching in  and . however, the same  method might also contain some unrelated logic that sets up event listeners, with cleanup performed in . mutually related code that changes together gets split apart, but completely unrelated code ends up combined in a single method. this makes it too easy to introduce bugs and inconsistencies.
in many cases it’s not possible to break these components into smaller ones because the stateful logic is all over the place. it’s also difficult to test them. this is one of the reasons many people prefer to combine react with a separate state management library. however, that often introduces too much abstraction, requires you to jump between different files, and makes reusing components more difficult.
to solve this, hooks let you split one component into smaller functions based on what pieces are related (such as setting up a subscription or fetching data), rather than forcing a split based on lifecycle methods. you may also opt into managing the component’s local state with a reducer to make it more predictable.
classes confuse both people and machines
in addition to making code reuse and code organization more difficult, we’ve found that classes can be a large barrier to learning react. you have to understand how  works in javascript, which is very different from how it works in most languages. you have to remember to bind the event handlers. without , the code is very verbose. people can understand props, state, and top-down data flow perfectly well but still struggle with classes. the distinction between function and class components in react and when to use each one leads to disagreements even between experienced react developers.
additionally, react has been out for about five years, and we want to make sure it stays relevant in the next five years. as , , , and others show,  of components has a lot of future potential. especially if it’s not limited to templates. recently, we’ve been experimenting with  using , and we’ve seen promising early results. however, we found that class components can encourage unintentional patterns that make these optimizations fall back to a slower path. classes present issues for today’s tools, too. for example, classes don’t minify very well, and they make hot reloading flaky and unreliable. we want to present an api that makes it more likely for code to stay on the optimizable path.
to solve these problems, hooks let you use more of react’s features without classes. conceptually, react components have always been closer to functions. hooks embrace functions, but without sacrificing the practical spirit of react. hooks provide access to imperative escape hatches and don’t require you to learn complex functional or reactive programming techniques.
is a good place to start learning hooks.
gradual adoption strategy
tldr: there are no plans to remove classes from react.
we know that react developers are focused on shipping products and don’t have time to look into every new api that’s being released. hooks are very new, and it might be better to wait for more examples and tutorials before considering learning or adopting them.
we also understand that the bar for adding a new primitive to react is extremely high. for curious readers, we have prepared a  that dives into the motivation with more details, and provides extra perspective on the specific design decisions and related prior art.
crucially, hooks work side-by-side with existing code so you can adopt them gradually. there is no rush to migrate to hooks. we recommend avoiding any “big rewrites”, especially for existing, complex class components. it takes a bit of a mind shift to start “thinking in hooks”. in our experience, it’s best to practice using hooks in new and non-critical components first, and ensure that everybody on your team feels comfortable with them. after you give hooks a try, please feel free to , positive or negative.
we intend for hooks to cover all existing use cases for classes, but we will keep supporting class components for the foreseeable future. at facebook, we have tens of thousands of components written as classes, and we have absolutely no plans to rewrite them. instead, we are starting to use hooks in the new code side by side with classes.
we’ve prepared a  that answers the most common questions about hooks.
next steps
by the end of this page, you should have a rough idea of what problems hooks are solving, but many details are probably unclear. don’t worry! let’s now go to  where we start learning about hooks by example.is this page useful?
hooks at a glance
hooks are . this page provides an overview of hooks for experienced react users. this is a fast-paced overview. if you get confused, look for a yellow box like this:
detailed explanation
read the  to learn why we’re introducing hooks to react.
↑↑↑ each section ends with a yellow box like this. they link to detailed explanations.
📌 state hook
this example renders a counter. when you click the button, it increments the value:
here,  is a hook (we’ll talk about what this means in a moment). we call it inside a function component to add some local state to it. react will preserve this state between re-renders.  returns a pair: the current state value and a function that lets you update it. you can call this function from an event handler or somewhere else. it’s similar to  in a class, except it doesn’t merge the old and new state together. (we’ll show an example comparing  to  in .)
the only argument to  is the initial state. in the example above, it is  because our counter starts from zero. note that unlike , the state here doesn’t have to be an object — although it can be if you want. the initial state argument is only used during the first render.
declaring multiple state variables
you can use the state hook more than once in a single component:
the  syntax lets us give different names to the state variables we declared by calling . these names aren’t a part of the  api. instead, react assumes that if you call  many times, you do it in the same order during every render. we’ll come back to why this works and when this is useful later.
but what is a hook?
hooks are functions that let you “hook into” react state and lifecycle features from function components. hooks don’t work inside classes — they let you use react without classes. (we  rewriting your existing components overnight but you can start using hooks in the new ones if you’d like.)
react provides a few built-in hooks like . you can also create your own hooks to reuse stateful behavior between different components. we’ll look at the built-in hooks first.
you can learn more about the state hook on a dedicated page: .
⚡️ effect hook
you’ve likely performed data fetching, subscriptions, or manually changing the dom from react components before. we call these operations “side effects” (or “effects” for short) because they can affect other components and can’t be done during rendering.
the effect hook, , adds the ability to perform side effects from a function component. it serves the same purpose as , , and  in react classes, but unified into a single api. (we’ll show examples comparing  to these methods in .)
for example, this component sets the document title after react updates the dom:
when you call , you’re telling react to run your “effect” function after flushing changes to the dom. effects are declared inside the component so they have access to its props and state. by default, react runs the effects after every render — including the first render. (we’ll talk more about how this compares to class lifecycles in .)
effects may also optionally specify how to “clean up” after them by returning a function. for example, this component uses an effect to subscribe to a friend’s online status, and cleans up by unsubscribing from it:
in this example, react would unsubscribe from our  when the component unmounts, as well as before re-running the effect due to a subsequent render. (if you want, there’s a way to  if the  we passed to  didn’t change.)
just like with , you can use more than a single effect in a component:
hooks let you organize side effects in a component by what pieces are related (such as adding and removing a subscription), rather than forcing a split based on lifecycle methods.
you can learn more about  on a dedicated page: .
✌️ rules of hooks
hooks are javascript functions, but they impose two additional rules:
only call hooks at the top level. don’t call hooks inside loops, conditions, or nested functions.
only call hooks from react function components. don’t call hooks from regular javascript functions. (there is just one other valid place to call hooks — your own custom hooks. we’ll learn about them in a moment.)
we provide a  to enforce these rules automatically. we understand these rules might seem limiting or confusing at first, but they are essential to making hooks work well.
you can learn more about these rules on a dedicated page: .
💡 building your own hooks
sometimes, we want to reuse some stateful logic between components. traditionally, there were two popular solutions to this problem:  and . custom hooks let you do this, but without adding more components to your tree.
earlier on this page, we introduced a  component that calls the  and  hooks to subscribe to a friend’s online status. let’s say we also want to reuse this subscription logic in another component.
first, we’ll extract this logic into a custom hook called :
it takes  as an argument, and returns whether our friend is online.
now we can use it from both components:
the state of each component is completely independent. hooks are a way to reuse stateful logic, not state itself. in fact, each call to a hook has a completely isolated state — so you can even use the same custom hook twice in one component.
custom hooks are more of a convention than a feature. if a function’s name starts with ”” and it calls other hooks, we say it is a custom hook. the  naming convention is how our linter plugin is able to find bugs in the code using hooks.
you can write custom hooks that cover a wide range of use cases like form handling, animation, declarative subscriptions, timers, and probably many more we haven’t considered. we are excited to see what custom hooks the react community will come up with.
you can learn more about custom hooks on a dedicated page: .
🔌 other hooks
there are a few less commonly used built-in hooks that you might find useful. for example,  lets you subscribe to react context without introducing nesting:
and  lets you manage local state of complex components with a reducer:
you can learn more about all the built-in hooks on a dedicated page: .
phew, that was fast! if some things didn’t quite make sense or you’d like to learn more in detail, you can read the next pages, starting with the  documentation.
you can also check out the  and the .
finally, don’t miss the  which explains why we’re adding hooks and how we’ll start using them side by side with classes — without rewriting our apps.is this page useful?
using the state hook
the  used this example to get familiar with hooks:
we’ll start learning about hooks by comparing this code to an equivalent class example.
equivalent class example
if you used classes in react before, this code should look familiar:
the state starts as , and we increment  when the user clicks a button by calling . we’ll use snippets from this class throughout the page.
you might be wondering why we’re using a counter here instead of a more realistic example. this is to help us focus on the api while we’re still making our first steps with hooks.
hooks and function components
as a reminder, function components in react look like this:
or this:
you might have previously known these as “stateless components”. we’re now introducing the ability to use react state from these, so we prefer the name “function components”.
hooks don’t work inside classes. but you can use them instead of writing classes.
what’s a hook?
our new example starts by importing the  hook from react:
what is a hook? a hook is a special function that lets you “hook into” react features. for example,  is a hook that lets you add react state to function components. we’ll learn other hooks later.
when would i use a hook? if you write a function component and realize you need to add some state to it, previously you had to convert it to a class. now you can use a hook inside the existing function component. we’re going to do that right now!
there are some special rules about where you can and can’t use hooks within a component. we’ll learn them in .
declaring a state variable
in a class, we initialize the  state to  by setting  to  in the constructor:
in a function component, we have no , so we can’t assign or read . instead, we call the  hook directly inside our component:
what does calling  do? it declares a “state variable”. our variable is called  but we could call it anything else, like . this is a way to “preserve” some values between the function calls —  is a new way to use the exact same capabilities that  provides in a class. normally, variables “disappear” when the function exits but state variables are preserved by react.
what do we pass to  as an argument? the only argument to the  hook is the initial state. unlike with classes, the state doesn’t have to be an object. we can keep a number or a string if that’s all we need. in our example, we just want a number for how many times the user clicked, so pass  as initial state for our variable. (if we wanted to store two different values in state, we would call  twice.)
what does  return? it returns a pair of values: the current state and a function that updates it. this is why we write . this is similar to  and  in a class, except you get them in a pair. if you’re not familiar with the syntax we used, we’ll come back to it .
now that we know what the  hook does, our example should make more sense:
we declare a state variable called , and set it to . react will remember its current value between re-renders, and provide the most recent one to our function. if we want to update the current , we can call .
you might be wondering: why is  not named  instead?
“create” wouldn’t be quite accurate because the state is only created the first time our component renders. during the next renders,  gives us the current state. otherwise it wouldn’t be “state” at all! there’s also a reason why hook names always start with . we’ll learn why later in the .
reading state
when we want to display the current count in a class, we read :
in a function, we can use  directly:
updating state
in a class, we need to call  to update the  state:
in a function, we already have  and  as variables so we don’t need :
recap
let’s now recap what we learned line by line and check our understanding.
line 1: we import the  hook from react. it lets us keep local state in a function component.
line 4: inside the  component, we declare a new state variable by calling the  hook. it returns a pair of values, to which we give names. we’re calling our variable  because it holds the number of button clicks. we initialize it to zero by passing  as the only  argument. the second returned item is itself a function. it lets us update the  so we’ll name it .
line 9: when the user clicks, we call  with a new value. react will then re-render the  component, passing the new  value to it.
this might seem like a lot to take in at first. don’t rush it! if you’re lost in the explanation, look at the code above again and try to read it from top to bottom. we promise that once you try to “forget” how state works in classes, and look at this code with fresh eyes, it will make sense.
tip: what do square brackets mean?
you might have noticed the square brackets when we declare a state variable:
the names on the left aren’t a part of the react api. you can name your own state variables:
this javascript syntax is called . it means that we’re making two new variables  and , where  is set to the first value returned by , and  is the second. it is equivalent to this code:
when we declare a state variable with , it returns a pair — an array with two items. the first item is the current value, and the second is a function that lets us update it. using  and  to access them is a bit confusing because they have a specific meaning. this is why we use array destructuring instead.
you might be curious how react knows which component  corresponds to since we’re not passing anything like  back to react. we’ll answer  and many others in the faq section.
tip: using multiple state variables
declaring state variables as a pair of  is also handy because it lets us give different names to different state variables if we want to use more than one:
in the above component, we have , , and  as local variables, and we can update them individually:
you don’t have to use many state variables. state variables can hold objects and arrays just fine, so you can still group related data together. however, unlike  in a class, updating a state variable always replaces it instead of merging it.
we provide more recommendations on splitting independent state variables .
on this page we’ve learned about one of the hooks provided by react, called . we’re also sometimes going to refer to it as the “state hook”. it lets us add local state to react function components — which we did for the first time ever!
we also learned a little bit more about what hooks are. hooks are functions that let you “hook into” react features from function components. their names always start with , and there are more hooks we haven’t seen yet.
now let’s continue by  it lets you perform side effects in components, and is similar to lifecycle methods in classes.is this page useful?
using the effect hook
the effect hook lets you perform side effects in function components:
this snippet is based on the , but we added a new feature to it: we set the document title to a custom message including the number of clicks.
data fetching, setting up a subscription, and manually changing the dom in react components are all examples of side effects. whether or not you’re used to calling these operations “side effects” (or just “effects”), you’ve likely performed them in your components before.
if you’re familiar with react class lifecycle methods, you can think of  hook as , , and  combined.
there are two common kinds of side effects in react components: those that don’t require cleanup, and those that do. let’s look at this distinction in more detail.
effects without cleanup
sometimes, we want to run some additional code after react has updated the dom. network requests, manual dom mutations, and logging are common examples of effects that don’t require a cleanup. we say that because we can run them and immediately forget about them. let’s compare how classes and hooks let us express such side effects.
example using classes
in react class components, the  method itself shouldn’t cause side effects. it would be too early — we typically want to perform our effects after react has updated the dom.
this is why in react classes, we put side effects into  and . coming back to our example, here is a react counter class component that updates the document title right after react makes changes to the dom:
note how we have to duplicate the code between these two lifecycle methods in class.
this is because in many cases we want to perform the same side effect regardless of whether the component just mounted, or if it has been updated. conceptually, we want it to happen after every render — but react class components don’t have a method like this. we could extract a separate method but we would still have to call it in two places.
now let’s see how we can do the same with the  hook.
example using hooks
we’ve already seen this example at the top of this page, but let’s take a closer look at it:
what does  do? by using this hook, you tell react that your component needs to do something after render. react will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the dom updates. in this effect, we set the document title, but we could also perform data fetching or call some other imperative api.
why is  called inside a component? placing  inside the component lets us access the  state variable (or any props) right from the effect. we don’t need a special api to read it — it’s already in the function scope. hooks embrace javascript closures and avoid introducing react-specific apis where javascript already provides a solution.
does  run after every render? yes! by default, it runs both after the first render and after every update. (we will later talk about .) instead of thinking in terms of “mounting” and “updating”, you might find it easier to think that effects happen “after render”. react guarantees the dom has been updated by the time it runs the effects.
now that we know more about effects, these lines should make sense:
we declare the  state variable, and then we tell react we need to use an effect. we pass a function to the  hook. this function we pass is our effect. inside our effect, we set the document title using the  browser api. we can read the latest  inside the effect because it’s in the scope of our function. when react renders our component, it will remember the effect we used, and then run our effect after updating the dom. this happens for every render, including the first one.
experienced javascript developers might notice that the function passed to  is going to be different on every render. this is intentional. in fact, this is what lets us read the  value from inside the effect without worrying about it getting stale. every time we re-render, we schedule a different effect, replacing the previous one. in a way, this makes the effects behave more like a part of the render result — each effect “belongs” to a particular render. we will see more clearly why this is useful .
unlike  or , effects scheduled with  don’t block the browser from updating the screen. this makes your app feel more responsive. the majority of effects don’t need to happen synchronously. in the uncommon cases where they do (such as measuring the layout), there is a separate  hook with an api identical to .
effects with cleanup
earlier, we looked at how to express side effects that don’t require any cleanup. however, some effects do. for example, we might want to set up a subscription to some external data source. in that case, it is important to clean up so that we don’t introduce a memory leak! let’s compare how we can do it with classes and with hooks.
in a react class, you would typically set up a subscription in , and clean it up in . for example, let’s say we have a  module that lets us subscribe to a friend’s online status. here’s how we might subscribe and display that status using a class:
notice how  and  need to mirror each other. lifecycle methods force us to split this logic even though conceptually code in both of them is related to the same effect.
eagle-eyed readers may notice that this example also needs a  method to be fully correct. we’ll ignore this for now but will come back to it in a  of this page.
let’s see how we could write this component with hooks.
you might be thinking that we’d need a separate effect to perform the cleanup. but code for adding and removing a subscription is so tightly related that  is designed to keep it together. if your effect returns a function, react will run it when it is time to clean up:
why did we return a function from our effect? this is the optional cleanup mechanism for effects. every effect may return a function that cleans up after it. this lets us keep the logic for adding and removing subscriptions close to each other. they’re part of the same effect!
when exactly does react clean up an effect? react performs the cleanup when the component unmounts. however, as we learned earlier, effects run for every render and not just once. this is why react also cleans up effects from the previous render before running the effects next time. we’ll discuss  and  later below.
we don’t have to return a named function from the effect. we called it  here to clarify its purpose, but you could return an arrow function or call it something different.
we’ve learned that  lets us express different kinds of side effects after a component renders. some effects might require cleanup so they return a function:
other effects might not have a cleanup phase, and don’t return anything.
the effect hook unifies both use cases with a single api.
if you feel like you have a decent grasp on how the effect hook works, or if you feel overwhelmed, you can jump to the  now.
tips for using effects
we’ll continue this page with an in-depth look at some aspects of  that experienced react users will likely be curious about. don’t feel obligated to dig into them now. you can always come back to this page to learn more details about the effect hook.
tip: use multiple effects to separate concerns
one of the problems we outlined in the  for hooks is that class lifecycle methods often contain unrelated logic, but related logic gets broken up into several methods. here is a component that combines the counter and the friend status indicator logic from the previous examples:
note how the logic that sets  is split between  and . the subscription logic is also spread between  and . and  contains code for both tasks.
so, how can hooks solve this problem? just like , you can also use several effects. this lets us separate unrelated logic into different effects:
hooks let us split the code based on what it is doing rather than a lifecycle method name. react will apply every effect used by the component, in the order they were specified.
explanation: why effects run on each update
if you’re used to classes, you might be wondering why the effect cleanup phase happens after every re-render, and not just once during unmounting. let’s look at a practical example to see why this design helps us create components with fewer bugs.
, we introduced an example  component that displays whether a friend is online or not. our class reads  from , subscribes to the friend status after the component mounts, and unsubscribes during unmounting:
but what happens if the  prop changes while the component is on the screen? our component would continue displaying the online status of a different friend. this is a bug. we would also cause a memory leak or crash when unmounting since the unsubscribe call would use the wrong friend id.
in a class component, we would need to add  to handle this case:
forgetting to handle  properly is a common source of bugs in react applications.
now consider the version of this component that uses hooks:
it doesn’t suffer from this bug. (but we also didn’t make any changes to it.)
there is no special code for handling updates because  handles them by default. it cleans up the previous effects before applying the next effects. to illustrate this, here is a sequence of subscribe and unsubscribe calls that this component could produce over time:
this behavior ensures consistency by default and prevents bugs that are common in class components due to missing update logic.
tip: optimizing performance by skipping effects
in some cases, cleaning up or applying the effect after every render might create a performance problem. in class components, we can solve this by writing an extra comparison with  or  inside :
this requirement is common enough that it is built into the  hook api. you can tell react to skip applying an effect if certain values haven’t changed between re-renders. to do so, pass an array as an optional second argument to :
in the example above, we pass  as the second argument. what does this mean? if the  is , and then our component re-renders with  still equal to , react will compare  from the previous render and  from the next render. because all items in the array are the same (), react would skip the effect. that’s our optimization.
when we render with  updated to , react will compare the items in the  array from the previous render to items in the  array from the next render. this time, react will re-apply the effect because . if there are multiple items in the array, react will re-run the effect even if just one of them is different.
this also works for effects that have a cleanup phase:
in the future, the second argument might get added automatically by a build-time transformation.
if you use this optimization, make sure the array includes all values from the component scope (such as props and state) that change over time and that are used by the effect. otherwise, your code will reference stale values from previous renders. learn more about  and .
if you want to run an effect and clean it up only once (on mount and unmount), you can pass an empty array () as a second argument. this tells react that your effect doesn’t depend on any values from props or state, so it never needs to re-run. this isn’t handled as a special case — it follows directly from how the dependencies array always works.
if you pass an empty array (), the props and state inside the effect will always have their initial values. while passing  as the second argument is closer to the familiar  and  mental model, there are usually   to avoid re-running effects too often. also, don’t forget that react defers running  until after the browser has painted, so doing extra work is less of a problem.
we recommend using the  rule as part of our  package. it warns when dependencies are specified incorrectly and suggests a fix.
congratulations! this was a long page, but hopefully by the end most of your questions about effects were answered. you’ve learned both the state hook and the effect hook, and there is a lot you can do with both of them combined. they cover most of the use cases for classes — and where they don’t, you might find the  helpful.
we’re also starting to see how hooks solve problems outlined in . we’ve seen how effect cleanup avoids duplication in  and , brings related code closer together, and helps us avoid bugs. we’ve also seen how we can separate effects by their purpose, which is something we couldn’t do in classes at all.
at this point you might be questioning how hooks work. how can react know which  call corresponds to which state variable between re-renders? how does react “match up” previous and next effects on every update? on the next page we will learn about the  — they’re essential to making hooks work.is this page useful?
rules of hooks
hooks are javascript functions, but you need to follow two rules when using them. we provide a  to enforce these rules automatically:
only call hooks at the top level
don’t call hooks inside loops, conditions, or nested functions. instead, always use hooks at the top level of your react function, before any early returns. by following this rule, you ensure that hooks are called in the same order each time a component renders. that’s what allows react to correctly preserve the state of hooks between multiple  and  calls. (if you’re curious, we’ll explain this in depth .)
only call hooks from react functions
don’t call hooks from regular javascript functions. instead, you can:
✅ call hooks from react function components.
✅ call hooks from custom hooks (we’ll learn about them ).
by following this rule, you ensure that all stateful logic in a component is clearly visible from its source code.
eslint plugin
we released an eslint plugin called  that enforces these two rules. you can add this plugin to your project if you’d like to try it:
this plugin is included by default in .
you can skip to the next page explaining how to write  now. on this page, we’ll continue by explaining the reasoning behind these rules.
explanation
as we , we can use multiple state or effect hooks in a single component:
so how does react know which state corresponds to which  call? the answer is that react relies on the order in which hooks are called. our example works because the order of the hook calls is the same on every render:
as long as the order of the hook calls is the same between renders, react can associate some local state with each of them. but what happens if we put a hook call (for example, the  effect) inside a condition?
the  condition is  on the first render, so we run this hook. however, on the next render the user might clear the form, making the condition . now that we skip this hook during rendering, the order of the hook calls becomes different:
react wouldn’t know what to return for the second  hook call. react expected that the second hook call in this component corresponds to the  effect, just like during the previous render, but it doesn’t anymore. from that point, every next hook call after the one we skipped would also shift by one, leading to bugs.
this is why hooks must be called on the top level of our components. if we want to run an effect conditionally, we can put that condition inside our hook:
note that you don’t need to worry about this problem if you use the . but now you also know why hooks work this way, and which issues the rule is preventing.
finally, we’re ready to learn about ! custom hooks let you combine hooks provided by react into your own abstractions, and reuse common stateful logic between different components.is this page useful?
building your own hooks
building your own hooks lets you extract component logic into reusable functions.
when we were learning about , we saw this component from a chat application that displays a message indicating whether a friend is online or offline:
now let’s say that our chat application also has a contact list, and we want to render names of online users with a green color. we could copy and paste similar logic above into our  component but it wouldn’t be ideal:
instead, we’d like to share this logic between  and .
traditionally in react, we’ve had two popular ways to share stateful logic between components:  and . we will now look at how hooks solve many of the same problems without forcing you to add more components to the tree.
extracting a custom hook
when we want to share logic between two javascript functions, we extract it to a third function. both components and hooks are functions, so this works for them too!
a custom hook is a javascript function whose name starts with ”” and that may call other hooks. for example,  below is our first custom hook:
there’s nothing new inside of it — the logic is copied from the components above. just like in a component, make sure to only call other hooks unconditionally at the top level of your custom hook.
unlike a react component, a custom hook doesn’t need to have a specific signature. we can decide what it takes as arguments, and what, if anything, it should return. in other words, it’s just like a normal function. its name should always start with  so that you can tell at a glance that the  apply to it.
the purpose of our  hook is to subscribe us to a friend’s status. this is why it takes  as an argument, and returns whether this friend is online:
now let’s see how we can use our custom hook.
using a custom hook
in the beginning, our stated goal was to remove the duplicated logic from the  and  components. both of them want to know whether a friend is online.
now that we’ve extracted this logic to a  hook, we can just use it:
is this code equivalent to the original examples? yes, it works in exactly the same way. if you look closely, you’ll notice we didn’t make any changes to the behavior. all we did was to extract some common code between two functions into a separate function. custom hooks are a convention that naturally follows from the design of hooks, rather than a react feature.
do i have to name my custom hooks starting with “”? please do. this convention is very important. without it, we wouldn’t be able to automatically check for violations of  because we couldn’t tell if a certain function contains calls to hooks inside of it.
do two components using the same hook share state? no. custom hooks are a mechanism to reuse stateful logic (such as setting up a subscription and remembering the current value), but every time you use a custom hook, all state and effects inside of it are fully isolated.
how does a custom hook get isolated state? each call to a hook gets isolated state. because we call  directly, from react’s point of view our component just calls  and . and as we  , we can call  and  many times in one component, and they will be completely independent.
tip: pass information between hooks
since hooks are functions, we can pass information between them.
to illustrate this, we’ll use another component from our hypothetical chat example. this is a chat message recipient picker that displays whether the currently selected friend is online:
we keep the currently chosen friend id in the  state variable, and update it if the user chooses a different friend in the  picker.
because the  hook call gives us the latest value of the  state variable, we can pass it to our custom  hook as an argument:
this lets us know whether the currently selected friend is online. if we pick a different friend and update the  state variable, our  hook will unsubscribe from the previously selected friend, and subscribe to the status of the newly selected one.
custom hooks offer the flexibility of sharing logic that wasn’t possible in react components before. you can write custom hooks that cover a wide range of use cases like form handling, animation, declarative subscriptions, timers, and probably many more we haven’t considered. what’s more, you can build hooks that are just as easy to use as react’s built-in features.
try to resist adding abstraction too early. now that function components can do more, it’s likely that the average function component in your codebase will become longer. this is normal — don’t feel like you have to immediately split it into hooks. but we also encourage you to start spotting cases where a custom hook could hide complex logic behind a simple interface, or help untangle a messy component.
for example, maybe you have a complex component that contains a lot of local state that is managed in an ad-hoc way.  doesn’t make centralizing the update logic any easier so you might prefer to write it as a  reducer:
reducers are very convenient to test in isolation, and scale to express complex update logic. you can further break them apart into smaller reducers if necessary. however, you might also enjoy the benefits of using react local state, or might not want to install another library.
so what if we could write a  hook that lets us manage the local state of our component with a reducer? a simplified version of it might look like this:
now we could use it in our component, and let the reducer drive its state management:
the need to manage local state with a reducer in a complex component is common enough that we’ve built the  hook right into react. you’ll find it together with other built-in hooks in the .is this page useful?
hooks api reference
this page describes the apis for the built-in hooks in react.
if you’re new to hooks, you might want to check out  first. you may also find useful information in the  section.
basic hooks
returns a stateful value, and a function to update it.
during the initial render, the returned state () is the same as the value passed as the first argument ().
the  function is used to update the state. it accepts a new state value and enqueues a re-render of the component.
during subsequent re-renders, the first value returned by  will always be the most recent state after applying updates.
react guarantees that  function identity is stable and won’t change on re-renders. this is why it’s safe to omit from the  or  dependency list.
functional updates
if the new state is computed using the previous state, you can pass a function to . the function will receive the previous value, and return an updated value. here’s an example of a counter component that uses both forms of :
the ”+” and ”-” buttons use the functional form, because the updated value is based on the previous value. but the “reset” button uses the normal form, because it always sets the count back to the initial value.
if your update function returns the exact same value as the current state, the subsequent rerender will be skipped completely.
unlike the  method found in class components,  does not automatically merge update objects. you can replicate this behavior by combining the function updater form with object spread syntax:
another option is , which is more suited for managing state objects that contain multiple sub-values.
lazy initial state
the  argument is the state used during the initial render. in subsequent renders, it is disregarded. if the initial state is the result of an expensive computation, you may provide a function instead, which will be executed only on the initial render:
bailing out of a state update
if you update a state hook to the same value as the current state, react will bail out without rendering the children or firing effects. (react uses the .)
note that react may still need to render that specific component again before bailing out. that shouldn’t be a concern because react won’t unnecessarily go “deeper” into the tree. if you’re doing expensive calculations while rendering, you can optimize them with .
batching of state updates
react may group several state updates into a single re-render to improve performance. normally, this improves performance and shouldn’t affect your application’s behavior.
before react 18, only updates inside react event handlers were batched. starting with react 18, . note that react makes sure that updates from several different user-initiated events — for example, clicking a button twice — are always processed separately and do not get batched. this prevents logical mistakes.
in the rare case that you need to force the dom update to be applied synchronously, you may wrap it in . however, this can hurt performance so do this only where needed.
accepts a function that contains imperative, possibly effectful code.
mutations, subscriptions, timers, logging, and other side effects are not allowed inside the main body of a function component (referred to as react’s render phase). doing so will lead to confusing bugs and inconsistencies in the ui.
instead, use . the function passed to  will run after the render is committed to the screen. think of effects as an escape hatch from react’s purely functional world into the imperative world.
by default, effects run after every completed render, but you can choose to fire them .
cleaning up an effect
often, effects create resources that need to be cleaned up before the component leaves the screen, such as a subscription or timer id. to do this, the function passed to  may return a clean-up function. for example, to create a subscription:
the clean-up function runs before the component is removed from the ui to prevent memory leaks. additionally, if a component renders multiple times (as they typically do), the previous effect is cleaned up before executing the next effect. in our example, this means a new subscription is created on every update. to avoid firing an effect on every update, refer to the next section.
timing of effects
unlike  and , the function passed to  fires after layout and paint, during a deferred event. this makes it suitable for the many common side effects, like setting up subscriptions and event handlers, because most types of work shouldn’t block the browser from updating the screen.
however, not all effects can be deferred. for example, a dom mutation that is visible to the user must fire synchronously before the next paint so that the user does not perceive a visual inconsistency. (the distinction is conceptually similar to passive versus active event listeners.) for these types of effects, react provides one additional hook called . it has the same signature as , and only differs in when it is fired.
additionally, starting in react 18, the function passed to  will fire synchronously before layout and paint when it’s the result of a discrete user input such as a click, or when it’s the result of an update wrapped in . this behavior allows the result of the effect to be observed by the event system, or by the caller of .
this only affects the timing of when the function passed to  is called - updates scheduled inside these effects are still deferred. this is different than , which fires the function and processes the updates inside of it immediately.
even in cases where  is deferred until after the browser has painted, it’s guaranteed to fire before any new renders. react will always flush a previous render’s effects before starting a new update.
conditionally firing an effect
the default behavior for effects is to fire the effect after every completed render. that way an effect is always recreated if one of its dependencies changes.
however, this may be overkill in some cases, like the subscription example from the previous section. we don’t need to create a new subscription on every update, only if the  prop has changed.
to implement this, pass a second argument to  that is the array of values that the effect depends on. our updated example now looks like this:
now the subscription will only be recreated when  changes.
if you use this optimization, make sure the array includes all values from the component scope (such as props and state) that change over time and that are used by the effect. otherwise, your code will reference stale values from previous renders. learn more about  and what to do when the .
the array of dependencies is not passed as arguments to the effect function. conceptually, though, that’s what they represent: every value referenced inside the effect function should also appear in the dependencies array. in the future, a sufficiently advanced compiler could create this array automatically.
accepts a context object (the value returned from ) and returns the current context value for that context. the current context value is determined by the  prop of the nearest  above the calling component in the tree.
when the nearest  above the component updates, this hook will trigger a rerender with the latest context  passed to that  provider. even if an ancestor uses  or , a rerender will still happen starting at the component itself using .
don’t forget that the argument to  must be the context object itself:
correct:
incorrect:
a component calling  will always re-render when the context value changes. if re-rendering the component is expensive, you can .
if you’re familiar with the context api before hooks,  is equivalent to  in a class, or to .
only lets you read the context and subscribe to its changes. you still need a  above in the tree to provide the value for this context.
putting it together with context.provider
this example is modified for hooks from a previous example in the , where you can find more information about when and how to use context.
additional hooks
the following hooks are either variants of the basic ones from the previous section, or only needed for specific edge cases. don’t stress about learning them up front.
an alternative to . accepts a reducer of type , and returns the current state paired with a  method. (if you’re familiar with redux, you already know how this works.)
is usually preferable to  when you have complex state logic that involves multiple sub-values or when the next state depends on the previous one.  also lets you optimize performance for components that trigger deep updates because .
here’s the counter example from the  section, rewritten to use a reducer:
specifying the initial state
there are two different ways to initialize  state. you may choose either one depending on the use case. the simplest way is to pass the initial state as a second argument:
react doesn’t use the  argument convention popularized by redux. the initial value sometimes needs to depend on props and so is specified from the hook call instead. if you feel strongly about this, you can call  to emulate the redux behavior, but it’s not encouraged.
lazy initialization
you can also create the initial state lazily. to do this, you can pass an  function as the third argument. the initial state will be set to .
it lets you extract the logic for calculating the initial state outside the reducer. this is also handy for resetting the state later in response to an action:
bailing out of a dispatch
if you return the same value from a reducer hook as the current state, react will bail out without rendering the children or firing effects. (react uses the .)
returns a  callback.
pass an inline callback and an array of dependencies.  will return a memoized version of the callback that only changes if one of the dependencies has changed. this is useful when passing callbacks to optimized child components that rely on reference equality to prevent unnecessary renders (e.g. ).
is equivalent to .
the array of dependencies is not passed as arguments to the callback. conceptually, though, that’s what they represent: every value referenced inside the callback should also appear in the dependencies array. in the future, a sufficiently advanced compiler could create this array automatically.
returns a  value.
pass a “create” function and an array of dependencies.  will only recompute the memoized value when one of the dependencies has changed. this optimization helps to avoid expensive calculations on every render.
remember that the function passed to  runs during rendering. don’t do anything there that you wouldn’t normally do while rendering. for example, side effects belong in , not .
if no array is provided, a new value will be computed on every render.
you may rely on  as a performance optimization, not as a semantic guarantee. in the future, react may choose to “forget” some previously memoized values and recalculate them on next render, e.g. to free memory for offscreen components. write your code so that it still works without  — and then add it to optimize performance.
the array of dependencies is not passed as arguments to the function. conceptually, though, that’s what they represent: every value referenced inside the function should also appear in the dependencies array. in the future, a sufficiently advanced compiler could create this array automatically.
returns a mutable ref object whose  property is initialized to the passed argument (). the returned object will persist for the full lifetime of the component.
a common use case is to access a child imperatively:
essentially,  is like a “box” that can hold a mutable value in its  property.
you might be familiar with refs primarily as a way to . if you pass a ref object to react with , react will set its  property to the corresponding dom node whenever that node changes.
however,  is useful for more than the  attribute. it’s  similar to how you’d use instance fields in classes.
this works because  creates a plain javascript object. the only difference between  and creating a  object yourself is that  will give you the same ref object on every render.
keep in mind that  doesn’t notify you when its content changes. mutating the  property doesn’t cause a re-render. if you want to run some code when react attaches or detaches a ref to a dom node, you may want to use a  instead.
customizes the instance value that is exposed to parent components when using . as always, imperative code using refs should be avoided in most cases.  should be used with :
in this example, a parent component that renders  would be able to call .
the signature is identical to , but it fires synchronously after all dom mutations. use this to read layout from the dom and synchronously re-render. updates scheduled inside  will be flushed synchronously, before the browser has a chance to paint.
prefer the standard  when possible to avoid blocking visual updates.
if you’re migrating code from a class component, note  fires in the same phase as  and . however, we recommend starting with  first and only trying  if that causes a problem.
if you use server rendering, keep in mind that neither  nor  can run until the javascript is downloaded. this is why react warns when a server-rendered component contains . to fix this, either move that logic to  (if it isn’t necessary for the first render), or delay showing that component until after the client renders (if the html looks broken until  runs).
to exclude a component that needs layout effects from the server-rendered html, render it conditionally with  and defer showing it with . this way, the ui doesn’t appear broken before hydration.
can be used to display a label for custom hooks in react devtools.
for example, consider the  custom hook described in :
we don’t recommend adding debug values to every custom hook. it’s most valuable for custom hooks that are part of shared libraries.
defer formatting debug values
in some cases formatting a value for display might be an expensive operation. it’s also unnecessary unless a hook is actually inspected.
for this reason  accepts a formatting function as an optional second parameter. this function is only called if the hooks are inspected. it receives the debug value as a parameter and should return a formatted display value.
for example a custom hook that returned a  value could avoid calling the  function unnecessarily by passing the following formatter:
accepts a value and returns a new copy of the value that will defer to more urgent updates. if the current render is the result of an urgent update, like user input, react will return the previous value and then render the new value after the urgent render has completed.
this hook is similar to user-space hooks which use debouncing or throttling to defer updates. the benefits to using  is that react will work on the update as soon as other work finishes (instead of waiting for an arbitrary amount of time), and like , deferred values can suspend without triggering an unexpected fallback for existing content.
memoizing deferred children
only defers the value that you pass to it. if you want to prevent a child component from re-rendering during an urgent update, you must also memoize that component with  or :
memoizing the children tells react that it only needs to re-render them when  changes and not when  changes. this caveat is not unique to , and it’s the same pattern you would use with similar hooks that use debouncing or throttling.
returns a stateful value for the pending state of the transition, and a function to start it.
lets you mark updates in the provided callback as transitions:
indicates when a transition is active to show a pending state:
updates in a transition will not show a fallback for re-suspended content. this allows the user to continue interacting with the current content while rendering the update.
is a hook for generating unique ids that are stable across the server and client, while avoiding hydration mismatches.
is not for generating . keys should be generated from your data.
for a basic example, pass the  directly to the elements that need it:
for multiple ids in the same component, append a suffix using the same :
generates a string that includes the  token. this helps ensure that the token is unique, but is not supported in css selectors or apis like .
supports an  to prevent collisions in multi-root apps. to configure, see the options for  and .
library hooks
the following hooks are provided for library authors to integrate libraries deeply into the react model, and are not typically used in application code.
is a hook recommended for reading and subscribing from external data sources in a way that’s compatible with concurrent rendering features like selective hydration and time slicing.
this method returns the value of the store and accepts three arguments:
: function to register a callback that is called whenever the store changes.
: function that returns the current value of the store.
: function that returns the snapshot used during server rendering.
the most basic example simply subscribes to the entire store:
however, you can also subscribe to a specific field:
when server rendering, you must serialize the store value used on the server, and provide it to . react will use this snapshot during hydration to prevent server mismatches:
must return a cached value. if getsnapshot is called multiple times in a row, it must return the same exact value unless there was a store update in between.
a shim is provided for supporting multiple react versions published as . this shim will prefer  when available, and fallback to a user-space implementation when it’s not.
as a convenience, we also provide a version of the api with automatic support for memoizing the result of getsnapshot published as .
the signature is identical to , but it fires synchronously before all dom mutations. use this to inject styles into the dom before reading layout in . since this hook is limited in scope, this hook does not have access to refs and cannot schedule updates.
should be limited to css-in-js library authors. prefer  or  instead.
hooks faq
this page answers some of the frequently asked questions about .
adoption strategy
which versions of react include hooks?
starting with 16.8.0, react includes a stable implementation of react hooks for:
react dom
react native
react dom server
react test renderer
react shallow renderer
note that to enable hooks, all react packages need to be 16.8.0 or higher. hooks won’t work if you forget to update, for example, react dom.
and above support hooks.
do i need to rewrite all my class components?
no. there are  to remove classes from react — we all need to keep shipping products and can’t afford rewrites. we recommend trying hooks in new code.
what can i do with hooks that i couldn’t with classes?
hooks offer a powerful and expressive new way to reuse functionality between components.  provides a glimpse of what’s possible.  by a react core team member dives deeper into the new capabilities unlocked by hooks.
how much of my react knowledge stays relevant?
hooks are a more direct way to use the react features you already know — such as state, lifecycle, context, and refs. they don’t fundamentally change how react works, and your knowledge of components, props, and top-down data flow is just as relevant.
hooks do have a learning curve of their own. if there’s something missing in this documentation,  and we’ll try to help.
should i use hooks, classes, or a mix of both?
when you’re ready, we’d encourage you to start trying hooks in new components you write. make sure everyone on your team is on board with using them and familiar with this documentation. we don’t recommend rewriting your existing classes to hooks unless you planned to rewrite them anyway (e.g. to fix bugs).
you can’t use hooks inside a class component, but you can definitely mix classes and function components with hooks in a single tree. whether a component is a class or a function that uses hooks is an implementation detail of that component. in the longer term, we expect hooks to be the primary way people write react components.
do hooks cover all use cases for classes?
our goal is for hooks to cover all use cases for classes as soon as possible. there are no hook equivalents to the uncommon ,  and  lifecycles yet, but we plan to add them soon.
do hooks replace render props and higher-order components?
often, render props and higher-order components render only a single child. we think hooks are a simpler way to serve this use case. there is still a place for both patterns (for example, a virtual scroller component might have a  prop, or a visual container component might have its own dom structure). but in most cases, hooks will be sufficient and can help reduce nesting in your tree.
what do hooks mean for popular apis like redux  and react router?
you can continue to use the exact same apis as you always have; they’ll continue to work.
react redux since v7.1.0  and exposes hooks like  or .
react router  since v5.1.
other libraries might support hooks in the future too.
do hooks work with static typing?
hooks were designed with static typing in mind. because they’re functions, they are easier to type correctly than patterns like higher-order components. the latest flow and typescript react definitions include support for react hooks.
importantly, custom hooks give you the power to constrain react api if you’d like to type them more strictly in some way. react gives you the primitives, but you can combine them in different ways than what we provide out of the box.
how to test components that use hooks?
from react’s point of view, a component using hooks is just a regular component. if your testing solution doesn’t rely on react internals, testing components with hooks shouldn’t be different from how you normally test components.
include many examples that you can copy and paste.
for example, let’s say we have this counter component:
we’ll test it using react dom. to make sure that the behavior matches what happens in the browser, we’ll wrap the code rendering and updating it into  calls:
the calls to  will also flush the effects inside of them.
if you need to test a custom hook, you can do so by creating a component in your test, and using your hook from it. then you can test the component you wrote.
to reduce the boilerplate, we recommend using  which is designed to encourage writing tests that use your components as the end users do.
for more information, check out .
what exactly do the  enforce?
we provide an  that enforces  to avoid bugs. it assumes that any function starting with ”” and a capital letter right after it is a hook. we recognize this heuristic isn’t perfect and there may be some false positives, but without an ecosystem-wide convention there is just no way to make hooks work well — and longer names will discourage people from either adopting hooks or following the convention.
in particular, the rule enforces that:
calls to hooks are either inside a  function (assumed to be a component) or another  function (assumed to be a custom hook).
hooks are called in the same order on every render.
there are a few more heuristics, and they might change over time as we fine-tune the rule to balance finding bugs with avoiding false positives.
from classes to hooks
how do lifecycle methods correspond to hooks?
: function components don’t need a constructor. you can initialize the state in the  call. if computing the initial state is expensive, you can pass a function to .
: schedule an update  instead.
: see  .
: this is the function component body itself.
, , : the  can express all combinations of these (including   cases).
,  and : there are no hook equivalents for these methods yet, but they will be added soon.
how can i do data fetching with hooks?
here is a  to get you started. to learn more, check out  about data fetching with hooks.
is there something like instance variables?
yes! the  hook isn’t just for dom refs. the “ref” object is a generic container whose  property is mutable and can hold any value, similar to an instance property on a class.
you can write to it from inside :
if we just wanted to set an interval, we wouldn’t need the ref ( could be local to the effect), but it’s useful if we want to clear the interval from an event handler:
conceptually, you can think of refs as similar to instance variables in a class. unless you’re doing , avoid setting refs during rendering — this can lead to surprising behavior. instead, typically you want to modify refs in event handlers and effects.
should i use one or many state variables?
if you’re coming from classes, you might be tempted to always call  once and put all state into a single object. you can do it if you’d like. here is an example of a component that follows the mouse movement. we keep its position and size in the local state:
now let’s say we want to write some logic that changes  and  when the user moves their mouse. note how we have to merge these fields into the previous state object manually:
this is because when we update a state variable, we replace its value. this is different from  in a class, which merges the updated fields into the object.
if you miss automatic merging, you could write a custom  hook that merges object state updates. however, we recommend to split state into multiple state variables based on which values tend to change together.
for example, we could split our component state into  and  objects, and always replace the  with no need for merging:
separating independent state variables also has another benefit. it makes it easy to later extract some related logic into a custom hook, for example:
note how we were able to move the  call for the  state variable and the related effect into a custom hook without changing their code. if all state was in a single object, extracting it would be more difficult.
both putting all state in a single  call, and having a  call per each field can work. components tend to be most readable when you find a balance between these two extremes, and group related state into a few independent state variables. if the state logic becomes complex, we recommend  or a custom hook.
can i run an effect only on updates?
this is a rare use case. if you need it, you can  to manually store a boolean value corresponding to whether you are on the first or a subsequent render, then check that flag in your effect. (if you find yourself doing this often, you could create a custom hook for it.)
how to get the previous props or state?
there are two cases in which you might want to get previous props or state.
sometimes, you need previous props to clean up an effect. for example, you might have an effect that subscribes to a socket based on the  prop. if the  prop changes, you want to unsubscribe from the previous  and subscribe to the next one. you don’t need to do anything special for this to work:
in the above example, if  changes from  to ,  will run first, and then  will run. there is no need to get “previous”  because the cleanup function will capture it in a closure.
other times, you might need to adjust state based on a change in props or other state. this is rarely needed and is usually a sign you have some duplicate or redundant state. however, in the rare case that you need this pattern, you can .
we have previously suggested a custom hook called  to hold the previous value. however, we’ve found that most use cases fall into the two patterns described above. if your use case is different, you can  and manually update it when needed. avoid reading and updating refs during rendering because this makes your component’s behavior difficult to predict and understand.
why am i seeing stale props or state inside my function?
any function inside a component, including event handlers and effects, “sees” the props and state from the render it was created in. for example, consider code like this:
if you first click “show alert” and then increment the counter, the alert will show the  variable at the time you clicked the “show alert” button. this prevents bugs caused by the code assuming props and state don’t change.
if you intentionally want to read the latest state from some asynchronous callback, you could keep it in , mutate it, and read from it.
finally, another possible reason you’re seeing stale props or state is if you use the “dependency array” optimization but didn’t correctly specify all the dependencies. for example, if an effect specifies  as the second argument but reads  inside, it will keep “seeing” the initial value of . the solution is to either remove the dependency array, or to fix it. here’s , and here’s  to run effects less often without incorrectly skipping dependencies.
we provide an  eslint rule as a part of the  package. it warns when dependencies are specified incorrectly and suggests a fix.
how do i implement ?
while you probably , in rare cases that you do (such as implementing a  component), you can update the state right during rendering. react will re-run the component with updated state immediately after exiting the first render so it wouldn’t be expensive.
here, we store the previous value of the  prop in a state variable so that we can compare:
this might look strange at first, but an update during rendering is exactly what  has always been like conceptually.
is there something like forceupdate?
both  and  hooks  if the next value is the same as the previous one. mutating state in place and calling  will not cause a re-render.
normally, you shouldn’t mutate local state in react. however, as an escape hatch, you can use an incrementing counter to force a re-render even if the state has not changed:
try to avoid this pattern if possible.
can i make a ref to a function component?
while you shouldn’t need this often, you may expose some imperative methods to a parent component with the  hook.
how can i measure a dom node?
one rudimentary way to measure the position or size of a dom node is to use a . react will call that callback whenever the ref gets attached to a different node. here is a :
we didn’t choose  in this example because an object ref doesn’t notify us about changes to the current ref value. using a callback ref ensures that  (e.g. in response to a click), we still get notified about it in the parent component and can update the measurements.
note that we pass  as a dependency array to . this ensures that our ref callback doesn’t change between the re-renders, and so react won’t call it unnecessarily.
in this example, the callback ref will be called only when the component mounts and unmounts, since the rendered  component stays present throughout any rerenders. if you want to be notified any time a component resizes, you may want to use  or a third-party hook built on it.
if you want, you can  into a reusable hook:
what does  mean?
if you’re not familiar with this syntax, check out the  in the state hook documentation.
performance optimizations
can i skip an effect on updates?
yes. see . note that forgetting to handle updates often , which is why this isn’t the default behavior.
is it safe to omit functions from the list of dependencies?
generally speaking, no.
it’s difficult to remember which props or state are used by functions outside of the effect. this is why usually you’ll want to declare functions needed by an effect inside of it. then it’s easy to see what values from the component scope that effect depends on:
if after that we still don’t use any values from the component scope, it’s safe to specify :
depending on your use case, there are a few more options described below.
we provide the  eslint rule as a part of the  package. it helps you find components that don’t handle updates consistently.
let’s see why this matters.
if you specify a  as the last argument to , , , , or , it must include all values that are used inside the callback and participate in the react data flow. that includes props, state, and anything derived from them.
it is only safe to omit a function from the dependency list if nothing in it (or the functions called by it) references props, state, or values derived from them. this example has a bug:
the recommended fix is to move that function inside of your effect. that makes it easy to see which props or state your effect uses, and to ensure they’re all declared:
this also allows you to handle out-of-order responses with a local variable inside the effect:
we moved the function inside the effect so it doesn’t need to be in its dependency list.
check out  and  to learn more about data fetching with hooks.
if for some reason you can’t move a function inside an effect, there are a few more options:
you can try moving that function outside of your component. in that case, the function is guaranteed to not reference any props or state, and also doesn’t need to be in the list of dependencies.
if the function you’re calling is a pure computation and is safe to call while rendering, you may call it outside of the effect instead, and make the effect depend on the returned value.
as a last resort, you can add a function to effect dependencies but wrap its definition into the  hook. this ensures it doesn’t change on every render unless its own dependencies also change:
note that in the above example we need to keep the function in the dependencies list. this ensures that a change in the  prop of  automatically triggers a refetch in the  component.
what can i do if my effect dependencies change too often?
sometimes, your effect may be using state that changes too often. you might be tempted to omit that state from a list of dependencies, but that usually leads to bugs:
the empty set of dependencies, , means that the effect will only run once when the component mounts, and not on every re-render. the problem is that inside the  callback, the value of  does not change, because we’ve created a closure with the value of  set to  as it was when the effect callback ran. every second, this callback then calls , so the count never goes above 1.
specifying  as a list of dependencies would fix the bug, but would cause the interval to be reset on every change. effectively, each  would get one chance to execute before being cleared (similar to a .) that may not be desirable. to fix this, we can use the . it lets us specify how the state needs to change without referencing the current state:
(the identity of the  function is guaranteed to be stable so it’s safe to omit.)
now, the  callback executes once a second, but each time the inner call to  can use an up-to-date value for  (called  in the callback here.)
in more complex cases (such as if one state depends on another state), try moving the state update logic outside the effect with the .  offers an example of how you can do this. the identity of the  function from  is always stable — even if the reducer function is declared inside the component and reads its props.
as a last resort, if you want something like  in a class, you can  to hold a mutable variable. then you can write and read to it. for example:
only do this if you couldn’t find a better alternative, as relying on mutation makes components less predictable. if there’s a specific pattern that doesn’t translate well,  with a runnable example code and we can try to help.
you can wrap a function component with  to shallowly compare its props:
it’s not a hook because it doesn’t compose like hooks do.  is equivalent to , but it only compares props. (you can also add a second argument to specify a custom comparison function that takes the old and new props. if it returns true, the update is skipped.)
doesn’t compare state because there is no single state object to compare. but you can make children pure too, or even .
how to memoize calculations?
the  hook lets you cache calculations between multiple renders by “remembering” the previous computation:
this code calls . but if the dependencies  haven’t changed since the last value,  skips calling it a second time and simply reuses the last value it returned.
you may rely on  as a performance optimization, not as a semantic guarantee. in the future, react may choose to “forget” some previously memoized values and recalculate them on next render, e.g. to free memory for offscreen components. write your code so that it still works without  — and then add it to optimize performance. (for rare cases when a value must never be recomputed, you can  a ref.)
conveniently,  also lets you skip an expensive re-render of a child:
note that this approach won’t work in a loop because hook calls  be placed inside loops. but you can extract a separate component for the list item, and call  there.
how to create expensive objects lazily?
lets you  if the dependencies are the same. however, it only serves as a hint, and doesn’t guarantee the computation won’t re-run. but sometimes you need to be sure an object is only created once.
the first common use case is when creating the initial state is expensive:
to avoid re-creating the ignored initial state, we can pass a function to :
react will only call this function during the first render. see the .
you might also occasionally want to avoid re-creating the  initial value. for example, maybe you want to ensure some imperative class instance only gets created once:
does not accept a special function overload like . instead, you can write your own function that creates and sets it lazily:
this avoids creating an expensive object until it’s truly needed for the first time. if you use flow or typescript, you can also give  a non-nullable type for convenience.
are hooks slow because of creating functions in render?
no. in modern browsers, the raw performance of closures compared to classes doesn’t differ significantly except in extreme scenarios.
in addition, consider that the design of hooks is more efficient in a couple ways:
hooks avoid a lot of the overhead that classes require, like the cost of creating class instances and binding event handlers in the constructor.
idiomatic code using hooks doesn’t need the deep component tree nesting that is prevalent in codebases that use higher-order components, render props, and context. with smaller component trees, react has less work to do.
traditionally, performance concerns around inline functions in react have been related to how passing new callbacks on each render breaks  optimizations in child components. hooks approach this problem from three sides.
the  hook lets you keep the same callback reference between re-renders so that  continues to work:
the  hook makes it easier to control when individual children update, reducing the need for pure components.
finally, the  hook reduces the need to pass callbacks deeply, as explained below.
how to avoid passing callbacks down?
we’ve found that most people don’t enjoy manually passing callbacks through every level of a component tree. even though it is more explicit, it can feel like a lot of “plumbing”.
in large component trees, an alternative we recommend is to pass down a  function from  via context:
any child in the tree inside  can use the  function to pass actions up to :
this is both more convenient from the maintenance perspective (no need to keep forwarding callbacks), and avoids the callback problem altogether. passing  down like this is the recommended pattern for deep updates.
note that you can still choose whether to pass the application state down as props (more explicit) or as context (more convenient for very deep updates). if you use context to pass down the state too, use two different context types — the  context never changes, so components that read it don’t need to rerender unless they also need the application state.
how to read an often-changing value from ?
we recommend to  rather than individual callbacks in props. the approach below is only mentioned here for completeness and as an escape hatch.
in some rare cases you might need to memoize a callback with  but the memoization doesn’t work very well because the inner function has to be re-created too often. if the function you’re memoizing is an event handler and isn’t used during rendering, you can use , and save the last committed value into it manually:
this is a rather convoluted pattern but it shows that you can do this escape hatch optimization if you need it. it’s more bearable if you extract it to a custom hook:
in either case, we don’t recommend this pattern and only show it here for completeness. instead, it is preferable to .
under the hood
how does react associate hook calls with components?
react keeps track of the currently rendering component. thanks to the , we know that hooks are only called from react components (or custom hooks — which are also only called from react components).
there is an internal list of “memory cells” associated with each component. they’re just javascript objects where we can put some data. when you call a hook like , it reads the current cell (or initializes it during the first render), and then moves the pointer to the next one. this is how multiple  calls each get independent local state.
what is the prior art for hooks?
hooks synthesize ideas from several different sources:
our old experiments with functional apis in the  repository.
react community’s experiments with render prop apis, including ’s .
’s  proposal as a sugar syntax for render props.
state variables and state cells in .
in reasonreact.
in rx.
in multicore ocaml.
came up with the original design for hooks, later refined by , , , and other members of the react team.is this page useful?
testing overview
you can test react components similar to testing other javascript code.
there are a few ways to test react components. broadly, they divide into two categories:
rendering component trees in a simplified test environment and asserting on their output.
running a complete app in a realistic browser environment (also known as “end-to-end” tests).
this documentation section focuses on testing strategies for the first case. while full end-to-end tests can be very useful to prevent regressions to important workflows, such tests are not concerned with react components in particular, and are out of the scope of this section.
when choosing testing tools, it is worth considering a few tradeoffs:
iteration speed vs realistic environment: some tools offer a very quick feedback loop between making a change and seeing the result, but don’t model the browser behavior precisely. other tools might use a real browser environment, but reduce the iteration speed and are flakier on a continuous integration server.
how much to mock: with components, the distinction between a “unit” and “integration” test can be blurry. if you’re testing a form, should its test also test the buttons inside of it? or should a button component have its own test suite? should refactoring a button ever break the form test?
different answers may work for different teams and products.
recommended tools
is a javascript test runner that lets you access the dom via . while jsdom is only an approximation of how the browser works, it is often good enough for testing react components. jest provides a great iteration speed combined with powerful features like mocking  and  so you can have more control over how the code executes.
is a set of helpers that let you test react components without relying on their implementation details. this approach makes refactoring a breeze and also nudges you towards best practices for accessibility. although it doesn’t provide a way to “shallowly” render a component without its children, a test runner like jest lets you do this by .
learn more
this section is divided in two pages:
: common patterns when writing tests for react components.
: what to consider when setting up a testing environment for react components.
testing recipes
common testing patterns for react components.
this page assumes you’re using  as a test runner. if you use a different test runner, you may need to adjust the api, but the overall shape of the solution will likely be the same. read more details on setting up a testing environment on the  page.
on this page, we will primarily use function components. however, these testing strategies don’t depend on implementation details, and work just as well for class components too.
setup/teardown
for each test, we usually want to render our react tree to a dom element that’s attached to . this is important so that it can receive dom events. when the test ends, we want to “clean up” and unmount the tree from the .
a common way to do it is to use a pair of  and  blocks so that they’ll always run and isolate the effects of a test to itself:
you may use a different pattern, but keep in mind that we want to execute the cleanup even if a test fails. otherwise, tests can become “leaky”, and one test can change the behavior of another test. that makes them difficult to debug.
when writing ui tests, tasks like rendering, user events, or data fetching can be considered as “units” of interaction with a user interface.  provides a helper called  that makes sure all updates related to these “units” have been processed and applied to the dom before you make any assertions:
this helps make your tests run closer to what real users would experience when using your application. the rest of these examples use  to make these guarantees.
you might find using  directly a bit too verbose. to avoid some of the boilerplate, you could use a library like , whose helpers are wrapped with .
the name  comes from the  pattern.
rendering
commonly, you might want to test whether a component renders correctly for given props. consider a simple component that renders a message based on a prop:
we can write a test for this component:
data fetching
instead of calling real apis in all your tests, you can mock requests with dummy data. mocking data fetching with “fake” data prevents flaky tests due to an unavailable backend, and makes them run faster. note: you may still want to run a subset of tests using an  framework that tells whether the whole app is working together.
we can write tests for it:
mocking modules
some modules might not work well inside a testing environment, or may not be as essential to the test itself. mocking out these modules with dummy replacements can make it easier to write tests for your own code.
consider a  component that embeds a third-party  component:
if we don’t want to load this component in our tests, we can mock out the dependency itself to a dummy component, and run our tests:
events
we recommend dispatching real dom events on dom elements, and then asserting on the result. consider a  component:
we could write tests for it:
different dom events and their properties are described in . note that you need to pass  in each event you create for it to reach the react listener because react automatically delegates events to the root.
react testing library offers a  for firing events.
timers
your code might use timer-based functions like  to schedule more work in the future. in this example, a multiple choice panel waits for a selection and advances, timing out if a selection isn’t made in 5 seconds:
we can write tests for this component by leveraging , and testing the different states it can be in.
you can use fake timers only in some tests. above, we enabled them by calling . the main advantage they provide is that your test doesn’t actually have to wait five seconds to execute, and you also didn’t need to make the component code more convoluted just for testing.
snapshot testing
frameworks like jest also let you save “snapshots” of data with . with these, we can “save” the rendered component output and ensure that a change to it has to be explicitly committed as a change to the snapshot.
in this example, we render a component and format the rendered html with the  package, before saving it as an inline snapshot:
it’s typically better to make more specific assertions than to use snapshots. these kinds of tests include implementation details so they break easily, and teams can get desensitized to snapshot breakages. selectively  can help reduce the size of snapshots and keep them readable for the code review.
multiple renderers
in rare cases, you may be running a test on a component that uses multiple renderers. for example, you may be running snapshot tests on a component with , that internally uses  from  inside a child component to render some content. in this scenario, you can wrap updates with s corresponding to their renderers.
if some common scenario is not covered, please let us know on the  for the documentation website.is this page useful?
testing environments
this document goes through the factors that can affect your environment and recommendations for some scenarios.
test runners
test runners like , ,  let you write test suites as regular javascript, and run them as part of your development process. additionally, test suites are run as part of continuous integration.
jest is widely compatible with react projects, supporting features like mocked  and , and  support. if you use create react app,  with useful defaults.
libraries like  work well in real browser environments, and could help for tests that explicitly need it.
end-to-end tests are used for testing longer flows across multiple pages, and require a .
mocking a rendering surface
tests often run in an environment without access to a real rendering surface like a browser. for these environments, we recommend simulating a browser with , a lightweight browser implementation that runs inside node.js.
in most cases, jsdom behaves like a regular browser would, but doesn’t have features like . this is still useful for most web-based component tests, since it runs quicker than having to start up a browser for each test. it also runs in the same process as your tests, so you can write code to examine and assert on the rendered dom.
just like in a real browser, jsdom lets us model user interactions; tests can dispatch events on dom nodes, and then observe and assert on the side effects of these actions .
a large portion of ui tests can be written with the above setup: using jest as a test runner, rendered to jsdom, with user interactions specified as sequences of browser events, powered by the  helper . for example, a lot of react’s own tests are written with this combination.
if you’re writing a library that tests mostly browser-specific behavior, and requires native browser behavior like layout or real inputs, you could use a framework like
in an environment where you can’t simulate a dom (e.g. testing react native components on node.js), you could use  to simulate interactions with elements. alternately, you could use the  helper from .
frameworks like ,  and  are useful for running .
mocking functions
when writing tests, we’d like to mock out the parts of our code that don’t have equivalents inside our testing environment (e.g. checking  status inside node.js). tests could also spy on some functions, and observe how other parts of the test interact with them. it is then useful to be able to selectively mock these functions with test-friendly versions.
this is especially useful for data fetching. it is usually preferable to use “fake” data for tests to avoid the slowness and flakiness due to fetching from real api endpoints . this helps make the tests predictable. libraries like  and , among others, support mocked functions. for end-to-end tests, mocking network can be more difficult, but you might also want to test the real api endpoints in them anyway.
some components have dependencies for modules that may not work well in test environments, or aren’t essential to our tests. it can be useful to selectively mock these modules out with suitable replacements .
on node.js, runners like jest . you could also use libraries like .
mocking timers
components might be using time-based functions like , , or . in testing environments, it can be helpful to mock these functions out with replacements that let you manually “advance” time. this is great for making sure your tests run fast! tests that are dependent on timers would still resolve in order, but quicker . most frameworks, including ,  and , let you mock timers in your tests.
sometimes, you may not want to mock timers. for example, maybe you’re testing an animation, or interacting with an endpoint that’s sensitive to timing (like an api rate limiter). libraries with timer mocks let you enable and disable them on a per test/suite basis, so you can explicitly choose how these tests would run.
end-to-end tests
end-to-end tests are useful for testing longer workflows, especially when they’re critical to your business (such as payments or signups). for these tests, you’d probably want to test how a real browser renders the whole app, fetches data from the real api endpoints, uses sessions and cookies, navigates between different links. you might also likely want to make assertions not just on the dom state, but on the backing data as well (e.g. to verify whether the updates have been persisted to the database).
in this scenario, you would use a framework like ,  or a library like  so you can navigate between multiple routes and assert on side effects not just in the browser, but potentially on the backend as well.is this page useful?
how to contribute
react is one of facebook’s first open source projects that is both under very active development and is also being used to ship code to everybody on . we’re still working out the kinks to make contributing to this project as easy and transparent as possible, but we’re not quite there yet. hopefully this document makes the process for contributing clear and answers some questions that you may have.
facebook has adopted the  as its code of conduct, and we expect project participants to adhere to it. please read  so that you can understand what actions will and will not be tolerated.
open development
all work on react happens directly on . both core team members and external contributors send pull requests which go through the same review process.
semantic versioning
react follows . we release patch versions for critical bugfixes, minor versions for new features or non-essential changes, and major versions for any breaking changes. when we make breaking changes, we also introduce deprecation warnings in a minor version so that our users learn about the upcoming changes and migrate their code in advance. learn more about our commitment to stability and incremental migration in .
every significant change is documented in the .
branch organization
submit all changes directly to the . we don’t use separate branches for development or for upcoming releases. we do our best to keep  in good shape, with all tests passing.
code that lands in  must be compatible with the latest stable release. it may contain additional features, but no breaking changes. we should be able to release a new minor version from the tip of  at any time.
feature flags
to keep the  branch in a releasable state, breaking changes and experimental features must be gated behind a feature flag.
feature flags are defined in . some builds of react may enable different sets of feature flags; for example, the react native build may be configured differently than react dom. these flags are found in . feature flags are statically typed by flow, so you can run  to confirm that you’ve updated all the necessary files.
react’s build system will strip out disabled feature branches before publishing. a continuous integration job runs on every commit to check for changes in bundle size. you can use the change in size as a signal that a feature was gated correctly.
bugs
where to find known issues
we are using  for our public bugs. we keep a close eye on this and try to make it clear when we have an internal fix in progress. before filing a new task, try to make sure your problem doesn’t already exist.
reporting new issues
the best way to get your bug fixed is to provide a reduced test case. this  is a great starting point.
security bugs
facebook has a  for the safe disclosure of security bugs. with that in mind, please do not file public issues; go through the process outlined on that page.
how to get in touch
irc:
there is also  in case you need help with react.
proposing a change
if you intend to change the public api, or make any non-trivial changes to the implementation, we recommend . this lets us reach an agreement on your proposal before you put significant effort into it.
if you’re only fixing a bug, it’s fine to submit a pull request right away but we still recommend to file an issue detailing what you’re fixing. this is helpful in case we don’t accept that specific fix but want to keep track of the issue.
your first pull request
working on your first pull request? you can learn how from this free video series:
to help you get your feet wet and get you familiar with our contribution process, we have a list of  that contain bugs that have a relatively limited scope. this is a great place to get started.
if you decide to fix an issue, please be sure to check the comment thread in case somebody is already working on a fix. if nobody is working on it at the moment, please leave a comment stating that you intend to work on it so other people don’t accidentally duplicate your effort.
if somebody claims an issue but doesn’t follow up for more than two weeks, it’s fine to take it over but you should still leave a comment.
sending a pull request
the core team is monitoring for pull requests. we will review your pull request and either merge it, request changes to it, or close it with an explanation. for api changes we may need to fix our internal uses at facebook.com, which could cause some delay. we’ll do our best to provide updates and feedback throughout the process.
before submitting a pull request, please make sure the following is done:
fork  and create your branch from .
run  in the repository root.
if you’ve fixed a bug or added code that should be tested, add tests!
ensure the test suite passes (). tip:  is helpful in development.
run  to test in the production environment.
if you need a debugger, run , open , and press “inspect”.
format your code with  ().
make sure your code lints (). tip:  to only check changed files.
run the  typechecks ().
if you haven’t already, complete the cla.
contributor license agreement (cla)
in order to accept your pull request, we need you to submit a cla. you only need to do this once, so if you’ve done this for another facebook open source project, you’re good to go. if you are submitting a pull request for the first time, just let us know that you have completed the cla and we can cross-check with your github username.
contribution prerequisites
you have  installed at lts and  at v1.2.0+.
you have  installed.
you have  installed or are comfortable installing a compiler if needed. some of our dependencies may require a compilation step. on os x, the xcode command line tools will cover this. on ubuntu,  will install the required packages. similar commands should work on other linux distros. windows will require some additional steps, see the  for details.
you are familiar with git.
development workflow
after cloning react, run  to fetch its dependencies.
then, you can run several commands:
checks the code style.
is like  but faster because it only checks files that differ in your branch.
runs the complete test suite.
runs an interactive test watcher.
runs tests in the production environment.
runs tests with matching filenames.
is just like  but with a debugger. open  and press “inspect”.
runs the  typechecks.
creates a  folder with all the packages.
creates umd builds of just react and reactdom.
we recommend running  (or its variations above) to make sure you don’t introduce any regressions as you work on your change. however, it can be handy to try your build of react in a real project.
first, run . this will produce pre-built bundles in  folder, as well as prepare npm packages inside .
the easiest way to try your changes is to run  and then open . this file already uses  from the  folder so it will pick up your changes.
if you want to try your changes in your existing react project, you may copy , , or any other build products into your app and use them instead of the stable version.
if your project uses react from npm, you may delete  and  in its dependencies and use  to point them to your local  folder. note that instead of  you’ll want to pass  when building. you’ll also need to build the  package:
every time you run  in the react folder, the updated versions will appear in your project’s . you can then rebuild your project to try your changes.
if some package is still missing (e.g. maybe you use  in your project), you can always do a full build with . note that running  without options takes a long time.
we still require that your pull request contains unit tests for any new functionality. this way we can ensure that we don’t break your code in the future.
style guide
we use an automatic code formatter called .
run  after making any changes to the code.
then, our linter will catch most issues that may exist in your code.
you can check the status of your code styling by simply running .
however, there are still some styles that the linter cannot pick up. if you are unsure about something, looking at  will guide you in the right direction.
request for comments (rfc)
many changes, including bug fixes and documentation improvements can be implemented and reviewed via the normal github pull request workflow.
some changes though are “substantial”, and we ask that these be put through a bit of a design process and produce a consensus among the react core team.
the “rfc” (request for comments) process is intended to provide a consistent and controlled path for new features to enter the project. you can contribute by visiting the .
license
by contributing to react, you agree that your contributions will be licensed under its mit license.
what next?
read the  to learn how the codebase is organized.is this page useful?
codebase overview
this section will give you an overview of the react codebase organization, its conventions, and the implementation.
if you want to  we hope that this guide will help you feel more comfortable making changes.
we don’t necessarily recommend any of these conventions in react apps. many of them exist for historical reasons and might change with time.
top-level folders
after cloning the , you will see a few top-level folders in it:
contains metadata (such as ) and the source code ( subdirectory) for all packages in the react repository. if your change is related to the code, the  subdirectory of each package is where you’ll spend most of your time.
contains a few small react test applications for contributors.
is the build output of react. it is not in the repository but it will appear in your react clone after you  for the first time.
the documentation is hosted .
there are a few other top-level folders but they are mostly used for the tooling and you likely won’t ever encounter them when contributing.
colocated tests
we don’t have a top-level directory for unit tests. instead, we put them into a directory called  relative to the files that they test.
for example, a test for  is located in  right next to it.
warnings and invariants
the react codebase uses  to display warnings:
warnings are only enabled in development. in production, they are completely stripped out. if you need to forbid some code path from executing, use  module instead:
the invariant is thrown when the  condition is .
“invariant” is just a way of saying “this condition always holds true”. you can think about it as making an assertion.
it is important to keep development and production behavior similar, so  throws both in development and in production. the error messages are automatically replaced with error codes in production to avoid negatively affecting the byte size.
development and production
you can use  pseudo-global variable in the codebase to guard development-only blocks of code.
it is inlined during the compile step, and turns into  checks in the commonjs builds.
for standalone builds, it becomes  in the unminified build, and gets completely stripped out with the  blocks it guards in the minified build.
we recently started introducing  checks to the codebase. files marked with the  annotation in the license header comment are being typechecked.
we accept pull requests . flow annotations look like this:
when possible, new code should use flow annotations.
you can run  locally to check your code with flow.
multiple packages
react is a . its repository contains multiple separate packages so that their changes can be coordinated together, and issues live in one place.
react core
the “core” of react includes all the , for example:
react core only includes the apis necessary to define components. it does not include the  algorithm or any platform-specific code. it is used both by react dom and react native components.
the code for react core is located in  in the source tree. it is available on npm as the  package. the corresponding standalone browser build is called , and it exports a global called .
renderers
react was originally created for the dom but it was later adapted to also support native platforms with . this introduced the concept of “renderers” to react internals.
renderers manage how a react tree turns into the underlying platform calls.
renderers are also located in :
renders react components to the dom. it implements  and is available as  npm package. it can also be used as standalone browser bundle called  that exports a  global.
renders react components to native views. it is used internally by react native.
renders react components to json trees. it is used by the  feature of  and is available as  npm package.
the only other officially supported renderer is . it used to be in a separate  but we moved it into the main source tree for now.
technically the  is a very thin layer that teaches react to interact with react native implementation. the real platform-specific code managing the native views lives in the  together with its components.
reconcilers
even vastly different renderers like react dom and react native need to share a lot of logic. in particular, the  algorithm should be as similar as possible so that declarative rendering, custom components, state, lifecycle methods, and refs work consistently across platforms.
to solve this, different renderers share some code between them. we call this part of react a “reconciler”. when an update such as  is scheduled, the reconciler calls  on components in the tree and mounts, updates, or unmounts them.
reconcilers are not packaged separately because they currently have no public api. instead, they are exclusively used by renderers such as react dom and react native.
stack reconciler
the “stack” reconciler is the implementation powering react 15 and earlier. we have since stopped using it, but it is documented in detail in the .
fiber reconciler
the “fiber” reconciler is a new effort aiming to resolve the problems inherent in the stack reconciler and fix a few long-standing issues. it has been the default reconciler since react 16.
its main goals are:
ability to split interruptible work in chunks.
ability to prioritize, rebase and reuse work in progress.
ability to yield back and forth between parents and children to support layout in react.
ability to return multiple elements from .
better support for error boundaries.
you can read more about react fiber architecture  and . while it has shipped with react 16, the async features are not enabled by default yet.
its source code is located in .
event system
react implements a layer over native events to smooth out cross-browser differences. its source code is located in .
read the  to learn about the pre-react 16 implementation of reconciler in more detail. we haven’t documented the internals of the new reconciler yet.is this page useful?
implementation notes
this section is a collection of implementation notes for the .
it is very technical and assumes a strong understanding of react public api as well as how it’s divided into core, renderers, and the reconciler. if you’re not very familiar with the react codebase, read  first.
it also assumes an understanding of the .
the stack reconciler was used in react 15 and earlier. it is located at .
video: building react from scratch
gave a talk about  that largely inspired this document.
both this document and his talk are simplifications of the real codebase so you might get a better understanding by getting familiar with both of them.
the reconciler itself doesn’t have a public api.  like react dom and react native use it to efficiently update the user interface according to the react components written by the user.
mounting as a recursive process
let’s consider the first time you mount a component:
will pass  along to the reconciler. remember that  is a react element, that is, a description of what to render. you can think about it as a plain object:
the reconciler will check if  is a class or a function.
if  is a function, the reconciler will call  to get the rendered element.
if  is a class, the reconciler will instantiate an  with , call the  lifecycle method, and then will call the  method to get the rendered element.
either way, the reconciler will learn the element  “rendered to”.
this process is recursive.  may render to a ,  may render to a , and so on. the reconciler will “drill down” through user-defined components recursively as it learns what each component renders to.
you can imagine this process as a pseudocode:
this really is a pseudo-code. it isn’t similar to the real implementation. it will also cause a stack overflow because we haven’t discussed when to stop the recursion.
let’s recap a few key ideas in the example above:
react elements are plain objects representing the component type (e.g. ) and the props.
user-defined components (e.g. ) can be classes or functions but they all “render to” elements.
“mounting” is a recursive process that creates a dom or native tree given the top-level react element (e.g. ).
mounting host elements
this process would be useless if we didn’t render something to the screen as a result.
in addition to user-defined (“composite”) components, react elements may also represent platform-specific (“host”) components. for example,  might return a  from its render method.
if element’s  property is a string, we are dealing with a host element:
there is no user-defined code associated with host elements.
when the reconciler encounters a host element, it lets the renderer take care of mounting it. for example, react dom would create a dom node.
if the host element has children, the reconciler recursively mounts them following the same algorithm as above. it doesn’t matter whether children are host (like ), composite (like ), or both.
the dom nodes produced by the child components will be appended to the parent dom node, and recursively, the complete dom structure will be assembled.
the reconciler itself is not tied to the dom. the exact result of mounting (sometimes called “mount image” in the source code) depends on the renderer, and can be a dom node (react dom), a string (react dom server), or a number representing a native view (react native).
if we were to extend the code to handle host elements, it would look like this:
this is working but still far from how the reconciler is really implemented. the key missing ingredient is support for updates.
introducing internal instances
the key feature of react is that you can re-render everything, and it won’t recreate the dom or reset the state:
however, our implementation above only knows how to mount the initial tree. it can’t perform updates on it because it doesn’t store all the necessary information, such as all the s, or which dom s correspond to which components.
the stack reconciler codebase solves this by making the  function a method and putting it on a class. there are drawbacks to this approach, and we are going in the opposite direction in the . nevertheless this is how it works now.
instead of separate  and  functions, we will create two classes:  and .
both classes have a constructor accepting the , as well as a  method returning the mounted node. we will replace a top-level  function with a factory that instantiates the correct class:
first, let’s consider the implementation of :
this is not much different from our previous  implementation, but now we can save some information, such as , , and , for use during updates.
note that an instance of  is not the same thing as an instance of the user-supplied .  is an implementation detail of our reconciler, and is never exposed to the user. the user-defined class is the one we read from , and  creates an instance of it.
to avoid the confusion, we will call instances of  and  “internal instances”. they exist so we can associate some long-lived data with them. only the renderer and the reconciler are aware that they exist.
in contrast, we call an instance of the user-defined class a “public instance”. the public instance is what you see as  in the  and other methods of your custom components.
the  function, refactored to be a  method on  class, also looks familiar:
the main difference after refactoring from  is that we now keep  and  associated with the internal dom component instance. we will also use them for applying non-destructive updates in the future.
as a result, each internal instance, composite or host, now points to its child internal instances. to help visualize this, if a function  component renders a  class component, and  class renders a , the internal instance tree would look like this:
in the dom you would only see the . however the internal instance tree contains both composite and host internal instances.
the composite internal instances need to store:
the current element.
the public instance if element type is a class.
the single rendered internal instance. it can be either a  or a .
the host internal instances need to store:
the dom node.
all the child internal instances. each of them can be either a  or a .
if you’re struggling to imagine how an internal instance tree is structured in more complex applications,  can give you a close approximation, as it highlights host instances with grey, and composite instances with purple:
to complete this refactoring, we will introduce a function that mounts a complete tree into a container node and a public instance:
unmounting
now that we have internal instances that hold onto their children and the dom nodes, we can implement unmounting. for a composite component, unmounting calls a lifecycle method and recurses.
for , unmounting tells each child to unmount:
in practice, unmounting dom components also removes the event listeners and clears some caches, but we will skip those details.
we can now add a new top-level function called  that is similar to :
in order for this to work, we need to read an internal root instance from a dom node. we will modify  to add the  property to the root dom node. we will also teach  to destroy any existing tree so it can be called multiple times:
now, running , or running  repeatedly, removes the old tree and runs the  lifecycle method on components.
updating
in the previous section, we implemented unmounting. however react wouldn’t be very useful if each prop change unmounted and mounted the whole tree. the goal of the reconciler is to reuse existing instances where possible to preserve the dom and the state:
we will extend our internal instance contract with one more method. in addition to  and , both  and  will implement a new method called :
its job is to do whatever is necessary to bring the component (and any of its children) up to date with the description provided by the .
this is the part that is often described as “virtual dom diffing” although what really happens is that we walk the internal tree recursively and let each internal instance receive an update.
updating composite components
when a composite component receives a new element, we run the  lifecycle method.
then we re-render the component with the new props, and get the next rendered element:
next, we can look at the rendered element’s . if the  has not changed since the last render, the component below can also be updated in place.
for example, if it returned  the first time, and  the second time, we can just tell the corresponding internal instance to  the next element:
however, if the next rendered element has a different  than the previously rendered element, we can’t update the internal instance. a  can’t “become” an .
instead, we have to unmount the existing internal instance and mount the new one corresponding to the rendered element type. for example, this is what happens when a component that previously rendered a  renders an :
to sum this up, when a composite component receives a new element, it may either delegate the update to its rendered internal instance, or unmount it and mount a new one in its place.
there is another condition under which a component will re-mount rather than receive an element, and that is when the element’s  has changed. we don’t discuss  handling in this document because it adds more complexity to an already complex tutorial.
note that we needed to add a method called  to the internal instance contract so that it’s possible to locate the platform-specific node and replace it during the update. its implementation is straightforward for both classes:
updating host components
host component implementations, such as , update differently. when they receive an element, they need to update the underlying platform-specific view. in case of react dom, this means updating the dom attributes:
then, host components need to update their children. unlike composite components, they might contain more than a single child.
in this simplified example, we use an array of internal instances and iterate over it, either updating or replacing the internal instances depending on whether the received  matches their previous . the real reconciler also takes element’s  in the account and track moves in addition to insertions and deletions, but we will omit this logic.
we collect dom operations on children in a list so we can execute them in batch:
as the last step, we execute the dom operations. again, the real reconciler code is more complex because it also handles moves:
and that is it for updating host components.
top-level updates
now that both  and  implement the  method, we can change the top-level  function to use it when the element  is the same as it was the last time:
now calling  two times with the same type isn’t destructive:
these are the basics of how react works internally.
what we left out
this document is simplified compared to the real codebase. there are a few important aspects we didn’t address:
components can render , and the reconciler can handle “empty slots” in arrays and rendered output.
the reconciler also reads  from the elements, and uses it to establish which internal instance corresponds to which element in an array. a bulk of complexity in the actual react implementation is related to that.
in addition to composite and host internal instance classes, there are also classes for “text” and “empty” components. they represent text nodes and the “empty slots” you get by rendering .
renderers use  to pass the host internal class to the reconciler. for example, react dom tells the reconciler to use  as the host internal instance implementation.
the logic for updating the list of children is extracted into a mixin called  which is used by the host internal instance class implementations both in react dom and react native.
the reconciler also implements support for  in composite components. multiple updates inside event handlers get batched into a single update.
the reconciler also takes care of attaching and detaching refs to composite components and host nodes.
lifecycle methods that are called after the dom is ready, such as  and , get collected into “callback queues” and are executed in a single batch.
react puts information about the current update into an internal object called “transaction”. transactions are useful for keeping track of the queue of pending lifecycle methods, the current dom nesting for the warnings, and anything else that is “global” to a specific update. transactions also ensure react “cleans everything up” after updates. for example, the transaction class provided by react dom restores the input selection after any update.
jumping into the code
is where the code like  and  from this tutorial lives. it takes care of mounting and unmounting top-level components.  is its react native analog.
is the equivalent of  in this tutorial. it implements the host component class for react dom renderer.  is its react native analog.
is the equivalent of  in this tutorial. it handles calling user-defined components and maintaining their state.
contains the switch that picks the right internal instance class to construct for an element. it is equivalent to  in this tutorial.
is a wrapper with , , and  methods. it calls the underlying implementations on the internal instances, but also includes some code around them that is shared by all internal instance implementations.
implements the logic for mounting, updating, and unmounting children according to the  of their elements.
implements processing the operation queue for child insertions, deletions, and moves independently of the renderer.
, , and  are really called , , and  in react codebase for legacy reasons, but they receive elements.
properties on the internal instances start with an underscore, e.g. . they are considered to be read-only public fields throughout the codebase.
future directions
stack reconciler has inherent limitations such as being synchronous and unable to interrupt the work or split it in chunks. there is a work in progress on the  with a . in the future, we intend to replace stack reconciler with it, but at the moment it is far from feature parity.
read the  to learn about the guiding principles we use for react development.is this page useful?
design principles
we wrote this document so that you have a better idea of how we decide what react does and what react doesn’t do, and what our development philosophy is like. while we are excited to see community contributions, we are not likely to choose a path that violates one or more of these principles.
this document assumes a strong understanding of react. it describes the design principles of react itself, not react components or applications.
for an introduction to react, check out  instead.
composition
the key feature of react is composition of components. components written by different people should work well together. it is important to us that you can add functionality to a component without causing rippling changes throughout the codebase.
for example, it should be possible to introduce some local state into a component without changing any of the components using it. similarly, it should be possible to add some initialization and teardown code to any component when necessary.
there is nothing “bad” about using state or lifecycle methods in components. like any powerful feature, they should be used in moderation, but we have no intention to remove them. on the contrary, we think they are integral parts of what makes react useful. we might enable  in the future, but both local state and lifecycle methods will be a part of that model.
components are often described as “just functions” but in our view they need to be more than that to be useful. in react, components describe any composable behavior, and this includes rendering, lifecycle, and state. some external libraries like  augment components with other responsibilities such as describing data dependencies. it is possible that those ideas might make it back into react too in some form.
common abstraction
in general we  that can be implemented in userland. we don’t want to bloat your apps with useless library code. however, there are exceptions to this.
for example, if react didn’t provide support for local state or lifecycle methods, people would create custom abstractions for them. when there are multiple abstractions competing, react can’t enforce or take advantage of the properties of either of them. it has to work with the lowest common denominator.
this is why sometimes we add features to react itself. if we notice that many components implement a certain feature in incompatible or inefficient ways, we might prefer to bake it into react. we don’t do it lightly. when we do it, it’s because we are confident that raising the abstraction level benefits the whole ecosystem. state, lifecycle methods, cross-browser event normalization are good examples of this.
we always discuss such improvement proposals with the community. you can find some of those discussions by the  label on the react issue tracker.
escape hatches
react is pragmatic. it is driven by the needs of the products written at facebook. while it is influenced by some paradigms that are not yet fully mainstream such as functional programming, staying accessible to a wide range of developers with different skills and experience levels is an explicit goal of the project.
if we want to deprecate a pattern that we don’t like, it is our responsibility to consider all existing use cases for it and  before we deprecate it. if some pattern that is useful for building apps is hard to express in a declarative way, we will  for it. if we can’t figure out a perfect api for something that we found necessary in many apps, we will  as long as it is possible to get rid of it later and it leaves the door open for future improvements.
stability
we value api stability. at facebook, we have more than 50 thousand components using react. many other companies, including  and , are also heavy users of react. this is why we are usually reluctant to change public apis or behavior.
however we think stability in the sense of “nothing changes” is overrated. it quickly turns into stagnation. instead, we prefer the stability in the sense of “it is heavily used in production, and when something changes, there is a clear (and preferably automated) migration path.”
when we deprecate a pattern, we study its internal usage at facebook and add deprecation warnings. they let us assess the impact of the change. sometimes we back out if we see that it is too early, and we need to think more strategically about getting the codebases to the point where they are ready for this change.
if we are confident that the change is not too disruptive and the migration strategy is viable for all use cases, we release the deprecation warning to the open source community. we are closely in touch with many users of react outside of facebook, and we monitor popular open source projects and guide them in fixing those deprecations.
given the sheer size of the facebook react codebase, successful internal migration is often a good indicator that other companies won’t have problems either. nevertheless sometimes people point out additional use cases we haven’t thought of, and we add escape hatches for them or rethink our approach.
we don’t deprecate anything without a good reason. we recognize that sometimes deprecations warnings cause frustration but we add them because deprecations clean up the road for the improvements and new features that we and many people in the community consider valuable.
for example, we added a  in react 15.2.0. many projects were affected by this. however fixing this warning is important so that we can introduce the support for  to react. there is a reason like this behind every deprecation that we add.
when we add a deprecation warning, we keep it for the rest of the current major version, and . if there is a lot of repetitive manual work involved, we release a  script that automates most of the change. codemods enable us to move forward without stagnation in a massive codebase, and we encourage you to use them as well.
you can find the codemods that we released in the  repository.
interoperability
we place high value in interoperability with existing systems and gradual adoption. facebook has a massive non-react codebase. its website uses a mix of a server-side component system called xhp, internal ui libraries that came before react, and react itself. it is important to us that any product team can  rather than rewrite their code to bet on it.
this is why react provides escape hatches to work with mutable models, and tries to work well together with other ui libraries. you can wrap an existing imperative ui into a declarative component, and vice versa. this is crucial for gradual adoption.
scheduling
even when your components are described as functions, when you use react you don’t call them directly. every component returns a , and that description may include both user-written components like  and platform-specific components like . it is up to react to “unroll”  at some point in the future and actually apply changes to the ui tree according to the render results of the components recursively.
this is a subtle distinction but a powerful one. since you don’t call that component function but let react call it, it means react has the power to delay calling it if necessary. in its current implementation react walks the tree recursively and calls render functions of the whole updated tree during a single tick. however in the future it might start .
this is a common theme in react design. some popular libraries implement the “push” approach where computations are performed when the new data is available. react, however, sticks to the “pull” approach where computations can be delayed until necessary.
react is not a generic data processing library. it is a library for building user interfaces. we think that it is uniquely positioned in an app to know which computations are relevant right now and which are not.
if something is offscreen, we can delay any logic related to it. if data is arriving faster than the frame rate, we can coalesce and batch updates. we can prioritize work coming from user interactions (such as an animation caused by a button click) over less important background work (such as rendering new content just loaded from the network) to avoid dropping frames.
to be clear, we are not taking advantage of this right now. however the freedom to do something like this is why we prefer to have control over scheduling, and why  is asynchronous. conceptually, we think of it as “scheduling an update”.
the control over scheduling would be harder for us to gain if we let the user directly compose views with a “push” based paradigm common in some variations of . we want to own the “glue” code.
it is a key goal for react that the amount of the user code that executes before yielding back into react is minimal. this ensures that react retains the capability to schedule and split work in chunks according to what it knows about the ui.
there is an internal joke in the team that react should have been called “schedule” because react does not want to be fully “reactive”.
developer experience
providing a good developer experience is important to us.
for example, we maintain  which let you inspect the react component tree in chrome and firefox. we have heard that it brings a big productivity boost both to the facebook engineers and to the community.
we also try to go an extra mile to provide helpful developer warnings. for example, react warns you in development if you nest tags in a way that the browser doesn’t understand, or if you make a common typo in the api. developer warnings and the related checks are the main reason why the development version of react is slower than the production version.
the usage patterns that we see internally at facebook help us understand what the common mistakes are, and how to prevent them early. when we add new features, we try to anticipate the common mistakes and warn about them.
we are always looking out for ways to improve the developer experience. we love to hear your suggestions and accept your contributions to make it even better.
when something goes wrong, it is important that you have breadcrumbs to trace the mistake to its source in the codebase. in react, props and state are those breadcrumbs.
if you see something wrong on the screen, you can open react devtools, find the component responsible for rendering, and then see if the props and state are correct. if they are, you know that the problem is in the component’s  function, or some function that is called by . the problem is isolated.
if the state is wrong, you know that the problem is caused by one of the  calls in this file. this, too, is relatively simple to locate and fix because usually there are only a few  calls in a single file.
if the props are wrong, you can traverse the tree up in the inspector, looking for the component that first “poisoned the well” by passing bad props down.
this ability to trace any ui to the data that produced it in the form of current props and state is very important to react. it is an explicit design goal that state is not “trapped” in closures and combinators, and is available to react directly.
while the ui is dynamic, we believe that synchronous  functions of props and state turn debugging from guesswork into a boring but finite procedure. we would like to preserve this constraint in react even though it makes some use cases, like complex animations, harder.
configuration
we find global runtime configuration options to be problematic.
for example, it is occasionally requested that we implement a function like  or . however this poses multiple problems, and we are not aware of good solutions to them.
what if somebody calls such a function from a third-party component library? what if one react app embeds another react app, and their desired configurations are incompatible? how can a third-party component specify that it requires a particular configuration? we think that global configuration doesn’t work well with composition. since composition is central to react, we don’t provide global configuration in code.
we do, however, provide some global configuration on the build level. for example, we provide separate development and production builds. we may also  in the future, and we are open to considering other build flags.
beyond the dom
we see the value of react in the way it allows us to write components that have fewer bugs and compose together well. dom is the original rendering target for react but  is just as important both to facebook and the community.
being renderer-agnostic is an important design constraint of react. it adds some overhead in the internal representations. on the other hand, any improvements to the core translate across platforms.
having a single programming model lets us form engineering teams around products instead of platforms. so far the tradeoff has been worth it for us.
implementation
we try to provide elegant apis where possible. we are much less concerned with the implementation being elegant. the real world is far from perfect, and to a reasonable extent we prefer to put the ugly code into the library if it means the user does not have to write it. when we evaluate new code, we are looking for an implementation that is correct, performant and affords a good developer experience. elegance is secondary.
we prefer boring code to clever code. code is disposable and often changes. so it is important that it . verbose code that is easy to move around, change and remove is preferred to elegant code that is prematurely abstracted and hard to change.
optimized for tooling
some commonly used apis have verbose names. for example, we use  instead of  or . this is . the goal is to make the points of interaction with the library highly visible.
in a massive codebase like facebook, being able to search for uses of specific apis is very important. we value distinct verbose names, and especially for the features that should be used sparingly. for example,  is hard to miss in a code review.
optimizing for search is also important because of our reliance on  to make breaking changes. we want it to be easy and safe to apply vast automated changes across the codebase, and unique verbose names help us achieve this. similarly, distinctive names make it easy to write custom  about using react without worrying about potential false positives.
plays a similar role. while it is not required with react, we use it extensively at facebook both for aesthetic and pragmatic reasons.
in our codebase, jsx provides an unambiguous hint to the tools that they are dealing with a react element tree. this makes it possible to add build-time optimizations such as , safely lint and codemod internal component usage, and  into the warnings.
dogfooding
we try our best to address the problems raised by the community. however we are likely to prioritize the issues that people are also experiencing internally at facebook. perhaps counter-intuitively, we think this is the main reason why the community can bet on react.
heavy internal usage gives us the confidence that react won’t disappear tomorrow. react was created at facebook to solve its problems. it brings tangible business value to the company and is used in many of its products.  it means that our vision stays sharp and we have a focused direction going forward.
this doesn’t mean that we ignore the issues raised by the community. for example, we added support for  and  to react even though we don’t rely on either of them internally. we are actively  and  to the best of our ability. the community is what makes react special to us, and we are honored to contribute back.
after releasing many open source projects at facebook, we have learned that trying to make everyone happy at the same time produced projects with poor focus that didn’t grow well. instead, we found that picking a small audience and focusing on making them happy brings a positive net effect. that’s exactly what we did with react, and so far solving the problems encountered by facebook product teams has translated well to the open source community.
the downside of this approach is that sometimes we fail to give enough focus to the things that facebook teams don’t have to deal with, such as the “getting started” experience. we are acutely aware of this, and we are thinking of how to improve in a way that would benefit everyone in the community without making the same mistakes we did with open source projects before.is this page useful?
ajax and apis
how can i make an ajax call?
you can use any ajax library you like with react. some popular ones are , , and the browser built-in .
where in the component lifecycle should i make an ajax call?
you should populate data with ajax calls in the  lifecycle method. this is so you can use  to update your component when the data is retrieved.
example: using ajax results to set local state
the component below demonstrates how to make an ajax call in  to populate local component state.
the example api returns a json object like this:
here is the equivalent with :
babel, jsx, and build steps
do i need to use jsx with react?
no! check out  to learn more.
do i need to use es6 (+) with react?
how can i write comments in jsx?
passing functions to components
how do i pass an event handler (like onclick) to a component?
pass event handlers and other functions as props to child components:
if you need to have access to the parent component in the handler, you also need to bind the function to the component instance (see below).
how do i bind a function to a component instance?
there are several ways to make sure functions have access to component attributes like  and , depending on which syntax and build steps you are using.
bind in constructor (es2015)
class properties (es2022)
bind in render
using  in render creates a new function each time the component renders, which may have performance implications (see below).
arrow function in render
using an arrow function in render creates a new function each time the component renders, which may break optimizations based on strict identity comparison.
is it ok to use arrow functions in render methods?
generally speaking, yes, it is ok, and it is often the easiest way to pass parameters to callback functions.
if you do have performance issues, by all means, optimize!
why is binding necessary at all?
in javascript, these two code snippets are not equivalent:
binding methods helps ensure that the second snippet works the same way as the first one.
with react, typically you only need to bind the methods you pass to other components. for example,  passes  so you want to bind it. however, it is unnecessary to bind the  method or the lifecycle methods: we don’t pass them to other components.
explains what binding is, and how functions work in javascript, in detail.
why is my function being called every time the component renders?
make sure you aren’t calling the function when you pass it to the component:
instead, pass the function itself (without parens):
how do i pass a parameter to an event handler or callback?
you can use an arrow function to wrap around an event handler and pass parameters:
this is equivalent to calling :
example: passing params using arrow functions
example: passing params using data-attributes
alternately, you can use dom apis to store data needed for event handlers. consider this approach if you need to optimize a large number of elements or have a render tree that relies on react.purecomponent equality checks.
how can i prevent a function from being called too quickly or too many times in a row?
if you have an event handler such as  or  and want to prevent the callback from being fired too quickly, then you can limit the rate at which callback is executed. this can be done by using:
throttling: sample changes based on a time based frequency (eg )
debouncing: publish changes after a period of inactivity (eg )
throttling: sample changes based on  (eg )
see  for a comparison of  and  functions.
,  and  provide a  method to cancel delayed callbacks. you should either call this method from  or check to ensure that the component is still mounted within the delayed function.
throttle
throttling prevents a function from being called more than once in a given window of time. the example below throttles a “click” handler to prevent calling it more than once per second.
debounce
debouncing ensures that a function will not be executed until after a certain amount of time has passed since it was last called. this can be useful when you have to perform some expensive calculation in response to an event that might dispatch rapidly (eg scroll or keyboard events). the example below debounces text input with a 250ms delay.
throttling
is a way of queuing a function to be executed in the browser at the optimal time for rendering performance. a function that is queued with  will fire in the next frame. the browser will work hard to ensure that there are 60 frames per second (60 fps). however, if the browser is unable to it will naturally limit the amount of frames in a second. for example, a device might only be able to handle 30 fps and so you will only get 30 frames in that second. using  for throttling is a useful technique in that it prevents you from doing more than 60 updates in a second. if you are doing 100 updates in a second this creates additional work for the browser that the user will not see anyway.
using this technique will only capture the last published value in a frame. you can see an example of how this optimization works on
testing your rate limiting
when testing your rate limiting code works correctly it is helpful to have the ability to fast forward time. if you are using  then you can use  to fast forward time. if you are using  throttling then you may find  to be a useful tool to control the ticking of animation frames.is this page useful?
component state
what does  do?
schedules an update to a component’s  object. when state changes, the component responds by re-rendering.
what is the difference between  and ?
(short for “properties”) and  are both plain javascript objects. while both hold information that influences the output of render, they are different in one important way:  get passed to the component (similar to function parameters) whereas  is managed within the component (similar to variables declared within a function).
here are some good resources for further reading on when to use  vs :
why is  giving me the wrong value?
in react, both  and  represent the rendered values, i.e. what’s currently on the screen.
calls to  are asynchronous - don’t rely on  to reflect the new value immediately after calling . pass an updater function instead of an object if you need to compute values based on the current state (see below for details).
example of code that will not behave as expected:
see below for how to fix this problem.
how do i update state with values that depend on the current state?
pass a function instead of an object to  to ensure the call always uses the most updated version of state (see below).
what is the difference between passing an object or a function in ?
passing an update function allows you to access the current state value inside the updater. since  calls are batched, this lets you chain updates and ensure they build on top of each other instead of conflicting:
when is  asynchronous?
currently,  is asynchronous inside event handlers.
this ensures, for example, that if both  and  call  during a click event,  isn’t re-rendered twice. instead, react “flushes” the state updates at the end of the browser event. this results in significant performance improvements in larger apps.
this is an implementation detail so avoid relying on it directly. in the future versions, react will batch updates by default in more cases.
why doesn’t react update  synchronously?
as explained in the previous section, react intentionally “waits” until all components call  in their event handlers before starting to re-render. this boosts performance by avoiding unnecessary re-renders.
however, you might still be wondering why react doesn’t just update  immediately without re-rendering.
there are two main reasons:
this would break the consistency between  and , causing issues that are very hard to debug.
this would make some of the new features we’re working on impossible to implement.
this  dives deep into the specific examples.
should i use a state management library like redux or mobx?
it’s a good idea to get to know react first, before adding in additional libraries. you can build quite complex applications using only react.is this page useful?
styling and css
how do i add css classes to components?
pass a string as the  prop:
it is common for css classes to depend on the component props or state:
if you often find yourself writing code like this,  package can simplify it.
can i use inline styles?
yes, see the docs on styling .
are inline styles bad?
css classes are generally better for performance than inline styles.
what is css-in-js?
“css-in-js” refers to a pattern where css is composed using javascript instead of defined in external files.
note that this functionality is not a part of react, but provided by third-party libraries. react does not have an opinion about how styles are defined; if in doubt, a good starting point is to define your styles in a separate  file as usual and refer to them using .
can i do animations in react?
react can be used to power animations. see , , , or , for example.is this page useful?
file structure
is there a recommended way to structure react projects?
react doesn’t have opinions on how you put files into folders. that said there are a few common approaches popular in the ecosystem you may want to consider.
grouping by features or routes
one common way to structure projects is to locate css, js, and tests together inside folders grouped by feature or route.
the definition of a “feature” is not universal, and it is up to you to choose the granularity. if you can’t come up with a list of top-level folders, you can ask the users of your product what major parts it consists of, and use their mental model as a blueprint.
grouping by file type
another popular way to structure projects is to group similar files together, for example:
some people also prefer to go further, and separate components into different folders depending on their role in the application. for example,  is a design methodology built on this principle. remember that it’s often more productive to treat such methodologies as helpful examples rather than strict rules to follow.
avoid too much nesting
there are many pain points associated with deep directory nesting in javascript projects. it becomes harder to write relative imports between them, or to update those imports when the files are moved. unless you have a very compelling reason to use a deep folder structure, consider limiting yourself to a maximum of three or four nested folders within a single project. of course, this is only a recommendation, and it may not be relevant to your project.
don’t overthink it
if you’re just starting a project,  on choosing a file structure. pick any of the above approaches (or come up with your own) and start writing code! you’ll likely want to rethink it anyway after you’ve written some real code.
if you feel completely stuck, start by keeping all files in a single folder. eventually it will grow large enough that you will want to separate some files from the rest. by that time you’ll have enough knowledge to tell which files you edit together most often. in general, it is a good idea to keep files that often change together close to each other. this principle is called “colocation”.
as projects grow larger, they often use a mix of both of the above approaches in practice. so choosing the “right” one in the beginning isn’t very important.is this page useful?
versioning policy
react follows  principles.
that means that with a version number x.y.z:
when releasing critical bug fixes, we make a patch release by changing the z number (ex: 15.6.2 to 15.6.3).
when releasing new features or non-critical fixes, we make a minor release by changing the y number (ex: 15.6.2 to 15.7.0).
when releasing breaking changes, we make a major release by changing the x number (ex: 15.6.2 to 16.0.0).
major releases can also contain new features, and any release can include bug fixes.
minor releases are the most common type of release.
this versioning policy does not apply to prerelease builds in the next or experimental channels.
breaking changes
breaking changes are inconvenient for everyone, so we try to minimize the number of major releases – for example, react 15 was released in april 2016 and react 16 was released in september 2017, and react 17 was released in october 2020.
instead, we release new features in minor versions. that means that minor releases are often more interesting and compelling than majors, despite their unassuming name.
commitment to stability
as we change react over time, we try to minimize the effort required to take advantage of new features. when possible, we’ll keep an older api working, even if that means putting it in a separate package. for example,  but they’re supported to this day  and many codebases continue to use them in stable, legacy code.
over a million developers use react, collectively maintaining millions of components. the facebook codebase alone has over 50,000 react components. that means we need to make it as easy as possible to upgrade to new versions of react; if we make large changes without a migration path, people will be stuck on old versions. we test these upgrade paths on facebook itself – if our team of less than 10 people can update 50,000+ components alone, we hope the upgrade will be manageable for anyone using react. in many cases, we write  to upgrade component syntax, which we then include in the open-source release for everyone to use.
gradual upgrades via warnings
development builds of react include many helpful warnings. whenever possible, we add warnings in preparation for future breaking changes. that way, if your app has no warnings on the latest release, it will be compatible with the next major release. this allows you to upgrade your apps one component at a time.
development warnings won’t affect the runtime behavior of your app. that way, you can feel confident that your app will behave the same way between the development and production builds — the only differences are that the production build won’t log the warnings and that it is more efficient. (if you ever notice otherwise, please file an issue.)
what counts as a breaking change?
in general, we don’t bump the major version number for changes to:
development warnings. since these don’t affect production behavior, we may add new warnings or modify existing warnings in between major versions. in fact, this is what allows us to reliably warn about upcoming breaking changes.
apis starting with . these are provided as experimental features whose apis we are not yet confident in. by releasing these with an  prefix, we can iterate faster and get to a stable api sooner.
alpha and canary versions of react. we provide alpha versions of react as a way to test new features early, but we need the flexibility to make changes based on what we learn in the alpha period. if you use these versions, note that apis may change before the stable release.
undocumented apis and internal data structures. if you access internal property names like  or , there is no warranty.  you are on your own.
this policy is designed to be pragmatic: certainly, we don’t want to cause headaches for you. if we bumped the major version for all of these changes, we would end up releasing more major versions and ultimately causing more versioning pain for the community. it would also mean that we can’t make progress in improving react as fast as we’d like.
that said, if we expect that a change on this list will cause broad problems in the community, we will still do our best to provide a gradual migration path.
if a minor release includes no new features, why isn’t it a patch?
it’s possible that a minor release will not include new features. , which states ”[a minor version] may be incremented if substantial new functionality or improvements are introduced within the private code. it may include patch level changes.”
however, it does raise the question of why these releases aren’t versioned as patches instead.
the answer is that any change to react (or other software) carries some risk of breaking in unexpected ways. imagine a scenario where a patch release that fixes one bug accidentally introduces a different bug. this would not only be disruptive to developers, but also harm their confidence in future patch releases. it’s especially regrettable if the original fix is for a bug that is rarely encountered in practice.
we have a pretty good track record for keeping react releases free of bugs, but patch releases have an even higher bar for reliability because most developers assume they can be adopted without adverse consequences.
for these reasons, we reserve patch releases only for the most critical bugs and security vulnerabilities.
if a release includes non-essential changes — such as internal refactors, changes to implementation details, performance improvements, or minor bugfixes — we will bump the minor version even when there are no new features.is this page useful?
virtual dom and internals
what is the virtual dom?
the virtual dom (vdom) is a programming concept where an ideal, or “virtual”, representation of a ui is kept in memory and synced with the “real” dom by a library such as reactdom. this process is called .
this approach enables the declarative api of react: you tell react what state you want the ui to be in, and it makes sure the dom matches that state. this abstracts out the attribute manipulation, event handling, and manual dom updating that you would otherwise have to use to build your app.
since “virtual dom” is more of a pattern than a specific technology, people sometimes say it to mean different things. in react world, the term “virtual dom” is usually associated with  since they are the objects representing the user interface. react, however, also uses internal objects called “fibers” to hold additional information about the component tree. they may also be considered a part of “virtual dom” implementation in react.
is the shadow dom the same as the virtual dom?
no, they are different. the shadow dom is a browser technology designed primarily for scoping variables and css in web components. the virtual dom is a concept implemented by libraries in javascript on top of browser apis.
what is “react fiber”?
fiber is the new reconciliation engine in react 16. its main goal is to enable incremental rendering of the virtual dom. .is this page useful?
installing tailwind cli
the simplest and fastest way to get up and running with tailwind css from scratch is with the tailwind cli tool. the cli is also available as a  if you want to use it without installing node.js.
syntax support
tailwind css uses a lot of custom css at-rules like , , and , and in many editors this can trigger warnings or errors where these rules aren’t recognized.
the solution to this is almost always to install a plugin for your editor/ide for postcss language support instead of regular css.
if you’re using vs code, our official  plugin includes a dedicated tailwind css language mode that has support for all of the custom at-rules and functions tailwind uses.
in some cases, you may need to disable native css linting/validations if your editor is very strict about the syntax it expects in your css files.
intellisense for vs code
the official  extension for visual studio code enhances the tailwind development experience by providing users with advanced features such as autocomplete, syntax highlighting, and linting.
autocomplete. intelligent suggestions for class names, as well as .linting. highlights errors and potential bugs in both your css and your markup.hover previews. see the complete css for a tailwind class name by hovering over it.syntax highlighting. provides syntax definitions so that tailwind features are highlighted correctly.
check out the project  to learn more, or  to get started now.
automatic class sorting with prettier
we maintain an official  for tailwind css that automatically sorts your classes following our .
it works seamlessly with custom tailwind configurations, and because it’s just a prettier plugin, it works anywhere prettier works — including every popular editor and ide, and of course on the command line.
check out the plugin  learn more and get started.
jetbrains ides
jetbrains ides like webstorm, phpstorm, and others include support for intelligent tailwind css completions in your html and when using .
since tailwind is a postcss plugin, there’s nothing stopping you from using it with sass, less, stylus, or other preprocessors, just like you can with other postcss plugins like .
it’s important to note that you don’t need to use a preprocessor with tailwind — you typically write very little css on a tailwind project anyways so using a preprocessor just isn’t as beneficial as it would be in a project where you write a lot of custom css.
this guide only exists as a reference for people who need to integrate tailwind with a preprocessor for reasons outside of their control, not because it is a recommended practice.
using postcss as your preprocessor
if you’re using tailwind for a brand new project and don’t need to integrate it with any existing sass/less/stylus stylesheets, you should highly consider relying on other postcss plugins to add the preprocessor features you use instead of using a separate preprocessor.
this has a few benefits:
your builds will be faster. since your css doesn’t have to be parsed and processed by multiple tools, your css will compile much quicker using only postcss.no quirks or workarounds. because tailwind adds some new non-standard keywords to css (like , , , etc.), you often have to write your css in annoying, unintuitive ways to get a preprocessor to give you the expected output. working exclusively with postcss avoids this.
for a fairly comprehensive list of available postcss plugins see the , but here are a few important ones we use on our own projects and can recommend.
build-time imports
one of the most useful features preprocessors offer is the ability to organize your css into multiple files and combine them at build time by processing  statements in advance, instead of in the browser.
the canonical plugin for handling this with postcss is .
to use it, install the plugin via npm:
then add it as the very first plugin in your postcss configuration:
one important thing to note about  is that it strictly adheres to the css spec and disallows  statements anywhere except at the very top of a file.
won't work, `@import` statements must come first
the easiest solution to this problem is to never mix regular css and imports in the same file. instead, create one main entry-point file for your imports, and keep all of your actual css in separate files.
use separate files for imports and actual css
the place you are most likely to run into this situation is in your main css file that includes your  declarations.
you can solve this by creating separate files for each  declaration, and then importing those files in your main stylesheet. to make this easy, we provide separate files for each  declaration out of the box that you can import directly from .
the  plugin is smart enough to look for files in the  folder automatically, so you don’t need to provide the entire path —  for example is enough.
import our provided css files
nesting
to add support for nested declarations, we recommend our bundled  plugin, which is a postcss plugin that wraps  or  and acts as a compatibility layer to make sure your nesting plugin of choice properly understands tailwind’s custom syntax like  and .
it’s included directly in the  package itself, so to use it all you need to do is add it to your postcss configuration, somewhere before tailwind:
by default, it uses the  plugin under the hood, which uses a sass-like syntax and is the plugin that powers nesting support in the .
if you’d rather use  (which is based on the work-in-progress  specification), first install the plugin:
then pass the plugin itself as an argument to  in your postcss configuration:
this can also be helpful if for whatever reason you need to use a very specific version of  and want to override the version we bundle with  itself.
note that if you are using  in your project, you should make sure to disable nesting and let  handle it for you instead:
variables
these days css variables (officially known as custom properties) have really good , so you don’t need a preprocessor to use variables at all.
we use css variables extensively within tailwind itself, so if you can use tailwind, you can use native css variables.
you may also find that most of the things you’ve used variables for in the past can be replaced with tailwind’s  function, which gives you access to all of your design tokens from your  file directly in your css:
learn more about the  function in our ;
vendor prefixes
for automatically managing vendor prefixes in your css, you should use .
to use it, install it via npm:
then add it to the very end of your plugin list in your postcss configuration:
using sass, less, or stylus
for the best development experience, we highly recommended that you , and that you don’t use preprocessors like sass or less in your tailwind projects.
to use tailwind with a preprocessing tool like sass, less, or stylus, you’ll need to add an additional build step to your project that lets you run your preprocessed css through postcss. if you’re using autoprefixer in your project, you already have something like this set up.
see our documentation on  to learn more about integrating tailwind into your existing build process.
the most important thing to understand about using tailwind with a preprocessor is that preprocessors like sass, less, and stylus run separately, before tailwind. this means that you can’t feed output from tailwind’s  function into a sass color function for example, because the  function isn’t actually evaluated until your sass has been compiled to css and fed into postcss.
won't work, sass is processed first
aside from that, each preprocessor has its own quirk or two when used with tailwind, which are outlined with workarounds below.
sass
when using tailwind with sass, using  with  requires you to use interpolation to compile properly.
won't work, sass complains about !important
use interpolation as a workaround
less
when using tailwind with less, you cannot nest tailwind’s  directive.
won't work, less doesn't realise it's a media query
instead, use a regular media query along with the  function to reference your screen sizes, or simply don’t nest your  directives.
use a regular media query and theme()
use the @screen directive at the top-level
stylus
when using tailwind with stylus, you can’t use tailwind’s  feature without wrapping the entire css rule in  so that stylus treats it as literal css:
won't work, stylus complains about @apply
use @css to avoid processing as stylus
this comes with a significant cost however, which is that you cannot use any stylus features inside a  block.
another option is to use the  function instead of , and write out the actual css properties in long form:
use theme() instead of @apply
in addition to this, stylus doesn’t support nesting the  directive (just like less).
won't work, stylus doesn't realise it's a media query
tailwind css is incredibly performance focused and aims to produce the smallest css file possible by only generating the css you are actually using in your project.
combined with minification and network compression, this usually leads to css files that are less than 10kb, even for large projects. for example, netflix uses tailwind for  and the entire website delivers only 6.5kb of css over the network.
with css files this small, you don’t have to worry about complex solutions like code-splitting your css for each page, and can instead just ship a single small css file that’s downloaded once and cached until you redeploy your site.
for the smallest possible production build, we recommend minifying your css with a tool like , and compressing your css with .
if you’re using tailwind cli, you can minify your css by adding the  flag:
if you’ve installed tailwind as a postcss plugin, add  to the end of your plugin list:
postcss.config.js
if you’re using a framework, check the documentation as this is often handled for you in production automatically and you don’t even need to configure it.
in general, tailwind css v3.0 is designed for and tested on the latest stable versions of chrome, firefox, edge, and safari. it does not support any version of ie, including ie 11.
while most of the features in tailwind css will work in all modern browsers, tailwind also includes apis for several bleeding-edge features that aren’t yet supported by all browsers, for example the  pseudo-class and  utilities.
since tailwind is such a low-level framework, it’s easy to avoid these features if you can’t use them by simply not using the utility or modifier that’s not supported, much like how you just wouldn’t use those css features in your css.
the  database is a great resource when you’re unsure about the support for an unfamiliar css feature.
many css properties require vendor prefixes to be understood by browsers, for example  needs the  prefix to work in most browsers:
if you’re using the tailwind cli tool, vendor prefixes like this will be added automatically.
if not, we recommend that you use , which is a postcss plugin that automatically adds any necessary vendor prefixes based on the browsers you tell it you need to support.
to learn more about specifying which browsers you need to support, check out  which is the standard way to define target browsers in front-end tooling.
tailwind css v3.0 is a major update to the framework with a brand new internal engine and as such includes a small number of breaking changes.
we take stability very seriously and have worked hard to make any breaking changes as painless as possible. for most projects, upgrading to tailwind css v3.0 should take less than 30 minutes.
to learn more about what’s new in tailwind css v3.0, read the  on our blog.
upgrade packages
update tailwind, as well as postcss and autoprefixer, using npm:
note that tailwind css v3.0 requires postcss 8, and no longer supports postcss 7. if you can’t upgrade to postcss 8, we recommend using  instead of installing tailwind as a postcss plugin.
if you are using nesting in your custom css (in combination with a postcss nesting plugin), you should also  in your postcss configuration to ensure compatibility with tailwind css v3.0.
official plugins
all of our first-party plugins have been updated for compatibility with v3.0.
if you’re using any of our plugins, make sure to update them all to the latest version at the same time to avoid version constraint errors.
play cdn
for tailwind css v3.0, the css-based cdn build we’ve offered in the past has been replaced by the new , which gives you the full power of the new engine right in the browser with no build step.
to try it out, throw this  tag in your :
the play cdn is designed for development purposes only — compiling your own static css build is a much better choice in production.
migrating to the jit engine
the new  we announced in march has replaced the classic engine in tailwind css v3.0.
the new engine generates the styles you need for your project on-demand, and might necessitate some small changes to your project depending on how you have tailwind configured.
if you were already opting in to  in tailwind css v2.x, you can safely remove that from your configuration in v3.0:
tailwind.config.js
configure content sources
since tailwind no longer uses purgecss under the hood, we’ve renamed the  option to  to better reflect what it’s for:
if you weren’t already using the  option in your project, it’s crucial that you configure your template paths now or your compiled css will be empty.
since we’re not using purgecss under the hood anymore, some of the advanced purge options have changed. see the new  documentation for more information on advanced options.
remove dark mode configuration
the dark mode feature is now enabled using the  strategy by default, so you can remove this key entirely from your  file, unless you’re using the  strategy.
you can also safely remove this key if it’s currently set to :
remove variant configuration
in tailwind css v3.0, every variant is automatically available for every utility by default, so you can remove the  section from your  file:
replace @variants with @layer
since all variants are now enabled by default, you no longer need to explicity enable these for custom css using the  or  directives.
instead, add any custom css to appropriate “layer” using the  directive:
any custom css added to one of tailwind’s layers will automatically support variants.
see the documentation on  for more information.
automatic transforms and filters
in tailwind css v3.0, transform and filter utilities like  and  will automatically take effect without needing to add the , , or  classes:
while there’s no harm in leaving them in your html, they can safely be removed — with one exception. if you’re relying on  to create a new stacking context, you may want to leave it, otherwise you may run into z-order issues. alternatively, replace it with  or  to force a new stacking context.
new opacity modifier syntax
the new engine introduces  for changing the opacity of color utilities that we recommend migrating to from utilities like :
the old approach still works in all cases except when using a  utility with the default  class — in v3 you’ll need to explicitly specify your border color:
every other situation behaves the same, so aside from that change your projects will work exactly as they did before. we do recommend using the new syntax going forward though, and plan to disable utilities like  and  by default in v4, though you will still be able to enable them if needed.
this new syntax works for all color utilities, even utilities that didn’t have any way to change the opacity in the past like .
color palette changes
tailwind css v3.0 now includes every color from the extended color palette by default, including previously disabled colors like cyan, rose, fuchsia, and lime, and all five variations of gray.
removed color aliases
in v2.0, several of the default colors were actually aliases for the extended colors:
in v3.0, these colors use their extended names by default, so what was previously  is now , and  now refers to the green from the extended palette.
if you’re using these colors in your project, the simplest way to upgrade is to alias them back to their previous names in your  file:
if you are already using a custom color palette, this change doesn’t impact you at all.
renamed gray scales
as part of enabling all of the extended colors by default, we’ve given the different gray shades shorter single-word names to make them more practical to use and make it less awkward for them to co-exist at the same time.
if you were referencing any of the extended grays, you should update your references to the new names, for example:
if you weren’t referencing any of the grays from the extended color palette, this change doesn’t impact you at all.
class name changes
some class names in tailwind css v3.0 have changed to avoid naming collisions, improve the developer experience, or make it possible to support new features.
wherever possible we have preserved the old name as well so many of these changes are non-breaking, but you’re encouraged to update to the new class names.
overflow-clip/ellipsis
those damn browser developers added a real  property, so using  for  is a really bad idea now.
we’ve renamed  to , and renamed  to  to avoid the naming collision:
this is extremely unlikely to affect anyone, as there are very few use-cases for  and it’s only really included for the sake of completion.
flex-grow/shrink
we’ve added  and  as aliases for  and :
the old class names will always work but you’re encouraged to update to the new ones.
outline-black/white
since browsers are finally starting to respect border radius when rendering outlines, we’ve added separate utilities for the , ,  and  properties.
this means that  and  now only set the outline color, whereas in v2 they set the color, width, style, and offset.
if you are using  or  in your project, you can bring back the old styles by adding the following custom css to your project:
alternatively, you can update any usage of them in your css with the following classes:
decoration-clone/slice
we’ve added  and  as aliases for  and  to avoid confusion with all of the new  utilities that use the  namespace:
other minor changes
tailwind css v3.0 necessitates a couple of other small breaking changes that are unlikely to affect many people, but have been captured here.
separator cannot be a dash
the dash () character cannot be used as a custom separator in v3.0 because of a parsing ambiguity it introduces in the engine.
you’ll have to switch to another character like  instead:
prefix cannot be a function
prior to tailwind css v3.0, it was possible to define your class prefix as a function:
this isn’t possible in the new engine and we’ve had to remove support for this feature.
instead, use a static prefix that is the same for every class tailwind generates:
file modifier order reversed
super minor change since v3.0.0-alpha.2 where the  modifier was introduced — if you were combining it with other modifiers like  or , you’ll need to flip the modifier order:
learn more in the  documentation.
fill and stroke use color palette
the  and  utilities mirror your  key by default now. this isn’t a breaking change if you haven’t customized your color palette, but if you have, the  and  classes may not work if you don’t have  included in your own custom color palette.
add  to your custom color palette to resolve this:
negative values removed
the negative prefix in utilites like  is a first class feature in tailwind now, rather than something driven by your theme, so you can add  in front of any utility that support negative values and it will just work.
the negative values have been removed from the default theme, so if you were referencing them with , you will see an error when trying to compile your css.
use the  function to update any affected code:
base layer must be present
in tailwind css v3.0, the  directive must be present for utilities like transforms, filters, and shadows to work as expected.
if you were previously disabling tailwind’s base styles by not including this directive, you should add it back and disable  in your  configuration instead:
main.css
this will disable tailwind’s global base styles without affecting utilities that rely on adding their own base styles to function correctly.
screens layer has been renamed
the  layer has been renamed to :
i think you are more likely to be attacked by a shark while working at your desk than you are to be affected by this change.
traditionally, whenever you need to style something on the web, you write css.
using a traditional approach where custom designs require custom css
chitchat
you have a new message!
with tailwind, you style elements by applying pre-existing classes directly in your html.
using utility classes to build custom designs without writing css
in the example above, we’ve used:
tailwind’s  and  utilities (, , and ) to control the overall card layoutthe  and  utilities ( and ) to constrain the card width and center it horizontallythe , , and  utilities (, , and ) to style the card’s appearancethe  and  utilities ( and ) to size the logo imagethe  utilities () to handle the spacing between the logo and the textthe , , and  utilities (, , , etc.) to style the card text
this approach allows us to implement a completely custom component design without writing a single line of custom css.
now i know what you’re thinking, “this is an atrocity, what a horrible mess!” and you’re right, it’s kind of ugly. in fact it’s just about impossible to think this is a good idea the first time you see it — you have to actually try it.
but once you’ve actually built something this way, you’ll quickly notice some really important benefits:
you aren’t wasting energy inventing class names. no more adding silly class names like  just to be able to style something, and no more agonizing over the perfect abstract name for something that’s really just a flex container.your css stops growing. using a traditional approach, your css files get bigger every time you add a new feature. with utilities, everything is reusable so you rarely need to write new css.making changes feels safer. css is global and you never know what you’re breaking when you make a change. classes in your html are local, so you can change them without worrying about something else breaking.
when you realize how productive you can be working exclusively in html with predefined utility classes, working any other way will feel like torture.
why not just use inline styles?
a common reaction to this approach is wondering, “isn’t this just inline styles?” and in some ways it is — you’re applying styles directly to elements instead of assigning them a class name and then styling that class.
but using utility classes has a few important advantages over inline styles:
designing with constraints. using inline styles, every value is a magic number. with utilities, you’re choosing styles from a predefined , which makes it much easier to build visually consistent uis.responsive design. you can’t use media queries in inline styles, but you can use tailwind’s  to build fully responsive interfaces easily.hover, focus, and other states. inline styles can’t target states like hover or focus, but tailwind’s  make it easy to style those states with utility classes.
this component is fully responsive and includes a button with hover and focus styles, and is built entirely with utility classes:
erin lindford
product engineer
message
maintainability concerns
the biggest maintainability concern when using a utility-first approach is managing commonly repeated utility combinations.
this is easily solved by , and using  like multi-cursor editing and simple loops.
aside from that, maintaining a utility-first css project turns out to be a lot easier than maintaining a large css codebase, simply because html is so much easier to maintain than css. large companies like github, netflix, heroku, kickstarter, twitch, segment, and more are using this approach with great success.
if you’d like to hear about others’ experiences with this approach, check out the following resources:
by john polacek by sarah dayan of algolia by sarah dayan of algolia, a podcast interview
for even more, check out , curated by .
every utility class in tailwind can be applied conditionally by adding a modifier to the beginning of the class name that describes the condition you want to target.
for example, to apply the  class on hover, use the  class:
hover over this button to see the background color change
save changes
when writing css the traditional way, a single class name would do different things based on the current state.traditionally the same class name applies different styles on hoverin tailwind, rather than adding the styles for a hover state to an existing class, you add another class to the element that only does something on hover.in tailwind, separate classes are used for the default state and the hover statenotice how  only defines styles for the  state? it does nothing by default, but as soon as you hover over an element with that class, the background color will change to .this is what we mean when we say a utility class can be applied conditionally — by using modifiers you can control exactly how your design behaves in different states, without ever leaving your html.
tailwind includes modifiers for just about everything you’ll ever need, including:
, like , , , and , like , , , and , like responsive breakpoints, dark mode, and , like  and
these modifiers can even be  to target more specific situations, for example changing the background color in dark mode, at the medium breakpoint, on hover:
in this guide you’ll learn about every modifier available in the framework, how to use them with your own custom classes, and even how to create your own.
pseudo-classes
hover, focus, and active
style elements on hover, focus, and active using the , , and  modifiers:
try interacting with this button to see the hover, focus, and active states
tailwind also includes modifiers for other interactive states like , , , and more.
see the  for a complete list of available pseudo-class modifiers.
first, last, odd, and even
style an element when it is the first-child or last-child using the  and  modifiers:
kristen ramos
kristen.ramos@example.com
floyd miles
floyd.miles@example.com
courtney henry
courtney.henry@example.com
ted fox
ted.fox@example.com
you can also style an element when it’s an odd or even child using the  and  modifiers:
tailwind also includes modifiers for other structural pseudo-classes like , , , and more.
form states
style form elements in different states using modifiers like , , and :
try making the email address valid to see the styles change
username
email
password
using modifiers for this sort of thing can reduce the amount of conditional logic in your templates, letting you use the same set of classes regardless of what state an input is in and letting the browser apply the right styles for you.
tailwind also includes modifiers for other form states like , , ,  and more.
styling based on parent state (group-{modifier})
when you need to style an element based on the state of some parent element, mark the parent with the  class, and use  modifiers like  to style the target element:
hover over the card to see both text elements change color
this pattern works with every pseudo-class modifier, for example , , or even .
differentiating nested groups
when nesting groups, you can style something based on the state of a specific parent group by giving that parent a unique group name using a  class, and including that name in modifiers using classes like :
co-founder / ceo
vp, marketing
account coordinator
groups can be named however you like and don’t need to be configured in any way — just name your groups directly in your markup and tailwind will automatically generate the necessary css.
arbitrary groups
you can create one-off  modifiers on the fly by providing your own selector as an  between square brackets:
htmlgenerated css
for more control, you can use the  character to mark where  should end up in the final selector relative to the selector you are passing in:
styling based on sibling state (peer-{modifier})
when you need to style an element based on the state of a sibling element, mark the sibling with the  class, and use  modifiers like  to style the target element:
try making the email address valid to see the warning disappear
please provide a valid email address.
this makes it possible to do all sorts of neat tricks, like  for example without any js.
this pattern works with every pseudo-class modifier, for example , , and .
it’s important to note that the  marker can only be used on previous siblings because of how the  works in css.
won't work, only previous siblings can be marked as peers
differentiating peers
when using multiple peers, you can style something on the state of a specific peer by giving that peer a unique name using a  class, and including that name in modifiers using classes like :
published status
draft published
drafts are only visible to administrators.
your post will be publicly visible on your site.
peers can be named however you like and don’t need to be configured in any way — just name your peers directly in your markup and tailwind will automatically generate the necessary css.
arbitrary peers
pseudo-elements
before and after
style the  and  pseudo-elements using the  and  modifiers:
when using these modifiers, tailwind will automatically add  by default so you don’t have to specify it unless you want a different value:
when you look annoyed all the time, people think that you're busy.
it’s worth noting that you don’t really need  and  pseudo-elements for most things in tailwind projects — it’s usually simpler to just use a real html element.
for example, here’s the same design from above but using a  instead of the  pseudo-element, which is a little easier to read and is actually less code:
save  and  for situations where it’s important that the content of the pseudo-element is not actually in the dom and can’t be selected by the user.
note that if you’ve disabled our , the content property will not be set to an empty string by default, and you will need to include  any time you use the  and  modifiers.
if you've disabled preflight make sure to set the content manually
placeholder text
style the placeholder text of any input or textarea using the  modifier:
search
file input buttons
style the button in file inputs using the  modifier:
choose profile photo
note that tailwind’s  is not applied to file input buttons. this means that to add a border to a file input button, you need to explicitly set the  using a class like  alongside any  utility:
list markers
style the counters or bullets in lists using the  modifier:
ingredients
5 cups chopped porcini mushrooms
1/2 cup of olive oil
3lb of celery
we’ve designed the  modifier to be inheritable, so although you can use it directly on an  element, you can also use it on a parent to avoid repeating yourself.
highlighted text
style the active text selection using the  modifier:
try selecting some of this text with your mouse
so i started to walk into the water. i won't lie to you boys, i was terrified. but i pressed on, and as i made my way past the breakers a strange calm came over me. i don't know if it was divine intervention or the kinship of all living things but i tell you jerry at that moment, i was a marine biologist.
we’ve designed the  modifier to be inheritable, so you can add it anywhere in the tree and it will be applied to all descendant elements.
this makes it easy to set the selection color to match your brand across your entire site:
first-line and first-letter
style the first line in a block of content using the  modifier, and the first letter using the  modifier:
well, let me tell you something, funny boy. y'know that little stamp, the one that says "new york public library"? well that may not mean anything to you, but that means a lot to me. one whole hell of a lot.
sure, go ahead, laugh if you want to. i've seen your type before: flashy, making the scene, flaunting convention. yeah, i know what you're thinking. what's this guy making such a big stink about old library books? well, let me give you a hint, junior.
dialog backdrops
style the backdrop of a  using the  modifier:
if you’re using native  elements in your project, you may also want to read about  using the  modifier.
media and feature queries
responsive breakpoints
to style an element at a specific breakpoint, use responsive modifiers like  and .
for example, this will render a 3-column grid on mobile, a 4-column grid on medium-width screens, and a 6-column grid on large-width screens:
check out the  documentation for an in-depth look at how these features work.
prefers color scheme
the  media query tells you whether the user prefers a light theme or dark theme, and is usually configured at the operating system level.
use utilities with no modifier to target light mode, and use the  modifier to provide overrides for dark mode:
light mode
writes upside-down
the zero gravity pen can be used to write in any orientation, including upside-down. it even works in outer space.
dark mode
check out the  documentation for an in-depth look at how this feature works.
prefers reduced motion
the  media query tells you if the user has requested that you minimize non-essential motion.
use the  modifier to conditionally add styles when the user has requested reduced motion:
try emulating `prefers-reduced-motion: reduce` in your developer tools to hide the spinner
processing...
tailwind also includes a  modifier that only adds styles when the user has not requested reduced motion. this can be useful when using the  helper would mean having to “undo” a lot of styles:
prefers contrast
the  media query tells you if the user has requested more or less contrast.
use the  modifier to conditionally add styles when the user has requested more contrast:
try emulating `prefers-contrast: more` in your developer tools to see the changes
social security number
we need this to steal your identity.
tailwind also includes a  modifier you can use to conditionally add styles when the user has requested less contrast.
viewport orientation
use the  and  modifiers to conditionally add styles when the viewport is in a specific orientation:
print styles
use the  modifier to conditionally add styles that only apply when the document is being printed:
supports rules
use the  modifier to style things based on whether a certain feature is supported in the user’s browser.
under the hood the  modifier generates  and takes anything you’d use with  between the square brackets, like a property/value pair, and even expressions using  and .
for terseness, if you only need to check if a property is supported (and not a specific value), you can just specify the property name:
you can configure shortcuts for common  rules you’re using in your project in the  section of your  file:
you can then use these custom  modifiers in your project:
attribute selectors
aria states
use the  modifier to conditionally style things based on .
for example, to apply the  class when the  attribute is set to , use the  class:
by default we’ve included modifiers for the most common boolean aria attributes:
you can customize which  modifiers are available by editing  or  in your  file:
if you need to use a one-off  modifier that doesn’t make sense to include in your theme, or for more complex aria attributes that take specific values, use square brackets to generate a property on the fly using any arbitrary value.
aria state modifiers can also target parent and sibling elements using the  and  modifiers:
data attributes
use the  modifier to conditionally apply styles based on .
since there are no standard  attributes by definition, by default we only support arbitrary values out of the box, for example:
you can configure shortcuts for common data attribute selectors you’re using in your project in the  section of your  file:
rtl support
right-to-left support is experimental and the details of how it works may change. share your feedback in the  to help us get it ready for prime time.
use the  and  modifiers conditionally add styles in right-to-left and left-to-right modes respectively when building multi-directional layouts:
left-to-right
tom cook
director of operations
right-to-left
تامر كرم
الرئيس التنفيذي
note that the  modifier will not take effect unless the  attribute is explicitly set to , so if you are building a multi-directional site make sure to always set a direction, not just in  mode.
always set the direction, even if left-to-right is your default
remember, these modifiers are only useful if you are building a site that needs to support both left-to-right and right-to-left layouts. if you’re building a site that only needs to support a single direction, you don’t need these modifiers — just apply the styles that make sense for your content.
open/closed state
use the  modifier to conditionally add styles when a  or  element is in an open state:
try toggling the disclosure to see the styles change
the mug is round. the jar is round. they should call it roundtine.
custom modifiers
using arbitrary variants
just like  let you use custom values with your utility classes, arbitrary variants let you write custom selector modifiers directly in your html.
arbitrary variants are just format strings that represent the selector, wrapped in square brackets. for example, this arbitrary modifier selects an element only when it is the third child:
the format string is the same as what you’d use with the , with the  representing the selector being modified.
arbitrary variants can be stacked with built-in modifiers or with each other, just like the rest of the modifiers in tailwind:
if you need spaces in your selector, you can use an underscore. for example, this arbitrary modifier selects all  elements within the element where you’ve added the class:
you can also use at-rules like  or  in arbitrary variants:
with at-rule custom modifiers the  placeholder isn’t necessary, just like when nesting with a preprocessor.
you can even combine at-rules and regular selector modifiers by including the selector modifier within curly braces after the at-rule:
creating a plugin
if you find yourself using the same arbitrary modifier multiple times in your project, it might be worth extracting it to a plugin using the  api:
advanced topics
using with your own classes
all of tailwind’s modifiers are available to use with your own custom classes as long as you’ve defined them in one of tailwind’s  or added them using a :
html
ordering stacked modifiers
when stacking modifiers, they are applied from the inside-out, like nested function calls:
for the most part this doesn’t actually matter, but there are a few situations where the order you use actually generates meaningfully different css.
for example, if you have  configured to , combining the  and  modifiers generates a different result depending on the order you use:
in the first example, the  element needs to be a parent of the  element, but in the second example it’s reversed.
another place this is important is when using modifiers like  that are included with the official typography plugin:
in the first example, every single heading is underlined when you hover over the article itself, whereas in the second example each heading is only underlined when you hover over that heading.
appendix
quick reference
a quick reference table of every single modifier included in tailwind by default.
pseudo-class reference
this is a comprehensive list of examples for all the pseudo-class modifiers included in tailwind to complement the  at the beginning of this guide.
hover (:hover)
style an element when the user hovers over it with the mouse cursor using the  modifier:
focus (:focus)
style an element when it has focus the  modifier:
focus-within (:focus-within)
style an element when it or one of its descendants has focus using the  modifier:
focus-visible (:focus-visible)
style an element when it has been focused using the keyboard using the  modifier:
active (:active)
style an element when it is being pressed using the  modifier:
visited (:visited)
style a link when it has already been visited using the  modifier:
target (:target)
style an element if its id matches the current url fragment using the  modifier:
first (:first-child)
style an element if it’s the first child using the  modifier:
last (:last-child)
style an element if it’s the last child using the  modifier:
only (:only-child)
style an element if it’s the only child using the  modifier:
odd (:nth-child(odd))
style an element if it’s an oddly numbered child using the  modifier:
even (:nth-child(even))
style an element if it’s an evenly numbered child using the  modifier:
first-of-type (:first-of-type)
style an element if it’s the first child of its type using the  modifier:
last-of-type (:last-of-type)
style an element if it’s the last child of its type using the  modifier:
only-of-type (:only-of-type)
style an element if it’s the only child of its type using the  modifier:
empty (:empty)
style an element if it has no content using the  modifier:
disabled (:disabled)
style an input when it’s disabled using the  modifier:
enabled (:enabled)
style an input when it’s enabled using the  modifier, most helpful when you only want to apply another style when an element is not disabled:
checked (:checked)
style a checkbox or radio button when it’s checked using the  modifier:
indeterminate (:indeterminate)
style a checkbox or radio button in an indeterminate state using the  modifier:
default (:default)
style an option, checkbox or radio button that was the default value when the page initially loaded using the  modifier:
required (:required)
style an input when it’s required using the  modifier:
valid (:valid)
style an input when it’s valid using the  modifier:
invalid (:invalid)
style an input when it’s invalid using the  modifier:
in-range (:in-range)
style an input when it’s value is within a specified range limit using the  modifier:
out-of-range (:out-of-range)
style an input when it’s value is outside of a specified range limit using the  modifier:
placeholder-shown (:placeholder-shown)
style an input when the placeholder is shown using the  modifier:
autofill (:autofill)
style an input when it has been autofilled by the browser using the  modifier:
read-only (:read-only)
style an input when it is read-only using the  modifier:
every utility class in tailwind can be applied conditionally at different breakpoints, which makes it a piece of cake to build complex responsive interfaces without ever leaving your html.
there are five breakpoints by default, inspired by common device resolutions:
to add a utility but only have it take effect at a certain breakpoint, all you need to do is prefix the utility with the breakpoint name, followed by the  character:
this works for every utility class in the framework, which means you can change literally anything at a given breakpoint — even things like letter spacing or cursor styles.
here’s a simple example of a marketing page component that uses a stacked layout on small screens, and a side-by-side layout on larger screens (resize your browser to see it in action):
here’s how the example above works:
by default, the outer  is , but by adding the  utility, it becomes  on medium screens and larger.when the parent is a flex container, we want to make sure the image never shrinks, so we’ve added  to prevent shrinking on medium screens and larger. technically we could have just used  since it would do nothing on smaller screens, but since it only matters on  screens, it’s a good idea to make that clear in the class name.on small screens the image is automatically full width by default. on medium screens and up, we’ve constrained the width to a fixed size and ensured the image is full height using .
we’ve only used one breakpoint in this example, but you could easily customize this component at other sizes using the , , , or  responsive prefixes as well.
working mobile-first
by default, tailwind uses a mobile-first breakpoint system, similar to what you might be used to in other frameworks like bootstrap.
what this means is that unprefixed utilities (like ) take effect on all screen sizes, while prefixed utilities (like ) only take effect at the specified breakpoint and above.
targeting mobile screens
where this approach surprises people most often is that to style something for mobile, you need to use the unprefixed version of a utility, not the  prefixed version. don’t think of  as meaning “on small screens”, think of it as “at the small breakpoint“.
don't use  to target mobile devices
use unprefixed utilities to target mobile, and override them at larger breakpoints
for this reason, it’s often a good idea to implement the mobile layout for a design first, then layer on any changes that make sense for  screens, followed by  screens, etc.
targeting a breakpoint range
by default, styles applied by rules like  will apply at that breakpoint and stay applied at larger breakpoints.
if you’d like to apply a utility only when a specific breakpoint range is active, stack a responsive modifier like  with a  modifier to limit that style to a specific range:
tailwind generates a corresponding  modifier for each breakpoint, so out of the box the following modifiers are available:
targeting a single breakpoint
to target a single breakpoint, target the range for that breakpoint by stacking a responsive modifier like  with the  modifier for the next breakpoint:
read about  to learn more.
using custom breakpoints
customizing your theme
you can completely customize your breakpoints in your  file:
learn more in the .
arbitrary values
if you need to use a one-off breakpoint that doesn’t make sense to include in your theme, use the  or  modifiers to generate custom breakpoint on the fly using any arbitrary value.
learn more about arbitrary value support in the  documentation.
basic usage
now that dark mode is a first-class feature of many operating systems, it’s becoming more and more common to design a dark version of your website to go along with the default design.
to make this as easy as possible, tailwind includes a  variant that lets you style your site differently when dark mode is enabled:
by default this uses the  css media feature, but you can also build sites that support toggling dark mode manually using the .
toggling dark mode manually
if you want to support toggling dark mode manually instead of relying on the operating system preference, use the  strategy instead of the  strategy:
now instead of  classes being applied based on , they will be applied whenever  class is present earlier in the html tree.
if you’ve set  in your tailwind config, be sure to add that to the  class. for example, if you have a prefix of , you’ll need to use the  class to enable dark mode.
how you add the  class to the  element is up to you, but a common approach is to use a bit of js that reads a preference from somewhere (like ) and updates the dom accordingly.
supporting system preference and manual selection
the  strategy can be used to support both the user’s system preference or a manually selected mode by using the .
here’s a simple example of how you can support light mode, dark mode, as well as respecting the operating system preference:
spaghetti.js
again you can manage this however you like, even storing the preference server-side in a database and rendering the class on the server — it’s totally up to you.
customizing the class name
some frameworks (like nativescript) have their own approach to enabling dark mode and add a different class name when dark mode is active.
you can customize the dark mode selector name by setting  to an array with your custom selector as the second item:
tailwind encourages a  workflow, where designs are implemented using only low-level utility classes. this is a powerful way to avoid premature abstraction and the pain points that come with it.
but of course as a project grows, you’ll inevitably find yourself repeating common utility combinations to recreate the same design in many different places.
for example, in the template below you can see the utility classes for each avatar image are repeated five separate times:
contributors
don’t panic! in this guide, you’ll learn about different strategies for reusing styles in your project, as well as best practices for when to employ each one.
using editor and language features
a lot of the time, duplication like this isn’t even a real problem because it’s all together in one place, or doesn’t even actually exist because you’re iterating over an array of items and only writing the markup once.
if the styles you need to reuse only need to be reused within a single file, multi-cursor editing and loops are the simplest way to manage any duplication.
multi-cursor editing
when duplication is localized to a group of elements in a single file, the easiest way to deal with it to use  to quickly select and edit the class list for each element at once:
you’d be surprised at how often this ends up being the best solution. if you can quickly edit all of the duplicated class lists simultaneously, there’s no benefit to introducing any additional abstraction.
loops
before you assume you’re going to need to extract a component or create a custom class for something, make sure you’re actually using it more than once in your template.
a lot of the time a design element that shows up more than once in the rendered page is only actually authored once because the actual markup is rendered in a loop.
for example, the duplicate avatars at the beginning of this guide would almost certainly be rendered in a loop in a real project:
you could even rewrite the navigation example using a loop or  if you preferred as well:
when elements are rendered in a loop like this, the actual class list is only written once so there’s no actual duplication problem to solve.
extracting components and partials
if you need to reuse some styles across multiple files, the best strategy is to create a component if you’re using a front-end framework like react, svelte, or vue, or a template partial if you’re using a templating language like blade, erb, twig, or nunjucks.
private villa
$299 usd per night
vacationcard.vue
now you can use this component in as many places as you like, while still having a single source of truth for the styles so they can easily be updated together in one place.
compared to css abstractions
unless a component is a single html element, the information needed to define it can’t be captured in css alone. for anything even remotely complex, the html structure is just as important as the css.
don't rely on css classes to extract complex components
even if you create classes for the different elements in a component like this, you still have to duplicate the html every time you want to use this component. sure you can update the font-size for every instance in a single place, but what if you need to turn the title into a link?
components and template partials solve this problem much better than css-only abstractions because a component can encapsulate the html and the styles. changing the font-size for every instance is just as easy as it is with css, but now you can turn all of the titles into links in a single place too.
create a template partial or javascript component
notification.jsx
when you create components and template partials like this, there’s no reason to use anything other than utility classes because you already have a single source of truth for the styles.
extracting classes with @apply
if you’re using a traditional templating language like erb or twig, creating a template partial for something as small as a button can feel like overkill compared to a simple css class like .
while it’s highly recommended that you create proper template partials for more complex components, you can use tailwind’s  directive to extract repeated utility patterns to custom css classes when a template partial feels heavy-handed.
here’s what a  class might look like using  to compose it from existing utilities:
learn more about  and  in the  documentation.
avoiding premature abstraction
whatever you do, don’t use  just to make things look “cleaner”. yes, html templates littered with tailwind classes are kind of ugly. making changes in a project that has tons of custom css is worse.
if you start using  for everything, you are basically just writing css again and throwing away all of the workflow and maintainability advantages tailwind gives you, for example:
you have to think up class names all the time — nothing will slow you down or drain your energy like coming up with a class name for something that doesn’t deserve to be named.you have to jump between multiple files to make changes — which is a way bigger workflow killer than you’d think before co-locating everything together.changing styles is scarier — css is global, are you sure you can change the min-width value in that class without breaking something in another part of the site?your css bundle will be bigger — oof.
if you’re going to use , use it for very small, highly reusable things like buttons and form controls — and even then only if you’re not using a framework like react where a component would be a better choice.
often the biggest challenge when working with a framework is figuring out what you’re supposed to do when there’s something you need that the framework doesn’t handle for you.
tailwind has been designed from the ground up to be extensible and customizable, so that no matter what you’re building you never feel like you’re fighting the framework.
this guide covers topics like customizing your design tokens, how to break out of those constraints when necessary, adding your own custom css, and extending the framework with plugins.
if you want to change things like your color palette, spacing scale, typography scale, or breakpoints, add your customizations to the  section of your  file:
learn more about customizing your theme in the  documentation.
using arbitrary values
while you can usually build the bulk of a well-crafted design using a constrained set of design tokens, once in a while you need to break out of those constraints to get things pixel-perfect.
when you find yourself really needing something like  to get a background image in just the right spot, use tailwind’s square bracket notation to generate a class on the fly with any arbitrary value:
this is basically like inline styles, with the major benefit that you can combine it with interactive modifiers like  and responsive modifiers like :
this works for everything in the framework, including things like background colors, font sizes, pseudo-element content, and more:
it’s even possible to use the  to reference the design tokens in your  file:
arbitrary properties
if you ever need to use a css property that tailwind doesn’t include a utility for out of the box, you can also use square bracket notation to write completely arbitrary css:
this is really like inline styles, but again with the benefit that you can use modifiers:
this can be useful for things like css variables as well, especially when they need to change under different conditions:
arbitrary variants
arbitrary variants are like arbitrary values but for doing on-the-fly selector modification, like you can with built-in pseudo-class variants like  or responsive variants like  but using square bracket notation directly in your html.
handling whitespace
when an arbitrary value needs to contain a space, use an underscore () instead and tailwind will automatically convert it to a space at build-time:
in situations where underscores are common but spaces are invalid, tailwind will preserve the underscore instead of converting it to a space, for example in urls:
in the rare case that you actually need to use an underscore but it’s ambiguous because a space is valid as well, escape the underscore with a backslash and tailwind won’t convert it to a space:
if you’re using something like jsx where the backslash is stripped from the rendered html, use  so the backslash isn’t treated as a javascript escape character:
resolving ambiguities
many utilities in tailwind share a common namespace but map to different css properties. for example  and  both share the  namespace, but one is for  and the other is for .
when using arbitrary values, tailwind can generally handle this ambiguity automatically based on the value you pass in:
sometimes it really is ambiguous though, for example when using css variables:
in these situations, you can “hint” the underlying type to tailwind by adding a  before the value:
using css and @layer
when you need to add truly custom css rules to a tailwind project, the easiest approach is to just add the custom css to your stylesheet:
for more power, you can also use the  directive to add styles to tailwind’s , , and  layers:
in css, the order of the rules in your stylesheet decides which declaration wins when two selectors have the same specificity:here, both buttons will be black since  comes after  in the css:to manage this, tailwind organizes the styles it generates into three different “layers” — a concept popularized by .the  layer is for things like reset rules or default styles applied to plain html elements.the  layer is for class-based styles that you want to be able to override with utilities.the  layer is for small, single-purpose classes that should always take precedence over any other styles.being explicit about this makes it easier to understand how your styles will interact with each other, and using the  directive lets you control the final declaration order while still organizing your actual code in whatever way you like.
the  directive helps you control declaration order by automatically relocating your styles to the corresponding  directive, and also enables features like  and  for your own custom css.
adding base styles
if you just want to set some defaults for the page (like the text color, background color, or font family), the easiest option is just adding some classes to the  or  elements:
this keeps your base styling decisions in your markup alongside all of your other styles, instead of hiding them in a separate file.
if you want to add your own default base styles for specific html elements, use the  directive to add those styles to tailwind’s  layer:
use the  function or  directive when adding custom base styles if you want to refer to any of the values defined in your .
adding component classes
use the  layer for any more complicated classes you want to add to your project that you’d still like to be able to override with utility classes.
traditionally these would be classes like , ,  — that kind of thing.
by defining component classes in the  layer, you can still use utility classes to override them when necessary:
using tailwind you probably don’t need these types of classes as often as you think. read our guide on  for our recommendations.
the  layer is also a good place to put custom styles for any third-party components you’re using:
use the  function or  directive when adding custom component styles if you want to refer to any of the values defined in your .
adding custom utilities
add any of your own custom utility classes to tailwind’s  layer:
this can be useful when there’s a css feature you’d like to use in your project that tailwind doesn’t include utilities for out of the box.
using modifiers with custom css
any custom styles you add to tailwind with  will automatically support tailwind’s modifier syntax for handling things like hover states, responsive breakpoints, dark mode, and more.
learn more about how these modifiers work in the  documentation.
removing unused custom css
any custom styles you add to the , , or  layers will only be included in your compiled css if those styles are actually used in your html.
if you want to add some custom css that should always be included, add it to your stylesheet without using the  directive:
make sure to put your custom styles where they need to go to get the precedence behavior you want. in the example above, we’ve added the  class before  to make sure utilities can still override it.
using multiple css files
if you are writing a lot of css and organizing it into multiple files, make sure those files are combined into a single stylesheet before processing them with tailwind, or you’ll see errors about using  without the corresponding  directive.
the easiest way to do this is using the  plugin:
learn more in our  documentation.
layers and per-component css
component frameworks like vue and svelte support adding per-component styles within a  block that lives in each component file.
while you can use features like  and  inside component styles like this, the  directive will not work and you’ll see an error about  being used without a matching  directive:
don't use `@layer` in component styles
card.svelte
this is because under-the-hood, frameworks like vue and svelte are processing every single  block independently, and running your postcss plugin chain against each one in isolation.
that means if you have 10 components that each have a  block, tailwind is being run 10 separate times, and each run has zero knowledge about the other runs. because of this, tailwind can’t take the styles you define in a  and move them to the corresponding  directive, because as far as tailwind can tell there is no  directive to move it to.
one solution to this is to simply not use  inside your component styles:
add your styles without using `@layer`
you lose the ability to control the precedence of your styles, but unfortunately that’s totally out of our control because of how these tools work.
our recommendation is that you just don’t use component styles like this at all and instead use tailwind the way it’s intended to be used — as a single global stylesheet where you use the classes directly in your html:
use tailwind's utilities instead of component styles
writing plugins
you can also add custom styles to your project using tailwind’s plugin system instead of using a css file:
learn more about writing your own plugins in the  documentation.
directives
directives are custom tailwind-specific  you can use in your css that offer special functionality for tailwind css projects.
@tailwind
use the  directive to insert tailwind’s , ,  and  styles into your css.
@layer
use the  directive to tell tailwind which “bucket” a set of custom styles belong to. valid layers are , , and .
tailwind will automatically move any css within a  directive to the same place as the corresponding  rule, so you don’t have to worry about authoring your css in a specific order to avoid specificity issues.
any custom css added to a layer will only be included in the final build if that css is actually used in your html, just like all of the classes built in to tailwind by default.
wrapping any custom css in a  directive also makes it possible to use modifiers with those rules, like  and  or responsive modifiers like  and .
@apply
use  to inline any existing utility classes into your own custom css.
this is useful when you need to write custom css (like to override the styles in a third-party library) but still want to work with your design tokens and use the same syntax you’re used to using in your html.
any rules inlined with  will have  removed by default to avoid specificity issues:
if you’d like to  an existing class and make it , simply add  to the end of the declaration:
note that if you’re using sass/scss, you’ll need to use sass’ interpolation feature to get this to work:
using @apply with per-component css
if you try to  a custom class you’ve defined in your global css in one of these per-component  blocks, you’ll get an error about the class not existing:
that means if you have 10 components that each have a  block, tailwind is being run 10 separate times, and each run has zero knowledge about the other runs. because of this, when you try to  in  it fails, because tailwind has no idea that the  class exists since svelte processed  and  in total isolation from each other.
the solution to this problem is to define any custom styles you want to  in your components using the  instead:
this way any file processed by tailwind that uses this config file will have access to those styles.
honestly though the best solution is to just not do weird stuff like this at all. use tailwind’s utilities directly in your markup the way they are intended to be used, and don’t abuse the  feature to do things like this and you will have a much better experience.
@config
use the  directive to specify which config file tailwind should use when compiling that css file. this is useful for projects that need to use different configuration files for different css entry points.
site.cssadmin.css
the path you provide to the  directive is relative to that css file, and will take precedence over a path defined in your postcss configuration or in the tailwind cli.
functions
tailwind adds a few custom functions you can use in your css to access tailwind-specific values. these functions are evaluated at build-time, and are replaced by static values in your final css.
theme()
use the  function to access your tailwind config values using dot notation.
if you need to access a value that contains a dot (like the  value in the spacing scale), you can use square bracket notation:
since tailwind uses a  to define its default color palette, make sure to use dot notation to access the nested colors.
don't use the dash syntax when accessing nested color values
use dot notation to access nested color values
to adjust the opacity of a color retrieved with , use a slash followed by the opacity value you want to use:
screen()
the  function allows you to create media queries that reference your breakpoints by name instead of duplicating their values in your own css.
this will resolve to the underlying screen value at build-time, generating a regular media query that matches specified breakpoint:
because tailwind is a framework for building bespoke user interfaces, it has been designed from the ground up with customization in mind.
by default, tailwind will look for an optional  file at the root of your project where you can define any customizations.
every section of the config file is optional, so you only have to specify what you’d like to change. any missing sections will fall back to tailwind’s .
creating your configuration file
generate a tailwind config file for your project using the tailwind cli utility included when you install the  npm package:
this will create a minimal  file at the root of your project:
using a different file name
to use a name other than , pass it as an argument on the command-line:
when you use a custom file name, you will need to specify it as a command-line argument when compiling your css with the tailwind cli tool:
if you’re using tailwind as a postcss plugin, you will need to specify your custom configuration path in your postcss configuration:
alternatively, you can specify your custom configuration path using the  directive:
learn more about the  directive in the  documentation.
generating a postcss configuration file
use the  flag if you’d like to also generate a basic  file alongside your  file:
this will generate a  file in your project that looks like this:
scaffolding the entire default configuration
for most users we encourage you to keep your config file as minimal as possible, and only specify the things you want to customize.
if you’d rather scaffold a complete configuration file that includes all of tailwind’s default configuration, use the  option:
you’ll get a file that matches the  tailwind uses internally.
configuration options
content
the  section is where you configure the paths to all of your html templates, js components, and any other files that contain tailwind class names.
learn more about configuring your content sources in the  documentation.
theme
the  section is where you define your color palette, fonts, type scale, border sizes, breakpoints — anything related to the visual design of your site.
learn more about the default theme and how to customize it in the .
plugins
the  section allows you to register plugins with tailwind that can be used to generate extra utilities, components, base styles, or custom variants.
learn more about writing your own plugins in the .
presets
the  section allows you to specify your own custom base configuration instead of using tailwind’s default base configuration.
learn more about presets in the .
prefix
the  option allows you to add a custom prefix to all of tailwind’s generated utility classes. this can be really useful when layering tailwind on top of existing css where there might be naming conflicts.
for example, you could add a  prefix by setting the  option like so:
now every class will be generated with the configured prefix:
it’s important to understand that this prefix is added after any variant modifiers. that means that classes with responsive or state modifiers like  or  will still have the responsive or state modifier first, with your custom prefix appearing after the colon:
the dash modifier for negative values should be added before your prefix, so  would become  if you’ve configured  as your prefix:
prefixes are only added to classes generated by tailwind; no prefix will be added to your own custom classes.
that means if you add your own custom utility like this:
…the generated variants will not have your configured prefix:
if you’d like to prefix your own utilities as well, just add the prefix to the class definition:
important
the  option lets you control whether or not tailwind’s utilities should be marked with . this can be really useful when using tailwind with existing css that has high specificity selectors.
to generate utilities as , set the  key in your configuration options to :
now all of tailwind’s utility classes will be generated as :
this also applies to any custom utilities you define in your css using the  directive:
selector strategy
setting  to  can introduce some issues when incorporating third-party js libraries that add inline styles to your elements. in those cases, tailwind’s  utilities defeat the inline styles, which can break your intended design.
to get around this, you can set  to an id selector like  instead:
this configuration will prefix all of your utilities with the given selector, effectively increasing their specificity without actually making them .
after you specify the  selector, you’ll need to ensure that the root element of your site matches it.  using the example above, we would need to set our root element’s  attribute to  in order for styles to work properly.
after your configuration is all set up and your root element matches the selector in your tailwind config, all of tailwind’s utilities will have a high enough specificity to defeat other classes used in your project, without interfering with inline styles:
when using the selector strategy, be sure that the template file that includes your root selector is included in your , otherwise all of your css will be removed when building for production.
important modifier
alternatively, you can make any utility important by adding a  character to the beginning:
the  always goes at the beginning of the utility name, after any variants, but before any prefix:
this can be useful in rare situations where you need to increase specificity because you’re at war with some styles you don’t control.
separator
the  option lets you customize which character should be used to separate modifiers (screen sizes, , , etc.) from utility names (, , etc.).
we use a colon by default (), but it can be useful to change this if you’re using a templating language like  that doesn’t support special characters in class names.
core plugins
the  section lets you completely disable classes that tailwind would normally generate by default if you don’t need them for your project.
to disable specific core plugins, provide an object for  that sets those plugins to :
if you’d like to safelist which core plugins should be enabled, provide an array that includes a list of the core plugins you’d like to use:
if you’d like to disable all of tailwind’s core plugins and simply use tailwind as a tool for processing your own custom plugins, provide an empty array:
here’s a list of every core plugin for reference:
using multiple configurations
for projects that need to generate multiple css files using different tailwind configurations, use the  directive to specify which config file should be used for each css entry point:
referencing in javascript
it can often be useful to reference your configuration values in your own client-side javascript — for example to access some of your theme values when dynamically applying inline styles in a react or vue component.
to make this easy, tailwind provides a  helper you can use to generate a fully merged version of your configuration object:
note that this will transitively pull in a lot of our build-time dependencies, resulting in bigger client-side bundle size. to avoid this, we recommend using a tool like  to generate a static version of your configuration at build-time.
typescript types
we ship first-party typescript types for the  file which give you all sorts of useful ide support, and makes it a lot easier to make changes to your configuration without referencing the documentation quite as much.
configuration files generated with tailwind cli include the necessary type annotation by default, but to configure typescript types manually, just add the type annotation above your configuration object:
the  section of your  file is where you configure the paths to all of your html templates, javascript components, and any other source files that contain tailwind class names.
this guide covers everything you need to know to make sure tailwind generates all of the css needed for your project.
configuring source paths
tailwind css works by scanning all of your html, javascript components, and any other template files for class names, then generating all of the corresponding css for those styles.
in order for tailwind to generate all of the css you need, it needs to know about every single file in your project that contains any tailwind class names.
configure the paths to all of your content files in the  section of your configuration file:
paths are configured as , making it easy to match all of the content files in your project without a ton of configuration:
use  to match anything except slashes and hidden filesuse  to match zero or more directoriesuse comma separate values between  to match against a list of options
tailwind uses the  library under-the-hood — check out their documentation for other supported pattern features.
paths are relative to your project root, not your  file, so if your  file is in a custom location, you should still write your paths relative to the root of your project.
pattern recommendations
for the best performance and to avoid false positives, be as specific as possible with your content configuration.
if you use a really broad pattern like this one, tailwind will even scan  for content which is probably not what you want:
don't use extremely broad patterns
if you have any files you need to scan that are at the root of your project (often an  file), list that file independently so your other patterns can be more specific:
be specific with your content patterns
some frameworks hide their main html entry point in a different place than the rest of your templates (often ), so if you are adding tailwind classes to that file make sure it’s included in your configuration as well:
don't forget your html entry point if applicable
if you have any javascript files that manipulate your html to add classes, make sure you include those as well:
src/spaghetti.js
it’s also important that you don’t scan any css files — configure tailwind to scan your templates where your class names are being used, never the css file that tailwind is generating.
never include css files in your content configuration
class detection in-depth
the way tailwind scans your source code for classes is intentionally very simple — we don’t actually parse or execute any of your code in the language it’s written in, we just use regular expressions to extract every string that could possibly be a class name.
for example, here’s some html with every potential class name string individually highlighted:
we don’t just limit our search to  attributes because you could be using classes anywhere, like in some javascript for toggling a menu:
by using this very simple approach, tailwind works extremely reliably with any programming language, like jsx for example:
button.jsx
dynamic class names
the most important implication of how tailwind extracts class names is that it will only find classes that exist as complete unbroken strings in your source files.
if you use string interpolation or concatenate partial class names together, tailwind will not find them and therefore will not generate the corresponding css:
don't construct class names dynamically
in the example above, the strings  and  do not exist, so tailwind will not generate those classes.
instead, make sure any class names you’re using exist in full:
always use complete class names
as long as you always use complete class names in your code, tailwind will generate all of your css perfectly every time.
working with third-party libraries
if you’re working with any third-party libraries (for example ) and styling that library with your own custom css, we recommend writing those styles without using tailwind’s  feature:
this will ensure that tailwind always includes those styles in your css, which is a lot easier than configuring tailwind to scan the source code of a third-party library.
if you’ve created your own reusable set of components that are styled with tailwind and are importing them in multiple projects, make sure to configure tailwind to scan those components for class names:
this will make sure tailwind generates all of the css needed for those components as well.
if you’re working in a monorepo with workspaces, you may need to use  to make sure tailwind can see your content files:
using relative paths
by default tailwind resolves non-absolute content paths relative to the current working directory, not the  file. this can lead to unexpected results if you run tailwind from a different directory.
to always resolve paths relative to the  file, use the object notation for your  configuration and set the  property to :
this will likely become the default behavior in the next major version of the framework.
configuring raw content
if for whatever reason you need to configure tailwind to scan some raw content rather than the contents of a file, use an object with a  key instead of a path:
there aren’t many valid use-cases for this —  is usually what you really want instead.
safelisting classes
for the smallest file size and best development experience, we highly recommend relying on your  configuration to tell tailwind which classes to generate as much as possible.
safelisting is a last-resort, and should only be used in situations where it’s impossible to scan certain content for class names. these situations are rare, and you should almost never need this feature.
if you need to make sure tailwind generates certain class names that don’t exist in your content files, use the  option:
one example of where this can be useful is if your site displays user-generated content and you want users to be able to use a constrained set of tailwind classes in their content that might not exist in your own site’s source files.
tailwind supports pattern-based safelisting for situations where you need to safelist a lot of classes:
patterns can only match against base utility names like , and won’t match if the pattern includes a variant modifier like .
if you want to force tailwind to generate variants for any matched classes, include them using the  option:
discarding classes
since tailwind uses a very simple approach to detecting class names in your content, you may find that some classes are being generated that you don’t actually need.
for example, this html would still generate the  class, even though that class is not actually being used:
you may also want to prevent tailwind from generating certain classes when those classes would conflict with some existing css, but you don’t want to go so far as to prefix all of your tailwind classes.
in these situations, you can use the  option to tell tailwind to ignore specific classes that it detects in your content:
the  option only affects css that would be generated by tailwind, not custom css you’ve authored yourself or are importing from another library.
unlike , the  option only supports strings, and you cannot block classes using regular expressions.
transforming source files
if you’re authoring content in a format that compiles to html (like markdown), it often makes sense to compile that content to html before scanning it for class names.
use the  option to transform any content matching a specific file extension before extracting classes:
when using , you’ll need to provide your source paths using  instead of as a top-level array under .
customizing extraction logic
use the  option to override the logic tailwind uses to detect class names for specific file extensions:
this is an advanced feature and most users won’t need it — the default extraction logic in tailwind works extremely well for almost all projects.
as with transforming, when using , you’ll need to provide your source paths using  instead of as a top-level array under .
troubleshooting
classes aren't generated
if tailwind isn’t generating classes, make sure your  configuration is correct and matches all of the right source files.
a common mistake is missing a file extension, for example if you’re using  instead of  for your react components:
or creating a new folder mid-project that wasn’t covered originally and forgetting to add it to your configuration:
it could also be that you are trying to use dynamic class names, which won’t work because tailwind doesn’t actually evaluate your source code and can only detect static unbroken class strings.
make sure you always use complete class names in your code:
read our documentation on  for more details.
styles rebuild in an infinite loop
if your css seems to be rebuilding in an infinite loop, there’s a good chance it’s because your build tool doesn’t support the  option when .
many build tools (such as webpack) don’t support this option, and as a result we can only tell them to watch specific files or entire directories. we can’t tell webpack to only watch  files in a directory for example.
that means that if building your css causes any files in those directories to change, a rebuild will be triggered, even if the changed file doesn’t match the extension in your glob.
so if you are watching  for changes, but you are writing your css output file to , you will get an infinite rebuild loop using some tools.
ideally we could warn you about this in the console, but many tools support it perfectly fine (including our own cli tool), and we have no reliable way to detect what build tool you are using.
to solve this problem, use more specific paths in your  config, making sure to only include directories that won’t change when your css builds:
if necessary, adjust your actual project directory structure to make sure you can target your template files without accidentally catching your css file or other build artifacts like manifest files.
if you absolutely can’t change your content config or directory structure, your best bet is to compile your css separately with a tool that has complete glob support. we recommend using , which is a fast, simple, purpose-built tool for compiling your css with tailwind.
it just isn't working properly
if you are experiencing weird, hard to describe issues with the output, or things just don’t seem like they are working at all, there’s a good chance it’s due to your build tool not supporting postcss dependency messages properly (or at all). one known example of this currently is .
when you are having these sorts of issues, we recommend using  to compile your css separately instead of trying to integrate tailwind into your existing tooling.
you can use packages like  or  to compile your css alongside your usual development command by adding some scripts to your project like this:
either way, please be sure to  or  so we can figure out the problem and try to improve compatibility with whatever tool you are using.
the  section of your  file is where you define your project’s color palette, type scale, fonts, breakpoints, border radius values, and more.
we provide a sensible  with a very generous set of values to get you started, but don’t be afraid to change it or extend it; you’re encouraged to customize it as much as you need to fit the goals of your design.
theme structure
the  object contains keys for , , and , as well as a key for each customizable .
see the  or the  for a complete list of theme options.
screens
the  key allows you to customize the responsive breakpoints in your project.
to learn more, see the .
colors
the  key allows you to customize the global color palette for your project.
by default, these colors are inherited by all color-related core plugins, like , , , and others.
spacing
the  key allows you to customize the global spacing and sizing scale for your project.
by default, these values are inherited by the , , , , , , , , , , , , and  core plugins.
the rest of the  section is used to configure which values are available for each individual core plugin.
for example, the  key lets you customize which border radius utilities will be generated:
the keys determine the suffix for the generated classes, and the values determine the value of the actual css declaration.
the example  configuration above would generate the following css classes:
you’ll notice that using a key of  in the theme configuration created the class  with no suffix. this is a common convention in tailwind and is supported by all core plugins.
to learn more about customizing a specific core plugin, visit the documentation for that plugin.
for a complete reference of available theme properties and their default values, .
customizing the default theme
out of the box, your project will automatically inherit the values from . if you would like to customize the default theme, you have a few different options depending on your goals.
extending the default theme
if you’d like to preserve the default values for a theme option but also add new values, add your extensions under the  key in the  section of your configuration file.
for example, if you wanted to add an extra breakpoint but preserve the existing ones, you could extend the  property:
overriding the default theme
to override an option in the default theme, add your overrides directly under the  section of your :
this will completely replace tailwind’s default configuration for that key, so in the example above none of the default opacity utilities would be generated.
any keys you do not provide will be inherited from the default theme, so in the above example, the default theme configuration for things like colors, spacing, border-radius, background-position, etc. would be preserved.
you can of course both override some parts of the default theme and extend other parts of the default theme within the same configuration:
referencing other values
if you need to reference another value in your theme, you can do so by providing a closure instead of a static value. the closure will receive an object that includes a  function that you can use to look up other values in your theme using dot notation.
for example, you could generate  utilities for every value in your spacing scale by referencing  in your  configuration:
the  function attempts to find the value you are looking for from the fully merged theme object, so it can reference your own customizations as well as the default theme values. it also works recursively, so as long as there is a static value at the end of the chain it will be able to resolve the value you are looking for.
note that you can only use this kind of closure with top-level theme keys, not the keys inside of each section.
you can't use functions for individual values
use functions for top-level theme keys
referencing the default theme
if you’d like to reference a value in the default theme for any reason, you can import it from .
one example of where this is useful is if you’d like to add a font family to one of tailwind’s default font stacks:
disabling an entire core plugin
if you don’t want to generate any classes for a certain core plugin, it’s better to set that plugin to false in your  configuration than to provide an empty object for that key in your  configuration.
don't assign an empty object in your theme configuration
do disable the plugin in your coreplugins configuration
the end result is the same, but since many core plugins expose no configuration they can only be disabled using  anyways, so it’s better to be consistent.
configuration reference
except for , , and , all of the keys in the  object map to one of tailwind’s . since many plugins are responsible for css properties that only accept a static set of values (like  for example), note that not every plugin has a corresponding key in the  object.
all of these keys are also available under the  key to enable .
configuring custom screens
you define your project’s breakpoints in the  section of your  file. the keys become your  (like ), and the values are the  where that breakpoint should start.
the default breakpoints are inspired by common device resolutions:
feel free to have as few or as many screens as you want, naming them in whatever way you’d prefer for your project.
overriding the defaults
to completely replace the default breakpoints, add your custom  configuration directly under the  key:
any default screens you haven’t overridden (such as  using the above example) will be removed and will not be available as screen modifiers.
overriding a single screen
to override a single screen size (like ), add your custom  value under the  key:
this will replace the default  value with the same name, without changing the order of your breakpoints.
adding larger breakpoints
the easiest way to add an additional larger breakpoint is using the  key:
this will add your custom screen to the end of the default breakpoint list.
adding smaller breakpoints
if you want to add an additional small breakpoint, you can’t use  because the small breakpoint would be added to the end of the breakpoint list, and breakpoints need to be sorted from smallest to largest in order to work as expected with a min-width breakpoint system.
instead, override the entire  key, re-specifying the default breakpoints:
we expose the default theme at  so you don’t have to maintain the list of default breakpoints yourself.
using custom screen names
you can name your custom screens whatever you like, and are not limited to following the ////  convention that tailwind uses by default.
your responsive modifiers will reflect these custom screen names, so using them in your html would now look like this:
advanced configuration
by default, breakpoints are min-width to encourage a  workflow. if you need more control over your media queries, you can also define them using an object syntax that lets you specify explicit min-width and max-width values.
max-width breakpoints
if you want to work with max-width breakpoints instead of min-width, you can specify your screens as objects with a  key:
make sure to list max-width breakpoints in descending order so that they override each other as expected.
fixed-range breakpoints
if you want your breakpoints to specify both a  and a , use the  and  keys together:
unlike regular min-width or max-width breakpoints, breakpoints defined this way will only take effect when the viewport size is explicitly within the defined range.
multi-range breakpoints
sometimes it can be useful to have a single breakpoint definition apply in multiple ranges.
for example, say you have a sidebar and want your breakpoints to be based on the content-area width rather than the entire viewport. you can simulate this by having one of your breakpoints fall back to a smaller breakpoint when the sidebar becomes visible and shrinks the content area:
custom media queries
if you want full control over the generated media query, use the  key:
media queries defined using the  key will be output as-is, and the  and  keys will be ignored.
default color palette
tailwind includes an expertly-crafted default color palette out-of-the-box that is a great starting point if you don’t have your own specific branding in mind.
slate50#f8fafc100#f1f5f9200#e2e8f0300#cbd5e1400#94a3b8500#64748b600#475569700#334155800#1e293b900#0f172agray50#f9fafb100#f3f4f6200#e5e7eb300#d1d5db400#9ca3af500#6b7280600#4b5563700#374151800#1f2937900#111827zinc50#fafafa100#f4f4f5200#e4e4e7300#d4d4d8400#a1a1aa500#71717a600#52525b700#3f3f46800#27272a900#18181bneutral50#fafafa100#f5f5f5200#e5e5e5300#d4d4d4400#a3a3a3500#737373600#525252700#404040800#262626900#171717stone50#fafaf9100#f5f5f4200#e7e5e4300#d6d3d1400#a8a29e500#78716c600#57534e700#44403c800#292524900#1c1917red50#fef2f2100#fee2e2200#fecaca300#fca5a5400#f87171500#ef4444600#dc2626700#b91c1c800#991b1b900#7f1d1dorange50#fff7ed100#ffedd5200#fed7aa300#fdba74400#fb923c500#f97316600#ea580c700#c2410c800#9a3412900#7c2d12amber50#fffbeb100#fef3c7200#fde68a300#fcd34d400#fbbf24500#f59e0b600#d97706700#b45309800#92400e900#78350fyellow50#fefce8100#fef9c3200#fef08a300#fde047400#facc15500#eab308600#ca8a04700#a16207800#854d0e900#713f12lime50#f7fee7100#ecfccb200#d9f99d300#bef264400#a3e635500#84cc16600#65a30d700#4d7c0f800#3f6212900#365314green50#f0fdf4100#dcfce7200#bbf7d0300#86efac400#4ade80500#22c55e600#16a34a700#15803d800#166534900#14532demerald50#ecfdf5100#d1fae5200#a7f3d0300#6ee7b7400#34d399500#10b981600#059669700#047857800#065f46900#064e3bteal50#f0fdfa100#ccfbf1200#99f6e4300#5eead4400#2dd4bf500#14b8a6600#0d9488700#0f766e800#115e59900#134e4acyan50#ecfeff100#cffafe200#a5f3fc300#67e8f9400#22d3ee500#06b6d4600#0891b2700#0e7490800#155e75900#164e63sky50#f0f9ff100#e0f2fe200#bae6fd300#7dd3fc400#38bdf8500#0ea5e9600#0284c7700#0369a1800#075985900#0c4a6eblue50#eff6ff100#dbeafe200#bfdbfe300#93c5fd400#60a5fa500#3b82f6600#2563eb700#1d4ed8800#1e40af900#1e3a8aindigo50#eef2ff100#e0e7ff200#c7d2fe300#a5b4fc400#818cf8500#6366f1600#4f46e5700#4338ca800#3730a3900#312e81violet50#f5f3ff100#ede9fe200#ddd6fe300#c4b5fd400#a78bfa500#8b5cf6600#7c3aed700#6d28d9800#5b21b6900#4c1d95purple50#faf5ff100#f3e8ff200#e9d5ff300#d8b4fe400#c084fc500#a855f7600#9333ea700#7e22ce800#6b21a8900#581c87fuchsia50#fdf4ff100#fae8ff200#f5d0fe300#f0abfc400#e879f9500#d946ef600#c026d3700#a21caf800#86198f900#701a75pink50#fdf2f8100#fce7f3200#fbcfe8300#f9a8d4400#f472b6500#ec4899600#db2777700#be185d800#9d174d900#831843rose50#fff1f2100#ffe4e6200#fecdd3300#fda4af400#fb7185500#f43f5e600#e11d48700#be123c800#9f1239900#881337
but when you do need to customize your palette, you can configure your colors under the  key in the  section of your  file:
when it comes to building a custom color palette, you can either  from scratch if you know exactly what you want, or  from our extensive included color palette if you want a head start.
using custom colors
if you’d like to completely replace the default color palette with your own custom colors, add your colors directly under the  section of your configuration file:
by default, these colors will be made available everywhere in the framework where you use colors, like the  utilities,  utilities,  utilities, and more.
don’t forget to include values like  and  if you want to use them in your project.
color object syntax
when your palette includes multiple shades of the same color, it can be convenient to group them together using our nested color object syntax:
the nested keys will be combined with the parent key to form class names like .
like many other places in tailwind, the special  key can be used when you want to define a value with no suffix:
this will create classes like , , and .
if you need a one-off custom color in your project, consider using tailwind’s arbitrary value notation to generate a class for that color on-demand instead of adding it to your theme:
share on twitter
generating colors
if you’re wondering how to automatically generate the 50–900 shades of your own custom colors, bad news — color is complicated and despite trying dozens of different tools, we’ve yet to find one that does a good job generating color palettes like this automatically.
we picked all of tailwind’s default colors by hand, meticulously balancing them by eye and testing them in real designs to make sure we were happy with them.
two useful tools we can recommend are  and  — they won’t do the work for you but their interfaces are well-designed for doing this sort of work.
using the default colors
if you don’t have a set of completely custom colors in mind for your project, you can curate your colors from our default palette by importing  in your configuration file and choosing the colors you want to use:
this can be helpful if you want to deliberately limit your color palette and reduce the number of class names suggested by .
aliasing color names
you can also alias the colors in our default palette to make the names simpler and easier to remember:
this is especially common for grays, as you usually only use one set in any given project and it’s nice to be able to type  instead of  for example.
adding additional colors
if you’d like to add a brand new color to the default palette, add it in the  section of your configuration file:
you can also use  to add additional shades to an existing color if it’s needed for your design:
disabling a default color
if you’d like to disable any of the default colors, the best approach is to override the default color palette and just include the colors you do want:
naming your colors
tailwind uses literal color names (like red, green, etc.) and a numeric scale (where 50 is light and 900 is dark) by default. we think this is the best choice for most projects, and have found it easier to maintain than using abstract names like  or .
that said, you can name your colors in tailwind whatever you like, and if you’re working on a project that needs to support multiple themes for example, it might make sense to use more abstract names:
you can configure those colors explicitly like we have above, or you can pull in colors from our default color palette and alias them:
again, we recommend sticking to the default naming convention for most projects, and only using abstract names if you have a really good reason.
using css variables
if you’d like to define your colors as css variables, you’ll need to define those variables as just the color channels if you want them to work with the :
define your css variables as channels with no color space function
don't include the color space function or opacity modifiers won't work
then define your colors in your configuration file, being sure to include the color space you’re using, and the special  placeholder that tailwind will use to inject the alpha value when using an opacity modifier:
when defining your colors this way, make sure that the format of your css variables is correct for the color function you are using. you’ll want to use spaces if using the modern , and commas if using legacy functions like  or :
use the  key in the  section of your  file to customize tailwind’s .
by default the spacing scale is inherited by the , , , , , , , , and  core plugins.
extending the default spacing scale
as described in the , if you’d like to extend the default spacing scale, you can do so using the  section of your  file:
this will generate classes like , , and  in addition to all of tailwind’s default spacing/sizing utilities.
overriding the default spacing scale
as described in the , if you’d like to override the default spacing scale, you can do so using the  section of your  file:
this will disable tailwind’s default spacing scale and generate classes like , , , and  instead.
default spacing scale
by default, tailwind includes a generous and comprehensive numeric spacing scale. the values are proportional, so  is twice as much spacing as  for example. one spacing unit is equal to , which translates to  by default in common browsers.
plugins let you register new styles for tailwind to inject into the user’s stylesheet using javascript instead of css.
to get started with your first plugin, import tailwind’s  function from . then inside your  array, call the imported  function with an anonymous function as the first argument.
plugin functions receive a single object argument that can be  into several helper functions:
, for registering new static utility styles, for registering new dynamic utility styles, for registering new static component styles, for registering new dynamic component styles, for registering new base styles, for registering custom variants, for looking up values in the user’s theme configuration, for looking up values in the user’s tailwind configuration, for checking if a core plugin is enabled, for manually escaping strings meant to be used in class names
we’ve developed a handful of official plugins for popular features that for one reason or another don’t belong in core yet.
plugins can be added to your project by installing them via npm, then adding them to your  file:
typography
the  plugin adds a set of  classes that can be used to quickly add sensible typographic styles to content blocks that come from sources like markdown or a cms database.
the  plugin adds an opinionated form reset layer that makes it easier to style form elements with utility classes.
line-clamp
the  plugin adds  classes you can use to truncate text to a fixed number of lines.
aspect ratio
the  plugin adds  and  classes that can be combined to give an element a fixed aspect ratio.
adding utilities
the  and  functions allow you to register new styles in tailwind’s  layer.
like with the utilities tailwind includes by default, utilities added by a plugin will only be included in the generated css if they are actually being used in the project.
static utilities
use the  function to register simple static utilities that don’t support user-provided values:
learn more about to represent your styles in javascript in the  reference.
dynamic utilities
use the  function to register utilities that map to values defined in the user’s  configuration:
utilities defined this way also support , which means you can use values not present in the theme using square bracket notation:
prefix and important
by default, plugin utilities automatically respect the user’s  and  preferences.
that means that given this tailwind configuration:
…the example plugin above would generate the following css:
using with modifiers
any custom utilities added using  can automatically be used with modifiers:
providing default values
utility plugins can provide default values by including a configuration object as the second argument to the  function:
./plugins/tab-size.js
these values behave just like the values in the default configuration, and can be overridden or extended by the end user.
adding components
the  function allows you to register new styles in tailwind’s  layer.
use it to add more opinionated, complex classes like buttons, form controls, alerts, etc; the sort of pre-built components you often see in other frameworks that you might need to override with utility classes.
to add new component styles from a plugin, call , passing in your styles using :
like with other component classes in tailwind, component classes added by a plugin will only be included in the generated css if they are actually being used in the project.
by default, component classes automatically respect the user’s  preference, but they are not affected by the user’s  preference.
although there’s rarely a good reason to make component declarations important, if you really need to do it you can always add  manually:
all classes in a selector will be prefixed by default, so if you add a more complex style like:
…the following css would be generated:
any component classes added using  can automatically be used with modifiers:
the  function allows you to register new styles in tailwind’s  layer. use it to add things like base typography styles, opinionated global resets, or  rules.
to add new base styles from a plugin, call , passing in your styles using :
since base styles are meant to target bare selectors like  or , they do not respect the user’s  or  configuration.
adding variants
the  function allows you to register your own custom  that can be used just like the built-in hover, focus, active, etc. variants.
to add a new variant, call the  function, passing in the name of your custom variant, and a format string that represents how the selector should be modified.
the first argument is the modifier name that users will use in their html, so the above example would make it possible to write classes like these:
parent and sibling states
your custom modifiers won’t automatically work with tailwind’s  and  state modifiers.
to support the  and  versions of your own custom modifiers, register them as separate variants using the special  directive to ensure the  and  classes only appear once in the final selector.
extending the configuration
plugins can merge their own set of configuration values into the user’s  configuration by providing an object as the second argument to the  function:
this can be useful for things like providing default  values for the classes your plugin generates.
exposing options
sometimes it makes sense for a plugin to be configurable in a way that doesn’t really belong under , like perhaps you want users to be able to customize the class name your plugin uses.
for cases like this, you can use  to define a plugin that can be invoked with a configuration object. this api is similar to the regular  api, except each argument should be a function that receives the user’s  and returns the value that you would have normally passed in using the regular api:
./plugins/markdown.js
the user would invoke your plugin passing along their options when registering it in their  configuration:
the user can also register plugins created this way normally without invoking them if they don’t need to pass in any custom options:
css-in-js syntax
tailwind’s plugin system expect css rules written as javascript objects, using the same sort of syntax you might recognize from css-in-js libraries like , powered by  under-the-hood.
consider this simple css rule:
translating this to a css-in-js object would look like this:
for convenience, property names can also be written in camelcase and will be automatically translated to dash-case:
nesting is also supported (powered by ), using the same syntax you might be familiar with from sass or less:
multiple rules can be defined in the same object:
…or as an array of objects in case you need to repeat the same key:
by default, any configuration you add in your own  file is intelligently merged with the , with your own configuration acting as a set of overrides and extensions.
the  option lets you specify a different configuration to use as your base, making it easy to package up a set of customizations that you’d like to reuse across projects.
this can be very useful for teams that manage multiple tailwind projects for the same brand where they want a single source of truth for colors, fonts, and other common customizations.
creating a preset
presets are just regular tailwind configuration objects, taking the exact same shape as the configuration you would add in your  file.
my-preset.js
as you can see, presets can contain all of the configuration options you’re used to, including theme overrides and extensions, adding plugins, configuring a prefix, and so on. read about  for more details.
assuming this preset was saved at , you would use it by adding it to the  file in your actual project under the  key:
by default, presets themselves extend tailwind’s  just like your own configuration would. if you’d like to create a preset that completely replaces the default configuration, include an empty  key in the preset itself:
merging logic in-depth
project-specific configurations (those found in your  file) are merged against presets the same way they are merged against the default configuration.
the following options in  simply replace the same option if present in a preset:
the remaining options are each carefully merged in the way that makes the most sense for that option, explained in more detail below.
the  object is merged shallowly, with top-level keys in  replacing the same top-level keys in any presets. the exception to this is the  key, which is collected across all configurations and applied on top of the rest of the theme configuration.
learn more about how the  option works in the .
the  array is merged across configurations, allowing presets to include their own presets, which can also include their own presets.
the  array is merged across configurations to make it possible for a preset to register plugins while also allowing you to add additional plugins at the project-level.
this means it’s not possible to disable a plugin that has been added by a preset. if you find yourself wanting to disable a plugin in a preset, it’s a sign that you should probably remove that plugin from the preset and include it on a project-by-project basis instead, or .
the  option behaves differently depending on whether you configure it as an object or as an array.
if you configure  as an object, it is merged across configurations.
if you configure  as an array, it replaces any  configuration provided by your configured preset(s).
extending multiple presets
the  option is an array and can accept multiple presets. this is useful if you want to split your reusable customizations up into composable chunks that can be imported independently.
when adding multiple presets, it’s important to note that if they overlap in any way, they are resolved the same way your own customizations are resolved against a preset, and the last configuration wins.
for example, if both of these configurations provided a custom color palette (and were not using ), the color palette from  would be used:
disabling the default configuration
if you’d like to completely disable the default configuration and start with no base configuration at all, set  to an empty array:
this will completely disable all of tailwind’s defaults, so no colors, font families, font sizes, spacing values, etc. will be generated at all.
you can also do this from within a preset if you’d like your preset to provide a complete design system on its own that doesn’t extend tailwind’s defaults:
built on top of , preflight is a set of base styles for tailwind projects that are designed to smooth over cross-browser inconsistencies and make it easier for you to work within the constraints of your design system.
tailwind automatically injects these styles when you include  in your css:
while most of the styles in preflight are meant to go unnoticed — they simply make things behave more like you’d expect them to — some are more opinionated and can be surprising when you first encounter them.
for a complete reference of all the styles applied by preflight, .
default margins are removed
preflight removes all of the default margins from elements like headings, blockquotes, paragraphs, etc.
this makes it harder to accidentally rely on margin values applied by the user-agent stylesheet that are not part of your spacing scale.
headings are unstyled
all heading elements are completely unstyled by default, and have the same font-size and font-weight as normal text.
the reason for this is two-fold:
it helps you avoid accidentally deviating from your type scale. by default, the browsers assigns sizes to headings that don’t exist in tailwind’s default type scale, and aren’t guaranteed to exist in your own type scale.in ui development, headings should often be visually de-emphasized. making headings unstyled by default means any styling you apply to headings happens consciously and deliberately.
you can always add default header styles to your project by .
if you’d like to selectively introduce sensible default heading styles into article-style parts of a page, we recommend the .
lists are unstyled
ordered and unordered lists are unstyled by default, with no bullets/numbers and no margin or padding.
if you’d like to style a list, you can do so using the  and  utilities:
you can always add default list styles to your project by .
if you’d like to selectively introduce default list styles into article-style parts of a page, we recommend the .
accessibility considerations
unstyled lists are . if your content is truly a list but you would like to keep it unstyled,  to the element:
images are block-level
images and other replaced elements (like , , , and others) are  by default.
this helps to avoid unexpected alignment issues that you often run into using the browser default of .
if you ever need to make one of these elements  instead of , simply use the  utility:
images are constrained to the parent width
images and videos are constrained to the parent width in a way that preserves their intrinsic aspect ratio.
this prevents them from overflowing their containers and makes them responsive by default. if you ever need to override this behavior, use the  utility:
border styles are reset globally
in order to make it easy to add a border by simply adding the  class, tailwind overrides the default border styles for all elements with the following rules:
since the  class only sets the  property, this reset ensures that adding that class always adds a solid 1px border using your configured default border color.
this can cause some unexpected results when integrating certain third-party libraries, like  for example.
when you run into situations like this, you can work around them by overriding the preflight styles with your own custom css:
buttons have a default outline
to ensure that we provide accessible styles out of the box, we made sure
that buttons have a default outline. you can of course override this by
applying  or similar utilities to your buttons.
extending preflight
if you’d like to add your own base styles on top of preflight, simply add them to your css within a  directive:
disabling preflight
if you’d like to completely disable preflight — perhaps because you’re integrating tailwind into an existing project or because you’d like to provide your own base styles — all you need to do is set  to  in the  section of your  file:
setting the aspect ratio
use the  utilities to set the desired aspect ratio of an element.
tailwind doesn’t include a large set of aspect ratio values out of the box since it’s easier to just use arbitrary values. see the  section for more information.
the  utilities use the native  css property, which was not supported in safari until version 15. until safari 15 is popularized, tailwind’s  plugin is a good alternative.
applying conditionally
hover, focus, and other states
tailwind lets you conditionally apply utility classes in different states using variant modifiers. for example, use  to only apply the  utility on hover.
for a complete list of all available state modifiers, check out the  documentation.
breakpoints and media queries
you can also use variant modifiers to target media queries like responsive breakpoints, dark mode, prefers-reduced-motion, and more. for example, use  to apply the  utility at only medium screen sizes and above.
to learn more, check out the documentation on ,  and .
using custom values
by default, tailwind provides a minimal set of  utilities. you can customize these values by editing  or  in your  file.
learn more about customizing the default theme in the  documentation.
if you need to use a one-off  value that doesn’t make sense to include in your theme, use square brackets to generate a property on the fly using any arbitrary value.
using the container
the  class sets the  of an element to match the  of the current breakpoint. this is useful if you’d prefer to design for a fixed set of screen sizes instead of trying to accommodate a fully fluid viewport.
note that unlike containers you might have used in other frameworks, tailwind’s container does not center itself automatically and does not have any built-in horizontal padding.
to center a container, use the  utility:
to add horizontal padding, use the  utilities:
if you’d like to center your containers by default or include default horizontal padding, see the  below.
responsive variants
the  class also includes responsive variants like  by default that allow you to make something behave like a container at only a certain breakpoint and up:
customizing
centering by default
to center containers by default, set the  option to  in the  section of your config file:
adding horizontal padding
to add horizontal padding by default, specify the amount of padding you’d like using the  option in the  section of your config file:
if you want to specify a different padding amount for each breakpoint, use an object to provide a  value and any breakpoint-specific overrides:
adding based on column count
use the  utilities to set the number of columns that should be created for the content within an element. the column width will be automatically adjusted to accommodate that number.
adding based on column width
use the  utilities to set the ideal column width for the content within an element, with the number of columns (the count) automatically adjusting to accommodate that value.
this “t-shirt” scale is the same as the  scale, with the addition of  and , since smaller columns may be desirable.
resize the example to see the expected behaviour
setting the column gap
to specify the width between columns, you can use the  utilities:
by default, tailwind provides a column count scale from  as well as a column t-shirt scale from . you can customize these values by editing  or  in your  file.
setting the break-after behavior
use the  utilities to control how a column or page break should behave after an element. for example, use the  utility to force a column break after an element.
setting the break-before behavior
use the  utilities to control how a column or page break should behave before an element. for example, use the  utility to force a column break before an element.
setting the break-inside behavior
use the  utilities to control how a column or page break should behave within an element. for example, use the  utility to avoid a column break within an element.
setting the box decoration break
use the  and  utilities to control whether properties like background, border, border-image, box-shadow, clip-page, margin, and padding should be rendered as if the element were one continuous fragment, or distinct blocks.
box-decoration-slice
hello
world
box-decoration-clone
including borders and padding
use  to set an element’s  to , telling the browser to include the element’s borders and padding when you give it a height or width.
this means a 100px × 100px element with a 2px border and 4px of padding on all sides will be rendered as 100px × 100px, with an internal content area of 88px × 88px.
tailwind makes this the default for all elements in our .
128px
excluding borders and padding
use  to set an element’s  to , telling the browser to add borders and padding on top of the element’s specified width or height.
this means a 100px × 100px element with a 2px border and 4px of padding on all sides will actually be rendered as 112px × 112px, with an internal content area of 100px × 100px.
block & inline
use , , and  to control the flow of text and elements.
when controlling the flow of text, using the css property display: inline
will cause the text inside the element to wrap normally.
while using the property
display: inline-block
will wrap the element to prevent the text inside from extending beyond its parent.
lastly, using the property
display: block
will put the element on its own line and fill its parent.
flow root
use  to create a block-level element with its own .
flex
use  to create a block-level flex container.
andrew alfred
technical advisor
inline flex
use  to create an inline flex container that flows with text.
today i spent most of the day researching ways to take advantage of the fact that bottles can be returned for 10 cents in michigan, but only 5 cents here.
kramer
keeps telling me there is no way to make it work, that he has run the numbers on every possible approach, but i just have to believe there's a way to make it work, there's simply too much opportunity here.
grid
use  to create a grid container.
inline grid
use  to create an inline grid container.
contents
use  to create a “phantom” container whose children act like direct children of the parent.
table
use the , , , , , , , , and  utilities to create elements that behave like their respective table elements.
song
artist
year
the sliding mr. bones (next stop, pottersville)
malcolm lockyer
1961
witchy woman
the eagles
1972
shining star
earth, wind, and fire
1975
hidden
use  to set an element to  and remove it from the page layout (compare with  from the  documentation).
floating elements to the right
use  to float an element to the right of its container.
maybe we can live without libraries, people like you and me. maybe. sure, we're too old to change the world, but what about that kid, sitting down, opening a book, right now, in a branch at the local library and finding drawings of pee-pees and wee-wees on the cat in the hat and the five chinese brothers? doesn't he deserve better?  look. if you think this is about overdue fines and missing books, you'd better think again. this is about that kid's right to read a book without getting his mind warped! or: maybe that turns you on, seinfeld; maybe that's how y'get your kicks. you and your good-time buddies.
floating elements to the left
use  to float an element to the left of its container.
disabling a float
use  to reset any floats that are applied to an element. this is the default value for the float property.
clearing left-floated elements
use  to position an element below any preceding left-floated elements.
maybe we can live without libraries, people like you and me. maybe. sure, we're too old to change the world, but what about that kid, sitting down, opening a book, right now, in a branch at the local library and finding drawings of pee-pees and wee-wees on the cat in the hat and the five chinese brothers? doesn't he deserve better? look. if you think this is about overdue fines and missing books, you'd better think again. this is about that kid's right to read a book without getting his mind warped! or: maybe that turns you on, seinfeld; maybe that's how y'get your kicks. you and your good-time buddies.
clearing right-floated elements
use  to position an element below any preceding right-floated elements.
clearing all floated elements
use  to position an element below all preceding floated elements.
disabling applied clears
use  to reset any clears that are applied to an element. this is the default value for the clear property.
setting the stacking context
use the  and  utilities to control whether an element should explicitly create a new stacking context.
resizing to cover a container
resize an element’s content to cover its container using .
containing an element
resize an element’s content to stay contained within its container using .
stretching to fit a container
stretch an element’s content to fit its container using .
scaling down if too large
display an element’s content at its original size but scale it down to fit its container if necessary using .
using an element's original size
display an element’s content at its original size ignoring the container size using .
positioning a replaced element
use the  utilities to specify how a replaced element’s content should be positioned within its container.
object-left-top
object-top
object-right-top
object-left
object-center
object-right
object-left-bottom
object-bottom
object-right-bottom
by default, tailwind provides nine object position utilities. you can customize these values by editing  or  in your  file.
showing content that overflows
use  to prevent content within an element from being clipped. note that any content that overflows the bounds of the element will then be visible.
hiding content that overflows
use  to clip any content within an element that overflows the bounds of that element.
scrolling if needed
use  to add scrollbars to an element in the event that its content overflows the bounds of that element. unlike , which always shows scrollbars, this utility will only show them if scrolling is necessary.
debra houston
analyst
jane white
director, marketing
ray flint
scrolling horizontally if needed
use  to allow horizontal scrolling if needed.
andrew
emily
whitney
david
kristin
sarah
scrolling vertically if needed
use  to allow vertical scrolling if needed.
scrolling horizontally always
use  to allow horizontal scrolling and always show scrollbars unless always-visible scrollbars are disabled by the operating system.
scrolling vertically always
use  to allow vertical scrolling and always show scrollbars unless always-visible scrollbars are disabled by the operating system.
scrolling in all directions
use  to add scrollbars to an element. unlike , which only shows scrollbars if they are necessary, this utility always shows them. note that some operating systems (like macos) hide unnecessary scrollbars regardless of this setting.
5 am
6 am
7 am
8 am
9 am
10 am
11 am
12 pm
1 pm
2 pm
3 pm
4 pm
5 pm
6 pm
7 pm
8 pm
flight to vancouver
toronto yyz
breakfast
mel's diner
🎉 party party 🎉
we like to party!
preventing parent overscrolling
use  to prevent scrolling in the target area from triggering scrolling in the parent element, but preserve “bounce” effects when scrolling past the end of the container in operating systems that support it.
maybe we can live without libraries, people like you and me. maybe. sure, we're too old to change the world, but what about that kid, sitting down, opening a book, right now, in a branch at the local library and finding drawings of pee-pees and wee-wees on the cat in the hat and the five chinese brothers? doesn't he deserve better?
preventing overscroll bouncing
use  to prevent scrolling in the target area from triggering scrolling in the parent element, and also prevent “bounce” effects when scrolling past the end of the container.
using the default overscroll behavior
use  to make it possible for the user to continue scrolling a parent scroll area when they reach the boundary of the primary scroll area.
statically positioning elements
use  to position an element according to the normal flow of the document.
any  will be ignored and the element will not act as a position reference for absolutely positioned children.
static parent
absolute child
relatively positioning elements
any  are calculated relative to the element’s normal position and the element will act as a position reference for absolutely positioned children.
relative parent
absolutely positioning elements
use  to position an element outside of the normal flow of the document, causing neighboring elements to act as if the element doesn’t exist.
any  are calculated relative to the nearest parent that has a position other than , and the element will act as a position reference for other absolutely positioned children.
with static positioning
static child
static sibling
with absolute positioning
fixed positioning elements
use  to position an element relative to the browser window.
any  are calculated relative to the viewport and the element will act as a position reference for absolutely positioned children.
contacts
mindy albrect
david arnold
sticky positioning elements
use  to position an element as  until it crosses a specified threshold, then treat it as fixed until its parent is off screen.
aisha houston
anna white
andy flint
bob alfred
bianca houston
brianna white
bert flint
colton alfred
cynthia houston
cheyenne white
charlie flint
placing a positioned element
use the  utilities to set the horizontal or vertical position of a .
using negative values
to use a negative top/right/bottom/left value, prefix the class name with a dash to convert it to a negative value.
by default, tailwind provides top/right/bottom/left/inset utilities for a combination of the , ,  as well as some additional fraction values.
you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the top/right/bottom/left/inset scale by editing  or  in your  file.
making elements invisible
use  to hide an element, but still maintain its place in the dom, affecting the layout of other elements (compare with  from the  documentation).
collapsing elements
use  to hide table rows, row groups, columns, and column groups as if they were set to , but without impacting the size of other rows and columns.
this makes it possible to dynamically toggle rows and columns without affecting the table layout.
showing all rows
hiding a row using
making elements visible
use  to make an element visible. this is mostly useful for undoing the  utility at different screen sizes.
setting the z-index
control the stack order (or three-dimensional positioning) of an element in tailwind, regardless of order it has been displayed, using the  utilities.
to use a negative z-index value, prefix the class name with a dash to convert it to a negative value.
by default, tailwind provides six numeric  utilities and an  utility. you can customize these values by editing  or  in your  file.
setting the flex basis
use the  utilities to set the initial size of flex items.
the default flex basis scale is a combination of the  as well as a set of percentage based values.
alternatively, you can customize just the flex basis scale by editing  or  in your  file.
use  to position flex items horizontally in the same direction as text:
row reversed
use  to position flex items horizontally in the opposite direction:
column
use  to position flex items vertically:
column reversed
use  to position flex items vertically in the opposite direction:
don't wrap
use  to prevent flex items from wrapping, causing inflexible items to overflow the container if necessary:
wrap normally
use  to allow flex items to wrap:
wrap reversed
use  to wrap flex items in the reverse direction:
initial
use  to allow a flex item to shrink but not grow, taking into account its initial size:
flex 1
use  to allow a flex item to grow and shrink as needed, ignoring its initial size:
auto
use  to allow a flex item to grow and shrink, taking into account its initial size:
none
use  to prevent a flex item from growing or shrinking:
by default, tailwind provides four  utilities. you can customize these values by editing  or  in your  file.
grow
use  to allow a flex item to grow to fill any available space:
don't grow
use  to prevent a flex item from growing:
by default, tailwind provides two  utilities. you can customize these values by editing  or  in your  file.
shrink
use  to allow a flex item to shrink if needed:
don't shrink
use  to prevent a flex item from shrinking:
ordering flex and grid items
use  to render flex and grid items in a different order than they appear in the dom.
to use a negative order value, prefix the class name with a dash to convert it to a negative value.
by default, tailwind provides utilities for , , , and an  utility for the numbers 1 through 12. you can customize these values by editing  or  in your  file.
specifying the columns in a grid
use the  utilities to create grids with n equally sized columns.
by default, tailwind includes  utilities for creating basic grids with up to 12 equal width columns. you can customize these values by editing  or  in your  file.
you have direct access to the  css property here so you can make your custom column values as generic or as complicated and site-specific as you like.
spanning columns
use the  utilities to make an element span n columns.
starting and ending lines
use the  and  utilities to make an element start or end at the nth grid line. these can also be combined with the  utilities to span a specific number of columns.
note that css grid lines start at 1, not 0, so a full-width element in a 6-column grid would start at line 1 and end at line 7.
by default, tailwind includes grid-column utilities for working with grids with up to 12 columns. you change, add, or remove these by customizing the , , and  sections of your tailwind theme config.
for creating more  utilities that control the  shorthand property directly, customize the  section of your tailwind theme config:
we use this internally for our  utilities. note that since this configures the  shorthand property directly, we include the word  directly in the value name, it’s not baked into the class name automatically. that means you are free to add entries that do whatever you want here — they don’t just have to be  utilities.
to add new  utilities, use the  section of your tailwind theme config:
specifying the rows in a grid
use the  utilities to create grids with n equally sized rows.
by default, tailwind includes  utilities for creating basic grids with up to 6 equal width rows. you can customize these values by editing  or  in your  file.
you have direct access to the  css property here so you can make your custom rows values as generic or as complicated and site-specific as you like.
spanning rows
use the  utilities to make an element span n rows.
use the  and  utilities to make an element start or end at the nth grid line. these can also be combined with the  utilities to span a specific number of rows.
note that css grid lines start at 1, not 0, so a full-height element in a 3-row grid would start at line 1 and end at line 4.
by default, tailwind includes grid-row utilities for working with grids with up to 6 explicit rows. you can customize these values by editing , , , , , and  in your  file.
controlling grid element placement
use the  utilities to control how the auto-placement algorithm works for a grid layout.
sizing implicitly-created grid columns
use the  utilities to control the size of implicitly-created grid columns.
by default, tailwind includes four general purpose  utilities. you can customize these values by editing  or  in your  file.
sizing implicitly-created grid rows
use the  utilities to control the size implicitly-created grid rows.
setting the gap between elements
use  to change the gap between both rows and columns in grid and flexbox layouts.
changing row and column gaps independently
use  and  to change the gap between rows and columns independently.
by default, tailwind’s gap scale uses the . you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the gap scale by editing  or  in your  file.
start
use  to justify items against the start of the container’s main axis:
center
use  to justify items along the center of the container’s main axis:
use  to justify items against the end of the container’s main axis:
space between
use  to justify items along the container’s main axis such that there is an equal amount of space between each item:
space around
use  to justify items along the container’s main axis such that there is an equal amount of space on each side of each item:
space evenly
use  to justify items along the container’s main axis such that there is an equal amount of space around each item, but also accounting for the doubling of space you would normally see between each item when using :
use  to justify grid items against the start of their inline axis:
use  to justify grid items against the end of their inline axis:
use  to justify grid items along their inline axis:
stretch
use  to stretch items along their inline axis:
use  to align an item based on the value of the grid’s  property:
use  to align a grid item to the start its inline axis:
use  to align a grid item along the center its inline axis:
use  to align a grid item to the end its inline axis:
use  to stretch a grid item to fill the grid area on its inline axis:
use  to pack rows in a container against the start of the cross axis:
use  to pack rows in a container in the center of the cross axis:
use  to pack rows in a container against the end of the cross axis:
use  to distribute rows in a container such that there is an equal amount of space between each line:
use  to distribute rows in a container such that there is an equal amount of space around each line:
use  to distribute rows in a container such that there is an equal amount of space around each item, but also accounting for the doubling of space you would normally see between each item when using :
use  to stretch items to fill the container’s cross axis:
use  to align items to the start of the container’s cross axis:
use  to align items along the center of the container’s cross axis:
use  to align items to the end of the container’s cross axis:
baseline
use  to align items along the container’s cross axis such that all of their baselines align:
use  to align an item based on the value of the container’s  property:
use  to align an item to the start of the container’s cross axis, despite the container’s  value:
use  to align an item along the center of the container’s cross axis, despite the container’s  value:
use  to align an item to the end of the container’s cross axis, despite the container’s  value:
use  to stretch an item to fill the container’s cross axis, despite the container’s  value:
use  to pack items in the center of the block axis:
use  to pack items against the start of the block axis:
use  to to pack items against the end of the block axis:
use  to distribute grid items along the block axis so that that there is an equal amount of space between each row on the block axis.
use  distribute grid items such that there is an equal amount of space around each row on the block axis:
use  to distribute grid items such that they are evenly spaced on the block axis:
use  to stretch grid items along their grid areas on the block axis:
use  to place grid items on the start of their grid areas on both axes:
use  to place grid items on the end of their grid areas on both axes:
use  to place grid items on the center of their grid areas on both axes:
use  to stretch items along their grid areas on both axes:
use  to align an item to the start on both axes:
use  to align an item at the center on both axes:
use  to align an item to the end on both axes:
use  to stretch an item on both axes:
add padding to a single side
control the padding on one side of an element using the  utilities.
for example,  would add  of padding to the top of an element,  would add  of padding to the right of an element,  would add  of padding to the bottom of an element, and  would add  of padding to the left of an element.
pt-6
pr-4
pb-8
pl-2
add horizontal padding
control the horizontal padding of an element using the  utilities.
px-8
add vertical padding
control the vertical padding of an element using the  utilities.
py-8
add padding to all sides
control the padding on all sides of an element using the  utilities.
by default, tailwind’s padding scale uses the . you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the padding scale by editing  or  in your  file.
add margin to a single side
control the margin on one side of an element using the  utilities.
for example,  would add  of margin to the top of an element,  would add  of margin to the right of an element,  would add  of margin to the bottom of an element, and  would add  of margin to the left of an element.
mt-6
mr-4
mb-8
ml-2
add horizontal margin
control the horizontal margin of an element using the  utilities.
mx-8
add vertical margin
control the vertical margin of an element using the  utilities.
my-8
add margin to all sides
control the margin on all sides of an element using the  utilities.
to use a negative margin value, prefix the class name with a dash to convert it to a negative value.
-mt-8
by default, tailwind’s margin scale uses the . you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the margin scale by editing  or  in your  file.
add horizontal space between children
control the horizontal space between elements using the  utilities.
add vertical space between children
control the vertical space between elements using the  utilities.
reversing children order
if your elements are in reverse order (using say  or ), use the  or  utilities to ensure the space is added to the correct side of each element.
to use a negative space value, prefix the class name with a dash to convert it to a negative value.
limitations
these utilities are really just a shortcut for adding margin to all-but-the-first-item in a group, and aren’t designed to handle complex cases like grids, layouts that wrap, or situations where the children are rendered in a complex custom order rather than their natural dom order.
for those situations, it’s better to use the  when possible, or add margin to every element with a matching negative margin on the parent:
cannot be paired with divide utilities
the  utilities are not designed to work together with the . for those situations, consider adding margin/padding utilities to the children instead.
by default, tailwind’s space scale uses the . you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the space scale by editing  or  in your  file.
fixed widths
use  or  to set an element to a fixed width.
w-96
w-80
w-64
w-48
w-40
w-32
w-24
percentage widths
use  or  to set an element to a percentage based width.
w-1/2
w-2/5
w-3/5
w-1/3
w-2/3
w-1/4
w-3/4
w-1/5
w-4/5
w-1/6
w-5/6
w-full
viewport width
use  to make an element span the entire width of the viewport.
resetting the width
the  utility can be useful if you need to remove an element’s assigned width under a specific condition, like at a particular breakpoint:
by default, tailwind’s width scale is a combination of the  as well as some additional values specific to widths.
to customize width separately, use the  section of your  file.
setting the minimum width
set the minimum width of an element using the  utilities.
you can customize your  scale by editing  or  in your  file.
setting the maximum width
set the maximum width of an element using the  utilities.
assistant to the traveling secretary
reading width
the  utility gives an element a max-width optimized for readability and adapts based on the font size.
text-sm
oh yeah. it's the best part. it's crunchy, it's explosive, it's where the muffin breaks free of the pan and sort of does it's own thing. i'll tell you. that's a million dollar idea right there. just sell the tops.
text-base
text-xl
constraining to your breakpoints
the  classes can be used to give an element a max-width matching a specific breakpoint. these values are automatically derived from the  of your  file.
fixed heights
use  or  to set an element to a fixed height.
h-96
h-80
h-64
h-48
h-40
h-32
h-24
full height
use  to set an element’s height to 100% of its parent, as long as the parent has a defined height.
viewport height
use  to make an element span the entire height of the viewport.
by default, tailwind’s height scale is a combination of the  as well as some additional values specific to heights.
to customize height separately, use the  section of your  file.
setting the minimum height
set the minimum height of an element using the , , or  utilities.
setting the maximum height
set the maximum height of an element using the  or  utilities.
by default, tailwind’s max-height scale uses a combination of the  as well as some additional height related values.
alternatively, you can customize just the max-height scale by editing  or  in your  file.
setting the font family
you can control the typeface of text using the font family utilities.
font-sans
the quick brown fox jumps over the lazy dog.
font-serif
font-mono
by default, tailwind provides three font family utilities: a cross-browser sans-serif stack, a cross-browser serif stack, and a cross-browser monospaced stack. you can change, add, or remove these by editing the  section of your tailwind config.
font families can be specified as an array or as a simple comma-delimited string:
note that tailwind does not automatically escape font names for you. if you’re using a font that contains an invalid identifier, wrap it in quotes or escape the invalid characters.
providing default font-feature-settings
you can optionally provide default  for each font in your project using a tuple of the form  when configuring custom fonts.
customizing the default font
for convenience,  sets the font family on the  element to match your configured  font, so one way to change the default font for your project is to customize the  key in your  configuration:
you can also customize the default font used in your project by  that sets the  property explicitly:
this is the best approach if you have customized your font family utilities to have different names and don’t want there to be  utility available in your project.
setting the font size
control the font size of an element using the  utilities.
text-lg
text-2xl
you can configure your own custom set of font size utilities using the  section of your  file.
providing a default line-height
tailwind’s default theme configures a sensible default  for each  utility. you can configure your own default line heights when using custom font sizes by defining each size using a tuple of the form  in your  file.
you can also specify a default line height using the object syntax, which allows you to also provide default  and  values. you can do this using a tuple of the form .
applying font smoothing
use the  utility to render text using subpixel antialiasing and the  utility to render text using grayscale antialiasing.
subpixel-antialiased
antialiased
italicizing text
the  utility can be used to make text italic. likewise, the  utility can be used to display text normally — typically to reset italic text at different breakpoints.
italic
not-italic
setting the font weight
control the font weight of an element using the  utilities.
font-light
font-normal
font-medium
font-semibold
font-bold
by default, tailwind provides nine  utilities. you change, add, or remove these by editing the  section of your tailwind config.
applying numeric variants
use the  utilities to enable additional glyphs for numbers, fractions, and ordinal markers (in fonts that support them).
these utilities are composable so you can enable multiple  features by combining multiple classes in your html:
note that many fonts don’t support these features (stacked fractions support for example is especially rare), so some of these utilities may have no effect depending on the font family you are using.
ordinaluse the  utility to enable special glyphs for the ordinal markers.
slashed zerouse the  utility to force a 0 with a slash; this is useful when a clear distinction between o and 0 is needed.
lining figuresuse the  utility to use the numeric glyphs that are all aligned by their baseline. this corresponds to the  opentype feature. this is the default for most fonts.
1234567890
oldstyle figuresuse the  utility to use numeric glyphs where some numbers have descenders. this corresponds to the  opentype feature.
proportional figuresuse the  utility to use numeric glyphs that have proportional widths (rather than uniform/tabular). this corresponds to the  opentype feature.
12121
90909
tabular figuresuse the  utility to use numeric glyphs that have uniform/tabular widths (rather than proportional). this corresponds to the  opentype feature.
diagonal fractionsuse the  utility to replace numbers separated by a slash with common diagonal fractions. this corresponds to the  opentype feature.
1/2 3/4 5/6
stacked fractionsuse the  utility to replace numbers separated by a slash with common stacked fractions. this corresponds to the  opentype feature. very few fonts seem to support this feature — we’ve used ubuntu mono here.
resetting numeric font variants
use the  propery to reset numeric font variants. this is usually useful for resetting a font feature at a particular breakpoint:
setting the letter spacing
control the letter spacing of an element using the  utilities.
tracking-tight
tracking-normal
tracking-wide
to use a negative letter-spacing value, prefix the class name with a dash to convert it to a negative value.
using negative values doesn’t make a ton of sense with the letter-spacing scale tailwind includes out of the box, but if you’ve opted to customize your letter-spacing scale to use numbers instead of descriptive words like “wide” the negative value modifier can be useful.
htmltailwind.config.js
by default, tailwind provides six tracking utilities. you can change, add, or remove these by editing the  section of your tailwind config.
relative line-heights
use the , , , , , and  utilities to give an element a relative line-height based on its current font-size.
leading-normal
leading-relaxed
leading-loose
fixed line-heights
use the  utilities to give an element a fixed line-height, irrespective of the current font-size. these are useful when you need very precise control over an element’s final size.
leading-6
leading-7
leading-8
overriding default line-heights
it’s important to note that by default, tailwind’s  utilities each set their own default line-height. this means that any time you use a responsive font-size utility like , any explicit line-height you have set for a smaller breakpoint will be overridden.
if you want to override the default line-height after setting a breakpoint-specific font-size, make sure to set a breakpoint-specific line-height as well:
using the same line-height across different font sizes is generally not going to give you good typographic results. line-height should typically get smaller as font-size increases, so the default behavior here usually saves you a ton of work. if you find yourself fighting it, you can always  to not include default line-heights.
by default, tailwind provides six relative and eight fixed  utilities. you change, add, or remove these by customizing the  section of your tailwind theme config.
setting the list style type
to create bulleted or numeric lists, use the  and  utilities.
list-disc
now this is a story all about how, my life got flipped-turned upside down
and i'd like to take a minute just sit right there
i'll tell you how i became the prince of a town called bel-air
list-decimal
list-none
by default, tailwind provides three utilities for the most common list style types. you change, add, or remove these by editing the  section of your tailwind config.
setting the list style position
control the position of the markers and text indentation in a list using the  and  utilities.
list-inside
list-outside
setting the text alignment
control the text alignment of an element using the , , , and  utilities.
setting the text color
control the text color of an element using the  utilities.
changing the opacity
control the opacity of an element’s text color using the color opacity modifier.
you can use any value defined in your , or use arbitrary values if you need to deviate from your design tokens.
try hovering over the text to see the expected behaviour
by default, tailwind makes the entire  available as text colors. you can  by editing  or  in your  file.
alternatively, you can customize just your text colors by editing  or  in your  file.
setting the text decoration
control how text is decorated with the , , and  utilities.
underline
overline
line-through
no-underline
setting the text decoration color
use the  utilities to change the color of an element’s .
i’m derek, an astro-engineer based in tattooine. i like to build x-wings at . outside of work, i like to  and have  fights.
control the opacity of an element’s text decoration color using the color opacity modifier.
by default, tailwind makes the entire  available as text decoration colors. you can  by editing  or  in your  file.
alternatively, you can customize just your text decoration colors by editing  or  in your  file.
setting the text decoration style
use the  utilities to change the style of an element’s .
decoration-solid
decoration-double
decoration-dotted
decoration-dashed
decoration-wavy
setting the text decoration thickness
use the  utilities to change the thickness of an element’s .
decoration-1
decoration-2
decoration-4
you can customize the  utilities by editing  or  in your  file.
setting the underline offset
use the  utilities to change the offset of a text underline.
underline-offset-1
underline-offset-2
underline-offset-4
underline-offset-8
transforming text
the  and  will uppercase and lowercase text respectively, whereas  utility will convert text to title-case. the  utility can be used to preserve the original casing — typically to reset capitalization at different breakpoints.
normal-case
uppercase
lowercase
capitalize
truncate
use  to truncate overflowing text with an ellipsis () if needed.
the longest word in any of the major english language dictionaries is pneumonoultramicroscopicsilicovolcanoconiosis, a word that refers to a lung disease contracted from the inhalation of very fine silica particles, specifically from a volcano; medically, it is the same as silicosis.
ellipsis
clip
use  to truncate the text at the limit of the content area.
adding a text indent
use the  utilities to set the amount of empty space (indentation) that’s shown before text in a block.
so i started to walk into the water. i won't lie to you boys, i was
terrified. but i pressed on, and as i made my way past the breakers
a strange calm came over me. i don't know if it was divine intervention
or the kinship of all living things but i tell you jerry at that moment,
i was a marine biologist.
to use a negative text indent value, prefix the class name with a dash to convert it to a negative value.
the default text indent scale is based on the . you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the text indent scale by editing  or  in your  file.
use  to align the baseline of an element with the baseline of its parent.
use  to align the top of an element and its descendants with the top of the entire line.
middle
use  to align the middle of an element with the baseline plus half the x-height of the parent.
bottom
use  to align the bottom of an element and its descendants with the bottom of the entire line.
text top
use  to align the top of an element with the top of the parent element’s font.
text bottom
use  to align the bottom of an element with the bottom of the parent element’s font.
if you need to use a one-off  value that isn't included in tailwind by default, use square brackets to generate a property on the fly using any arbitrary value.
normal
use  to cause text to wrap normally within an element. newlines and spaces will be collapsed.
hey everyone!
it's almost 2022       and we still don't know if there is aliens living among us, or do we? maybe the person writing this is an alien.
you will never know.
no wrap
use  to prevent text from wrapping within an element. newlines and spaces will be collapsed.
use  to preserve newlines and spaces within an element. text will not be wrapped.
pre line
use  to preserve newlines but not spaces within an element. text will be wrapped normally.
pre wrap
use  to preserve newlines and spaces within an element. text will be wrapped normally.
use  to only add line breaks at normal word break points.
the longest word in any of the major english language dictionaries is
pneumonoultramicroscopicsilicovolcanoconiosis,
a word that refers to a lung disease contracted from the inhalation of very fine silica particles, specifically from a volcano; medically, it is the same as silicosis.
break words
use  to add line breaks mid-word if needed.
break all
use  to add line breaks whenever necessary, without trying to preserve whole words.
setting a pseudo-element's content
use the  utilities along with the  and  variant modifiers to set the contents of the  and  pseudo-elements.
out of the box,  is the only available preconfigured content utility. and while you can add additional utilities by , it generally makes more sense to just use an arbitrary value.
use the square bracket notation to define any arbitrary content value on the fly.
higher resolution means more than just a better-quality image. with a retina 6k display,  gives you nearly 40 percent more screen real estate than a 5k display.
referencing an attribute value
these content utilities even support css features like the  function, which you can use to reference a value stored in an attribute:
using spaces and underscores
since whitespace denotes the end of a class in html, replace any spaces in an arbitrary value with an underscore:
if you need to include an actual underscore, you can do this by escaping it with a backslash:
by default, tailwind only provides the  utility. you can customize these values by editing  or  in your  file.
fixed
use  to fix the background image relative to the viewport.
my trip to the summit
november 16, 2021 · 4 min read
look. if you think this is about overdue fines and missing books, you'd better think again. this is about that kid's right to read a book without getting his mind warped! or: maybe that turns you on, seinfeld; maybe that's how y'get your kicks. you and your good-time buddies.
local
use  to scroll the background image with the container and the viewport.
"because the mail never stops. it just keeps coming and coming and coming, there's never a let-up. it's relentless. every day it piles up more and more and more. and you gotta get it out but the more you get it out the more it keeps coming in. and then the barcode reader breaks and it's publisher's clearing house day."
— newman
scroll
use  to scroll the background image with the viewport, but not with the container.
setting the background clip
use the  utilities to control the bounding box of an element’s background.
bg-clip-border
bg-clip-padding
bg-clip-content
cropping to text
use  to crop an element’s background to match the shape of the text. useful for effects where you want a background image to be visible through the text.
setting the background color
control the background color of an element using the  utilities.
control the opacity of an element’s background color using the color opacity modifier.
bg-sky-500
button a
bg-sky-500/75
button b
bg-sky-500/50
button c
try hovering over the button to see the background color change
subscribe
by default, tailwind makes the entire  available as background colors. you can  by editing  or  in your  file.
alternatively, you can customize just your background colors by editing  or  in your  file.
setting the background origin
use , , and  to control where an element’s background is rendered.
bg-origin-border
bg-origin-padding
bg-origin-content
setting the background position
use the  utilities to control the position of an element’s background image.
bg-left-top
bg-top
bg-right-top
bg-left
bg-center
bg-right
bg-left-bottom
bg-bottom
bg-right-bottom
repeat
use  to repeat the background image both vertically and horizontally.
no repeat
use  when you don’t want to repeat the background image.
repeat horizontally
use  to repeat the background image only horizontally.
repeat vertically
use  to repeat the background image only vertically.
use  to display the background image at its default size.
cover
use  to scale the background image until it fills the background layer.
contain
use  to scale the background image to the outer edges without cropping or stretching.
by default, tailwind provides utilities for , , and  background sizes. you can change, add, or remove these by editing the  section of your config.
linear gradients
to give an element a linear gradient background, use one of the  utilities, in combination with the  utilities.
by default, tailwind includes background image utilities for creating linear gradient backgrounds in eight directions.
you can add your own background images by editing the  section of your  file:
these don’t just have to be gradients — they can be any background images you need.
starting color
set the starting color of a gradient using the  utilities.
ending color
set the ending color of a gradient using the  utilities.
gradients to do not fade in from transparent by default. to fade in from transparent, reverse the gradient direction and use a  utility.
middle color
add a middle color to a gradient using the  utilities.
gradients with a  and  will fade out to transparent by default if no  is present.
fading to transparent
gradients fade out to transparent by default, without requiring you to add  explicitly. tailwind intelligently calculates the right “transparent” value to use based on the starting color to avoid  that causes fading to simply the  keyword to fade through gray, which just looks awful.
don't add `to-transparent` explicitly
only specify a from color and fade to transparent automatically
hover me
by default, tailwind makes the entire  available as gradient colors. you can  by editing  or  in your  file.
alternatively, you can customize just your gradient colors by editing  or  in your  file.
rounded corners
use utilities like , , or  to apply different border radius sizes to an element.
rounded
rounded-md
rounded-lg
rounded-full
pill buttons
use the  utility to create pill buttons.
no rounding
use  to remove an existing border radius from an element.
this is most commonly used to remove a border radius that was applied at a smaller breakpoint.
rounded-none
rounding sides separately
use  to only round one side of an element.
rounded-t-lg
rounded-r-lg
rounded-b-lg
rounded-l-lg
rounding corners separately
use  to only round one corner an element.
rounded-tl-lg
rounded-tr-lg
rounded-br-lg
rounded-bl-lg
by default, tailwind provides five border radius size utilities. you can change, add, or remove these by editing the  section of your tailwind config.
all sides
use the , , , , or  utilities to set the border width for all sides of an element.
border
border-2
border-4
border-8
individual sides
use the , , , , or  utilities to set the border width for one side of an element.
border-t-4
border-r-4
border-b-4
border-l-4
horizontal and vertical sides
use the  utilities to set the border width on two sides of an element at the same time.
border-x-4
border-y-4
between elements
you can also add borders between child elements using the  and  utilities.
learn more in the  and  documentation.
by default, tailwind provides five  utilities, and the same number of utilities per side (horizontal, vertical, top, right, bottom, and left). you change, add, or remove these by editing the  section of your tailwind config.
setting the border color
control the border color of an element using the  utilities.
email address
this field is required.
control the opacity of an element’s border color using the color opacity modifier.
border-indigo-500/100
border-indigo-500/75
border-indigo-500/50
use the  utilities to set the border color for one side of an element.
border-t-indigo-500
border-r-indigo-500
border-b-indigo-500
border-l-indigo-500
use the  utilities to set the border color on two sides of an element at the same time.
border-x-indigo-500
border-y-indigo-500
send email
by default, tailwind makes the entire  available as border colors. you can  by editing  or  in your  file.
alternatively, you can customize just your border colors by editing  or  in your  file.
setting the border style
use  to control an element’s border style.
border-solid
border-dashed
border-dotted
border-double
no style
use  to remove an existing border style from an element.
this is most commonly used to remove a border style that was applied at a smaller breakpoint.
add borders between horizontal children
add borders between horizontal elements using the  utilities.
add borders between stacked children
add borders between stacked elements using the  utilities.
if your elements are in reverse order (using say  or ), use the  or  utilities to ensure the border is added to the correct side of each element.
the divide width scale inherits its values from the  scale by default, so if you’d like to customize your values for both border width and divide width together, use the  section of your  file.
to customize only the divide width values, use the  section of your  file.
setting the divide color
control the border color between elements using the  utilities.
control the opacity of the divide color using the color opacity modifier.
by default, tailwind makes the entire  available as divide colors. you can  by editing  or  in your  file.
alternatively, you can customize just your divide colors by editing  or  in your  file.
set the divide style
control the border style between elements using the  utilities.
setting the outline width
use the  utilities to change the width of an element’s outline.
outline-1
outline-2
outline-4
the default outline width is .
setting the outline color
use the  utilities to change the color of an element’s outline.
outline-blue-500
outline-cyan-500
outline-pink-500
control the opacity of an element’s outline color using the color opacity modifier.
outline-blue-500/50
by default, tailwind makes the entire  available as outline colors. you can  by editing  or  in your  file.
alternatively, you can customize just your outline colors by editing  or  in your  file.
setting the outline style
use the  utilities to change the style of an element’s outline.
outline
outline-dashed
outline-dotted
outline-double
button d
removing outlines
use  to hide the default browser outline on focused elements.
it is highly recommended to apply your own focus styling for accessibility when using this utility.
focus the input to see the expected behaviour
the  utility is implemented using a transparent outline under the hood to ensure elements are still visibly focused to  users.
setting the outline offset
use the  utilities to change the offset of an element’s outline.
outline-offset-0
outline-offset-2
outline-offset-4
adding a ring
use the  utilities to apply solid box-shadow of a specific thickness to an element. rings are a semi-transparent blue color by default, similar to the default focus ring style in many systems.
ring-2
ring
ring-4
ring utilities compose gracefully with regular  utilities and can be combined on the same element.
you can also control the color, opacity, and offset of rings using the , , and  utilities.
focus rings
the ring width utilities make it easy to use custom focus rings by adding  to the beginning of any  utility.
focus this button to see the ring appear
the  variant is enabled by default for the , , , and  utilities as well.
inset rings
use the  utility to force a ring to render on the inside of an element instead of the outside. this can be useful for elements at the edge of the screen where part of the ring wouldn’t be visible.
by default, tailwind includes a handful of general purpose  utilities. you can customize these values by editing  or  in your  file.
setting the ring color
use the  utilities to set the color of an .
create account
by default, tailwind makes the entire  available as ring colors. you can  by editing  or  in your  file.
alternatively, you can customize just your ring colors by editing  or  in your  file.
setting the ring offset width
use the  utilities to simulate an offset by adding solid white box-shadow and increasing the thickness of the accompanying outline ring to accommodate the offset.
ring-offset-0
ring-offset-2
ring-offset-4
changing the offset color
you can’t actually offset a box-shadow in css, so we have to fake it using a solid color shadow that matches the parent background color. we use white by default, but if you are adding a ring offset over a different background color, you should use the  utilities to match the parent background color:
ring-offset-slate-50
dark:ring-offset-slate-900
for more information, see the  documentation.
to customize which ring offset width utilities are generated, add your custom values under  key in the  section of your  file.
setting the ring offset color
use the  utilities to change the color of a ring offset. usually this is done to try and match the offset to the parent background color, since true box-shadow offsets aren’t actually possible in css.
control the opacity of an element’s ring offset color using the color opacity modifier.
by default, tailwind makes the entire  available as ring offset color colors. you can  by editing  or  in your  file.
alternatively, you can customize just your ring offset color colors by editing  or  in your  file.
adding an outer shadow
use the , , , , , or  utilities to apply different sized outer box shadows to an element.
shadow-md
shadow-lg
shadow-xl
shadow-2xl
adding an inner shadow
use the  utility to apply a subtle inset box shadow to an element. this can be useful for things like form controls or wells.
shadow-inner
removing the shadow
use  to remove an existing box shadow from an element. this is most commonly used to remove a shadow that was applied at a smaller breakpoint.
shadow-none
by default, tailwind provides six drop shadow utilities, one inner shadow utility, and a utility for removing existing shadows. you can customize these values by editing  or  in your  file.
if a  shadow is provided, it will be used for the non-suffixed  utility. any other keys will be used as suffixes, for example the key  will create a corresponding  utility.
setting the box shadow color
use the  utilities to change the color of an existing box shadow. by default colored shadows have an opacity of 100%, but you can adjust this using the opacity modifier.
shadow-cyan-500/50
shadow-blue-500/50
shadow-indigo-500/50
using shadows on colored backgrounds
when using shadows on a colored background, colored shadows can often look more natural than the default black-based shadows, which tend to appear gray and washed-out.
default shadow
colored shadow
preserving shadow color when changing the shadow size
note that if you change the  size at a different breakpoint, or on another state like hover, it will override any existing shadow colors.
to preserve the shadow color in these states, it needs to be respecified:
it’s necessary for tailwind to work this way, as otherwise there would be no way to switch back to the default shadow.
by default, tailwind makes the entire  available as box shadow colors. you can  by editing  or  in your  file.
alternatively, you can customize just your box shadow colors by editing  or  in your  file.
changing an element's opacity
control the opacity of an element using the  utilities.
opacity-100
opacity-75
opacity-50
opacity-25
by default, tailwind provides several opacity utilities for general purpose use. you can customize these values by editing  or  in your  file.
setting an element's blend mode
use the  utilities to control how an element’s content should be blended with the background.
setting the background blend mode
use the  utilities to control how an element’s background image(s) should blend with its background color.
blurring elements
use the  utilities to blur an element.
blur-none
blur-sm
blur-lg
blur-2xl
removing filters
to remove all of the filters on an element at once, use the  utility:
this can be useful when you want to remove filters conditionally, such as on hover or at a particular breakpoint.
changing element brightness
use the  utilities to control an element’s brightness.
brightness-50
brightness-100
brightness-125
brightness-200
changing element contrast
use the  utilities to control an element’s contrast.
contrast-50
contrast-100
contrast-125
contrast-200
adding a drop shadow
use the  utilities to add a drop shadow to an element.
drop-shadow-md
drop-shadow-lg
drop-shadow-xl
drop-shadow-2xl
making an element grayscale
use the  and  utilities to control whether an element should be rendered as grayscale or in full color.
grayscale-0
grayscale
rotating an element's hue
use the  utilities to rotate the hue of an element.
hue-rotate-15
hue-rotate-90
hue-rotate-180
-hue-rotate-60
to use a negative hue-rotate value, prefix the class name with a dash to convert it to a negative value.
inverting an element's color
use the  and  utilities to control whether an element should be rendered with inverted colors or normally.
invert-0
invert
changing element saturation
use the  utilities to control an element’s saturation.
saturate-50
saturate-100
saturate-150
saturate-200
adding sepia filters
use the  and  utilities to control whether an element should be rendered as sepia or in full color.
sepia-0
sepia
blurring behind an element
use the  utilities to control an element’s backdrop blur.
backdrop-blur-sm
backdrop-blur-md
backdrop-blur-xl
removing backdrop filters
to remove all of the backdrop filters on an element at once, use the  utility:
this can be useful when you want to remove backdrop filters conditionally, such as on hover or at a particular breakpoint.
controlling backdrop brightness
use the  utilities to control an element’s backdrop brightness.
backdrop-brightness-50
backdrop-brightness-125
backdrop-brightness-200
controlling backdrop contrast
use the  utilities to control an element’s backdrop contrast.
backdrop-contrast-50
backdrop-contrast-125
backdrop-contrast-200
use the  and  utilities to control whether an element’s backdrop should be rendered as grayscale or in full color.
backdrop-grayscale-0
backdrop-grayscale
rotating the backdrop's hue
use the  utilities to rotate the hue of an element’s backdrop.
backdrop-hue-rotate-90
backdrop-hue-rotate-180
-backdrop-hue-rotate-60
to use a negative backdrop hue rotate value, prefix the class name with a dash to convert it to a negative value.
inverting an element's backdrop
use the  and  utilities to control whether an element should be rendered with inverted backdrop colors or normally.
backdrop-invert-0
backdrop-invert
controlling opacity of backdrop filters
use the  utilities to control the opacity of other backdrop filters applied to an element.
backdrop-opacity-10
backdrop-opacity-60
backdrop-opacity-95
changing backdrop saturation
use the  utilities to control an element’s backdrop saturation.
backdrop-saturate-50
backdrop-saturate-125
backdrop-saturate-200
adding backdrop sepia filters
use the  and  utilities to control whether an element’s backdrop should be rendered as sepia or in full color.
backdrop-sepia-0
backdrop-sepia
collapse
use  to combine adjacent cell borders into a single border when possible. note that this includes collapsing borders on the top-level  tag.
separate
use  to force each cell to display its own separate borders.
setting the border spacing
use  to control the space between the borders of table cells with .
by default, tailwind’s border spacing utilities use the . you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the border spacing scale by editing  or  in your  file.
use  to allow the table to automatically size columns to fit the contents of the cell.
use  to allow the table to ignore the content and use fixed widths for columns. the width of the first row will set the column widths for the whole table.
you can manually set the widths for some columns and the rest of the available width will be divided evenly amongst the columns without explicit width.
controlling transitioned properties
use the  utilities to specify which properties should transition when they change.
hover the button to see the expected behaviour
prefers-reduced-motion
for situations where the user has specified that they prefer reduced motion, you can conditionally apply animations and transitions using the  and  variants:
by default, tailwind provides transition-property utilities for seven common property combinations. you can customize these values by editing  or  in your  file.
changing transition duration
use the  utilities to control an element’s transition-duration.
hover each button to see the expected behaviour
duration-150
duration-300
duration-700
by default, tailwind provides eight general purpose transition-duration utilities. you can customize these values by editing  or  in your  file.
controlling the easing curve
use the  utilities to control an element’s easing curve.
ease-in
ease-out
ease-in-out
by default, tailwind provides four general purpose transition-timing-function utilities. you can customize these values by editing  or  in your  file.
delaying transitions
use the  utilities to control an element’s transition-delay.
delay-150
delay-300
delay-700
by default, tailwind provides eight general purpose transition-delay utilities. you can customize these values by editing  or  in your  file.
spin
add the  utility to add a linear spin animation to elements like loading indicators.
ping
add the  utility to make an element scale and fade like a radar ping or ripple of water — useful for things like notification badges.
transactions
pulse
add the  utility to make an element gently fade in and out — useful for things like skeleton loaders.
bounce
add the  utility to make an element bounce up and down — useful for things like “scroll down” indicators.
animations by their very nature tend to be highly project-specific. the animations we include by default are best thought of as helpful examples, and you’re encouraged to customize your animations to better suit your needs.
by default, tailwind provides utilities for four different example animations, as well as the  utility. you can customize these values by editing  or  in your  file.
to add new animation , use the  section of your theme configuration:
you can then reference these keyframes by name in the  section of your theme configuration:
scaling an element
use the , , and  utilities to scale an element.
scale-75
scale-100
scale-125
to use a negative scale value, prefix the class name with a dash to convert it to a negative value.
removing transforms
to remove all of the transforms on an element at once, use the  utility:
this can be useful when you want to remove transforms conditionally, such as on hover or at a particular breakpoint.
hardware acceleration
if your transition performs better when rendered by the gpu instead of the cpu, you can force hardware acceleration by adding the  utility:
use  to force things back to the cpu if you need to undo this conditionally.
rotating an element
use the  utilities to rotate an element.
rotate-0
rotate-45
rotate-90
rotate-180
to use a negative rotate value, prefix the class name with a dash to convert it to a negative value.
translating an element
use the  and  utilities to translate an element.
translate-y-6
-translate-y-6
translate-x-6
to use a negative translate value, prefix the class name with a dash to convert it to a negative value.
by default, tailwind provides fixed value  utilities that match the , as well as 50% and 100% variations for translating relative to the element’s size.  you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the translate scale by editing  or  in your  file.
skewing an element
use the  and  utilities to skew an element.
skew-y-0
skew-y-3
skew-y-6
skew-y-12
to use a negative skew value, prefix the class name with a dash to convert it to a negative value.
changing the transform origin
specify an element’s transform origin using the  utilities.
origin-center
origin-top-left
origin-bottom
by default, tailwind provides  utilities for all of the built-in browser keyword options. you can customize these values by editing  or  in your  file.
setting the accent color
use the  utilities to change the accent color of an element. this is helpful for styling elements like checkboxes and radio groups by overriding the browser’s default color.
browser default
customized
while it’s possible to control the opacity of the accent color using the color opacity modifier, the  alpha value is only supported in firefox at this time (last tested november 2021).
note, while it’s possible to set an accent color using the  and  modifiers, the resulting color will be slightly different than what you set, as browsers automatically adjust the brightness of the accent color for these two states.
by default, tailwind makes the entire  available as accent colors. you can  by editing  or  in your  file.
alternatively, you can customize just your accent colors by editing  or  in your  file.
removing default element appearance
use  to reset any browser specific styling on an element. this utility is often used when creating .
maybe
default browser styles applied
default styles removed
setting the cursor style
use the  to control which cursor is displayed when hovering over an element.
hover over each button to see the cursor change
submit
saving...
confirm
tailwind lets you conditionally apply utility classes in different states using variant modifiers. for example, use  to only apply the  utility on focus.
by default, tailwind includes  utilities for many built in options. you can customize these values by editing  or  in your  file.
setting the caret color
use the  utilities to change the color of the text input cursor.
focus the textarea to see the new caret color
by default, tailwind makes the entire  available as caret colors. you can  by editing  or  in your  file.
alternatively, you can customize just your caret colors by editing  or  in your  file.
controlling pointer event behavior
use  to revert to the default browser behavior for pointer events (like  and ).
use  to make an element ignore pointer events. the pointer events will still trigger on child elements and pass-through to elements that are “beneath” the target.
try clicking the search icons to see the expected behaviour
pointer-events-auto
pointer-events-none
resizing in all directions
use  to make an element horizontally and vertically resizable.
drag the textarea handle in the demo to see the expected behaviour
resizing vertically
use  to make an element vertically resizable.
resizing horizontally
use  to make an element horizontally resizable.
preventing resizing
use  to prevent an element from being resizable.
notice that the textarea handle is gone
adding smooth scrolling
use the  utilities to enable smooth scrolling within an element.
setting the scroll margin
use the  utilities to set the scroll offset around items within a snap container.
scroll in the grid of images to see the expected behaviour
to use a negative scroll margin value, prefix the class name with a dash to convert it to a negative value.
by default, tailwind’s scroll margin scale uses the . you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the scroll margin scale by editing  or  in your  file.
setting the scroll padding
use the  utilities to set the scroll offset of an element within a snap container.
by default, tailwind’s scroll padding scale uses the . you can customize your spacing scale by editing  or  in your  file.
alternatively, you can customize just the scroll padding scale by editing  or  in your  file.
snapping to the center
use the  utility to snap an element to its center when being scrolled inside a snap container.
snap point
snapping to the start
use the  utility to snap an element to its start when being scrolled inside a snap container.
snapping to the end
use the  utility to snap an element to its end when being scrolled inside a snap container.
forcing snap position stops
use the  utility together with the  utility to force a snap container to always stop on an element before the user can continue scrolling to the next item.
skipping snap position stops
use the  utility to allow a snap container to skip past possible scroll snap positions.
horizontal scroll snapping
use the  utility to enable horizontal scroll snapping within an element.
for scroll snapping to work, you need to also set the  on the children within the element.
mandatory scroll snapping
use the  utility to force a snap container to always come to rest on a snap point.
proximity scroll snapping
use the  utility to make a snap container come to rest on snap points that are close in proximity. this is the browser default.
setting the touch action
use the  utilities to control how an element can be scrolled (panned) and zoomed (pinched) on touchscreens.
try panning these images on a touchscreen
touch-auto
touch-none
touch-pan-x
touch-pan-y
disabling text selection
use  to prevent selecting text in an element and its children.
try selecting the text to see the expected behaviour
allowing text selection
use  to allow selecting text in an element and its children.
selecting all text in one click
use  to automatically select all the text in an element when a user clicks.
using auto select behaviour
use  to use the default browser behavior for selecting text. useful for undoing other classes like  at different breakpoints.
optimizing with will change
use ,  and  to optimize an element that’s expected to change in the near future by instructing the browser to prepare the necessary animation before it actually begins.
it’s recommended that you apply these utilities just before an element changes, and then remove it shortly after it finishes using .
the will-change property is intended to be used as a last resort when dealing with known performance problems. avoid using these utilities too much, or simply in anticipation of performance issues, as it could actually cause the page to be less performant.
setting the fill color
use the  utilities to change the fill color of an svg.
this can be useful for styling icon sets like .
by default, tailwind makes the entire  available as fill colors. you can  by editing  or  in your  file.
alternatively, you can customize just your fill colors by editing  or  in your  file.
setting the stroke color
use the  utilities to change the stroke color of an svg.
by default, tailwind makes the entire  available as stroke colors. you can  by editing  or  in your  file.
alternatively, you can customize just your stroke colors by editing  or  in your  file.
setting the stroke width
use the  utilities to set the stroke width of an svg.
screen-reader-only elements
use  to hide an element visually without hiding it from screen readers:
undoing screen-reader-only elements
use  to undo , making an element visible to sighted users as well as screen readers. this can be useful when you want to visually hide something on small screens but show it on larger screens for example:
the official tailwind css typography plugin provides a set of  classes you can use to add beautiful typographic defaults to any vanilla html you don’t control, like html rendered from markdown, or pulled from a cms.
to see what it looks like in action, check out our  on tailwind play.
installation
install the plugin from npm:
then add the plugin to your  file:
now you can use the  classes to add sensible typography styles to any vanilla html:
choosing a gray scale
this plugin includes a modifier class for each of the five gray scales tailwind includes by default so you can easily style your content to match the grays you’re using in your project.
here are the classes that are generated using a totally default tailwind css v2.0 build:
modifier classes are designed to be used with the  and must be used in conjunction with the base  class.
always include the `prose` class when adding a gray scale modifier
to learn about creating your own color themes, read the  documentation.
applying a type scale
size modifiers allow you to adjust the overall size of your typography for different contexts.
five different typography sizes are included out of the box:
these can be used in combination with tailwind’s  to change the overall font size of a piece of content at different viewport sizes:
everything about the provided size modifiers has been hand-tuned by professional designers to look as beautiful as possible, including the relationships between font sizes, heading spacing, code block padding, and more.
size modifiers are designed to be used with the  and must be used in conjunction with the base  class.
always include the `prose` class when adding a size modifier
to learn about customizing the included type scales, read the documentation on .
adapting to dark mode
each default color theme includes a hand-designed dark mode version that you can trigger by adding the  class:
element modifiers
use element modifiers to customize the style of individual elements in your content directly in your html:
this makes it easy to do things like style links to match your brand, add a border radius to images, and tons more.
here’s a complete list of available element modifiers:
when stacking these modifiers with other modifiers like , you most likely want the other modifier to come first:
read the tailwind css documentation on  to learn more.
overriding max-width
each size modifier comes with a baked in  designed to keep the content as readable as possible. this isn’t always what you want though, and sometimes you’ll want the content to just fill the width of its container.
in those cases, all you need to do is add  to your content to override the embedded max-width:
undoing typography styles
if you have a block of markup embedded in some content that shouldn’t inherit the  styles, use the  class to sandbox it:
note that you can’t nest new  instances within a  block at this time.
adding custom color themes
you can create your own color theme by adding a new key in the  section of your  file and providing your colors under the  key:
see our internal  for some more examples.
changing the default class name
if you need to use a class name other than  for any reason, you can do so using the  option when registering the plugin:
now every instance of  in the default class names will be replaced by your custom class name:
customizing the css
if you want to customize the raw css generated by this plugin, add your overrides under the  key in the  section of your  file:
like with all theme customizations in tailwind, you can also define the  key as a function if you need access to the  helper:
customizations should be applied to a specific modifier like  or , and must be added under the  property. customizations are authored in the same  used to write tailwind plugins.
see  for this plugin for more in-depth examples of configuring each modifier.
about this documentation
welcome to the official api reference documentation for node.js!
node.js is a javascript runtime built on the .
contributing
report errors in this documentation in . see
for directions on how to submit pull requests.
stability index
throughout the documentation are indications of a section's stability. some apis
are so proven and so relied upon that they are unlikely to ever change at all.
others are brand new and experimental, or known to be hazardous.
the stability indices are as follows:
stability: 0 - deprecated. the feature may emit warnings. backward
compatibility is not guaranteed.
stability: 1 - experimental. the feature is not subject to
rules. non-backward compatible changes or removal may
occur in any future release. use of the feature is not recommended in
production environments.
stability: 2 - stable. compatibility with the npm ecosystem is a high
priority.
stability: 3 - legacy. although this feature is unlikely to be removed and is
still covered by semantic versioning guarantees, it is no longer actively
maintained, and other alternatives are available.
features are marked as legacy rather than being deprecated if their use does no
harm, and they are widely relied upon within the npm ecosystem. bugs found in
legacy features are unlikely to be fixed.
use caution when making use of experimental features, particularly within
modules. users may not be aware that experimental features are being used.
bugs or behavior changes may surprise users when experimental api
modifications occur. to avoid surprises, use of an experimental feature may need
a command-line flag. experimental features may also emit a .
stability overview
json output
added in: v0.6.12
every  document has a corresponding  document. this is for ides
and other utilities that consume the documentation.
system calls and man pages
node.js functions which wrap a system call will document that. the docs link
to the corresponding man pages which describe how the system call works.
most unix system calls have windows analogues. still, behavior differences may
be unavoidable.
usage and example
please see the  document for more information.
an example of a  written with node.js which responds with
commands in this document start with  or  to replicate how they would
appear in a user's terminal. do not include the  and  characters. they are
there to show the start of each command.
lines that don't start with  or  character show the output of the previous
command.
first, make sure to have downloaded and installed node.js. see
for further install information.
now, create an empty project folder called , then navigate into it.
linux and mac:
windows cmd:
windows powershell:
next, create a new source file in the
folder and call it .
open  in any preferred text editor and
paste in the following content:
save the file, go back to the terminal window, and enter the following command:
output like this should appear in the terminal:
now, open any preferred web browser and visit .
if the browser displays the string , that indicates
the server is working.
assert
- stable
source code:
the  module provides a set of assertion functions for verifying
invariants.
strict assertion mode
in strict assertion mode, non-strict methods behave like their corresponding
strict methods. for example,  will behave like
in strict assertion mode, error messages for objects display a diff. in legacy
assertion mode, error messages for objects display the objects, often truncated.
to use strict assertion mode:
example error diff:
to deactivate the colors, use the  or
environment variables. this will also deactivate the colors in the repl. for
more on color support in terminal environments, read the tty
documentation.
legacy assertion mode
legacy assertion mode uses the  in:
to use legacy assertion mode:
legacy assertion mode may have surprising results, especially when using
class: assert.assertionerror
extends:
indicates the failure of an assertion. all errors thrown by the
module will be instances of the  class.
added in: v0.1.21
if provided, the error message is set to this value.
the  property on the error instance.
if provided, the generated stack trace omits
frames before this function.
a subclass of  that indicates the failure of an assertion.
all instances contain the built-in  properties ( and )
and:
set to the  argument for methods such as
set to the  value for methods such as
indicates if the message was auto-generated
() or not.
value is always  to show that the error is an
assertion error.
set to the passed in operator value.
class:
added in: v14.2.0, v12.19.0
- experimental
this feature is currently experimental and behavior might still change.
creates a new  object which can be used to track if functions
were called a specific number of times. the  must be called
for the verification to take place. the usual pattern would be to call it in a
handler.
default: a no-op function.
default: .
returns:  that wraps .
the wrapper function is expected to be called exactly  times. if the
function has not been called exactly  times when
is called, then  will throw an
error.
added in: v18.8.0
returns:  with all the calls to a tracked function.
object
the arguments passed to the tracked function
returns:  of objects containing information about the wrapper functions
returned by .
the actual number of times the function was called.
the number of times the function was expected to be
called.
the name of the function that is wrapped.
a stack trace of the function.
the arrays contains information about the expected and actual number of calls of
the functions that have not been called the expected number of times.
a tracked function to reset.
reset calls of the call tracker.
if a tracked function is passed as an argument, the calls will be reset for it.
if no arguments are passed, all tracked functions will be reset
iterates through the list of functions passed to
and will throw an error for functions that
have not been called the expected number of times.
added in: v0.5.9
the input that is checked for being truthy.
an alias of .
- legacy: use  instead.
tests for deep equality between the  and  parameters. consider
using  instead.  can have
surprising results.
deep equality means that the enumerable "own" properties of child objects
are also recursively evaluated by the following rules.
comparison details
primitive values are compared with the ,
with the exception of . it is treated as being identical in case
both sides are .
of objects should be the same.
only  are considered.
names and messages are always compared, even if these are not
enumerable properties.
are compared both as objects and unwrapped values.
properties are compared unordered.
keys and  items are compared unordered.
recursion stops when both sides differ or both sides encounter a circular
reference.
implementation does not test the  of
objects.
properties are not compared.
and  comparison does not rely on their values.
lastindex, flags, and source are always compared, even if these
are not enumerable properties.
the following example does not throw an  because the
primitives are compared using the .
"deep" equality means that the enumerable "own" properties of child objects
are evaluated also:
if the values are not equal, an  is thrown with a
property set equal to the value of the  parameter. if the
parameter is undefined, a default error message is assigned. if the
parameter is an instance of an  then it will be thrown instead of the
tests for deep equality between the  and  parameters.
are recursively evaluated also by the following rules.
primitive values are compared using .
of objects are compared using
the .
enumerable own  properties are compared as well.
and  comparison does not rely on their values. see
below for further details.
expects the  input not to match the regular expression.
if the values do match, or if the  argument is of another type than
, an  is thrown with a  property set equal
to the value of the  parameter. if the  parameter is
undefined, a default error message is assigned. if the  parameter is an
instance of an  then it will be thrown instead of the
added in: v10.0.0
awaits the  promise or, if  is a function, immediately
calls the function and awaits the returned promise to complete. it will then
check that the promise is not rejected.
if  is a function and it throws an error synchronously,
will return a rejected  with that error. if
the function does not return a promise,  will return a
rejected  with an  error. in both cases
the error handler is skipped.
using  is actually not useful because there is little
benefit in catching a rejection and then rejecting it again. instead, consider
adding a comment next to the specific code path that should not reject and keep
error messages as expressive as possible.
if specified,  can be a , , or a validation
function. see  for more details.
besides the async nature to await the completion behaves identically to
asserts that the function  does not throw an error.
using  is actually not useful because there
is no benefit in catching an error and then rethrowing it. instead, consider
adding a comment next to the specific code path that should not throw and keep
when  is called, it will immediately call the
function.
if an error is thrown and it is the same type as that specified by the
parameter, then an  is thrown. if the error is of a
different type, or if the  parameter is undefined, the error is
propagated back to the caller.
the following, for instance, will throw the  because there is no
matching error type in the assertion:
however, the following will result in an  with the message
'got unwanted exception...':
if an  is thrown and a value is provided for the
parameter, the value of  will be appended to the
message:
tests shallow, coercive equality between the  and  parameters
using the .  is specially handled
and treated as being identical if both sides are .
|  default:
throws an  with the provided error message or a default
error message. if the  parameter is an instance of an  then
it will be thrown instead of the .
using  with more than two arguments is possible but deprecated.
see below for further details.
- deprecated: use  or other assert
functions instead.
default:
if  is falsy, the error message is set as the values of  and
separated by the provided . if just the two  and
arguments are provided,  will default to . if
is provided as third argument it will be used as the error message and
the other arguments will be stored as properties on the thrown object. if
is provided, all stack frames above that function will be
removed from stacktrace (see ). if no arguments are
given, the default message  will be used.
in the last three cases , , and  have no
influence on the error message.
example use of  for truncating the exception's stacktrace:
throws  if  is not  or . this is useful when
testing the  argument in callbacks. the stack trace contains all frames
from the error passed to  including the potential new frames for
itself.
expects the  input to match the regular expression.
if the values do not match, or if the  argument is of another type than
tests for any deep inequality. opposite of .
if the values are deeply equal, an  is thrown with a
parameter is an instance of an  then it will be thrown
instead of the .
tests for deep strict inequality. opposite of .
if the values are deeply and strictly equal, an  is thrown
with a  property set equal to the value of the  parameter. if
the  parameter is undefined, a default error message is assigned. if
the  parameter is an instance of an  then it will be thrown
tests shallow, coercive inequality with the .  is
specially handled and treated as being identical if both sides are .
if the values are equal, an  is thrown with a
tests strict inequality between the  and  parameters as
determined by .
if the values are strictly equal, an  is thrown with a
tests if  is truthy. it is equivalent to
if  is not truthy, an  is thrown with a
parameter is , a default error message is assigned. if the
if no arguments are passed in at all  will be set to the string:
be aware that in the  the error message will be different to the one
thrown in a file! see below for further details.
|  |  |
check that the promise is rejected.
will return a rejected  with that error. if the
function does not return a promise,  will return a rejected
with an  error. in both cases the error
handler is skipped.
if specified,  can be a , , a validation function,
an object where each property will be tested for, or an instance of error where
each property will be tested for including the non-enumerable  and
properties.
if specified,  will be the message provided by the
if the  fails to reject.
cannot be a string. if a string is provided as the second
argument, then  is assumed to be omitted and the string will be used for
instead. this can lead to easy-to-miss mistakes. please read the
example in  carefully if using a string as the second
argument gets considered.
the value to snapshot.
the name of the snapshot.
returns:
reads the  snapshot from a file and compares  to the snapshot.
is serialized with . if the value is not strictly
equal to the snapshot,  returns a rejected  with an
the snapshot filename uses the same basename as the application's main
entrypoint with a  extension. if the snapshot file does not exist,
it is created. the  command line flag can be used
to force the update of an existing snapshot.
tests strict equality between the  and  parameters as
if the values are not strictly equal, an  is thrown with a
expects the function  to throw an error.
a validation object where each property will be tested for strict deep equality,
or an instance of error where each property will be tested for strict deep
equality including the non-enumerable  and  properties. when
using an object, it is also possible to use a regular expression, when
validating against a string property. see below for examples.
if specified,  will be appended to the message provided by the
if the  call fails to throw or in case the error validation
fails.
custom validation object/error instance:
validate instanceof using constructor:
validate error message using :
using a regular expression runs  on the error object, and will
therefore also include the error name.
custom error validation:
the function must return  to indicate all internal validations passed.
it will otherwise fail with an .
instead. this can lead to easy-to-miss mistakes. using the same
message as the thrown error message is going to result in an
error. please read the example below carefully if using
a string as the second argument gets considered:
due to the confusing error-prone notation, avoid a string as the second
argument.
asynchronous context tracking
introduction
these classes are used to associate state and propagate it throughout
callbacks and promise chains.
they allow storing data throughout the lifetime of a web request
or any other asynchronous duration. it is similar to thread-local storage
in other languages.
the  and  classes are part of the
module:
this class creates stores that stay coherent through asynchronous operations.
while you can create your own implementation on top of the
module,  should be preferred as it is a performant and memory
safe implementation that involves significant optimizations that are non-obvious
to implement.
the following example uses  to build a simple logger
that assigns ids to incoming http requests and includes them in messages
logged within each request.
each instance of  maintains an independent storage context.
multiple instances can safely exist simultaneously without risk of interfering
with each other's data.
added in: v13.10.0, v12.17.0
creates a new instance of . store is only provided within a
call or after an  call.
disables the instance of . all subsequent calls
to  will return  until
or  is called again.
when calling , all current contexts linked to the
instance will be exited.
calling  is required before the
can be garbage collected. this does not apply to stores
provided by the , as those objects are garbage collected
along with the corresponding async resources.
use this method when the  is not in use anymore
in the current process.
returns the current store.
if called outside of an asynchronous context initialized by
calling  or , it
returns .
added in: v13.11.0, v12.17.0
transitions into the context for the remainder of the current
synchronous execution and then persists the store through any following
asynchronous calls.
this transition will continue for the entire synchronous execution.
this means that if, for example, the context is entered within an event
handler subsequent event handlers will also run within that context unless
specifically bound to another context with an . that is why
should be preferred over  unless there are strong reasons
to use the latter method.
runs a function synchronously within a context and returns its
return value. the store is not accessible outside of the callback function.
the store is accessible to any asynchronous operations created within the
callback.
the optional  are passed to the callback function.
if the callback function throws an error, the error is thrown by  too.
the stacktrace is not impacted by this call and the context is exited.
runs a function synchronously outside of a context and returns its
return value. the store is not accessible within the callback function or
the asynchronous operations created within the callback. any
call done within the callback function will always return .
the stacktrace is not impacted by this call and the context is re-entered.
usage with
if, within an async function, only one  call is to run within a context,
the following pattern should be used:
in this example, the store is only available in the callback function and the
functions called by . outside of , calling  will return
troubleshooting: context loss
in most cases,  works without issues. in rare situations, the
current store is lost in one of the asynchronous operations.
if your code is callback-based, it is enough to promisify it with
so it starts working with native promises.
if you need to use a callback-based api or your code assumes
a custom thenable implementation, use the  class
to associate the asynchronous operation with the correct execution context.
find the function call responsible for the context loss by logging the content
of  after the calls you suspect are responsible
for the loss. when the code logs , the last callback called is
probably responsible for the context loss.
the class  is designed to be extended by the embedder's async
resources. using this, users can easily trigger the lifetime events of their
own resources.
the  hook will trigger when an  is instantiated.
the following is an overview of the  api.
the type of async event.
the id of the execution context that created this
async event. default: .
if set to , disables
when the object is garbage collected. this usually does not need to be set
(even if  is called manually), unless the resource's
is retrieved and the sensitive api's  is called with it.
when set to , the  call on garbage collection
will only take place if there is at least one active  hook.
example usage:
static method:
the function to bind to the current execution context.
an optional name to associate with the underlying
binds the given function to the current execution context.
the returned function will have an  property referencing
the  to which the function is bound.
the function to bind to the current .
binds the given function to execute to this 's scope.
added in: v9.6.0
the function to call in the execution context of this async
resource.
the receiver to be used for the function call.
optional arguments to pass to the function.
call the provided function with the provided arguments in the execution context
of the async resource. this will establish the context, trigger the asynchooks
before callbacks, call the function, trigger the asynchooks after callbacks, and
then restore the original execution context.
returns:  a reference to .
call all  hooks. this should only ever be called once. an error will
be thrown if it is called more than once. this must be manually called. if
the resource is left to be collected by the gc then the  hooks will
never be called.
returns:  the unique  assigned to the resource.
returns:  the same  that is passed to the
constructor.
using  for a  thread pool
the following example shows how to use the  class to properly
provide async tracking for a  pool. other resource pools, such as
database connection pools, can follow a similar model.
assuming that the task is adding two numbers, using a file named
with the following content:
a worker pool around it could use the following structure:
without the explicit tracking added by the  objects,
it would appear that the callbacks are associated with the individual
objects. however, the creation of the s is not associated with the
creation of the tasks and does not provide information about when tasks
were scheduled.
this pool could be used as follows:
integrating  with
event listeners triggered by an  may be run in a different
execution context than the one that was active when  was
associate an event listener with the correct execution context. the same
approach can be applied to a  or a similar event-driven class.
async hooks
the  module provides an api to track asynchronous resources.
it can be accessed using:
terminology
an asynchronous resource represents an object with an associated callback.
this callback may be called multiple times, such as the
event in , or just a single time like in .
a resource can also be closed before the callback is called.  does
not explicitly distinguish between these different cases but will represent them
as the abstract concept that is a resource.
if s are used, each thread has an independent
interface, and each thread will use a new set of async ids.
following is a simple overview of the public api.
added in: v8.1.0
the  to register
returns:  instance used for disabling and enabling hooks
registers functions to be called for different lifetime events of each async
operation.
the callbacks /// are called for the
respective asynchronous event during a resource's lifetime.
all callbacks are optional. for example, if only resource cleanup needs to
be tracked, then only the  callback needs to be passed. the
specifics of all functions that can be passed to  is in the
section.
the callbacks will be inherited via the prototype chain:
because promises are asynchronous resources whose lifecycle is tracked
via the async hooks mechanism, the , , , and
callbacks must not be async functions that return promises.
if any  callbacks throw, the application will print the stack trace
and exit. the exit path does follow that of an uncaught exception, but
all  listeners are removed, thus forcing the process to
exit. the  callbacks will still be called unless the application is run
with , in which case a stack trace will be
printed and the application exits, leaving a core file.
the reason for this error handling behavior is that these callbacks are running
at potentially volatile points in an object's lifetime, for example during
class construction and destruction. because of this, it is deemed necessary to
bring down the process quickly in order to prevent an unintentional abort in the
future. this is subject to change in the future if a comprehensive analysis is
performed to ensure an exception can follow the normal control flow without
unintentional side effects.
printing in  callbacks
because printing to the console is an asynchronous operation,
will cause  callbacks to be called. using  or
similar asynchronous operations inside an  callback function will
cause an infinite recursion. an easy solution to this when debugging is to use a
synchronous logging operation such as .
this will print to the file and will not invoke  recursively because
it is synchronous.
if an asynchronous operation is needed for logging, it is possible to keep
track of what caused the asynchronous operation using the information
provided by  itself. the logging should then be skipped when
it was the logging itself that caused the  callback to be called. by
doing this, the otherwise infinite recursion is broken.
the class  exposes an interface for tracking lifetime events
of asynchronous operations.
enable the callbacks for a given  instance. if no callbacks are
provided, enabling is a no-op.
the  instance is disabled by default. if the  instance
should be enabled immediately after creation, the following pattern can be used.
disable the callbacks for a given  instance from the global pool of
callbacks to be executed. once a hook has been disabled it will not
be called again until enabled.
for api consistency  also returns the  instance.
hook callbacks
key events in the lifetime of asynchronous events have been categorized into
four areas: instantiation, before/after the callback is called, and when the
instance is destroyed.
a unique id for the async resource.
the type of the async resource.
the unique id of the async resource in whose
execution context this async resource was created.
reference to the resource representing the async
operation, needs to be released during destroy.
called when a class is constructed that has the possibility to emit an
asynchronous event. this does not mean the instance must call
/ before  is called, only that the possibility
exists.
this behavior can be observed by doing something like opening a resource then
closing it before the resource can be used. the following snippet demonstrates
this.
every new resource is assigned an id that is unique within the scope of the
current node.js instance.
the  is a string identifying the type of resource that caused
to be called. generally, it will correspond to the name of the
resource's constructor.
valid values are:
these values can change in any node.js release. furthermore users of
likely provide other values.
there is also the  resource type, which is used to track
instances and asynchronous work scheduled by them.
users are able to define their own  when using the public embedder api.
it is possible to have type name collisions. embedders are encouraged to use
unique prefixes, such as the npm package name, to prevent collisions when
listening to the hooks.
is the  of the resource that caused (or "triggered")
the new resource to initialize and that caused  to call. this is different
from  that only shows when a resource was
created, while  shows why a resource was created.
the following is a simple demonstration of :
output when hitting the server with :
the  is the server which receives the connections.
the  is the new connection from the client. when a new
connection is made, the  instance is immediately constructed. this
happens outside of any javascript stack. (an  of  means
that it is being executed from c++ with no javascript stack above it.) with only
that information, it would be impossible to link resources together in
terms of what caused them to be created, so  is given the task
of propagating what resource is responsible for the new resource's existence.
is an object that represents the actual async resource that has
been initialized. this can contain useful information that can vary based on
the value of . for instance, for the  resource type,
provides the host name used when looking up the ip address for the
host in . the api for accessing this information is
not supported, but using the embedder api, users can provide
and document their own resource objects. for example, such a resource object
could contain the sql query being executed.
in some cases the resource object is reused for performance reasons, it is
thus not safe to use it as a key in a  or add properties to it.
asynchronous context example
the following is an example with additional information about the calls to
between the  and  calls, specifically what the
callback to  will look like. the output formatting is slightly more
elaborate to make calling context easier to see.
output from only starting the server:
as illustrated in the example,  and  each specify
the value of the current execution context; which is delineated by calls to
and .
only using  to graph resource allocation results in the following:
the  is not part of this graph, even though it was the reason for
being called. this is because binding to a port without a host
name is a synchronous operation, but to maintain a completely asynchronous
api the user's callback is placed in a . which is why
is present in the output and is a 'parent' for
the graph only shows when a resource was created, not why, so to track
the why use . which can be represented with the following
graph:
when an asynchronous operation is initiated (such as a tcp server receiving a
new connection) or completes (such as writing data to disk) a callback is
called to notify the user. the  callback is called just before said
callback is executed.  is the unique identifier assigned to the
resource about to execute the callback.
the  callback will be called 0 to n times. the  callback
will typically be called 0 times if the asynchronous operation was cancelled
or, for example, if no connections are received by a tcp server. persistent
asynchronous resources like a tcp server will typically call the
callback multiple times, while other operations like  will call
it only once.
called immediately after the callback specified in  is completed.
if an uncaught exception occurs during execution of the callback, then
will run after the  event is emitted or a 's
handler runs.
called after the resource corresponding to  is destroyed. it is also
called asynchronously from the embedder api .
some resources depend on garbage collection for cleanup, so if a reference is
made to the  object passed to  it is possible that
will never be called, causing a memory leak in the application. if the resource
does not depend on garbage collection, then this will not be an issue.
added in: v8.6.0
called when the  function passed to the  constructor is
invoked (either directly or through other means of resolving a promise).
does not do any observable synchronous work.
the  is not necessarily fulfilled or rejected at this point if the
was resolved by assuming the state of another .
calls the following callbacks:
added in: v13.9.0, v12.17.0
returns:  the resource representing the current execution.
useful to store data within the resource.
resource objects returned by  are most often internal
node.js handle objects with undocumented apis. using any functions or properties
on the object is likely to crash your application and should be avoided.
using  in the top-level execution context will
return an empty object as there is no handle or request object to use,
but having an object representing the top-level can be helpful.
this can be used to implement continuation local storage without the
use of a tracking  to store the metadata:
returns:  the  of the current execution context. useful to
track when something calls.
the id returned from  is related to execution timing, not
causality (which is covered by ):
promise contexts may not get precise  by default.
see the section on .
returns:  the id of the resource responsible for calling the callback
that is currently being executed.
promise contexts may not get valid s by default. see
the section on .
added in: v17.2.0, v16.14.0
returns: a map of provider types to the corresponding numeric id.
this map contains all the event types that might be emitted by the  event.
this feature suppresses the deprecated usage of .
see:
promise execution tracking
by default, promise executions are not assigned s due to the relatively
expensive nature of the  provided by
v8. this means that programs using promises or / will not get
correct execution and trigger ids for promise callback contexts by default.
observe that the  callback claims to have executed in the context of the
outer scope even though there was an asynchronous hop involved. also,
the  value is , which means that we are missing context about
the resource that caused (triggered) the  callback to be executed.
installing async hooks via  enables promise execution
tracking:
in this example, adding any actual hook function enabled the tracking of
promises. there are two promises in the example above; the promise created by
and the promise returned by the call to . in the
example above, the first promise got the   and the latter got
. during the execution of the  callback, we are executing
in the context of promise with  . this promise was triggered by
async resource .
another subtlety with promises is that  and  callbacks are run
only on chained promises. that means promises not created by /
will not have the  and  callbacks fired on them. for more details
see the details of the v8  api.
javascript embedder api
library developers that handle their own asynchronous resources performing tasks
like i/o, connection pooling, or managing callback queues may use the
javascript api so that all the appropriate callbacks are called.
the documentation for this class has moved .
buffer
objects are used to represent a fixed-length sequence of bytes. many
node.js apis support s.
the  class is a subclass of javascript's  class and
extends it with methods that cover additional use cases. node.js apis accept
plain s wherever s are supported as well.
while the  class is available within the global scope, it is still
recommended to explicitly reference it via an import or require statement.
buffers and character encodings
when converting between s and strings, a character encoding may be
specified. if no character encoding is specified, utf-8 will be used as the
node.js buffers accept all case variations of encoding strings that they
receive. for example, utf-8 can be specified as , , or .
the character encodings currently supported by node.js are the following:
(alias: ): multi-byte encoded unicode characters. many web
pages and other document formats use . this is the default character
encoding. when decoding a  into a string that does not exclusively
contain valid utf-8 data, the unicode replacement character  � will be
used to represent those errors.
(alias: ): multi-byte encoded unicode characters.
unlike , each character in the string will be encoded using either 2
or 4 bytes. node.js only supports the  variant of
: latin-1 stands for . this character encoding only
supports the unicode characters from  to . each character is
encoded using a single byte. characters that do not fit into that range are
truncated and will be mapped to characters in that range.
converting a  into a string using one of the above is referred to as
decoding, and converting a string into a  is referred to as encoding.
node.js also supports the following binary-to-text encodings. for
binary-to-text encodings, the naming convention is reversed: converting a
into a string is typically referred to as encoding, and converting a
string into a  as decoding.
:  encoding. when creating a  from a string,
this encoding will also correctly accept "url and filename safe alphabet" as
specified in . whitespace characters such as spaces,
tabs, and new lines contained within the base64-encoded string are ignored.
:  encoding as specified in
. when creating a  from a string, this
encoding will also correctly accept regular base64-encoded strings. when
encoding a  to a string, this encoding will omit padding.
: encode each byte as two hexadecimal characters. data truncation
may occur when decoding strings that do not exclusively consist of an even
number of hexadecimal characters. see below for an example.
the following legacy character encodings are also supported:
: for 7-bit  data only. when encoding a string into a
, this is equivalent to using . when decoding a
into a string, using this encoding will additionally unset the highest bit of
each byte before decoding as .
generally, there should be no reason to use this encoding, as
(or, if the data is known to always be ascii-only, ) will be a
better choice when encoding or decoding ascii-only text. it is only provided
for legacy compatibility.
: alias for . see  for more background
on this topic. the name of this encoding can be very misleading, as all of the
encodings listed here convert between strings and binary data. for converting
between strings and s, typically  is the right choice.
, : aliases of . ucs-2 used to refer to a variant
of utf-16 that did not support characters that had code points larger than
u+ffff. in node.js, these code points are always supported.
modern web browsers follow the  which aliases
both  and  to . this means that while doing
something like , if the returned charset is one of those listed in
the whatwg specification it is possible that the server actually returned
-encoded data, and using  encoding may incorrectly decode
the characters.
buffers and typedarrays
instances are also javascript  and
instances. all  methods are available on s. there are,
however, subtle incompatibilities between the  api and the
api.
in particular:
while  creates a copy of part of the ,
creates a view over the existing
without copying. this behavior can be surprising, and only exists for legacy
compatibility.  can be used to achieve
the behavior of  on both s
and other s and should be preferred.
is incompatible with its  equivalent.
a number of methods, e.g. , support additional arguments.
there are two ways to create new  instances from a :
passing a  to a  constructor will copy the s
contents, interpreted as an array of integers, and not as a byte sequence
of the target type.
passing the s underlying  will create a
that shares its memory with the .
it is possible to create a new  that shares the same allocated
memory as a  instance by using the  object's
property in the same way.
behaves like  in this context.
when creating a  using a 's , it is
possible to use only a portion of the underlying  by passing in
and  parameters.
the  and  have different signatures and
implementations. specifically, the  variants accept a second
argument that is a mapping function that is invoked on every element of the
typed array:
the  method, however, does not support the use of a mapping
function:
buffers and iteration
instances can be iterated over using  syntax:
additionally, the , , and
methods can be used to create iterators.
a  encapsulates immutable, raw data that can be safely shared across
multiple worker threads.
|  |  |  |  an
array of string, , , , or  objects,
or any mix of such objects, that will be stored within the .
one of either  or . when set
to , line endings in string source parts will be converted to
the platform native line-ending as specified by .
the blob content-type. the intent is for  to convey
the mime media type of the data, however no validation of the type format
is performed.
creates a new  object containing a concatenation of the given sources.
, , , and  sources are copied into
the 'blob' and can therefore be safely modified after the 'blob' is created.
string sources are encoded as utf-8 byte sequences and copied into the blob.
unmatched surrogate pairs within each string part will be replaced by unicode
u+fffd replacement characters.
added in: v15.7.0, v14.18.0
returns a promise that fulfills with an  containing a copy of
the  data.
the total size of the  in bytes.
the starting index.
the ending index.
the content-type for the new
creates and returns a new  containing a subset of this  objects
data. the original  is not altered.
added in: v16.7.0
returns a new  that allows the content of the  to be read.
returns a promise that fulfills with the contents of the  decoded as a
utf-8 string.
type:
the content-type of the .
objects and
once a  object is created, it can be sent via  to multiple
destinations without transferring or immediately copying the data. the data
contained by the  is copied only when the  or
methods are called.
the  class is a global type for dealing with binary data directly.
it can be constructed in a variety of ways.
the desired length of the new .
|  |  |  a value to pre-fill the new
with. default: .
if  is a string, this is its encoding.
allocates a new  of  bytes. if  is , the
will be zero-filled.
if  is larger than
or smaller than 0,
is thrown.
if  is specified, the allocated  will be initialized by calling
if both  and  are specified, the allocated  will be
initialized by calling .
calling  can be measurably slower than the alternative
but ensures that the newly created  instance
contents will never contain sensitive data from previous allocations, including
data that might not have been allocated for s.
a  will be thrown if  is not a number.
allocates a new  of  bytes. if  is larger than
the underlying memory for  instances created in this way is not
initialized. the contents of the newly created  are unknown and
may contain sensitive data. use  instead to initialize
instances with zeroes.
the  module pre-allocates an internal  instance of
size  that is used as a pool for the fast allocation of new
instances created using ,
, , and the deprecated
constructor only when  is less than or equal
to  (floor of  divided by two).
use of this pre-allocated internal memory pool is a key difference between
calling  vs. .
specifically,  will never use the internal
pool, while  will use the internal
pool if  is less than or equal to half . the
difference is subtle but can be important when an application requires the
additional performance that  provides.
is thrown. a zero-length  is created if  is 0.
may contain sensitive data. use  to initialize
such  instances with zeroes.
when using  to allocate new  instances,
allocations under 4 kib are sliced from a single pre-allocated . this
allows applications to avoid the garbage collection overhead of creating many
individually allocated  instances. this approach improves both
performance and memory usage by eliminating the need to track and clean up as
many individual  objects.
however, in the case where a developer may need to retain a small chunk of
memory from a pool for an indeterminate amount of time, it may be appropriate
to create an un-pooled  instance using  and
then copying out the relevant bits.
|  |  |  |  |  a
value to calculate the length of.
returns:  the number of bytes contained within .
returns the byte length of a string when encoded using .
this is not the same as , which does not account
for the encoding that is used to convert the string into bytes.
for , , and , this function assumes valid input.
for strings that contain non-base64/hex-encoded data (e.g. whitespace), the
return value might be greater than the length of a  created from the
string.
when  is a ////
, the byte length as reported by
is returned.
returns:  either , , or , depending on the result of the
comparison. see  for details.
compares  to , typically for the purpose of sorting arrays of
instances. this is equivalent to calling
|  list of  or
instances to concatenate.
total length of the  instances in
when concatenated.
returns a new  which is the result of concatenating all the
instances in the  together.
if the list has no items, or if the  is 0, then a new zero-length
if  is not provided, it is calculated from the  instances
in  by adding their lengths.
if  is provided, it is coerced to an unsigned integer. if the
combined length of the s in  exceeds , the result is
truncated to .
may also use the internal  pool like
does.
added in: v5.10.0
allocates a new  using an  of bytes in the range  – .
array entries outside that range will be truncated to fit into it.
a  will be thrown if  is not an  or another type
appropriate for  variants.
and  may also use the internal
pool like  does.
|  an ,
, for example the  property of a
index of first byte to expose. default: .
number of bytes to expose.
this creates a view of the  without copying the underlying
memory. for example, when passed a reference to the  property of a
instance, the newly created  will share the same
allocated memory as the 's underlying .
the optional  and  arguments specify a memory range within
the  that will be shared by the .
a  will be thrown if  is not an  or a
or another type appropriate for
variants.
it is important to remember that a backing  can cover a range
of memory that extends beyond the bounds of a  view. a new
created using the  property of a  may extend
beyond the range of the :
|  an existing  or  from
which to copy data.
copies the passed  data onto a new  instance.
a  will be thrown if  is not a  or another type
added in: v8.2.0
an object supporting  or .
|  a byte-offset or encoding.
a length.
for objects whose  function returns a value not strictly equal to
, returns .
for objects that support , returns
a  will be thrown if  does not have the mentioned methods or
is not of another type appropriate for  variants.
a string to encode.
the encoding of . default: .
creates a new  containing . the  parameter identifies
the character encoding to be used when converting  into bytes.
a  will be thrown if  is not a string or another type
added in: v0.1.101
returns  if  is a ,  otherwise.
added in: v0.9.1
a character encoding name to check.
returns  if  is the name of a supported character encoding,
or  otherwise.
class property:
added in: v0.11.3
this is the size (in bytes) of pre-allocated internal  instances used
for pooling. this value may be modified.
the index operator  can be used to get and set the octet at position
in . the values refer to individual bytes, so the legal value
range is between  and  (hex) or  and  (decimal).
this operator is inherited from , so its behavior on out-of-bounds
access is the same as . in other words,  returns
when  is negative or greater or equal to , and
does not modify the buffer if  is negative or
the underlying  object based on which this
object is created.
this  is not guaranteed to correspond exactly to the original
. see the notes on  for details.
the  of the s underlying  object.
when setting  in ,
or sometimes when allocating a  smaller than , the
buffer does not start from a zero offset on the underlying .
this can cause problems when accessing the underlying  directly
using , as other parts of the  may be unrelated
to the  object itself.
a common issue when creating a  object that shares its memory with
a  is that in this case one needs to specify the  correctly:
|  a  or  with which to
compare .
the offset within  at which to begin
comparison. default: .
the offset within  at which to end comparison
(not inclusive). default: .
the offset within  at which to begin comparison.
compares  with  and returns a number indicating whether
comes before, after, or is the same as  in sort order.
comparison is based on the actual sequence of bytes in each .
is returned if  is the same as
is returned if  should come before  when sorted.
is returned if  should come after  when sorted.
the optional , , , and
arguments can be used to limit the comparison to specific ranges within
and  respectively.
is thrown if , ,
, or .
added in: v0.1.90
|  a  or  to copy into.
writing. default: .
the offset within  from which to begin copying.
the offset within  at which to stop copying (not
inclusive). default: .
returns:  the number of bytes copied.
copies data from a region of  to a region in , even if the
memory region overlaps with .
performs the same operation, and is available
for all typedarrays, including node.js s, although it takes
different function arguments.
added in: v1.1.0
creates and returns an  of  pairs from the contents
of .
returns  if both  and  have exactly the same bytes,
otherwise. equivalent to
|  |  |  the value with which to fill .
number of bytes to skip before starting to fill .
where to stop filling  (not inclusive). default:
the encoding for  if  is a string.
fills  with the specified . if the  and  are not given,
the entire  will be filled:
is coerced to a  value if it is not a string, , or
integer. if the resulting integer is greater than  (decimal),  will be
filled with .
if the final write of a  operation falls on a multi-byte character,
then only the bytes of that character that fit into  are written:
if  contains invalid characters, it is truncated; if no valid
fill data remains, an exception is thrown:
added in: v5.3.0
|  |  |  what to search for.
where to begin searching in . if negative, then
offset is calculated from the end of . default: .
returns:   if  was found in ,  otherwise.
equivalent to .
if  is a string, this is the encoding used to
determine the binary representation of the string that will be searched for in
. default: .
returns:  the index of the first occurrence of  in , or
if  does not contain .
if  is:
a string,  is interpreted according to the character encoding in
a  or ,  will be used in its entirety.
to compare a partial , use .
a number,  will be interpreted as an unsigned 8-bit integer
value between  and .
if  is not a string, number, or , this method will throw a
. if  is a number, it will be coerced to a valid byte value,
an integer between 0 and 255.
if  is not a number, it will be coerced to a number. if the result
of coercion is  or , then the entire buffer will be searched. this
behavior matches .
if  is an empty string or empty  and  is less
than ,  will be returned. if  is empty and
is at least ,  will be returned.
creates and returns an  of  keys (indices).
offset is calculated from the end of . default:
returns:  the index of the last occurrence of  in , or
identical to , except the last occurrence of  is found
rather than the first occurrence.
if  is not a number, it will be coerced to a number. any arguments
that coerce to , like  or , will search the whole buffer.
this behavior matches .
if  is an empty string or empty ,  will be returned.
returns the number of bytes in .
deprecated since: v8.0.0
- deprecated: use  instead.
the  property is a deprecated alias for .
added in: v12.0.0, v10.20.0
number of bytes to skip before starting to read. must
satisfy: . default: .
reads a signed, big-endian 64-bit integer from  at the specified .
integers read from a  are interpreted as two's complement signed
values.
reads a signed, little-endian 64-bit integer from  at the specified
reads an unsigned, big-endian 64-bit integer from  at the specified
this function is also available under the  alias.
reads an unsigned, little-endian 64-bit integer from  at the specified
satisfy . default: .
reads a 64-bit, big-endian double from  at the specified .
reads a 64-bit, little-endian double from  at the specified .
reads a 32-bit, big-endian float from  at the specified .
reads a 32-bit, little-endian float from  at the specified .
reads a signed 8-bit integer from  at the specified .
integers read from a  are interpreted as two's complement signed values.
reads a signed, big-endian 16-bit integer from  at the specified .
reads a signed, little-endian 16-bit integer from  at the specified
reads a signed, big-endian 32-bit integer from  at the specified .
reads a signed, little-endian 32-bit integer from  at the specified
satisfy .
number of bytes to read. must satisfy
reads  number of bytes from  at the specified
and interprets the result as a big-endian, two's complement signed value
supporting up to 48 bits of accuracy.
and interprets the result as a little-endian, two's complement signed value
reads an unsigned 8-bit integer from  at the specified .
reads an unsigned, big-endian 16-bit integer from  at the specified
reads an unsigned, little-endian 16-bit integer from  at the specified
reads an unsigned, big-endian 32-bit integer from  at the specified
reads an unsigned, little-endian 32-bit integer from  at the specified
and interprets the result as an unsigned big-endian integer supporting
up to 48 bits of accuracy.
and interprets the result as an unsigned, little-endian integer supporting
added in: v3.0.0
where the new  will start. default: .
where the new  will end (not inclusive).
returns a new  that references the same memory as the original, but
offset and cropped by the  and  indices.
specifying  greater than  will return the same result as
that of  equal to .
this method is inherited from .
modifying the new  slice will modify the memory in the original
because the allocated memory of the two objects overlap.
specifying negative indexes causes the slice to be generated relative to the
end of  rather than the beginning.
this method is not compatible with the ,
which is a superclass of . to copy the slice, use
interprets  as an array of unsigned 16-bit integers and swaps the
byte order in-place. throws  if
is not a multiple of 2.
one convenient use of  is to perform a fast in-place conversion
between utf-16 little-endian and utf-16 big-endian:
interprets  as an array of unsigned 32-bit integers and swaps the
is not a multiple of 4.
added in: v6.3.0
interprets  as an array of 64-bit numbers and swaps byte order in-place.
throws  if  is not a multiple of 8.
added in: v0.9.2
returns a json representation of .  implicitly calls
this function when stringifying a  instance.
accepts objects in the format returned from this method.
in particular,  works like .
the character encoding to use. default: .
the byte offset to start decoding at. default: .
the byte offset to stop decoding at (not inclusive).
decodes  to a string according to the specified character encoding in
.  and  may be passed to decode only a subset of .
if  is  and a byte sequence in the input is not valid utf-8,
then each invalid byte is replaced with the replacement character .
the maximum length of a string instance (in utf-16 code units) is available
as .
creates and returns an  for  values (bytes). this function is
called automatically when a  is used in a  statement.
string to write to .
number of bytes to skip before starting to write .
maximum number of bytes to write (written bytes will not
exceed ). default: .
the character encoding of . default: .
returns:  number of bytes written.
writes  to  at  according to the character encoding in
. the  parameter is the number of bytes to write. if  did
not contain enough space to fit the entire string, only part of  will be
written. however, partially encoded characters will not be written.
number to be written to .
number of bytes to skip before starting to write. must
returns:   plus the number of bytes written.
writes  to  at the specified  as big-endian.
is interpreted and written as a two's complement signed integer.
writes  to  at the specified  as little-endian.
writes  to  at the specified  as little-endian
writes  to  at the specified  as big-endian. the
must be a javascript number. behavior is undefined when  is anything
other than a javascript number.
writes  to  at the specified  as little-endian. the
writes  to  at the specified  as big-endian. behavior is
undefined when  is anything other than a javascript number.
writes  to  at the specified  as little-endian. behavior is
writes  to  at the specified .  must be a valid
signed 8-bit integer. behavior is undefined when  is anything other than
a signed 8-bit integer.
writes  to  at the specified  as big-endian.  the
must be a valid signed 16-bit integer. behavior is undefined when  is
anything other than a signed 16-bit integer.
the  is interpreted and written as a two's complement signed integer.
writes  to  at the specified  as little-endian.  the
must be a valid signed 32-bit integer. behavior is undefined when  is
anything other than a signed 32-bit integer.
number of bytes to write. must satisfy
writes  bytes of  to  at the specified
as big-endian. supports up to 48 bits of accuracy. behavior is undefined when
is anything other than a signed integer.
as little-endian. supports up to 48 bits of accuracy. behavior is undefined
when  is anything other than a signed integer.
writes  to  at the specified .  must be a
valid unsigned 8-bit integer. behavior is undefined when  is anything
other than an unsigned 8-bit integer.
must be a valid unsigned 16-bit integer. behavior is undefined when
is anything other than an unsigned 16-bit integer.
must be a valid unsigned 16-bit integer. behavior is undefined when  is
anything other than an unsigned 16-bit integer.
must be a valid unsigned 32-bit integer. behavior is undefined when
is anything other than an unsigned 32-bit integer.
must be a valid unsigned 32-bit integer. behavior is undefined when  is
anything other than an unsigned 32-bit integer.
as big-endian. supports up to 48 bits of accuracy. behavior is undefined
when  is anything other than an unsigned integer.
an array of bytes to copy from.
see .
- deprecated: use
or the  property of a .
- deprecated: use  instead (also see
see  and . this variant of the
constructor is equivalent to .
- deprecated:
use  instead.
string to encode.
module apis
while, the  object is available as a global, there are additional
-related apis that are available only via the  module
accessed using .
added in: v15.13.0, v14.17.0
- legacy. use  instead.
the base64-encoded input string.
decodes a string of base64-encoded data into bytes, and encodes those bytes
into a string using latin-1 (iso-8859-1).
the  may be any javascript-value that can be coerced into a string.
this function is only provided for compatibility with legacy web platform apis
and should never be used in new code, because they use strings to represent
binary data and predate the introduction of typed arrays in javascript.
for code running using node.js apis, converting between base64-encoded strings
and binary data should be performed using  and
an ascii (latin1) string.
decodes a string into bytes using latin-1 (iso-8859), and encodes those bytes
into a string using base64.
added in: v0.5.4
returns the maximum number of bytes that will be returned when
is called. this can be overridden by user modules. see
for more details on  behavior.
the largest size allowed for a single  instance.
an alias for .
the largest length allowed for a single  instance.
a  url string returned by a prior call to
resolves a  an associated  object registered using
a prior call to .
|  a  or  instance.
the current encoding.
to target encoding.
re-encodes the given  or  instance from one character
encoding to another. returns a new  instance.
throws if the  or  specify invalid character encodings or if
conversion from  to  is not permitted.
encodings supported by  are: , ,
, , , and .
the transcoding process will use substitution characters if a given byte
sequence cannot be adequately represented in the target encoding. for instance:
because the euro () sign is not representable in us-ascii, it is replaced
with  in the transcoded .
deprecated since: v6.0.0
see . this was never a class in the sense that
the constructor always returned a  instance, rather than a
instance.
buffer constants
on 32-bit architectures, this value currently is 230 - 1 (about 1
gib).
on 64-bit architectures, this value currently is 232 (about 4 gib).
it reflects  under the hood.
this value is also available as .
represents the largest  that a  primitive can have, counted
in utf-16 code units.
this value may depend on the js engine that is being used.
in versions of node.js prior to 6.0.0,  instances were created using the
constructor function, which allocates the returned
differently based on what arguments are provided:
passing a number as the first argument to  (e.g. )
allocates a new  object of the specified size. prior to node.js 8.0.0,
the memory allocated for such  instances is not initialized and
can contain sensitive data. such  instances must be subsequently
initialized by using either  or by writing to the
entire  before reading data from the .
while this behavior is intentional to improve performance,
development experience has demonstrated that a more explicit distinction is
required between creating a fast-but-uninitialized  versus creating a
slower-but-safer . since node.js 8.0.0,  and  return a  with initialized memory.
passing a string, array, or  as the first argument copies the
passed object's data into the .
passing an  or a  returns a
that shares allocated memory with the given array buffer.
because the behavior of  is different depending on the type of the
first argument, security and reliability issues can be inadvertently introduced
into applications when argument validation or  initialization is not
performed.
for example, if an attacker can cause an application to receive a number where
a string is expected, the application may call
instead of , leading it to allocate a 100 byte buffer instead
of allocating a 3 byte buffer with content . this is commonly possible
using json api calls. since json distinguishes between numeric and string types,
it allows injection of numbers where a naively written application that does not
validate its input sufficiently might expect to always receive a string.
before node.js 8.0.0, the 100 byte buffer might contain
arbitrary pre-existing in-memory data, so may be used to expose in-memory
secrets to a remote attacker. since node.js 8.0.0, exposure of memory cannot
occur because the data is zero-filled. however, other attacks are still
possible, such as causing very large buffers to be allocated by the server,
leading to performance degradation or crashing on memory exhaustion.
to make the creation of  instances more reliable and less error-prone,
the various forms of the  constructor have been deprecated
and replaced by separate , , and
methods.
developers should migrate all existing uses of the  constructors
to one of these new apis.
returns a new  that contains a copy of the
provided octets.
returns a new  that shares the same allocated memory as the given
contents of the given .
returns a new
that contains a copy of the provided string.
initialized  of the specified size. this method is slower than
but guarantees that newly
created  instances never contain old data that is potentially
sensitive. a  will be thrown if  is not a number.
each return a
new uninitialized  of the specified . because the  is
uninitialized, the allocated segment of memory might contain old data that is
potentially sensitive.
instances returned by  and
may be allocated off a shared internal memory pool
if  is less than or equal to half . instances
returned by  never use the shared internal
memory pool.
the  command-line option
node.js can be started using the  command-line option to
cause all newly-allocated  instances to be zero-filled upon creation by
default. without the option, buffers created with ,
, and  are not zero-filled.
use of this flag can have a measurable negative impact on performance. use the
option only when necessary to enforce that newly allocated
instances cannot contain old data that is potentially sensitive.
what makes  and  "unsafe"?
when calling  and , the
segment of allocated memory is uninitialized (it is not zeroed-out). while
this design makes the allocation of memory quite fast, the allocated segment of
memory might contain old data that is potentially sensitive. using a
created by  without completely overwriting the
memory can allow this old data to be leaked when the  memory is read.
while there are clear performance advantages to using
, extra care must be taken in order to avoid
introducing security vulnerabilities into an application.
c++ addons
addons are dynamically-linked shared objects written in c++. the
function can load addons as ordinary node.js modules.
addons provide an interface between javascript and c/c++ libraries.
there are three options for implementing addons: node-api, nan, or direct
use of internal v8, libuv, and node.js libraries. unless there is a need for
direct access to functionality which is not exposed by node-api, use node-api.
refer to  for more information on
node-api.
when not using node-api, implementing addons is complicated,
involving knowledge of several components and apis:
: the c++ library node.js uses to provide the
javascript implementation. v8 provides the mechanisms for creating objects,
calling functions, etc. v8's api is documented mostly in the
header file ( in the node.js source
tree), which is also available .
: the c library that implements the node.js event loop, its worker
threads and all of the asynchronous behaviors of the platform. it also
serves as a cross-platform abstraction library, giving easy, posix-like
access across all major operating systems to many common system tasks, such
as interacting with the filesystem, sockets, timers, and system events. libuv
also provides a threading abstraction similar to posix threads for
more sophisticated asynchronous addons that need to move beyond the
standard event loop. addon authors should
avoid blocking the event loop with i/o or other time-intensive tasks by
offloading work via libuv to non-blocking system operations, worker threads,
or a custom use of libuv threads.
internal node.js libraries. node.js itself exports c++ apis that addons can
use, the most important of which is the  class.
node.js includes other statically linked libraries including openssl. these
other libraries are located in the  directory in the node.js source
tree. only the libuv, openssl, v8, and zlib symbols are purposefully
re-exported by node.js and may be used to various extents by addons. see
for additional information.
all of the following examples are available for  and may
be used as the starting-point for an addon.
this "hello world" example is a simple addon, written in c++, that is the
equivalent of the following javascript code:
first, create the file :
all node.js addons must export an initialization function following
the pattern:
there is no semi-colon after  as it's not a function (see
the  must match the filename of the final binary (excluding
the  suffix).
in the  example, then, the initialization function is
and the addon module name is .
when building addons with , using the macro  as
the first parameter of  will ensure that the name of the final
binary will be passed to .
context-aware addons
there are environments in which node.js addons may need to be loaded multiple
times in multiple contexts. for example, the  runtime runs multiple
instances of node.js in a single process. each instance will have its own
cache, and thus each instance will need a native addon to behave
correctly when loaded via . this means that the addon
must support multiple initializations.
a context-aware addon can be constructed by using the macro
, which expands to the name of a function which node.js
will expect to find when it loads an addon. an addon can thus be initialized as
in the following example:
another option is to use the macro , which will also
construct a context-aware addon. unlike , which is used to
construct an addon around a given addon initializer function,
serves as the declaration of such an initializer to be
followed by a function body.
the following three variables may be used inside the function body following an
invocation of :
, and
the choice to build a context-aware addon carries with it the responsibility of
carefully managing global static data. since the addon may be loaded multiple
times, potentially even from different threads, any global static data stored
in the addon must be properly protected, and must not contain any persistent
references to javascript objects. the reason for this is that javascript
objects are only valid in one context, and will likely cause a crash when
accessed from the wrong context or from a different thread than the one on which
they were created.
the context-aware addon can be structured to avoid global static data by
performing the following steps:
define a class which will hold per-addon-instance data and which has a static
member of the form
heap-allocate an instance of this class in the addon initializer. this can be
accomplished using the  keyword.
call , passing it the above-created
instance and a pointer to . this will ensure the instance is
deleted when the environment is torn down.
store the instance of the class in a , and
pass the  to all methods exposed to javascript by passing it
to  or  which creates the
native-backed javascript functions. the third parameter of
or   accepts the
and makes it available in the native callback using the
method.
this will ensure that the per-addon-instance data reaches each binding that can
be called from javascript. the per-addon-instance data must also be passed into
any asynchronous callbacks the addon may create.
the following example illustrates the implementation of a context-aware addon:
worker support
in order to be loaded from multiple node.js environments,
such as a main thread and a worker thread, an add-on needs to either:
be an node-api addon, or
be declared as context-aware using  as described above
in order to support  threads, addons need to clean up any resources
they may have allocated when such a thread exists. this can be achieved through
the usage of the  function:
this function adds a hook that will run before a given node.js instance shuts
down. if necessary, such hooks can be removed before they are run using
, which has the same signature. callbacks are
run in last-in first-out order.
if necessary, there is an additional pair of
and  overloads, where the cleanup hook takes a
callback function. this can be used for shutting down asynchronous resources,
such as any libuv handles registered by the addon.
the following  uses :
test in javascript by running:
building
once the source code has been written, it must be compiled into the binary
file. to do so, create a file called  in the
top-level of the project describing the build configuration of the module
using a json-like format. this file is used by , a tool written
specifically to compile node.js addons.
a version of the  utility is bundled and distributed with
node.js as part of . this version is not made directly available for
developers to use and is intended only to support the ability to use the
command to compile and install addons. developers who wish to
use  directly can install it using the command
. see the   for
more information, including platform-specific requirements.
once the  file has been created, use  to
generate the appropriate project build files for the current platform. this
will generate either a  (on unix platforms) or a  file
(on windows) in the  directory.
next, invoke the  command to generate the compiled
file. this will be put into the  directory.
when using  to install a node.js addon, npm uses its own bundled
version of  to perform this same set of actions, generating a
compiled version of the addon for the user's platform on demand.
once built, the binary addon can be used from within node.js by pointing
to the built  module:
because the exact path to the compiled addon binary can vary depending on how
it is compiled (i.e. sometimes it may be in ), addons can use
the  package to load the compiled module.
while the  package implementation is more sophisticated in how it
locates addon modules, it is essentially using a  pattern similar to:
linking to libraries included with node.js
node.js uses statically linked libraries such as v8, libuv, and openssl. all
addons are required to link to v8 and may link to any of the other dependencies
as well. typically, this is as simple as including the appropriate
statements (e.g. ) and  will locate
the appropriate headers automatically. however, there are a few caveats to be
aware of:
when  runs, it will detect the specific release version of node.js
and download either the full source tarball or just the headers. if the full
source is downloaded, addons will have complete access to the full set of
node.js dependencies. however, if only the node.js headers are downloaded,
then only the symbols exported by node.js will be available.
can be run using the  flag pointing at a local node.js
source image. using this option, the addon will have access to the full set of
dependencies.
loading addons using
the filename extension of the compiled addon binary is  (as opposed
to  or ). the  function is written to look for
files with the  file extension and initialize those as dynamically-linked
libraries.
when calling , the  extension can usually be
omitted and node.js will still find and initialize the addon. one caveat,
however, is that node.js will first attempt to locate and load modules or
javascript files that happen to share the same base name. for instance, if
there is a file  in the same directory as the binary ,
then  will give precedence to the  file
and load it instead.
native abstractions for node.js
each of the examples illustrated in this document directly use the
node.js and v8 apis for implementing addons. the v8 api can, and has, changed
dramatically from one v8 release to the next (and one major node.js release to
the next). with each change, addons may need to be updated and recompiled in
order to continue functioning. the node.js release schedule is designed to
minimize the frequency and impact of such changes but there is little that
node.js can do to ensure stability of the v8 apis.
the  (or ) provide a set of tools that
addon developers are recommended to use to keep compatibility between past and
future releases of v8 and node.js. see the   for an
illustration of how it can be used.
node-api
node-api is an api for building native addons. it is independent from
the underlying javascript runtime (e.g. v8) and is maintained as part of
node.js itself. this api will be application binary interface (abi) stable
across versions of node.js. it is intended to insulate addons from
changes in the underlying javascript engine and allow modules
compiled for one version to run on later versions of node.js without
recompilation. addons are built/packaged with the same approach/tools
outlined in this document (node-gyp, etc.). the only difference is the
set of apis that are used by the native code. instead of using the v8
or  apis, the functions available
in the node-api are used.
creating and maintaining an addon that benefits from the abi stability
provided by node-api carries with it certain
to use node-api in the above "hello world" example, replace the content of
with the following. all other instructions remain the same.
the functions available and how to use them are documented in
addon examples
following are some example addons intended to help developers get started. the
examples use the v8 apis. refer to the online
for help with the various v8 calls, and v8's  for an
explanation of several concepts used such as handles, scopes, function
templates, etc.
each of these examples using the following  file:
in cases where there is more than one  file, simply add the additional
filename to the  array:
once the  file is ready, the example addons can be configured and
built using :
function arguments
addons will typically expose objects and functions that can be accessed from
javascript running within node.js. when functions are invoked from javascript,
the input arguments and return value must be mapped to and from the c/c++
code.
the following example illustrates how to read function arguments passed from
javascript and how to return a result:
once compiled, the example addon can be required and used from within node.js:
callbacks
it is common practice within addons to pass javascript functions to a c++
function and execute them from there. the following example illustrates how
to invoke such callbacks:
this example uses a two-argument form of  that receives the full
object as the second argument. this allows the addon to completely
overwrite  with a single function instead of adding the function as a
property of .
to test it, run the following javascript:
in this example, the callback function is invoked synchronously.
object factory
addons can create and return new objects from within a c++ function as
illustrated in the following example. an object is created and returned with a
property  that echoes the string passed to :
to test it in javascript:
function factory
another common scenario is creating javascript functions that wrap c++
functions and returning those back to javascript:
to test:
wrapping c++ objects
it is also possible to wrap c++ objects/classes in a way that allows new
instances to be created using the javascript  operator:
then, in , the wrapper class inherits from :
in , implement the various methods that are to be exposed.
below, the method  is exposed by adding it to the constructor's
prototype:
to build this example, the  file must be added to the
test it with:
the destructor for a wrapper object will run when the object is
garbage-collected. for destructor testing, there are command-line flags that
can be used to make it possible to force garbage collection. these flags are
provided by the underlying v8 javascript engine. they are subject to change
or removal at any time. they are not documented by node.js or v8, and they
should never be used outside of testing.
during shutdown of the process or worker threads destructors are not called
by the js engine. therefore it's the responsibility of the user to track
these objects and ensure proper destruction to avoid resource leaks.
factory of wrapped objects
alternatively, it is possible to use a factory pattern to avoid explicitly
creating object instances using the javascript  operator:
first, the  method is implemented in :
in , the static method  is added to handle
instantiating the object. this method takes the place of using  in
javascript:
the implementation in  is similar to the previous example:
once again, to build this example, the  file must be added to the
passing wrapped objects around
in addition to wrapping and returning c++ objects, it is possible to pass
wrapped objects around by unwrapping them with the node.js helper function
. the following examples shows a function
that can take two  objects as input arguments:
in , a new public method is added to allow access to private values
after unwrapping the object.
the implementation of  is similar to before:
node-api (formerly n-api) is an api for building native addons. it is
independent from the underlying javascript runtime (for example, v8) and is
maintained as part of node.js itself. this api will be application binary
interface (abi) stable across versions of node.js. it is intended to insulate
addons from changes in the underlying javascript engine and allow modules
compiled for one major version to run on later major versions of node.js without
recompilation. the  guide provides a more in-depth explanation.
addons are built/packaged with the same approach/tools outlined in the section
titled . the only difference is the set of apis that are used by
the native code. instead of using the v8 or
apis, the functions available in node-api are used.
apis exposed by node-api are generally used to create and manipulate
javascript values. concepts and operations generally map to ideas specified
in the ecma-262 language specification. the apis have the following
all node-api calls return a status code of type . this
status indicates whether the api call succeeded or failed.
the api's return value is passed via an out parameter.
all javascript values are abstracted behind an opaque type named
in case of an error status code, additional information can be obtained
using . more information can be found in the error
handling section .
node-api is a c api that ensures abi stability across node.js versions
and different compiler levels. a c++ api can be easier to use.
to support using c++, the project maintains a
c++ wrapper module called .
this wrapper provides an inlineable c++ api. binaries built
with  will depend on the symbols for the node-api c-based
functions exported by node.js.  is a more
efficient way to write code that calls node-api. take, for example, the
following  code. the first section shows the
code and the second section shows what actually gets
used in the addon.
the end result is that the addon only uses the exported c apis. as a result,
it still gets the benefits of the abi stability provided by the c api.
when using  instead of the c apis, start with the api
for .
the  offers
an excellent orientation and tips for developers just getting started with
node-api and .
implications of abi stability
although node-api provides an abi stability guarantee, other parts of node.js do
not, and any external libraries used from the addon may not. in particular,
none of the following apis provide an abi stability guarantee across major
versions:
the node.js c++ apis available via any of
the libuv apis which are also included with node.js and available via
the v8 api available via
thus, for an addon to remain abi-compatible across node.js major versions, it
must use node-api exclusively by restricting itself to using
and by checking, for all external libraries that it uses, that the external
library makes abi stability guarantees similar to node-api.
unlike modules written in javascript, developing and deploying node.js
native addons using node-api requires an additional set of tools. besides the
basic tools required to develop for node.js, the native addon developer
requires a toolchain that can compile c and c++ code into a binary. in
addition, depending upon how the native addon is deployed, the user of
the native addon will also need to have a c/c++ toolchain installed.
for linux developers, the necessary c/c++ toolchain packages are readily
available.  is widely used in the node.js community to build and
test across a variety of platforms. for many developers, the
compiler infrastructure is also a good choice.
for mac developers,  offers all the required compiler tools.
however, it is not necessary to install the entire xcode ide. the following
command installs the necessary toolchain:
for windows developers,  offers all the required compiler
tools. however, it is not necessary to install the entire visual studio
ide. the following command installs the necessary toolchain:
the sections below describe the additional tools available for developing
and deploying node.js native addons.
build tools
both the tools listed here require that users of the native
addon have a c/c++ toolchain installed in order to successfully install
the native addon.
node-gyp
is a build system based on the  fork of
google's  tool and comes bundled with npm. gyp, and therefore node-gyp,
requires that python be installed.
historically, node-gyp has been the tool of choice for building native
addons. it has widespread adoption and documentation. however, some
developers have run into limitations in node-gyp.
cmake.js
is an alternative build system based on .
cmake.js is a good choice for projects that already use cmake or for
developers affected by limitations in node-gyp.
uploading precompiled binaries
the three tools listed here permit native addon developers and maintainers
to create and upload binaries to public or private servers. these tools are
typically integrated with ci/cd build systems like  and
to build and upload binaries for a variety of platforms and
architectures. these binaries are then available for download by users who
do not need to have a c/c++ toolchain installed.
node-pre-gyp
is a tool based on node-gyp that adds the ability to
upload binaries to a server of the developer's choice. node-pre-gyp has
particularly good support for uploading binaries to amazon s3.
prebuild
is a tool that supports builds using either node-gyp or
cmake.js. unlike node-pre-gyp which supports a variety of servers, prebuild
uploads binaries only to . prebuild is a good choice for
github projects using cmake.js.
prebuildify
is a tool based on node-gyp. the advantage of prebuildify is
that the built binaries are bundled with the native addon when it's
uploaded to npm. the binaries are downloaded from npm and are immediately
available to the module user when the native addon is installed.
in order to use the node-api functions, include the file  which
is located in the src directory in the node development tree:
this will opt into the default  for the given release of node.js.
in order to ensure compatibility with specific versions of node-api, the version
can be specified explicitly when including the header:
this restricts the node-api surface to just the functionality that was available
in the specified (and earlier) versions.
some of the node-api surface is experimental and requires explicit opt-in:
in this case the entire api surface, including any experimental apis, will be
available to the module code.
node-api version matrix
node-api versions are additive and versioned independently from node.js.
version 4 is an extension to version 3 in that it has all of the apis
from version 3 with some additions. this means that it is not necessary
to recompile for new versions of node.js which are
listed as supporting a later version.
* node-api was experimental.
** node.js 8.0.0 included node-api as experimental. it was released as
node-api version 1 but continued to evolve until node.js 8.6.0. the api is
different in versions prior to node.js 8.6.0. we recommend node-api version 3 or
later.
each api documented for node-api will have a header named , and apis
which are stable will have the additional header .
apis are directly usable when using a node.js version which supports
the node-api version shown in  or higher.
when using a node.js version that does not support the
listed or if there is no  listed,
then the api will only be available if
precedes the inclusion of
or . if an api appears not to be available on
a version of node.js which is later than the one shown in  then
this is most likely the reason for the apparent absence.
the node-apis associated strictly with accessing ecmascript features from native
code can be found separately in  and .
the apis defined in these headers are included in  and
. the headers are structured in this way in order to allow
implementations of node-api outside of node.js. for those implementations the
node.js specific apis may not be applicable.
the node.js-specific parts of an addon can be separated from the code that
exposes the actual functionality to the javascript environment so that the
latter may be used with multiple implementations of node-api. in the example
below,  and  refer only to . this ensures
that  can be reused to compile against either the node.js
implementation of node-api or any implementation of node-api outside of node.js.
is a separate file that contains the node.js specific entry point
to the addon and which instantiates the addon by calling into  when the
addon is loaded into a node.js environment.
environment life cycle apis
of the  defines the concept
of an "agent" as a self-contained environment in which javascript code runs.
multiple such agents may be started and terminated either concurrently or in
sequence by the process.
a node.js environment corresponds to an ecmascript agent. in the main process,
an environment is created at startup, and additional environments can be created
on separate threads to serve as . when node.js is embedded in
another application, the main thread of the application may also construct and
destroy a node.js environment multiple times during the life cycle of the
application process such that each node.js environment created by the
application may, in turn, during its life cycle create and destroy additional
environments as worker threads.
from the perspective of a native addon this means that the bindings it provides
may be called multiple times, from multiple contexts, and even concurrently from
multiple threads.
native addons may need to allocate global state which they use during
their entire life cycle such that the state must be unique to each instance of
the addon.
to this end, node-api provides a way to allocate data such that its life cycle
is tied to the life cycle of the agent.
added in: v12.8.0, v10.20.0
n-api version: 6
: the environment that the node-api call is invoked under.
: the data item to make available to bindings of this instance.
: the function to call when the environment is being torn
down. the function receives  so that it might free it.
provides more details.
: optional hint to pass to the finalize callback during
collection.
returns  if the api succeeded.
this api associates  with the currently running agent.  can later
be retrieved using . any existing data associated with
the currently running agent which was set by means of a previous call to
will be overwritten. if a  was provided
by the previous call, it will not be called.
: the data item that was previously associated with the currently
running agent by a call to .
this api retrieves data that was previously associated with the currently
running agent via . if no data is set, the call will
succeed and  will be set to .
basic node-api data types
node-api exposes the following fundamental datatypes as abstractions that are
consumed by the various apis. these apis should be treated as opaque,
introspectable only with other node-api calls.
added in: v8.0.0
n-api version: 1
integral status code indicating the success or failure of a node-api call.
currently, the following status codes are supported.
if additional information is required upon an api returning a failed status,
it can be obtained by calling .
: utf8-encoded string containing a vm-neutral description of
the error.
: reserved for vm-specific error details. this is currently
not implemented for any vm.
: vm-specific error code. this is currently
: the node-api status code that originated with the last error.
see the  section for additional information.
is used to represent a context that the underlying node-api
implementation can use to persist vm-specific state. this structure is passed
to native functions when they're invoked, and it must be passed back when
making node-api calls. specifically, the same  that was passed in when
the initial native function was called must be passed to any subsequent
nested node-api calls. caching the  for the purpose of general reuse,
and passing the  between instances of the same addon running on
different  threads is not allowed. the  becomes invalid
when an instance of a native addon is unloaded. notification of this event is
delivered through the callbacks given to  and
this is an opaque pointer that is used to represent a javascript value.
added in: v10.6.0
n-api version: 4
this is an opaque pointer that represents a javascript function which can be
called asynchronously from multiple threads via
a value to be given to  to indicate whether
the thread-safe function is to be closed immediately () or
merely released () and thus available for subsequent use via
the call should block whenever the queue associated with the thread-safe
function is full.
node-api memory management types
this is an abstraction used to control and modify the lifetime of objects
created within a particular scope. in general, node-api values are created
within the context of a handle scope. when a native method is called from
javascript, a default handle scope will exist. if the user does not explicitly
create a new handle scope, node-api values will be created in the default handle
scope. for any invocations of code outside the execution of a native method
(for instance, during a libuv callback invocation), the module is required to
create a scope before invoking any functions that can result in the creation
of javascript values.
handle scopes are created using  and are destroyed
using . closing the scope can indicate to the gc
that all s created during the lifetime of the handle scope are no
longer referenced from the current stack frame.
for more details, review the .
escapable handle scopes are a special type of handle scope to return values
created within a particular handle scope to a parent scope.
this is the abstraction to use to reference a . this allows for
users to manage the lifetimes of javascript values, including defining their
minimum lifetimes explicitly.
added in: v14.8.0, v12.19.0
n-api version: 8
a 128-bit value stored as two unsigned 64-bit integers. it serves as a uuid
with which javascript objects can be "tagged" in order to ensure that they are
of a certain type. this is a stronger check than , because
the latter can report a false positive if the object's prototype has been
manipulated. type-tagging is most useful in conjunction with
because it ensures that the pointer retrieved from a wrapped object can be
safely cast to the native type corresponding to the type tag that had been
previously applied to the javascript object.
added in: v14.10.0, v12.19.0
an opaque value returned by . it must be passed
to  when the chain of asynchronous cleanup
events completes.
node-api callback types
opaque datatype that is passed to a callback function. it can be used for
getting additional information about the context in which the callback was
invoked.
function pointer type for user-provided native functions which are to be
exposed to javascript via node-api. callback functions should satisfy the
following signature:
unless for reasons discussed in , creating a
handle and/or callback scope inside a  is not necessary.
function pointer type for add-on provided functions that allow the user to be
notified when externally-owned data is ready to be cleaned up because the
object with which it was associated with, has been garbage-collected. the user
must provide a function satisfying the following signature which would get
called upon the object's collection. currently,  can be used for
finding out when objects that have external data are collected.
handle and/or callback scope inside the function body is not necessary.
function pointer used with functions that support asynchronous
operations. callback functions must satisfy the following signature:
implementations of this function must avoid making node-api calls that execute
javascript or interact with javascript objects. node-api calls should be in the
instead. do not use the  parameter as
it will likely result in execution of javascript.
function pointer used with asynchronous thread-safe function calls. the callback
will be called on the main thread. its purpose is to use a data item arriving
via the queue from one of the secondary threads to construct the parameters
necessary for a call into javascript, usually via , and then
make the call into javascript.
the data arriving from the secondary thread via the queue is given in the
parameter and the javascript function to call is given in the
parameter.
node-api sets up the environment prior to calling this callback, so it is
sufficient to call the javascript function via  rather than
via .
callback functions must satisfy the following signature:
: the environment to use for api calls, or  if the thread-safe
function is being torn down and  may need to be freed.
: the javascript function to call, or  if the
thread-safe function is being torn down and  may need to be freed. it
may also be  if the thread-safe function was created without
: the optional data with which the thread-safe function was
created.
: data created by the secondary thread. it is the responsibility of
the callback to convert this native data to javascript values (with node-api
functions) that can be passed as parameters when  is invoked.
this pointer is managed entirely by the threads and this callback. thus this
callback should free the data.
function pointer used with . it will be called
when the environment is being torn down.
: the handle that must be passed to
after completion of the asynchronous
cleanup.
: the data that was passed to .
the body of the function should initiate the asynchronous cleanup actions at the
end of which  must be passed in a call to
node-api uses both return values and javascript exceptions for error handling.
the following sections explain the approach for each case.
return values
all of the node-api functions share the same error handling pattern. the
return type of all api functions is .
the return value will be  if the request was successful and
no uncaught javascript exception was thrown. if an error occurred and
an exception was thrown, the  value for the error
will be returned. if an exception was thrown, and no error occurred,
will be returned.
in cases where a return value other than  or
is returned,
must be called to check if an exception is pending.
see the section on exceptions for more details.
the full set of possible  values is defined
in .
the  return value provides a vm-independent representation of
the error which occurred. in some cases it is useful to be able to get
more detailed information, including a string representing the error as well as
vm (engine)-specific information.
in order to retrieve this information
is provided which returns a  structure.
the format of the  structure is as follows:
: textual representation of the error that occurred.
: opaque handle reserved for engine use only.
: vm specific error code.
: node-api status code for the last error.
returns the information for the last
node-api call that was made.
do not rely on the content or format of any of the extended information as it
is not subject to semver and may change at any time. it is intended only for
logging purposes.
: the environment that the api is invoked under.
: the  structure with more
information about the error.
this api retrieves a  structure with information
about the last error that occurred.
the content of the  returned is only valid up until
a node-api function is called on the same . this includes a call to
so it may often be necessary to make a copy
of the information so that it can be used later. the pointer returned
in  points to a statically-defined string so it is safe to use
that pointer if you have copied it out of the  field (which will
be overwritten) before another node-api function was called.
this api can be called even if there is a pending javascript exception.
exceptions
any node-api function call may result in a pending javascript exception. this is
the case for any of the api functions, even those that may not cause the
execution of javascript.
if the  returned by a function is  then no
exception is pending and no additional action is required. if the
returned is anything other than  or
, in order to try to recover and continue
instead of simply returning immediately,
must be called in order to determine if an exception is pending or not.
in many cases when a node-api function is called and an exception is
already pending, the function will return immediately with a
of . however, this is not the case
for all functions. node-api allows a subset of the functions to be
called to allow for some minimal cleanup before returning to javascript.
in that case,  will reflect the status for the function. it
will not reflect previous pending exceptions. to avoid confusion, check
the error status after every function call.
when an exception is pending one of two approaches can be employed.
the first approach is to do any appropriate cleanup and then return so that
execution will return to javascript. as part of the transition back to
javascript, the exception will be thrown at the point in the javascript
code where the native method was invoked. the behavior of most node-api calls
is unspecified while an exception is pending, and many will simply return
, so do as little as possible and then return to
javascript where the exception can be handled.
the second approach is to try to handle the exception. there will be cases
where the native code can catch the exception, take the appropriate action,
and then continue. this is only recommended in specific cases
where it is known that the exception can be safely handled. in these
cases  can be used to get and
clear the exception. on success, result will contain the handle to
the last javascript  thrown. if it is determined, after
retrieving the exception, the exception cannot be handled after all
it can be re-thrown it with  where error is the
javascript value to be thrown.
the following utility functions are also available in case native code
needs to throw an exception or determine if a  is an instance
of a javascript  object: ,
, ,  and .
the following utility functions are also available in case native
code needs to create an  object: ,
,  and ,
where result is the  that refers to the newly created
javascript  object.
the node.js project is adding error codes to all of the errors
generated internally. the goal is for applications to use these
error codes for all error checking. the associated error messages
will remain, but will only be meant to be used for logging and
display with the expectation that the message can change without
semver applying. in order to support this model with node-api, both
in internal functionality and for module specific functionality
(as its good practice), the  and  functions
take an optional code parameter which is the string for the code
to be added to the error object. if the optional parameter is
then no code will be associated with the error. if a code is provided,
the name associated with the error is also updated to be:
where  is the original name associated with the error
and  is the code that was provided. for example, if the code
is  and a  is being created the name will be:
: the javascript value to be thrown.
this api throws the javascript value provided.
: optional error code to be set on the error.
: c string representing the text to be associated with the error.
this api throws a javascript  with the text provided.
: the  to be checked.
: boolean value that is set to true if  represents
an error, false otherwise.
this api queries a  to check if it represents an error object.
: optional  with the string for the error code to be
associated with the error.
:  that references a javascript  to be used as
the message for the .
:  representing the error created.
this api returns a javascript  with the text provided.
: the exception if one is pending,  otherwise.
: boolean value that is set to true if an exception is pending.
added in: v9.10.0
n-api version: 3
: the error that is passed to .
trigger an  in javascript. useful if an async
callback throws an exception with no way to recover.
fatal errors
in the event of an unrecoverable error in a native addon, a fatal error can be
thrown to immediately terminate the process.
: optional location at which the error occurred.
: the length of the location in bytes, or
if it is null-terminated.
: the message associated with the error.
: the length of the message in bytes, or
the function call does not return, the process will be terminated.
object lifetime management
as node-api calls are made, handles to objects in the heap for the underlying
vm may be returned as . these handles must hold the
objects 'live' until they are no longer required by the native code,
otherwise the objects could be collected before the native code was
finished using them.
as object handles are returned they are associated with a
'scope'. the lifespan for the default scope is tied to the lifespan
of the native method call. the result is that, by default, handles
remain valid and the objects associated with these handles will be
held live for the lifespan of the native method call.
in many cases, however, it is necessary that the handles remain valid for
either a shorter or longer lifespan than that of the native method.
the sections which follow describe the node-api functions that can be used
to change the handle lifespan from the default.
making handle lifespan shorter than that of the native method
it is often necessary to make the lifespan of handles shorter than
the lifespan of a native method. for example, consider a native method
that has a loop which iterates through the elements in a large array:
this would result in a large number of handles being created, consuming
substantial resources. in addition, even though the native code could only
use the most recent handle, all of the associated objects would also be
kept alive since they all share the same scope.
to handle this case, node-api provides the ability to establish a new 'scope' to
which newly created handles will be associated. once those handles
are no longer required, the scope can be 'closed' and any handles associated
with the scope are invalidated. the methods available to open/close scopes are
node-api only supports a single nested hierarchy of scopes. there is only one
active scope at any time, and all new handles will be associated with that
scope while it is active. scopes must be closed in the reverse order from
which they are opened. in addition, all scopes created within a native method
must be closed before returning from that method.
taking the earlier example, adding calls to  and
would ensure that at most a single handle
is valid throughout the execution of the loop:
when nesting scopes, there are cases where a handle from an
inner scope needs to live beyond the lifespan of that scope. node-api supports
an 'escapable scope' in order to support this case. an escapable scope
allows one handle to be 'promoted' so that it 'escapes' the
current scope and the lifespan of the handle changes from the current
scope to that of the outer scope.
the methods available to open/close escapable scopes are
the request to promote a handle is made through  which
can only be called once.
:  representing the new scope.
this api opens a new scope.
:  representing the scope to be closed.
this api closes the scope passed in. scopes must be closed in the
reverse order from which they were created.
this api opens a new scope from which one object can be promoted
to the outer scope.
:  representing the current scope.
:  representing the javascript  to be
escaped.
:  representing the handle to the escaped
in the outer scope.
this api promotes the handle to the javascript object so that it is valid
for the lifetime of the outer scope. it can only be called once per scope.
if it is called more than once an error will be returned.
references to objects with a lifespan longer than that of the native method
in some cases an addon will need to be able to create and reference objects
with a lifespan longer than that of a single native method invocation. for
example, to create a constructor and later use that constructor
in a request to creates instances, it must be possible to reference
the constructor object across many different instance creation requests. this
would not be possible with a normal handle returned as a  as
described in the earlier section. the lifespan of a normal handle is
managed by scopes and all scopes must be closed before the end of a native
node-api provides methods to create persistent references to an object.
each persistent reference has an associated count with a value of 0
or higher. the count determines if the reference will keep
the corresponding object live. references with a count of 0 do not
prevent the object from being collected and are often called 'weak'
references. any count greater than 0 will prevent the object
from being collected.
references can be created with an initial reference count. the count can
then be modified through  and
. if an object is collected while the count
for a reference is 0, all subsequent calls to
get the object associated with the reference
will return  for the returned . an attempt to call
for a reference whose object has been collected
results in an error.
references must be deleted once they are no longer required by the addon. when
a reference is deleted, it will no longer prevent the corresponding object from
being collected. failure to delete a persistent reference results in
a 'memory leak' with both the native memory for the persistent reference and
the corresponding object on the heap being retained forever.
there can be multiple persistent references created which refer to the same
object, each of which will either keep the object live or not based on its
individual count. multiple persistent references to the same object
can result in unexpectedly keeping alive native memory. the native structures
for a persistent reference must be kept alive until finalizers for the
referenced object are executed. if a new persistent reference is created
for the same object, the finalizers for that object will not be
run and the native memory pointed by the earlier persistent reference
will not be freed. this can be avoided by calling
in addition to  when possible.
:  representing the  to which we want a
: initial reference count for the new reference.
:  pointing to the new reference.
this api creates a new reference with the specified reference count
to the  passed in.
:  to be deleted.
this api deletes the reference passed in.
:  for which the reference count will be incremented.
: the new reference count.
this api increments the reference count for the reference
passed in and returns the resulting reference count.
:  for which the reference count will be decremented.
this api decrements the reference count for the reference
the  in or out of these methods is a handle to the
object to which the reference is related.
:  for which we requesting the corresponding .
: the  for the  referenced by the
if still valid, this api returns the  representing the
javascript  associated with the . otherwise, result
cleanup on exit of the current node.js instance
while a node.js process typically releases all its resources when exiting,
embedders of node.js, or future worker support, may require addons to register
clean-up hooks that will be run once the current node.js instance exits.
node-api provides functions for registering and un-registering such callbacks.
when those callbacks are run, all resources that are being held by the addon
should be freed up.
added in: v10.2.0
registers  as a function to be run with the  parameter once the
current node.js environment exits.
a function can safely be specified multiple times with different
values. in that case, it will be called multiple times as well.
providing the same  and  values multiple times is not allowed
and will lead the process to abort.
the hooks will be called in reverse order, i.e. the most recently added one
will be called first.
removing this hook can be done by using .
typically, that happens when the resource for which this hook was added
is being torn down anyway.
for asynchronous cleanup,  is available.
unregisters  as a function to be run with the  parameter once the
current node.js environment exits. both the argument and the function value
need to be exact matches.
the function must have originally been registered
with , otherwise the process will abort.
: the function pointer to call at environment teardown.
: the pointer to pass to  when it gets called.
: optional handle that refers to the asynchronous cleanup
hook.
registers , which is a function of type , as
a function to be run with the  and  parameters once the
unlike , the hook is allowed to be asynchronous.
otherwise, behavior generally matches that of .
if  is not , an opaque value will be stored in it
that must later be passed to ,
regardless of whether the hook has already been invoked.
: the handle to an asynchronous cleanup hook that was
created with .
unregisters the cleanup hook corresponding to . this will prevent
the hook from being executed, unless it has already started executing.
this must be called on any  value obtained
from .
module registration
node-api modules are registered in a manner similar to other modules
except that instead of using the  macro the following
is used:
the next difference is the signature for the  method. for a node-api
module it is as follows:
the return value from  is treated as the  object for the module.
the  method is passed an empty object via the  parameter as a
convenience. if  returns , the parameter passed as  is
exported by the module. node-api modules cannot modify the  object but
can specify anything as the  property of the module.
to add the method  as a function so that it can be called as a method
provided by the addon:
to set a function to be returned by the  for the addon:
to define a class so that new instances can be created (often used with
you can also use the  macro, which acts as a shorthand
for  and defining an  function:
all node-api addons are context-aware, meaning they may be loaded multiple
times. there are a few design considerations when declaring such a module.
the documentation on  provides more details.
the variables  and  will be available inside the function body
following the macro invocation.
for more details on setting properties on objects, see the section on
for more details on building addon modules in general, refer to the existing
working with javascript values
node-api exposes a set of apis to create all types of javascript values.
some of these types are documented under
of the .
fundamentally, these apis are used to do one of the following:
create a new javascript object
convert from a primitive c type to a node-api value
convert from node-api value to a primitive c type
get global instances including  and
node-api values are represented by the type .
any node-api call that requires a javascript value takes in a .
in some cases, the api does check the type of the  up-front.
however, for better performance, it's better for the caller to make sure that
the  in question is of the javascript type expected by the api.
enum types
added in: v13.7.0, v12.17.0, v10.20.0
describes the  filter enums:
limits the range of collected properties.
limits the collected properties to the given
object only.  will include all keys
of the objects's prototype chain as well.
property filter bits. they can be or'ed to build a composite filter.
will convert integer indices to
strings.  will return numbers for integer
indices.
describes the type of a . this generally corresponds to the types
described in  of the ecmascript language specification.
in addition to types in that section,  can also represent
s and s with external data.
a javascript value of type  appears in javascript as a plain
object such that no properties can be set on it, and no prototype.
this represents the underlying binary scalar datatype of the .
elements of this enum correspond to
object creation functions
: a  representing a javascript .
this api returns a node-api value corresponding to a javascript  type.
javascript arrays are described in
of the ecmascript language specification.
: the initial length of the .
the 's length property is set to the passed-in length parameter.
however, the underlying buffer is not guaranteed to be pre-allocated by the vm
when the array is created. that behavior is left to the underlying vm
implementation. if the buffer must be a contiguous block of memory that can be
directly read and/or written via c, consider using
: the length in bytes of the array buffer to create.
: pointer to the underlying byte buffer of the .
can optionally be ignored by passing .
this api returns a node-api value corresponding to a javascript .
s are used to represent fixed-length binary data buffers. they are
normally used as a backing-buffer for  objects.
the  allocated will have an underlying byte buffer whose size is
determined by the  parameter that's passed in.
the underlying buffer is optionally returned back to the caller in case the
caller wants to directly manipulate the buffer. this buffer can only be
written to directly from native code. to write to this buffer from javascript,
a typed array or  object would need to be created.
javascript  objects are described in
: size in bytes of the underlying buffer.
: raw pointer to the underlying buffer.
: a  representing a .
this api allocates a  object. while this is still a
fully-supported data structure, in most cases using a  will suffice.
: size in bytes of the input buffer (should be the same as the size
of the new buffer).
: raw pointer to the underlying buffer to copy from.
: pointer to the new 's underlying data buffer.
this api allocates a  object and initializes it with data copied
from the passed-in buffer. while this is still a fully-supported data
structure, in most cases using a  will suffice.
added in: v11.11.0, v10.17.0
n-api version: 5
: ecmascript time value in milliseconds since 01 january, 1970 utc.
this api does not observe leap seconds; they are ignored, as
ecmascript aligns with posix time specification.
this api allocates a javascript  object.
: raw pointer to the external data.
: optional callback to call when the external value is being
collected.  provides more details.
: a  representing an external value.
this api allocates a javascript value with external data attached to it. this
is used to pass external data through javascript code, so it can be retrieved
later by native code using .
the api adds a  callback which will be called when the javascript
object just created is ready for garbage collection. it is similar to
except that:
the native data cannot be retrieved later using ,
nor can it be removed later using , and
the object created by the api can be used with .
the created value is not an object, and therefore does not support additional
properties. it is considered a distinct value type: calling  with
an external value yields .
: pointer to the underlying byte buffer of the
: the length in bytes of the underlying buffer.
: optional callback to call when the  is being
the underlying byte buffer of the  is externally allocated and
managed. the caller must ensure that the byte buffer remains valid until the
finalize callback is called.
javascript s are described in
: size in bytes of the input buffer (should be the same as the
size of the new buffer).
: raw pointer to the underlying buffer to expose to javascript.
this api allocates a  object and initializes it with data
backed by the passed in buffer. while this is still a fully-supported data
for node.js >=4  are s.
this api allocates a default javascript .
it is the equivalent of doing  in javascript.
the javascript  type is described in  of the
ecmascript language specification.
: optional  which refers to a javascript
to be set as the description for the symbol.
this api creates a javascript  value from a utf8-encoded c string.
the javascript  type is described in
added in: v17.5.0
: utf-8 c string representing the text to be used as the
description for the symbol.
: the length of the description string in bytes, or
this api searches in the global registry for an existing symbol with the given
description. if the symbol already exists it will be returned, otherwise a new
symbol will be created in the registry.
the javascript  type is described in  of the ecmascript
language specification.
: scalar datatype of the elements within the .
: number of elements in the .
:  underlying the typed array.
: the byte offset within the  from which to
start projecting the .
this api creates a javascript  object over an existing
.  objects provide an array-like view over an
underlying data buffer where each element has the same underlying binary scalar
datatype.
it's required that  should
be <= the size in bytes of the array passed in. if not, a  exception
is raised.
added in: v8.3.0
:  underlying the .
this api creates a javascript  object over an existing .
objects provide an array-like view over an underlying data buffer,
but one which allows items of different size and type in the .
it is required that  is less than or equal to the
size in bytes of the array passed in. if not, a  exception is
raised.
functions to convert from c types to node-api
added in: v8.4.0
: integer value to be represented in javascript.
this api is used to convert from the c  type to the javascript
type.
: unsigned integer value to be represented in javascript.
of the ecmascript language specification. note the complete range of
cannot be represented with full precision in javascript. integer values
outside the range of   -
will lose precision.
: double-precision value to be represented in javascript.
added in: v10.7.0
this api converts the c  type to the javascript  type.
: determines if the resulting  will be positive or
negative.
: the length of the  array.
: an array of  little-endian 64-bit words.
this api converts an array of unsigned 64-bit words into a single
value.
the resulting  is calculated as: (–1) (
× (264)0 +  × (264)1 + …)
: character buffer representing an iso-8859-1-encoded string.
: the length of the string in bytes, or  if it
is null-terminated.
this api creates a javascript  value from an iso-8859-1-encoded c
string. the native string is copied.
: character buffer representing a utf16-le-encoded string.
: the length of the string in two-byte code units, or
this api creates a javascript  value from a utf16-le-encoded c string.
the native string is copied.
: character buffer representing a utf8-encoded string.
functions to convert from node-api to c types
:  representing the javascript  whose length is
being queried.
:  representing length of the array.
this api returns the length of an array.
length is described in  of the ecmascript language
specification.
:  representing the  being queried.
: the underlying data buffer of the . if byte_length
is , this may be  or any other pointer value.
: length in bytes of the underlying data buffer.
this api is used to retrieve the underlying data buffer of an  and
its length.
warning: use caution while using this api. the lifetime of the underlying data
buffer is managed by the  even after it's returned. a
possible safe way to use this api is in conjunction with
, which can be used to guarantee control over the
lifetime of the . it's also safe to use the returned data buffer
within the same callback as long as there are no calls to other apis that might
trigger a gc.
: the underlying data buffer of the .
if length is , this may be  or any other pointer value.
this api is used to retrieve the underlying data buffer of a
and its length.
warning: use caution while using this api since the underlying data buffer's
lifetime is not guaranteed if it's managed by the vm.
:  representing javascript  whose prototype
to return. this returns the equivalent of  (which is
not the same as the function's  property).
:  representing prototype of the given object.
:  representing the  whose
properties to query.
: the number of elements in the .
: the data buffer underlying the  adjusted by
the  value so that it points to the first element in the
. if the length of the array is , this may be  or
any other pointer value.
: the  underlying the .
: the byte offset within the underlying native array
at which the first element of the arrays is located. the value for the data
parameter has already been adjusted so that data points to the first element
in the array. therefore, the first byte of the native array would be at
this api returns various properties of a typed array.
any of the out parameters may be  if that property is unneeded.
warning: use caution while using this api since the underlying data buffer
is managed by the vm.
: number of bytes in the .
: the data buffer underlying the .
if byte_length is , this may be  or any other pointer value.
: the byte offset within the data buffer from which
to start projecting the .
this api returns various properties of a .
:  representing a javascript .
: time value as a  represented as milliseconds since
midnight at the beginning of 01 january, 1970 utc.
returns  if the api succeeded. if a non-date  is passed
in it returns .
this api returns the c double primitive of time value for the given javascript
:  representing javascript .
: c boolean primitive equivalent of the given javascript
returns  if the api succeeded. if a non-boolean  is
passed in it returns .
this api returns the c boolean primitive equivalent of the given javascript
: c double primitive equivalent of the given javascript
returns  if the api succeeded. if a non-number  is passed
this api returns the c double primitive equivalent of the given javascript
: the environment that the api is invoked under
: c  primitive equivalent of the given javascript
: indicates whether the  value was converted
losslessly.
returns  if the api succeeded. if a non- is passed in it
this api returns the c  primitive equivalent of the given javascript
. if needed it will truncate the value, setting  to .
: integer representing if the javascript  is positive
or negative.
: must be initialized to the length of the
array. upon return, it will be set to the actual number of words that
would be needed to store this .
: pointer to a pre-allocated 64-bit word array.
this api converts a single  value into a sign bit, 64-bit little-endian
array, and the number of elements in the array.  and  may be
both set to , in order to get only .
:  representing javascript external value.
: pointer to the data wrapped by the javascript external value.
returns  if the api succeeded. if a non-external  is
this api retrieves the external data pointer that was previously passed to
returns  if the api succeeded. if a non-number
is passed in .
this api returns the c  primitive equivalent
of the given javascript .
if the number exceeds the range of the 32 bit integer, then the result is
truncated to the equivalent of the bottom 32 bits. this can result in a large
positive number becoming a negative number if the value is > 231 - 1.
non-finite number values (, , or ) set the
result to zero.
is passed in it returns .
values outside the range of
-   will lose
precision.
:  representing javascript string.
: buffer to write the iso-8859-1-encoded string into. if  is
passed in, the length of the string in bytes and excluding the null terminator
is returned in .
: size of the destination buffer. when this value is
insufficient, the returned string is truncated and null-terminated.
: number of bytes copied into the buffer, excluding the null
terminator.
returns  if the api succeeded. if a non-
this api returns the iso-8859-1-encoded string corresponding the value passed
: buffer to write the utf8-encoded string into. if  is passed
in, the length of the string in bytes and excluding the null terminator is
returned in .
this api returns the utf8-encoded string corresponding the value passed in.
: buffer to write the utf16-le-encoded string into. if  is
passed in, the length of the string in 2-byte code units and excluding the
null terminator is returned.
: number of 2-byte code units copied into the buffer, excluding
the null terminator.
this api returns the utf16-encoded string corresponding the value passed in.
: c primitive equivalent of the given  as a
this api returns the c primitive equivalent of the given  as a
functions to get global instances
: the value of the boolean to retrieve.
:  representing javascript  singleton to
retrieve.
this api is used to return the javascript singleton object that is used to
represent the given boolean value.
:  representing javascript  object.
this api returns the  object.
:  representing javascript undefined value.
this api returns the undefined object.
working with javascript values and abstract operations
node-api exposes a set of apis to perform some abstract operations on javascript
values. some of these operations are documented under
these apis support doing one of the following:
coerce javascript values to specific javascript types (such as  or
check the type of a javascript value.
check for equality between two javascript values.
: the javascript value to coerce.
:  representing the coerced javascript .
this api implements the abstract operation  as defined in
this function potentially runs js code if the passed-in value is an
object.
: the javascript value whose type to query.
: the type of the javascript value.
if the type of  is not a known ecmascript type and
is not an external value.
this api represents behavior similar to invoking the  operator on
the object as defined in  of the ecmascript language
specification. however, there are some differences:
it has support for detecting an external value.
it detects  as a separate type, while ecmascript  would detect
if  has a type that is invalid, an error is returned.
: the javascript value to check.
: the javascript function object of the constructor function
to check against.
: boolean that is set to true if
is true.
this api represents invoking the  operator on the object as
defined in  of the ecmascript language specification.
: whether the given object is an array.
this api represents invoking the  operation on the object
as defined in  of the ecmascript language specification.
: whether the given object is an .
this api checks if the  passed in is an array buffer.
: whether the given  represents a
this api checks if the  passed in is a buffer.
: whether the given  represents a javascript
this api checks if the  passed in is a date.
: whether the given  represents an  object.
this api checks if the  passed in is an .
: whether the given  represents a .
this api checks if the  passed in is a typed array.
this api checks if the  passed in is a .
: the javascript value to check against.
: whether the two  objects are equal.
this api represents the invocation of the strict equality algorithm as
added in: v13.0.0, v12.16.0, v10.22.0
n-api version: 7
: the javascript  to be detached.
returns  if the api succeeded. if a non-detachable  is
generally, an  is non-detachable if it has been detached before.
the engine may impose additional conditions on whether an  is
detachable. for example, v8 requires that the  be external,
that is, created with .
this api represents the invocation of the  detach operation as
added in: v13.3.0, v12.16.0, v10.22.0
: the javascript  to be checked.
: whether the  is detached.
the  is considered detached if its internal data is .
this api represents the invocation of the
operation as defined in  of the ecmascript language
working with javascript properties
node-api exposes a set of apis to get and set properties on javascript
objects. some of these types are documented under  of the
properties in javascript are represented as a tuple of a key and a value.
fundamentally, all property keys in node-api can be represented in one of the
following forms:
named: a simple utf8-encoded string
integer-indexed: an index value represented by
javascript value: these are represented in node-api by . this can
be a  representing a , , or .
however, it's the caller's responsibility to make sure that the
in question is of the javascript type expected by the api.
the apis documented in this section provide a simple interface to
get and set properties on arbitrary javascript objects represented by
for instance, consider the following javascript code snippet:
the equivalent can be done using node-api values with the following snippet:
indexed properties can be set in a similar manner. consider the following
javascript snippet:
properties can be retrieved using the apis described in this section.
consider the following javascript snippet:
the following is the approximate equivalent of the node-api counterpart:
finally, multiple properties can also be defined on an object for performance
reasons. consider the following javascript:
structures
are flags used to control the behavior of properties
set on a javascript object. other than  they correspond to the
attributes listed in
they can be one or more of the following bitflags:
: no explicit attributes are set on the property. by default, a
property is read only, not enumerable and not configurable.
: the property is writable.
: the property is enumerable.
: the property is configurable as defined in
: the property will be defined as a static property on a class as
opposed to an instance property, which is the default. this is used only by
. it is ignored by .
: like a method in a js class, the property is
configurable and writeable, but not enumerable.
: like a property set via assignment in javascript,
the property is writable, enumerable, and configurable.
: optional string describing the key for the property,
encoded as utf8. one of  or  must be provided for the
property.
: optional  that points to a javascript string or symbol
to be used as the key for the property. one of  or  must
be provided for the property.
: the value that's retrieved by a get access of the property if the
property is a data property. if this is passed in, set , ,
and  to  (since these members won't be used).
: a function to call when a get access of the property is performed.
if this is passed in, set  and  to  (since these members
won't be used). the given function is called implicitly by the runtime when
the property is accessed from javascript code (or if a get on the property is
performed using a node-api call).  provides more details.
: a function to call when a set access of the property is performed.
the property is set from javascript code (or if a set on the property is
: set this to make the property descriptor object's
property to be a javascript function represented by . if this is
passed in, set ,  and  to  (since these members
won't be used).  provides more details.
: the attributes associated with the particular property. see
: the callback data passed into ,  and  if this
function is invoked.
: the object from which to retrieve the properties.
: a  representing an array of javascript values
that represent the property names of the object. the api can be used to
iterate over  using
this api returns the names of the enumerable properties of  as an array
of strings. the properties of  whose key is a symbol will not be
included.
: whether to retrieve prototype properties as well.
: which properties to retrieve
(enumerable/readable/writable).
: whether to convert numbered property keys to strings.
that represent the property names of the object.
and  can be used to iterate over .
this api returns an array containing the names of the available properties
of this object.
: the object on which to set the property.
: the name of the property to set.
: the property value.
this api set a property on the  passed in.
: the object from which to retrieve the property.
: the name of the property to retrieve.
: the value of the property.
this api gets the requested property from the  passed in.
: the object to query.
: the name of the property whose existence to check.
: whether the property exists on the object or not.
this api checks if the  passed in has the named property.
: the name of the property to delete.
: whether the property deletion succeeded or not.  can
optionally be ignored by passing .
this api attempts to delete the  own property from .
: the name of the own property whose existence to check.
: whether the own property exists on the object or not.
this api checks if the  passed in has the named own property.  must
be a  or a , or an error will be thrown. node-api will not
perform any conversion between data types.
this method is equivalent to calling  with a
created from the string passed in as .
: the name of the property to get.
: the object from which to set the properties.
: the index of the property to set.
this api sets an element on the  passed in.
: the index of the property to get.
this api gets the element at the requested index.
: the index of the property whose existence to check.
this api returns if the  passed in has an element at the
requested index.
: the index of the property to delete.
: whether the element deletion succeeded or not.  can
this api attempts to delete the specified  from .
: the number of elements in the  array.
: the array of property descriptors.
this method allows the efficient definition of multiple properties on a given
object. the properties are defined using property descriptors (see
). given an array of such property descriptors,
this api will set the properties on the object one at a time, as defined by
(described in  of the ecma-262
specification).
added in: v14.14.0, v12.20.0
: the object to freeze.
this method freezes a given object. this prevents new properties from
being added to it, existing properties from being removed, prevents
changing the enumerability, configurability, or writability of existing
properties, and prevents the values of existing properties from being changed.
it also prevents the object's prototype from being changed. this is described
in  of the
ecma-262 specification.
: the object to seal.
this method seals a given object. this prevents new properties from being
added to it, as well as marking all existing properties as non-configurable.
this is described in
of the ecma-262 specification.
working with javascript functions
node-api provides a set of apis that allow javascript code to
call back into native code. node-apis that support calling back
into native code take in a callback functions represented by
the  type. when the javascript vm calls back to
native code, the  function provided is invoked. the apis
documented in this section allow the callback function to do the
following:
get information about the context in which the callback was invoked.
get the arguments passed into the callback.
return a  back from the callback.
additionally, node-api provides a set of functions which allow calling
javascript functions from native code. one can either call a function
like a regular javascript function call, or as a constructor
any non- data which is passed to this api via the  field of the
items can be associated with  and freed
whenever  is garbage-collected by passing both  and the data to
: the  value passed to the called function.
:  representing the javascript function to be invoked.
: the count of elements in the  array.
: array of  representing javascript values passed in
as arguments to the function.
:  representing the javascript object returned.
this method allows a javascript function object to be called from a native
add-on. this is the primary mechanism of calling back from the add-on's
native code into javascript. for the special case of calling into javascript
after an async operation, see .
a sample use case might look as follows. consider the following javascript
snippet:
then, the above function can be invoked from a native add-on using the
following code:
: optional name of the function encoded as utf8. this is
visible within javascript as the new function object's  property.
: the length of the  in bytes, or  if
it is null-terminated.
: the native function which should be called when this function
object is invoked.  provides more details.
: user-provided data context. this will be passed back into the
function when invoked later.
:  representing the javascript function object for
the newly created function.
this api allows an add-on author to create a function object in native code.
this is the primary mechanism to allow calling into the add-on's native code
from javascript.
the newly created function is not automatically visible from script after this
call. instead, a property must be explicitly set on any object that is visible
to javascript, in order for the function to be accessible from script.
in order to expose a function as part of the
add-on's module exports, set the newly created function on the exports
object. a sample module might look as follows:
given the above code, the add-on can be used from javascript as follows:
the string passed to  is the name of the target in
responsible for creating the  file.
any non- data which is passed to this api via the  parameter can
be associated with the resulting javascript function (which is returned in the
parameter) and freed whenever the function is garbage-collected by
passing both the javascript function and the data to .
javascript s are described in  of the ecmascript
: the callback info passed into the callback function.
: specifies the length of the provided  array and
receives the actual count of arguments.  can
: c array of s to which the arguments will be
copied. if there are more arguments than the provided count, only the
requested number of arguments are copied. if there are fewer arguments
provided than claimed, the rest of  is filled with  values
that represent .  can optionally be ignored by
passing .
: receives the javascript  argument for the call.
: receives the data pointer for the callback.  can
this method is used within a callback function to retrieve details about the
call like the arguments and the  pointer from a given callback info.
: the  of the constructor call.
this api returns the  of the constructor call. if the current
callback is not a constructor call, the result is .
:  representing the javascript function to be invoked
as a constructor.
: array of javascript values as  representing the
arguments to the constructor. if  is zero this parameter may be
omitted by passing in .
:  representing the javascript object returned,
which in this case is the constructed object.
this method is used to instantiate a new javascript value using a given
that represents the constructor for the object. for example,
consider the following snippet:
the following can be approximated in node-api using the following snippet:
object wrap
node-api offers a way to "wrap" c++ classes and instances so that the class
constructor and methods can be called from javascript.
the  api defines a javascript class with constructor,
static properties and methods, and instance properties and methods that
correspond to the c++ class.
when javascript code invokes the constructor, the constructor callback
uses  to wrap a new c++ instance in a javascript object,
then returns the wrapper object.
when javascript code invokes a method or property accessor on the class,
the corresponding  c++ function is invoked. for an instance
callback,  obtains the c++ instance that is the target of
the call.
for wrapped objects it may be difficult to distinguish between a function
called on a class prototype and a function called on an instance of a class.
a common pattern used to address this problem is to save a persistent
reference to the class constructor for later  checks.
the reference must be freed once it is no longer needed.
there are occasions where  is insufficient for ensuring that
a javascript object is a wrapper for a certain native type. this is the case
especially when wrapped javascript objects are passed back into the addon via
static methods rather than as the  value of prototype methods. in such
cases there is a chance that they may be unwrapped incorrectly.
in the above example  is a method that accepts two
arguments. the first is a database handle and the second is a query handle.
internally, it unwraps the first argument and casts the resulting pointer to a
native database handle. it then unwraps the second argument and casts the
resulting pointer to a query handle. if the arguments are passed in the wrong
order, the casts will work, however, there is a good chance that the underlying
database operation will fail, or will even cause an invalid memory access.
to ensure that the pointer retrieved from the first argument is indeed a pointer
to a database handle and, similarly, that the pointer retrieved from the second
argument is indeed a pointer to a query handle, the implementation of
has to perform a type validation. retaining the javascript
class constructor from which the database handle was instantiated and the
constructor from which the query handle was instantiated in s can
help, because  can then be used to ensure that the instances
passed into  are indeed of the correct type.
unfortunately,  does not protect against prototype
manipulation. for example, the prototype of the database handle instance can be
set to the prototype of the constructor for query handle instances. in this
case, the database handle instance can appear as a query handle instance, and it
will pass the  test for a query handle instance, while still
containing a pointer to a database handle.
to this end, node-api provides type-tagging capabilities.
a type tag is a 128-bit integer unique to the addon. node-api provides the
structure for storing a type tag. when such a value is passed
along with a javascript object stored in a  to
, the javascript object will be "marked" with the
type tag. the "mark" is invisible on the javascript side. when a javascript
object arrives into a native binding,  can be used
along with the original type tag to determine whether the javascript object was
previously "marked" with the type tag. this creates a type-checking capability
of a higher fidelity than  can provide, because such type-
tagging survives prototype manipulation and addon unloading/reloading.
continuing the above example, the following skeleton addon implementation
illustrates the use of  and
: name of the javascript constructor function; when wrapping a
c++ class, we recommend for clarity that this name be the same as that of
the c++ class.
: the length of the  in bytes, or
: callback function that handles constructing instances
of the class. when wrapping a c++ class, this method must be a static member
with the  signature. a c++ class constructor cannot be
used.  provides more details.
: optional data to be passed to the constructor callback as
the  property of the callback info.
: number of items in the  array argument.
: array of property descriptors describing static and
instance data properties, accessors, and methods on the class
: a  representing the constructor function for
the class.
defines a javascript class, including:
a javascript constructor function that has the class name. when wrapping a
corresponding c++ class, the callback passed via  can be used to
instantiate a new c++ class instance, which can then be placed inside the
javascript object instance being constructed using .
properties on the constructor function whose implementation can call
corresponding static data properties, accessors, and methods of the c++
class (defined by property descriptors with the  attribute).
properties on the constructor function's  object. when wrapping a
c++ class, non-static data properties, accessors, and methods of the c++
class can be called from the static functions given in the property
descriptors without the  attribute after retrieving the c++ class
instance placed inside the javascript object instance by using
when wrapping a c++ class, the c++ constructor callback passed via
should be a static method on the class that calls the actual class constructor,
then wraps the new c++ instance in a javascript object, and returns the wrapper
object. see  for details.
the javascript constructor function returned from  is
often saved and used later to construct new instances of the class from native
code, and/or to check whether provided values are instances of the class. in
that case, to prevent the function value from being garbage-collected, a
strong persistent reference to it can be created using
, ensuring that the reference count is kept >= 1.
any non- data which is passed to this api via the  parameter or via
the  field of the  array items can be associated
with the resulting javascript constructor (which is returned in the
parameter) and freed whenever the class is garbage-collected by passing both
the javascript function and the data to .
: the javascript object that will be the wrapper for the
native object.
: the native instance that will be wrapped in the
javascript object.
: optional native callback that can be used to free the
native instance when the javascript object is ready for garbage-collection.
: optional contextual hint that is passed to the
finalize callback.
: optional reference to the wrapped object.
wraps a native instance in a javascript object. the native instance can be
retrieved later using .
when javascript code invokes a constructor for a class that was defined using
, the  for the constructor is invoked.
after constructing an instance of the native class, the callback must then call
to wrap the newly constructed instance in the already-created
javascript object that is the  argument to the constructor callback.
(that  object was created from the constructor function's ,
so it already has definitions of all the instance properties and methods.)
typically when wrapping a class instance, a finalize callback should be
provided that simply deletes the native instance that is received as the
argument to the finalize callback.
the optional returned reference is initially a weak reference, meaning it
has a reference count of 0. typically this reference count would be incremented
temporarily during async operations that require the instance to remain valid.
caution: the optional returned reference (if obtained) should be deleted via
only in response to the finalize callback
invocation. if it is deleted before then, then the finalize callback may never
be invoked. therefore, when obtaining a reference a finalize callback is also
required in order to enable correct disposal of the reference.
finalizer callbacks may be deferred, leaving a window where the object has
been garbage collected (and the weak reference is invalid) but the finalizer
hasn't been called yet. when using  on weak
references returned by , you should still handle an empty result.
calling  a second time on an object will return an error. to
associate another native instance with the object, use
first.
: the object associated with the native instance.
: pointer to the wrapped native instance.
retrieves a native instance that was previously wrapped in a javascript
object using .
when javascript code invokes a method or property accessor on the class, the
corresponding  is invoked. if the callback is for an instance
method or accessor, then the  argument to the callback is the wrapper
object; the wrapped c++ instance that is the target of the call can be obtained
then by calling  on the wrapper object.
added in: v8.5.0
retrieves a native instance that was previously wrapped in the javascript
object  using  and removes the wrapping. if a finalize
callback was associated with the wrapping, it will no longer be called when the
javascript object becomes garbage-collected.
: the javascript object to be marked.
: the tag with which the object is to be marked.
associates the value of the  pointer with the javascript object.
can then be used to compare the tag that was
attached to the object with one owned by the addon to ensure that the object
has the right type.
if the object already has an associated type tag, this api will return
: the javascript object whose type tag to examine.
: the tag with which to compare any tag found on the object.
: whether the type tag given matched the type tag on the
object.  is also returned if no type tag was found on the object.
compares the pointer given as  with any that can be found on
. if no tag is found on  or, if a tag is found but it does
not match , then  is set to . if a tag is found and it
matches , then  is set to .
: the javascript object to which the native data will be
attached.
: the native data that will be attached to the javascript
: native callback that will be used to free the
native data when the javascript object is ready for garbage-collection.
: optional reference to the javascript object.
adds a  callback which will be called when the javascript object
in  is ready for garbage collection. this api is similar to
the api can be called multiple times with different data items in order to
attach each of them to the javascript object, and
the object manipulated by the api can be used with .
simple asynchronous operations
addon modules often need to leverage async helpers from libuv as part of their
implementation. this allows them to schedule work to be executed asynchronously
so that their methods can return in advance of the work being completed. this
allows them to avoid blocking overall execution of the node.js application.
node-api provides an abi-stable interface for these
supporting functions which covers the most common asynchronous use cases.
node-api defines the  structure which is used to manage
asynchronous workers. instances are created/deleted with
the  and  callbacks are functions that will be
invoked when the executor is ready to execute and when it completes its
task respectively.
the  function should avoid making any node-api calls
that could result in the execution of javascript or interaction with
javascript objects. most often, any code that needs to make node-api
calls should be made in  callback instead.
avoid using the  parameter in the execute callback as
it will likely execute javascript.
these functions implement the following interfaces:
when these methods are invoked, the  parameter passed will be the
addon-provided  data that was passed into the
call.
once created the async worker can be queued
for execution using the  function:
can be used if the work needs
to be cancelled before the work has started execution.
after calling , the  callback
will be invoked with a status value of .
the work should not be deleted before the
callback invocation, even when it was cancelled.
: an optional object associated with the async work
that will be passed to possible  .
: identifier for the kind of resource that is being
provided for diagnostic information exposed by the  api.
: the native function which should be called to execute the
logic asynchronously. the given function is called from a worker pool thread
and can execute in parallel with the main event loop thread.
: the native function which will be called when the
asynchronous logic is completed or is cancelled. the given function is called
from the main event loop thread.  provides
more details.
execute and complete functions.
:  which is the handle to the newly created
async work.
this api allocates a work object that is used to execute logic asynchronously.
it should be freed using  once the work is no longer
required.
should be a null-terminated, utf-8-encoded string.
the  identifier is provided by the user and should be
representative of the type of async work being performed. it is also recommended
to apply namespacing to the identifier, e.g. by including the module name. see
the  for more information.
: the handle returned by the call to .
this api frees a previously allocated work object.
this api requests that the previously allocated work be scheduled
for execution. once it returns successfully, this api must not be called again
with the same  item or the result will be undefined.
this api cancels queued work if it has not yet
been started. if it has already started executing, it cannot be
cancelled and  will be returned. if successful,
the  callback will be invoked with a status value of
. the work should not be deleted before the
callback invocation, even if it has been successfully cancelled.
custom asynchronous operations
the simple asynchronous work apis above may not be appropriate for every
scenario. when using any other asynchronous mechanism, the following apis
are necessary to ensure an asynchronous operation is properly tracked by
the runtime.
: object associated with the async work
that will be passed to possible   and can be
accessed by .
: the initialized async context.
the  object needs to be kept alive until
to keep  related api acts correctly. in
order to retain abi compatibility with previous versions, s
are not maintaining the strong reference to the  objects to
avoid introducing causing memory leaks. however, if the  is
garbage collected by javascript engine before the  was
destroyed by , calling  related apis
like  and  can cause
problems like loss of async context when using the  api.
in order to retain abi compatibility with previous versions, passing
for  does not result in an error. however, this is not
recommended as this will result poor results with
and  as the resource is
now required by the underlying  implementation in order to provide
the linkage between async callbacks.
: the async context to be destroyed.
: context for the async operation that is
invoking the callback. this should normally be a value previously
obtained from .
for  does not result in an error. however, this results
in incorrect operation of async hooks. potential issues include loss of
async context when using the  api.
arguments to the function. if  is zero this parameter may be
add-on. this api is similar to . however, it is used to call
from native code back into javascript after returning from an async
operation (when there is no other script on the stack). it is a fairly simple
wrapper around .
note it is not necessary to use  from within a
; in that situation the callback's async
context has already been set up, so a direct call to
is sufficient and appropriate. use of the  function
may be required when implementing custom async behavior that does not use
any s or promises scheduled on the microtask queue by
javascript during the callback are ran before returning back to c/c++.
: an object associated with the async work
that will be passed to possible  . this
parameter has been deprecated and is ignored at runtime. use the
parameter in  instead.
: context for the async operation that is invoking the callback.
this should be a value previously obtained from .
: the newly created scope.
there are cases (for example, resolving promises) where it is
necessary to have the equivalent of the scope associated with a callback
in place when making certain node-api calls. if there is no other script on
the stack the  and
functions can be used to open/close
the required scope.
: the scope to be closed.
version management
: a pointer to version information for node.js itself.
this function fills the  struct with the major, minor, and patch
version of node.js that is currently running, and the  field with the
value of .
the returned buffer is statically allocated and does not need to be freed.
: the highest version of node-api supported.
this api returns the highest node-api version supported by the
node.js runtime. node-api is planned to be additive such that
newer releases of node.js may support additional api functions.
in order to allow an addon to use a newer function when running with
versions of node.js that support it, while providing
fallback behavior when running with node.js versions that don't
support it:
call  to determine if the api is available.
if available, dynamically load a pointer to the function using .
use the dynamically loaded pointer to invoke the function.
if the function is not available, provide an alternate implementation
that does not use the function.
memory management
: the change in externally allocated memory that is kept
alive by javascript objects.
: the adjusted value
this function gives v8 an indication of the amount of externally allocated
memory that is kept alive by javascript objects (i.e. a javascript object
that points to its own memory allocated by a native addon). registering
externally allocated memory will trigger global garbage collections more
often than it would otherwise.
promises
node-api provides facilities for creating  objects as described in
of the ecma specification. it implements promises as a pair of
objects. when a promise is created by , a "deferred"
object is created and returned alongside the . the deferred object is
bound to the created  and is the only means to resolve or reject the
using  or . the
deferred object that is created by  is freed by
or . the  object may
be returned to javascript where it can be used in the usual fashion.
for example, to create a promise and pass it to an asynchronous worker:
the above function  would perform its asynchronous
action and then it would resolve or reject the deferred, thereby concluding the
promise and freeing the deferred:
: a newly created deferred object which can later be passed to
or  to resolve resp. reject
the associated promise.
: the javascript promise associated with the deferred object.
this api creates a deferred object and a javascript promise.
: the deferred object whose associated promise to resolve.
: the value with which to resolve the promise.
this api resolves a javascript promise by way of the deferred object
with which it is associated. thus, it can only be used to resolve javascript
promises for which the corresponding deferred object is available. this
effectively means that the promise must have been created using
and the deferred object returned from that call must
have been retained in order to be passed to this api.
the deferred object is freed upon successful completion.
: the value with which to reject the promise.
this api rejects a javascript promise by way of the deferred object
with which it is associated. thus, it can only be used to reject javascript
: the value to examine
: flag indicating whether  is a native promise
object (that is, a promise object created by the underlying engine).
script execution
node-api provides an api for executing a string containing javascript using the
underlying javascript engine.
: a javascript string containing the script to execute.
: the value resulting from having executed the script.
this function executes a string of javascript code and returns its result with
the following caveats:
unlike , this function does not allow the script to access the current
lexical scope, and therefore also does not allow to access the
, meaning that pseudo-globals such as  will not be
available.
the script can access the . function and  declarations
in the script will be added to the  object. variable declarations
made using  and  will be visible globally, but will not be added
to the  object.
the value of  is  within the script.
libuv event loop
node-api provides a function for getting the current event loop associated with
a specific .
added in: v9.3.0, v8.10.0
n-api version: 2
: the current libuv loop instance.
asynchronous thread-safe function calls
javascript functions can normally only be called from a native addon's main
thread. if an addon creates additional threads, then node-api functions that
require a , , or  must not be called from those
threads.
when an addon has additional threads and javascript functions need to be invoked
based on the processing completed by those threads, those threads must
communicate with the addon's main thread so that the main thread can invoke the
javascript function on their behalf. the thread-safe function apis provide an
easy way to do this.
these apis provide the type  as well as apis to
create, destroy, and call objects of this type.
creates a persistent reference to a
that holds a javascript function which can be called from multiple
threads. the calls happen asynchronously. this means that values with which the
javascript callback is to be called will be placed in a queue, and, for each
value in the queue, a call will eventually be made to the javascript function.
upon creation of a  a  callback can be
provided. this callback will be invoked on the main thread when the thread-safe
function is about to be destroyed. it receives the context and the finalize data
given during construction, and provides an opportunity for cleaning up after the
threads e.g. by calling . aside from the main loop thread,
no threads should be using the thread-safe function after the finalize callback
completes.
the  given during the call to  can
be retrieved from any thread with a call to
calling a thread-safe function
can be used for initiating a call into
javascript.  accepts a parameter which controls
whether the api behaves blockingly. if set to , the api
behaves non-blockingly, returning  if the queue was full,
preventing data from being successfully added to the queue. if set to
, the api blocks until space becomes available in the queue.
never blocks if the thread-safe function was
created with a maximum queue size of 0.
should not be called with
from a javascript thread, because, if the queue is full, it may cause the
javascript thread to deadlock.
the actual call into javascript is controlled by the callback given via the
parameter.  is invoked on the main thread once for each
value that was placed into the queue by a successful call to
. if such a callback is not given, a default
callback will be used, and the resulting javascript call will have no arguments.
the  callback receives the javascript function to call as a
in its parameters, as well as the  context pointer used when
creating the , and the next data pointer that was
created by one of the secondary threads. the callback can then use an api such
as  to call into javascript.
the callback may also be invoked with  and  both set to
to indicate that calls into javascript are no longer possible, while items
remain in the queue that may need to be freed. this normally occurs when the
node.js process exits while there is a thread-safe function still active.
it is not necessary to call into javascript via  because
node-api runs  in a context appropriate for callbacks.
reference counting of thread-safe functions
threads can be added to and removed from a  object
during its existence. thus, in addition to specifying an initial number of
threads upon creation,  can be called to
indicate that a new thread will start making use of the thread-safe function.
similarly,  can be called to indicate that an
existing thread will stop making use of the thread-safe function.
objects are destroyed when every thread which uses
the object has called  or has received a
return status of  in response to a call to
. the queue is emptied before the
is destroyed.
should be the last api call made in conjunction with a given
, because after the call completes, there is no
guarantee that the  is still allocated. for the same
reason, do not use a thread-safe function
after receiving a return value of  in response to a call to
. data associated with the
can be freed in its  callback which
was passed to . the parameter
of  marks the initial
number of acquisitions of the thread-safe functions, instead of calling
multiple times at creation.
once the number of threads making use of a  reaches
zero, no further threads can start making use of it by calling
. in fact, all subsequent api calls
associated with it, except , will return an
error value of .
the thread-safe function can be "aborted" by giving a value of
to . this will cause all subsequent apis
associated with the thread-safe function except
to return  even before its
reference count reaches zero. in particular,
will return , thus informing the threads that it is no longer
possible to make asynchronous calls to the thread-safe function. this can be
used as a criterion for terminating the thread. upon receiving a return value
of  from  a thread must not use
the thread-safe function anymore because it is no longer guaranteed to
be allocated.
deciding whether to keep the process running
similarly to libuv handles, thread-safe functions can be "referenced" and
"unreferenced". a "referenced" thread-safe function will cause the event loop on
the thread on which it is created to remain alive until the thread-safe function
is destroyed. in contrast, an "unreferenced" thread-safe function will not
prevent the event loop from exiting. the apis  and
exist for this purpose.
neither does  mark the thread-safe functions as
able to be destroyed nor does  prevent it from
being destroyed.
: an optional javascript function to call from another thread. it
must be provided if  is passed to .
: an optional object associated with the async work that
will be passed to possible  .
: a javascript string to provide an identifier for
the kind of resource that is being provided for diagnostic information exposed
by the  api.
: maximum size of the queue.  for no limit.
: the initial number of acquisitions, i.e. the
initial number of threads, including the main thread, which will be making use
of this function.
: optional data to be passed to .
: optional function to call when the
is being destroyed.
: optional data to attach to the resulting
: optional callback which calls the javascript function in
response to a call on a different thread. this callback will be called on the
main thread. if not given, the javascript function will be called with no
parameters and with  as its  value.
: the asynchronous thread-safe javascript function.
: the thread-safe function for which to retrieve the context.
: the location where to store the context.
this api may be called from any thread which makes use of .
: the asynchronous thread-safe javascript function to invoke.
: data to send into javascript via the callback
provided during the creation of the thread-safe javascript function.
: flag whose value can be either  to
indicate that the call should block if the queue is full or
to indicate that the call should return immediately
with a status of  whenever the queue is full.
this api should not be called with  from a javascript
thread, because, if the queue is full, it may cause the javascript thread to
deadlock.
this api will return  if  was
called with  set to  from any thread. the value is only
added to the queue if the api returns .
: the asynchronous thread-safe javascript function to start making
use of.
a thread should call this api before passing  to any other thread-safe
function apis to indicate that it will be making use of . this prevents
from being destroyed when all other threads have stopped making use of
this api may be called from any thread which will start making use of .
: the asynchronous thread-safe javascript function whose reference
count to decrement.
: flag whose value can be either  to indicate
that the current thread will make no further calls to the thread-safe
function, or  to indicate that in addition to the current
thread, no other thread should make any further calls to the thread-safe
function. if set to , further calls to
will return , and no further
values will be placed in the queue.
a thread should call this api when it stops making use of . passing
to any thread-safe apis after having called this api has undefined results, as
may have been destroyed.
this api may be called from any thread which will stop making use of .
: the thread-safe function to reference.
this api is used to indicate that the event loop running on the main thread
should not exit until  has been destroyed. similar to  it is
also idempotent.
being destroyed.  and
are available for that purpose.
this api may only be called from the main thread.
: the thread-safe function to unreference.
may exit before  is destroyed. similar to  it is also
idempotent.
miscellaneous utilities
added in: v15.9.0, v14.18.0, v12.22.0
: a url containing the absolute path of the
location from which the add-on was loaded. for a file on the local
file system it will start with . the string is null-terminated and
owned by  and must thus not be modified or freed.
may be an empty string if the add-on loading process fails to establish
the add-on's file name during loading.
c++ embedder api
node.js provides a number of c++ apis that can be used to execute javascript
in a node.js environment from other c++ software.
the documentation for these apis can be found in  in the node.js
source tree. in addition to the apis exposed by node.js, some required concepts
are provided by the v8 embedder api.
because using node.js as an embedded library is different from writing code
that is executed by node.js, breaking changes do not follow typical node.js
and may occur on each semver-major release without prior
warning.
example embedding application
the following sections will provide an overview over how to use these apis
to create an application from scratch that will perform the equivalent of
, i.e. that will take a piece of javascript and run it in
a node.js-specific environment.
the full code can be found .
setting up per-process state
node.js requires some per-process state management in order to run:
arguments parsing for node.js ,
v8 per-process requirements, such as a  instance.
the following example shows how these can be set up. some class names are from
the  and  c++ namespaces, respectively.
per-instance state
node.js has a concept of a “node.js instance”, that is commonly being referred
to as . each  is associated with:
exactly one , i.e. one js engine instance,
exactly one , i.e. one event loop, and
a number of s, but exactly one main .
one  instance that contains information that could be
shared by multiple s that use the same .
currently, no testing if performed for this scenario.
in order to set up a , an  needs
to be provided. one possible choice is the default node.js allocator, which
can be created through . using the node.js
allocator allows minor performance optimizations when addons use the node.js
c++  api, and is required in order to track  memory in
additionally, each  that is used for a node.js instance needs to
be registered and unregistered with the  instance, if one
is being used, in order for the platform to know which event loop to use
for tasks scheduled by the .
the  helper function creates a ,
sets it up with some node.js-specific hooks (e.g. the node.js error handler),
and registers it with the platform automatically.
child process
the  module provides the ability to spawn subprocesses in
a manner that is similar, but not identical, to . this capability
is primarily provided by the  function:
by default, pipes for , , and  are established between
the parent node.js process and the spawned subprocess. these pipes have
limited (and platform-specific) capacity. if the subprocess writes to
stdout in excess of that limit without the output being captured, the
subprocess blocks waiting for the pipe buffer to accept more data. this is
identical to the behavior of pipes in the shell. use the
option if the output will not be consumed.
the command lookup is performed using the  environment
variable if  is in the  object. otherwise,  is
used. if  is set without , lookup on unix is performed
on a default search path search of  (see your operating system's
manual for execvpe/execvp), on windows the current processes environment
variable  is used.
on windows, environment variables are case-insensitive. node.js
lexicographically sorts the  keys and uses the first one that
case-insensitively matches. only first (in lexicographic order) entry will be
passed to the subprocess. this might lead to issues on windows when passing
objects to the  option that have multiple variants of the same key, such as
the  method spawns the child process asynchronously,
without blocking the node.js event loop. the
function provides equivalent functionality in a synchronous manner that blocks
the event loop until the spawned process either exits or is terminated.
for convenience, the  module provides a handful of
synchronous and asynchronous alternatives to  and
. each of these alternatives are implemented on
top of  or .
: spawns a shell and runs a command within that
shell, passing the  and  to a callback function when
complete.
: similar to  except
that it spawns the command directly without first spawning a shell by
: spawns a new node.js process and invokes a
specified module with an ipc communication channel established that allows
sending messages between parent and child.
: a synchronous version of
that will block the node.js event loop.
for certain use cases, such as automating shell scripts, the
may be more convenient. in many cases, however,
the synchronous methods can have significant impact on performance due to
stalling the event loop while spawned processes complete.
asynchronous process creation
the , , ,
and  methods all follow the idiomatic asynchronous
programming pattern typical of other node.js apis.
each of the methods returns a  instance. these objects
implement the node.js  api, allowing the parent process to
register listener functions that are called when certain events occur during
the life cycle of the child process.
the  and  methods
additionally allow for an optional  function to be specified that is
invoked when the child process terminates.
spawning  and  files on windows
the importance of the distinction between  and
can vary based on platform. on unix-type
operating systems (unix, linux, macos)  can be
more efficient because it does not spawn a shell by default. on windows,
however,  and  files are not executable on their own without a
terminal, and therefore cannot be launched using .
when running on windows,  and  files can be invoked using
with the  option set, with
, or by spawning  and passing the  or
file as an argument (which is what the  option and
do). in any case, if the script filename contains
spaces it needs to be quoted.
the command to run, with space-separated arguments.
|  current working directory of the child process.
environment key-value pairs. default: .
shell to execute the command with. see
and . default:
on unix,  on windows.
allows aborting the child process using an
abortsignal.
largest amount of data in bytes allowed on stdout or
stderr. if exceeded, the child process is terminated and any output is
truncated. see caveat at .
sets the user identity of the process (see ).
sets the group identity of the process (see ).
hide the subprocess console window that would
normally be created on windows systems. default: .
called with the output when process terminates.
spawns a shell then executes the  within that shell, buffering any
generated output. the  string passed to the exec function is processed
directly by the shell and special characters (vary based on
need to be dealt with accordingly:
never pass unsanitized user input to this function. any input containing shell
metacharacters may be used to trigger arbitrary command execution.
if a  function is provided, it is called with the arguments
. on success,  will be . on error,
will be an instance of . the  property will be
the exit code of the process. by convention, any exit code other than
indicates an error.  will be the signal that terminated the
process.
the  and  arguments passed to the callback will contain the
stdout and stderr output of the child process. by default, node.js will decode
the output as utf-8 and pass strings to the callback. the  option
can be used to specify the character encoding used to decode the stdout and
stderr output. if  is , or an unrecognized character
encoding,  objects will be passed to the callback instead.
if  is greater than , the parent will send the signal
identified by the  property (the default is ) if the
child runs longer than  milliseconds.
unlike the  posix system call,  does not replace
the existing process and uses a shell to execute the command.
if this method is invoked as its ed version, it returns
a  for an  with  and  properties. the returned
instance is attached to the  as a  property. in
case of an error (including any error resulting in an exit code other than 0), a
rejected promise is returned, with the same  object given in the
callback, but with two additional properties  and .
if the  option is enabled, calling  on the corresponding
is similar to calling  on the child process except
the error passed to the callback will be an :
the name or path of the executable file to run.
list of string arguments.
no quoting or escaping of arguments is
done on windows. ignored on unix. default: .
|  if , runs  inside of a shell. uses
on unix, and  on windows. a different
shell can be specified as a string. see  and
. default:  (no shell).
the  function is similar to
except that it does not spawn a shell by default. rather, the specified
executable  is spawned directly as a new process making it slightly more
efficient than .
the same options as  are supported. since a shell is
not spawned, behaviors such as i/o redirection and file globbing are not
supported.
if the  option is enabled, do not pass unsanitized user input to this
function. any input containing shell metacharacters may be used to trigger
arbitrary command execution.
|  the module to run in the child.
prepare child to run independently of its parent
process. specific behavior depends on the platform, see
executable used to create the child process.
list of string arguments passed to the executable.
specify the kind of serialization used for sending
messages between processes. possible values are  and .
see  for more details. default: .
allows closing the child process using an
|  the signal value to be used when the spawned
process will be killed by timeout or abort signal. default: .
if , stdin, stdout, and stderr of the child will be
piped to the parent, otherwise they will be inherited from the parent, see
the  and  options for 's
for more details. default: .
|  see 's .
when this option is provided, it overrides . if the array variant
is used, it must contain exactly one item with value  or an error
will be thrown. for instance .
in milliseconds the maximum amount of time the process
is allowed to run. default: .
the  method is a special case of
used specifically to spawn new node.js processes.
like , a  object is returned. the
returned  will have an additional communication channel
built-in that allows messages to be passed back and forth between the parent and
child. see  for details.
keep in mind that spawned node.js child processes are
independent of the parent with exception of the ipc communication channel
that is established between the two. each process has its own memory, with
their own v8 instances. because of the additional resource allocations
required, spawning a large number of child node.js processes is not
recommended.
by default,  will spawn new node.js instances using the
of the parent process. the  property in the
object allows for an alternative execution path to be used.
node.js processes launched with a custom  will communicate with the
parent process using the file descriptor (fd) identified using the
environment variable  on the child process.
unlike the  posix system call,  does not clone the
current process.
the  option available in  is not supported by
and will be ignored if set.
the command to run.
explicitly set the value of  sent to the child
process. this will be set to  if not specified.
|  child's stdio configuration (see
done on windows. ignored on unix. this is set to  automatically
when  is specified and is cmd. default: .
the  method spawns a new process using the given
, with command-line arguments in . if omitted,  defaults
to an empty array.
a third argument may be used to specify additional options, with these defaults:
use  to specify the working directory from which the process is spawned.
if not given, the default is to inherit the current working directory. if given,
but the path does not exist, the child process emits an  error
and exits immediately.  is also emitted when the command
does not exist.
use  to specify environment variables that will be visible to the new
process, the default is .
values in  will be ignored.
example of running , capturing , , and the
exit code:
example: a very elaborate way to run
example of checking for failed :
certain platforms (macos, linux) will use the value of  for the process
title while others (windows, sunos) will use .
node.js overwrites  with  on startup, so
in a node.js child process will not match the
parameter passed to  from the parent. retrieve it with the
property instead.
added in: v0.7.10
on windows, setting  to  makes it possible for the
child process to continue running after the parent exits. the child will have
its own console window. once enabled for a child process, it cannot be
disabled.
on non-windows platforms, if  is set to , the child
process will be made the leader of a new process group and session. child
processes may continue running after the parent exits regardless of whether
they are detached or not. see  for more information.
by default, the parent will wait for the detached child to exit. to prevent the
parent from waiting for a given  to exit, use the
method. doing so will cause the parent's event loop to not
include the child in its reference count, allowing the parent to exit
independently of the child, unless there is an established ipc channel between
the child and the parent.
when using the  option to start a long-running process, the process
will not stay running in the background after the parent exits unless it is
provided with a  configuration that is not connected to the parent.
if the parent's  is inherited, the child will remain attached to the
controlling terminal.
example of a long-running process, by detaching and also ignoring its parent
file descriptors, in order to ignore the parent's termination:
alternatively one can redirect the child process' output into files:
the  option is used to configure the pipes that are established
between the parent and child process. by default, the child's stdin, stdout,
and stderr are redirected to corresponding ,
, and  streams on the
object. this is equivalent to setting the
equal to .
for convenience,  may be one of the following strings:
: equivalent to  (the default)
: equivalent to
: equivalent to  or
otherwise, the value of  is an array where each index corresponds
to an fd in the child. the fds 0, 1, and 2 correspond to stdin, stdout,
and stderr, respectively. additional fds can be specified to create additional
pipes between the parent and child. the value is one of the following:
: create a pipe between the child process and the parent process.
the parent end of the pipe is exposed to the parent as a property on the
object as . pipes
created for fds 0, 1, and 2 are also available as ,
and , respectively.
these are not actual unix pipes and therefore the child process
can not use them by their descriptor files,
e.g.  or .
: same as  except that the  flag
is set on the handle. this is necessary for overlapped i/o on the child
process's stdio handles. see the
for more details. this is exactly the same as  on non-windows
systems.
: create an ipc channel for passing messages/file descriptors
between parent and child. a  may have at most one ipc
stdio file descriptor. setting this option enables the
method. if the child is a node.js process, the
presence of an ipc channel will enable  and
methods, as well as  and
events within the child.
accessing the ipc channel fd in any way other than
or using the ipc channel with a child process that is not a node.js instance
is not supported.
: instructs node.js to ignore the fd in the child. while node.js
will always open fds 0, 1, and 2 for the processes it spawns, setting the fd
to  will cause node.js to open  and attach it to the
child's fd.
: pass through the corresponding stdio stream to/from the
parent process. in the first three positions, this is equivalent to
, , and , respectively. in
any other position, equivalent to .
object: share a readable or writable stream that refers to a tty,
file, socket, or a pipe with the child process. the stream's underlying
file descriptor is duplicated in the child process to the fd that
corresponds to the index in the  array. the stream must have an
underlying descriptor (file streams do not until the  event has
occurred).
positive integer: the integer value is interpreted as a file descriptor
that is open in the parent process. it is shared with the child
process, similar to how  objects can be shared. passing sockets
is not supported on windows.
, : use default value. for stdio fds 0, 1, and 2 (in other
words, stdin, stdout, and stderr) a pipe is created. for fd 3 and up, the
default is .
it is worth noting that when an ipc channel is established between the
parent and child processes, and the child is a node.js process, the child
is launched with the ipc channel unreferenced (using ) until the
child registers an event handler for the  event
or the  event. this allows the child to exit
normally without the process being held open by the open ipc channel.
on unix-like operating systems, the  method
performs memory operations synchronously before decoupling the event loop
from the child. applications with a large memory footprint may find frequent
calls to be a bottleneck. for more information,
see also:  and .
synchronous process creation
the , , and
methods are synchronous and will block the
node.js event loop, pausing execution of any additional code until the spawned
process exits.
blocking calls like these are mostly useful for simplifying general-purpose
scripting tasks and for simplifying the loading/processing of application
configuration at startup.
|  |  |  the value which will be passed
as stdin to the spawned process. supplying this value will override
|  child's stdio configuration.  by default will
be output to the parent process' stderr unless  is specified.
process will be killed. default: .
stderr. if exceeded, the child process is terminated. see caveat at
the encoding used for all stdio inputs and outputs.
returns:  |  the stdout from the command.
the  method is generally identical to
with the exception that the method will not
return until the child process has fully closed. when a timeout has been
encountered and  is sent, the method won't return until the process
has completely exited.
if the child process intercepts and handles the  signal and
does not exit, the parent process will still wait until the child process has
exited.
if the process times out or has a non-zero exit code, this method will throw an
that will include the full result of the underlying
sets the user identity of the process. (see ).
sets the group identity of the process. (see ).
with the exception that the method will not return
until the child process has fully closed. when a timeout has been encountered
and  is sent, the method won't return until the process has
completely exited. if the child process intercepts and handles the
signal and doesn't exit, the parent process will wait until the child process
has exited.
if the process times out or has a non-zero exit code, this method will throw.
the  object will contain the entire result from
|  child's stdio configuration.
pid of the child process.
array of results from stdio output.
|  the contents of .
|  the exit code of the subprocess, or  if the
subprocess terminated due to a signal.
|  the signal used to kill the subprocess, or  if
the subprocess did not terminate due to a signal.
the error object if the child process failed or timed out.
with the exception that the function will not return
completely exited. if the process intercepts and handles the  signal
and doesn't exit, the parent process will wait until the child process has
added in: v2.2.0
instances of the  represent spawned child processes.
instances of  are not intended to be created directly. rather,
use the , ,
, or  methods to create
instances of .
event:
added in: v0.7.7
the exit code if the child exited on its own.
the signal by which the child process was terminated.
the  event is emitted after a process has ended and the stdio
streams of a child process have been closed. this is distinct from the
event, since multiple processes might share the same stdio
streams. the  event will always emit after  was
already emitted, or  if the child failed to spawn.
added in: v0.7.2
the  event is emitted after calling the
method in parent process or
in child process. after disconnecting it is no longer
possible to send or receive messages, and the
property is .
the  event is emitted whenever:
the process could not be spawned, or
the process could not be killed, or
sending a message to the child process failed.
the  event may or may not fire after an error has occurred. when
listening to both the  and  events, guard
against accidentally invoking handler functions multiple times.
see also  and .
the  event is emitted after the child process ends. if the process
exited,  is the final exit code of the process, otherwise . if the
process terminated due to receipt of a signal,  is the string name of
the signal, otherwise . one of the two will always be non-.
when the  event is triggered, child process stdio streams might still be
open.
node.js establishes signal handlers for  and  and node.js
processes will not terminate immediately due to receipt of those signals.
rather, node.js will perform a sequence of cleanup actions and then will
re-raise the handled signal.
a parsed json object or primitive value.
a  or  object, or
undefined.
the  event is triggered when a child process uses
to send messages.
the message goes through serialization and parsing. the resulting
message might not be the same as what is originally sent.
if the  option was set to  used when spawning the
child process, the  argument can contain data that json is not able
to represent.
see  for more details.
added in: v15.1.0, v14.17.0
the  event is emitted once the child process has spawned successfully.
if the child process does not spawn successfully, the  event is not
emitted and the  event is emitted instead.
if emitted, the  event comes before all other events and before any
data is received via  or .
the  event will fire regardless of whether an error occurs within
the spawned process. for example, if  spawns successfully,
the  event will fire, though  may fail to spawn .
this caveat also applies when using .
a pipe representing the ipc channel to the child process.
the  property is a reference to the child's ipc channel. if
no ipc channel exists, this property is .
added in: v7.1.0
this method makes the ipc channel keep the event loop of the parent process
running if  has been called before.
this method makes the ipc channel not keep the event loop of the parent process
running, and lets it finish even while the channel is open.
set to  after  is called.
the  property indicates whether it is still possible to
send and receive messages from a child process. when  is
, it is no longer possible to send or receive messages.
closes the ipc channel between parent and child, allowing the child to exit
gracefully once there are no other connections keeping it alive. after calling
this method the  and  properties in
both the parent and child (respectively) will be set to , and it will be
no longer possible to pass messages between the processes.
the  event will be emitted when there are no messages in the
process of being received. this will most often be triggered immediately after
calling .
when the child process is a node.js instance (e.g. spawned using
), the  method can be invoked
within the child process to close the ipc channel as well.
the  property indicates the exit code of the child process.
if the child process is still running, the field will be .
the  method sends a signal to the child process. if no
argument is given, the process will be sent the  signal. see
for a list of available signals. this function returns  if
succeeds, and  otherwise.
the  object may emit an  event if the signal
cannot be delivered. sending a signal to a child process that has already exited
is not an error but may have unforeseen consequences. specifically, if the
process identifier (pid) has been reassigned to another process, the signal will
be delivered to that process instead which can have unexpected results.
while the function is called , the signal delivered to the child process
may not actually terminate the process.
see  for reference.
on windows, where posix signals do not exist, the  argument will be
ignored, and the process will be killed forcefully and abruptly (similar to
on linux, child processes of child processes will not be terminated
when attempting to kill their parent. this is likely to happen when running a
new process in a shell or with the use of the  option of :
added in: v0.5.10
set to  after  is used to successfully
send a signal to the child process.
the  property indicates whether the child process
successfully received a signal from . the  property
does not indicate that the child process has been terminated.
returns the process identifier (pid) of the child process. if the child process
fails to spawn due to errors, then the value is  and  is
emitted.
calling  after making a call to  will
restore the removed reference count for the child process, forcing the parent
to wait for the child to exit before exiting itself.
the  argument, if present, is an object used to
parameterize the sending of certain types of handles.  supports
the following properties:
a value that can be used when passing instances of
. when , the socket is kept open in the sending process.
when an ipc channel has been established between the parent and child (
i.e. when using ), the  method can
be used to send messages to the child process. when the child process is a
node.js instance, these messages can be received via the  event.
for example, in the parent script:
and then the child script,  might look like this:
child node.js processes will have a  method of their own
that allows the child to send messages back to the parent.
there is a special case when sending a  message. messages
containing a  prefix in the  property are reserved for use within
node.js core and will not be emitted in the child's
event. rather, such messages are emitted using the
event and are consumed internally by node.js.
applications should avoid using such messages or listening for
events as it is subject to change without notice.
the optional  argument that may be passed to  is
for passing a tcp server or socket object to the child process. the child will
receive the object as the second argument passed to the callback function
registered on the  event. any data that is received
and buffered in the socket will not be sent to the child.
the optional  is a function that is invoked after the message is
sent but before the child may have received it. the function is called with a
single argument:  on success, or an  object on failure.
if no  function is provided and the message cannot be sent, an
event will be emitted by the  object. this can
happen, for instance, when the child process has already exited.
will return  if the channel has closed or when the
backlog of unsent messages exceeds a threshold that makes it unwise to send
more. otherwise, the method returns . the  function can be
used to implement flow control.
example: sending a server object
the  argument can be used, for instance, to pass the handle of
a tcp server object to the child process as illustrated in the example below:
the child would then receive the server object as:
once the server is now shared between the parent and child, some connections
can be handled by the parent and some by the child.
while the example above uses a server created using the  module,
module servers use exactly the same workflow with the exceptions of
listening on a  event instead of  and using
instead of . this is, however, only
supported on unix platforms.
example: sending a socket object
similarly, the  argument can be used to pass the handle of a
socket to the child process. the example below spawns two children that each
handle connections with "normal" or "special" priority:
the  would receive the socket handle as the second argument
passed to the event callback function:
do not use  on a socket that has been passed to a subprocess.
the parent cannot track when the socket is destroyed.
any  handlers in the subprocess should verify that  exists,
as the connection may have been closed during the time it takes to send the
connection to the child.
the  property indicates the signal received by
the child process if any, else .
the  property represents the full list of command-line
arguments the child process was launched with.
the  property indicates the executable file name of
the child process that is launched.
for , its value will be equal to
for , its value will be the name of
the executable file.
for ,  its value will be the name of the shell
in which the child process is launched.
|  |
a  that represents the child process's .
if the child was spawned with  set to anything other than ,
then this will be .
is an alias for . both properties will
refer to the same value.
the  property can be  or
if the child process could not be successfully spawned.
if a child process waits to read all of its input, the child will not continue
until this stream has been closed via .
a sparse array of pipes to the child process, corresponding with positions in
the  option passed to  that have been set
to the value . , , and
are also available as ,
, and , respectively.
in the following example, only the child's fd  (stdout) is configured as a
pipe, so only the parent's  is a stream, all other values
in the array are .
the  property can be  if the child process could
not be successfully spawned.
and unicode
the  option specifies the largest number of bytes allowed on
or . if this value is exceeded, then the child process is terminated.
this impacts output that includes multibyte character encodings such as utf-8 or
utf-16. for instance,  will send 13 utf-8 encoded bytes
to  although there are only 4 characters.
shell requirements
the shell should understand the  switch. if the shell is , it
should understand the  switches and command-line parsing should be
compatible.
default windows shell
although microsoft specifies  must contain the path to
in the root environment, child processes are not always subject to
the same requirement. thus, in  functions where a shell can be
spawned,  is used as a fallback if  is
unavailable.
advanced serialization
added in: v13.2.0, v12.16.0
child processes support a serialization mechanism for ipc that is based on the
, based on the
. this is generally more powerful and
supports more built-in javascript object types, such as ,
and ,  and , , ,  etc.
however, this format is not a full superset of json, and e.g. properties set on
objects of such built-in types will not be passed on through the serialization
step. additionally, performance may not be equivalent to that of json, depending
on the structure of the passed data.
therefore, this feature requires opting in by setting the
option to  when calling
or .
cluster
clusters of node.js processes can be used to run multiple instances of node.js
that can distribute workloads among their application threads. when process
isolation is not needed, use the  module instead, which
allows running multiple application threads within a single node.js instance.
the cluster module allows easy creation of child processes that all share
server ports.
running node.js will now share port 8000 between the workers:
on windows, it is not yet possible to set up a named pipe server in a worker.
how it works
the worker processes are spawned using the  method,
so that they can communicate with the parent via ipc and pass server
handles back and forth.
the cluster module supports two methods of distributing incoming
connections.
the first one (and the default one on all platforms except windows)
is the round-robin approach, where the primary process listens on a
port, accepts new connections and distributes them across the workers
in a round-robin fashion, with some built-in smarts to avoid
overloading a worker process.
the second approach is where the primary process creates the listen
socket and sends it to interested workers. the workers then accept
incoming connections directly.
the second approach should, in theory, give the best performance.
in practice however, distribution tends to be very unbalanced due
to operating system scheduler vagaries. loads have been observed
where over 70% of all connections ended up in just two processes,
out of a total of eight.
because  hands off most of the work to the primary
process, there are three cases where the behavior between a normal
node.js process and a cluster worker differs:
because the message is passed to the primary,
file descriptor 7 in the parent will be listened on, and the
handle passed to the worker, rather than listening to the worker's
idea of what the number 7 file descriptor references.
listening on handles explicitly will cause
the worker to use the supplied handle, rather than talk to the primary
normally, this will cause servers to listen on a
random port. however, in a cluster, each worker will receive the
same "random" port each time they do . in essence, the
port is random the first time, but predictable thereafter. to listen
on a unique port, generate a port number based on the cluster worker id.
node.js does not provide routing logic. it is therefore important to design an
application such that it does not rely too heavily on in-memory data objects for
things like sessions and login.
because workers are all separate processes, they can be killed or
re-spawned depending on a program's needs, without affecting other
workers. as long as there are some workers still alive, the server will
continue to accept connections. if no workers are alive, existing connections
will be dropped and new connections will be refused. node.js does not
automatically manage the number of workers, however. it is the application's
responsibility to manage the worker pool based on its own needs.
although a primary use case for the  module is networking, it can
also be used for other use cases requiring worker processes.
added in: v0.7.0
a  object contains all public information and method about a worker.
in the primary it can be obtained using . in a worker
it can be obtained using .
similar to the  event, but specific to this worker.
added in: v0.7.3
this event is the same as the one provided by .
within a worker,  may also be used.
added in: v0.11.2
the exit code, if it exited normally.
the name of the signal (e.g. ) that caused
the process to be killed.
it is not emitted in the worker.
similar to the  event of , but specific to this worker.
here is an example using the message system. it keeps a count in the primary
process of the number of http requests received by the workers:
in a worker, this function will close all servers, wait for the  event
on those servers, and then disconnect the ipc channel.
in the primary, an internal message is sent to the worker causing it to call
on itself.
causes  to be set.
after a server is closed, it will no longer accept new connections,
but connections may be accepted by any other listening worker. existing
connections will be allowed to close as usual. when no more connections exist,
see , the ipc channel to the worker will close allowing it
to die gracefully.
the above applies only to server connections, client connections are not
automatically closed by workers, and disconnect does not wait for them to close
before exiting.
in a worker,  exists, but it is not this function;
it is .
because long living server connections may block workers from disconnecting, it
may be useful to send a message, so application specific actions may be taken to
close them. it also may be useful to implement a timeout, killing a worker if
the  event has not been emitted after some time.
added in: v6.0.0
this property is  if the worker exited due to .
if the worker exited any other way, it is . if the
worker has not exited, it is .
the boolean  allows distinguishing between
voluntary and accidental exit, the primary may choose not to respawn a worker
based on this value.
added in: v0.8.0
each new worker is given its own unique id, this id is stored in the
while a worker is alive, this is the key that indexes it in
added in: v0.11.14
this function returns  if the worker is connected to its primary via its
ipc channel,  otherwise. a worker is connected to its primary after it
has been created. it is disconnected after the  event is emitted.
this function returns  if the worker's process has terminated (either
because of exiting or being signaled). otherwise, it returns .
added in: v0.9.12
name of the kill signal to send to the worker
process. default:
this function will kill the worker. in the primary worker, it does this by
disconnecting the , and once disconnected, killing with
. in the worker, it does it by killing the process with .
the  function kills the worker process without waiting for a graceful
disconnect, it has the same behavior as .
this method is aliased as  for backwards compatibility.
all workers are created using , the returned object
from this function is stored as . in a worker, the global
is stored.
see: .
workers will call  if the  event occurs
on  and  is not . this protects against
accidental disconnection.
send a message to a worker or primary, optionally with a handle.
in the primary, this sends a message to a specific worker. it is identical to
in a worker, this sends a message to the primary. it is identical to
this example will echo back all messages from the primary:
added in: v0.7.9
emitted after the worker ipc channel has disconnected. this can occur when a
worker exits gracefully, is killed, or is disconnected manually (such as with
there may be a delay between the  and  events. these
events can be used to detect if the process is stuck in a cleanup or if there
are long-living connections.
when any of the workers die the cluster module will emit the  event.
this can be used to restart the worker by calling  again.
when a new worker is forked the cluster module will emit a  event.
this can be used to log worker activity, and create a custom timeout.
after calling  from a worker, when the  event is emitted
on the server, a  event will also be emitted on  in the
primary.
the event handler is executed with two arguments, the  contains the
worker object and the  object contains the following connection
properties: , , and . this is very useful if the
worker is listening on more than one address.
the  is one of:
(tcpv4)
(tcpv6)
(unix domain socket)
or  (udpv4 or udpv6)
emitted when the cluster primary receives a message from any worker.
after forking a new worker, the worker should respond with an online message.
when the primary receives an online message it will emit this event.
the difference between  and  is that fork is emitted when the
primary forks a worker, and  is emitted when the worker is running.
added in: v0.7.1
emitted every time  is called.
the  object is the  object at the time
was called and is advisory only, since multiple calls to
can be made in a single tick.
if accuracy is important, use .
called when all workers are disconnected and handles are
closed.
calls  on each worker in .
when they are disconnected all internal handles will be closed, allowing the
primary process to die gracefully if no other event is waiting.
the method takes an optional callback argument which will be called when
finished.
this can only be called from the primary process.
added in: v0.6.0
key/value pairs to add to worker process environment.
spawn a new worker process.
added in: v0.8.1deprecated since: v16.0.0
deprecated alias for .
added in: v16.0.0
true if the process is a primary. this is determined
by the . if  is
undefined, then  is .
true if the process is not a primary (it is the negation of ).
the scheduling policy, either  for round-robin or
to leave it to the operating system. this is a
global setting and effectively frozen once either the first worker is spawned,
or  is called, whichever comes first.
is the default on all operating systems except windows.
windows will change to  once libuv is able to effectively
distribute iocp handles without incurring a large performance hit.
can also be set through the
environment variable. valid
values are  and .
list of string arguments passed to the node.js
executable. default: .
file path to worker file. default: .
string arguments passed to worker.
current working directory of the worker process. default:
(inherits from parent process).
whether or not to send output to parent's stdio.
configures the stdio of forked processes. because the
cluster module relies on ipc to function, this configuration must contain an
entry. when this option is provided, it overrides .
sets the user identity of the process. (see .)
sets the group identity of the process. (see .)
|  sets inspector port of worker.
this can be a number, or a function that takes no arguments and returns a
number. by default each worker gets its own port, incremented from the
primary's .
hide the forked processes console window that would
after calling  (or ) this settings object will
contain the settings, including the default values.
this object is not intended to be changed or set manually.
is used to change the default 'fork' behavior. once called,
the settings will be present in .
any settings changes only affect future calls to  and have no
effect on workers that are already running.
the only attribute of a worker that cannot be set via  is
the  passed to .
the defaults above apply to the first call only; the defaults for later
calls are the current values at the time of  is called.
a reference to the current worker object. not available in the primary process.
a hash that stores the active worker objects, keyed by  field. this makes it
easy to loop through all the workers. it is only available in the primary
a worker is removed from  after the worker has disconnected
and exited. the order between these two events cannot be determined in
advance. however, it is guaranteed that the removal from the
list happens before the last  or  event is emitted.
command-line api
node.js comes with a variety of cli options. these options expose built-in
debugging, multiple ways to execute scripts, and other helpful runtime options.
to view this documentation as a manual page in a terminal, run .
synopsis
execute without arguments to start the .
for more info about , see the  documentation.
program entry point
the program entry point is a specifier-like string. if the string is not an
absolute path, it's resolved as a relative path from the current working
directory. that path is then resolved by  module loader. if no
corresponding file is found, an error is thrown.
if a file is found, its path will be passed to the
under any of the following conditions:
the program was started with a command-line flag that forces the entry
point to be loaded with ecmascript module loader.
the file has an  extension.
the file does not have a  extension, and the nearest parent
file contains a top-level  field with a value of
otherwise, the file is loaded using the commonjs module loader. see
for more details.
ecmascript modules loader entry point caveat
when loading  loads the program entry point, the
command will only accept as input only files with , , or
extensions; and with  extensions when
is enabled.
options
all options, including v8 options, allow words to be separated by both
dashes () or underscores (). for example,  is
if an option that takes a single value (such as ) is
passed more than once, then the last passed value is used. options from the
command line take precedence over options passed through the
environment variable.
alias for stdin. analogous to the use of  in other command-line utilities,
meaning that the script is read from stdin, and the rest of the options
are passed to that script.
added in: v6.11.0
indicate the end of node options. pass the rest of the arguments to the script.
if no script filename or eval/print script is supplied prior to this, then
the next argument is used as a script filename.
added in: v0.10.8
aborting instead of exiting causes a core file to be generated for post-mortem
analysis using a debugger (such as , , and ).
if this flag is passed, the behavior can still be set to not abort through
(and through usage of the
module that uses it).
generates a snapshot blob when the process exits and writes it to
disk, which can be loaded later with .
when building the snapshot, if  is not specified,
the generated blob will be written, by default, to
in the current working directory. otherwise it will be written to
the path specified by .
the  can be used to specify an entry point at
snapshot building time, thus avoiding the need of an additional entry
script at deserialization time:
for more information, check out the  documentation.
currently the support for run-time snapshot is experimental in that:
user-land modules are not yet supported in the snapshot, so only
one single file can be snapshotted. users can bundle their applications
into a single script with their bundler of choice before building
a snapshot, however.
only a subset of the built-in modules work in the snapshot, though the
node.js core test suite checks that a few fairly complex applications
can be snapshotted. support for more modules are being added. if any
crashes or buggy behaviors occur when building a snapshot, please file
a report in the  and link to it in the
added in: v10.12.0
print source-able bash completion script for node.js.
added in: v14.9.0, v12.19.0
enable experimental support for custom  resolution
conditions.
any number of custom string condition names are permitted.
the default node.js conditions of , , , and
will always apply as defined.
for example, to run a module with "development" resolutions:
added in: v12.0.0
starts the v8 cpu profiler on start up, and writes the cpu profile to disk
before exit.
if  is not specified, the generated profile is placed
in the current working directory.
if  is not specified, the generated profile is
named .
specify the directory where the cpu profiles generated by  will
be placed.
the default value is controlled by the
command-line option.
added in: v12.2.0
specify the sampling interval in microseconds for the cpu profiles generated
by . the default is 1000 microseconds.
specify the file name of the cpu profile generated by .
set the directory to which all diagnostic output files are written.
defaults to current working directory.
affects the default output directory of:
added in: v13.12.0, v12.17.0
disable the  property. if  is , the
property is removed entirely. if  is , accesses to the
property throw an exception with the code .
added in: v9.8.0
make built-in language features like  and  that generate
code from strings throw an exception instead. this does not affect the node.js
module.
set the default value of  in  and
. the value could be:
: sets default  .
the default is  and  have higher
priority than .
enable fips-compliant crypto at startup. (requires node.js to be built
against fips-compatible openssl.)
enable  support for stack traces.
when using a transpiler, such as typescript, stack traces thrown by an
application reference the transpiled code, not the original source position.
enables caching of source maps and makes a best
effort to report stack traces relative to the original source file.
overriding  prevents  from
modifying the stack trace.
note, enabling source maps can introduce latency to your application
when  is accessed. if you access  frequently
in your application, take into account the performance implications
added in: v18.7.0
expose the  on the global scope.
added in: v17.6.0
added in: v13.9.0, v12.16.2
enable experimental  support.
specify the  of a custom experimental .
may be any string accepted as an .
enable experimental support for the  protocol in  specifiers.
added in: v11.8.0
use the specified file as a security policy.
added in: v18.0.0
disable experimental support for the .
added in: v16.6.0
use this flag to disable top-level await in repl.
added in: v13.4.0, v12.16.0
sets the resolution algorithm for resolving es module specifiers. valid options
are  and .
the default is , which requires providing the full path to a
module. the  mode enables support for optional file extensions and
the ability to import a directory that has an index file.
see  for example usage.
enable experimental es module support in the  module.
enable experimental webassembly system interface (wasi) support.
added in: v12.3.0
enable experimental webassembly module support.
added in: v12.12.0
disable loading native addons that are not .
force fips-compliant crypto on startup. (cannot be disabled from script code.)
(same requirements as .)
added in: v11.12.0
enable experimental frozen intrinsics like  and .
only the root context is supported. there is no guarantee that
is indeed the default intrinsic reference. code may break
under this flag.
to allow polyfills to be added,  runs before freezing intrinsics.
added in: v18.3.0
enforces  event on node-api asynchronous callbacks.
to prevent from an existing add-on from crashing the process, this flag is not
enabled by default. in the future, this flag will be enabled by default to
enforce the correct behavior.
added in: v15.1.0, v14.18.0
writes a v8 heap snapshot to disk when the v8 heap usage is approaching the
heap limit.  should be a non-negative integer (in which case
node.js will write no more than  snapshots to disk).
when generating snapshots, garbage collection may be triggered and bring
the heap usage down. therefore multiple snapshots may be written to disk
before the node.js instance finally runs out of memory. these heap snapshots
can be compared to determine what objects are being allocated during the
time consecutive snapshots are taken. it's not guaranteed that node.js will
write exactly  snapshots to disk, but it will try
its best to generate at least one and up to  snapshots before the
node.js instance runs out of memory when  is greater than .
generating v8 snapshots takes time and memory (both memory managed by the
v8 heap and native memory outside the v8 heap). the bigger the heap is,
the more resources it needs. node.js will adjust the v8 heap to accommodate
the additional v8 heap memory overhead, and try its best to avoid using up
all the memory available to the process. when the process uses
more memory than the system deems appropriate, the process may be terminated
abruptly by the system, depending on the system configuration.
enables a signal handler that causes the node.js process to write a heap dump
when the specified signal is received.  must be a valid signal name.
disabled by default.
added in: v12.4.0
starts the v8 heap profiler on start up, and writes the heap profile to disk
specify the directory where the heap profiles generated by  will
specify the average sampling interval in bytes for the heap profiles generated
by . the default is 512 * 1024 bytes.
specify the file name of the heap profile generated by .
added in: v0.11.15
specify icu data load path. (overrides .)
this configures node.js to interpret string input as commonjs or as an es
module. string input is input via , , or .
valid values are  and . the default is .
the repl does not support this option.
added in: v7.6.0
activate inspector on  and break at start of user script.
default  is .
set the  to be used when the inspector is activated.
useful when activating the inspector by sending the  signal.
default host is .
see the  below regarding the
parameter usage.
activate inspector on . default is .
v8 inspector integration allows tools such as chrome devtools and ides to debug
and profile node.js instances. the tools attach to node.js instances via a
tcp port and communicate using the .
warning: binding inspector to a public ip:port combination is insecure
binding the inspector to a public ip (including ) with an open port is
insecure, as it allows external hosts to connect to the inspector and perform
a  attack.
if specifying a host, make sure that either:
the host is not accessible from public networks.
a firewall disallows unwanted connections on the port.
more specifically,  is insecure if the port ( by
default) is not firewall-protected.
see the  section for more information.
specify ways of the inspector web socket url exposure.
by default inspector websocket url is available in stderr and under
endpoint on .
added in: v13.4.0, v12.15.0, v10.19.0
use an insecure http parser that accepts invalid http headers. this may allow
interoperability with non-conformant http implementations. it may also allow
request smuggling and other http attacks that rely on invalid headers being
accepted. avoid using this option.
disable . this may be
required on some platforms for security reasons. it can also reduce attack
surface on other platforms, but the performance impact may be severe.
this flag is inherited from v8 and is subject to change upstream. it may
disappear in a non-semver-major release.
specify the maximum size, in bytes, of http headers. defaults to 16 kib.
added in: v7.10.0
this option is a no-op. it is kept for compatibility.
added in: v16.10.0, v14.19.0
disable the  exports condition as well as disable loading
native addons. when  is specified, calling  or
requiring a native c++ addon will fail and throw an exception.
silence deprecation warnings.
added in: v17.0.0
hide extra information on fatal exception that causes exit.
added in: v9.0.0
disables runtime checks for . these will still be enabled
dynamically when  is enabled.
added in: v16.10.0
do not search modules from global paths like  and
silence all process warnings (including deprecations).
added in: v15.0.0, v14.18.0
enable extra debug checks for memory leaks in node.js internals. this is
usually only useful for developers debugging node.js itself.
added in: v6.9.0
load an openssl configuration file on startup. among other uses, this can be
used to enable fips-compliant crypto if node.js is built
against fips-enabled openssl.
added in: v18.5.0
enable openssl default configuration section,  to be read from
the openssl configuration file. the default configuration file is named
but this can be changed using the environment variable
, or by using the command line option .
the location of the default openssl configuration file depends on how openssl
is being linked to node.js. sharing the openssl configuration may have unwanted
implications and it is recommended to use a configuration section specific to
node.js which is  and is default when this option is not used.
enable openssl 3.0 legacy provider. for more information please see
emit pending deprecation warnings.
pending deprecations are generally identical to a runtime deprecation with the
notable exception that they are turned off by default and will not be emitted
unless either the  command-line flag, or the
environment variable, is set. pending deprecations
are used to provide a kind of selective "early warning" mechanism that
developers may leverage to detect deprecated api usage.
added in: v12.7.0
instructs node.js to error prior to running any code if the policy does not have
the specified integrity. it expects a  string as a
instructs the module loader to preserve symbolic links when resolving and
caching modules.
by default, when node.js loads a module from a path that is symbolically linked
to a different on-disk location, node.js will dereference the link and use the
actual on-disk "real path" of the module as both an identifier and as a root
path to locate other dependency modules. in most cases, this default behavior
is acceptable. however, when using symbolically linked peer dependencies, as
illustrated in the example below, the default behavior causes an exception to
be thrown if  attempts to require  as a peer dependency:
the  command-line flag instructs node.js to use the
symlink path for modules as opposed to the real path, allowing symbolically
linked peer dependencies to be found.
note, however, that using  can have other side effects.
specifically, symbolically linked native modules can fail to load if those
are linked from more than one location in the dependency tree (node.js would
see those as two separate modules and would attempt to load the module multiple
times, causing an exception to be thrown).
the  flag does not apply to the main module, which allows
to work. to apply the same
behavior for the main module, also use .
caching the main module ().
this flag exists so that the main module can be opted-in to the same behavior
that  gives to all other imports; they are separate flags,
however, for backward compatibility with older node.js versions.
does not imply ; use
in addition to
when it is not desirable to follow symlinks before
resolving relative paths.
added in: v2.0.0
generate v8 profiler output.
added in: v5.2.0
process v8 profiler output generated using the v8 option .
write process warnings to the given file instead of printing to stderr. the
file will be created if it does not exist, and will be appended to if it does.
if an error occurs while attempting to write the warning to the file, the
warning will be written to stderr instead.
the  name may be an absolute path. if it is not, the default directory it
will be written to is controlled by the
write reports in a compact format, single-line json, more easily consumable
by log processing systems than the default multi-line format designed for
human consumption.
location at which the report will be generated.
name of the file to which the report will be written.
if the filename is set to  or , the report is written to
the stdout or stderr of the process respectively.
enables the report to be triggered on fatal errors (internal errors within
the node.js runtime such as out of memory) that lead to termination of the
application. useful to inspect various diagnostic data elements such as heap,
stack, event loop state, resource consumption etc. to reason about the fatal
enables report to be generated upon receiving the specified (or predefined)
signal to the running node.js process. the signal to trigger the report is
specified through .
sets or resets the signal for report generation (not supported on windows).
default signal is .
enables report to be generated when the process exits due to an uncaught
exception. useful when inspecting the javascript stack in conjunction with
native stack and other runtime environment data.
added in: v15.6.0
initializes an openssl secure heap of  bytes. when initialized, the
secure heap is used for selected types of allocations within openssl
during key generation and other operations. this is useful, for instance,
to prevent sensitive information from leaking due to pointer overruns
or underruns.
the secure heap is a fixed size and cannot be resized at runtime so,
if used, it is important to select a large enough heap to cover all
application uses.
the heap size given must be a power of two. any value less than 2
will disable the secure heap.
the secure heap is disabled by default.
the secure heap is not available on windows.
when using , the  flag specifies the
minimum allocation from the secure heap. the minimum value is .
the maximum value is the lesser of  or .
the value given must be a power of two.
when used with ,  specifies the path
where the generated snapshot blob will be written to. if not specified,
when used without ,  specifies the
path to the blob that will be used to restore the application state.
added in: v18.1.0
starts the node.js command line test runner. this flag cannot be combined with
, , , or the inspector. see the documentation
on  for more details.
added in: v18.11.0
a regular expression that configures the test runner to only execute tests
whose name matches the provided pattern. see the documentation on
configures the test runner to only execute top level tests that have the
option set.
throw errors for deprecations.
set  on startup.
added in: v4.0.0
specify an alternative default tls cipher list. requires node.js to be built
with crypto support (default).
log tls key material to a file. the key material is in nss
format and can be used by software (such as wireshark) to decrypt the tls
traffic.
set  to 'tlsv1.2'. use to disable support for
tlsv1.3.
set default  to 'tlsv1.3'. use to enable support
for tlsv1.3.
set default  to 'tlsv1'. use for compatibility with
old tls clients or servers.
set default  to 'tlsv1.1'. use for compatibility
with old tls clients or servers.
added in: v12.2.0, v10.20.0
set default  to 'tlsv1.2'. this is the default for
12.x and later, but the option is supported for compatibility with older node.js
versions.
set default  to 'tlsv1.3'. use to disable support
for tlsv1.2, which is not as secure as tlsv1.3.
added in: v14.3.0deprecated since: v18.8.0
- deprecated
print short summaries of calls to  to stderr.
the output could look like this:
the fields here correspond to:
the thread id as given by
the base address of the  in question, as well as the
byte offset corresponding to the index passed to
the expected value that was passed to
the timeout passed to
print stack traces for deprecations.
added in: v7.7.0
a comma separated list of categories that should be traced when trace event
tracing is enabled using .
template string specifying the filepath for the trace event data, it
supports  and .
enables the collection of trace event tracing information.
added in: v13.5.0, v12.16.0
prints a stack trace whenever an environment is exited proactively,
i.e. invoking .
prints a stack trace on sigint.
added in: v2.1.0
prints a stack trace whenever synchronous i/o is detected after the first turn
of the event loop.
prints tls packet trace information to . this can be used to debug tls
connection problems.
added in: v13.1.0
print stack traces for uncaught exceptions; usually, the stack trace associated
with the creation of an  is printed, whereas this makes node.js also
print the stack trace associated with throwing the value (which does not need
to be an  instance).
enabling this option may affect garbage collection behavior negatively.
print stack traces for process warnings (including deprecations).
added in: v2.4.0
track heap object allocations for heap snapshots.
using this flag allows to change what should happen when an unhandled rejection
occurs. one of the following modes can be chosen:
: emit . if this hook is not set, raise the
unhandled rejection as an uncaught exception. this is the default.
: raise the unhandled rejection as an uncaught exception. if the
exception is handled,  is emitted.
: always trigger a warning, no matter if the
hook is set or not but do not print the deprecation warning.
: emit . if this hook is not
set, trigger a warning, and set the process exit code to 1.
: silence all warnings.
if a rejection happens during the command line entry point's es module static
loading phase, it will always raise it as an uncaught exception.
updates snapshot files used by .
use bundled mozilla ca store as supplied by current node.js version
or use openssl's default ca store. the default store is selectable
at build-time.
the bundled ca store, as supplied by node.js, is a snapshot of mozilla ca store
that is fixed at release time. it is identical on all supported platforms.
using openssl store allows for external modifications of the store. for most
linux and bsd distributions, this store is maintained by the distribution
maintainers and system administrators. openssl ca store location is dependent on
configuration of the openssl library but this can be altered at runtime using
environment variables.
see  and .
added in: v13.6.0, v12.17.0
re-map the node.js static code to large memory pages at startup. if supported on
the target system, this will cause the node.js static code to be moved onto 2
mib pages instead of 4 kib pages.
the following values are valid for :
: no mapping will be attempted. this is the default.
: if supported by the os, mapping will be attempted. failure to map will
be ignored and a message will be printed to standard error.
: if supported by the os, mapping will be attempted. failure to map
will be ignored and will not be reported.
added in: v0.1.3
print v8 command-line options.
set v8's thread pool size which will be used to allocate background jobs.
if set to  then v8 will choose an appropriate size of the thread pool based
on the number of online processors.
if the value provided is larger than v8's maximum, then the largest value
will be chosen.
starts node.js in watch mode.
when in watch mode, changes in the watched files cause the node.js process to
restart.
by default, watch mode will watch the entry point
and any required or imported module.
use  to specify what paths to watch.
this flag cannot be combined with
, , , or the repl.
starts node.js in watch mode and specifies what paths to watch.
when in watch mode, changes in the watched paths cause the node.js process to
this will turn off watching of required or imported modules, even when used in
combination with .
this option is only supported on macos and windows.
an  exception will be thrown
when the option is used on a platform that does not support it.
automatically zero-fills all newly allocated  and
instances.
syntax check the script without executing.
evaluate the following argument as javascript. the modules which are
predefined in the repl can also be used in .
on windows, using  a single quote will not work correctly because it
only recognizes double  for quoting. in powershell or git bash, both
and  are usable.
print node command-line options.
the output of this option is less detailed than this document.
opens the repl even if stdin does not appear to be a terminal.
identical to  but prints the result.
added in: v1.6.0
preload the specified module at startup.
follows 's module resolution
rules.  may be either a path to a file, or a node module name.
only commonjs modules are supported. attempting to preload a
es6 module using  will fail with an error.
print node's version.
the  environment variable is used to
enable ansi colorized output. the value may be:
, , or the empty string  indicate 16-color support,
to indicate 256-color support, or
to indicate 16 million-color support.
when  is used and set to a supported value, both the ,
and  environment variables are ignored.
any other value will result in colorized output being disabled.
added in: v0.1.32
-separated list of core modules that should print debug information.
-separated list of core c++ modules that should print debug information.
added in: v0.3.0
when set, colors will not be used in the repl.
added in: v7.3.0
when set, the well known "root" cas (like verisign) will be extended with the
extra certificates in . the file should consist of one or more trusted
certificates in pem format. a message will be emitted (once) with
if the file is missing or
malformed, but any errors are otherwise ignored.
neither the well known nor extra certificates are used when the
options property is explicitly specified for a tls or https client or server.
this environment variable is ignored when  runs as setuid root or
has linux file capabilities set.
the  environment variable is only read when the node.js
process is first launched. changing the value at runtime using
has no effect on the current process.
data path for icu ( object) data. will extend linked-in data when compiled
with small-icu support.
when set to , process warnings are silenced.
a space-separated list of command-line options.  are interpreted
before command-line options, so command-line options will override or
compound after anything in . node.js will exit with an error if
an option that is not allowed in the environment is used, such as  or a
script file.
if an option value contains a space, it can be escaped using double quotes:
a singleton flag passed as a command-line option will override the same flag
passed into :
a flag that can be passed multiple times will be treated as if its
instances were passed first, and then its command-line
instances afterwards:
node.js options that are allowed are:
v8 options that are allowed are:
, and  are only available on linux.
-separated list of directories prefixed to the module search path.
on windows, this is a -separated list instead.
when set to , emit pending deprecation warnings.
set the number of pending pipe instance handles when the pipe server is waiting
for connections. this setting applies to windows only.
when set to , instructs the module loader to preserve symbolic links when
resolving and caching modules.
when set, process warnings will be emitted to the given file instead of
printing to stderr. the file will be created if it does not exist, and will be
appended to if it does. if an error occurs while attempting to write the
warning to the file, the warning will be written to stderr instead. this is
equivalent to using the  command-line flag.
path to the file used to store the persistent repl history. the default path is
, which is overridden by this variable. setting the value
to an empty string ( or ) disables persistent repl history.
added in: v13.0.0, v12.16.0
path to a node.js module which will be loaded in place of the built-in repl.
overriding this value to an empty string () will use the built-in repl.
added in: v14.5.0
if  equals , the check for a supported platform is skipped during
node.js startup. node.js might not execute correctly. any issues encountered
on unsupported platforms will not be fixed.
if  equals , certificate validation is disabled for tls connections.
this makes tls, and https by extension, insecure. the use of this environment
variable is strongly discouraged.
when set, node.js will begin outputting  and
data to the directory provided as an argument (coverage
information is written as json to files with a  prefix).
will automatically propagate to subprocesses, making it
easier to instrument applications that call the  family
of functions.  can be set to an empty string, to prevent
propagation.
coverage output
coverage is output as an array of  objects on the top-level
key :
source map cache
if found, source map data is appended to the top-level key
on the json coverage object.
is an object with keys representing the files source maps
were extracted from, and values which include the raw source-map url
(in the key ), the parsed source map v3 information (in the key ),
and the line lengths of the source file (in the key ).
is an alias for . the value of the
environment variable is arbitrary.
used to enable fips-compliant crypto if node.js is built with
if the  command-line option is used, the environment
variable is ignored.
if  is enabled, this overrides and sets openssl's directory
containing trusted certificates.
be aware that unless the child environment is explicitly set, this environment
variable will be inherited by any child processes, and if they use openssl, it
may cause them to trust the same cas as node.
if  is enabled, this overrides and sets openssl's file
the  environment variable is used to specify the timezone configuration.
while node.js does not support all of the various , it does support basic  (such as
, , or ).
it may support a few other abbreviations or aliases, but these are strongly
discouraged and not guaranteed.
set the number of threads used in libuv's threadpool to  threads.
asynchronous system apis are used by node.js whenever possible, but where they
do not exist, libuv's threadpool is used to create asynchronous node apis based
on synchronous system apis. node.js apis that use the threadpool are:
all  apis, other than the file watcher apis and those that are explicitly
synchronous
asynchronous crypto apis such as , ,
all  apis, other than those that are explicitly synchronous
because libuv's threadpool has a fixed size, it means that if for whatever
reason any of these apis takes a long time, other (seemingly unrelated) apis
that run in libuv's threadpool will experience degraded performance. in order to
mitigate this issue, one potential solution is to increase the size of libuv's
threadpool by setting the  environment variable to a value
greater than  (its current default value). for more information, see the
useful v8 options
v8 has its own set of cli options. any v8 cli option that is provided to
will be passed on to v8 to handle. v8's options have no stability guarantee.
the v8 team themselves don't consider them to be part of their formal api,
and reserve the right to change them at any time. likewise, they are not
covered by the node.js stability guarantees. many of the v8
options are of interest only to v8 developers. despite this, there is a small
set of v8 options that are widely applicable to node.js, and they are
documented here:
(in megabytes)
sets the max memory size of v8's old memory section. as memory
consumption approaches the limit, v8 will spend more time on
garbage collection in an effort to free unused memory.
on a machine with 2 gib of memory, consider setting this to
1536 (1.5 gib) to leave some memory for other uses and avoid swapping.
sets the maximum  size for v8's  in
mib (megabytes).
increasing the max size of a semi-space may improve throughput for node.js at
the cost of more memory consumption.
since the young generation size of the v8 heap is three times (see
in v8) the size of the semi-space,
an increase of 1 mib to semi-space applies to each of the three individual
semi-spaces and causes the heap size to increase by 3 mib. the throughput
improvement depends on your workload (see ).
the default value is 16 mib for 64-bit systems and 8 mib for 32-bit systems. to
get the best configuration for your application, you should try different
max-semi-space-size values when running benchmarks for your application.
for example, benchmark on a 64-bit systems:
console
the  module provides a simple debugging console that is similar to
the javascript console mechanism provided by web browsers.
the module exports two specific components:
a  class with methods such as , , and
that can be used to write to any node.js stream.
a global  instance configured to write to  and
. the global  can be used without calling
warning: the global console object's methods are neither consistently
synchronous like the browser apis they resemble, nor are they consistently
asynchronous like all other node.js streams. see the  for
more information.
example using the global :
example using the  class:
the  class can be used to create a simple logger with configurable
output streams and can be accessed using either
or  (or their destructured counterparts):
ignore errors when writing to the underlying
streams. default: .
|  set color support for this  instance.
setting to  enables coloring while inspecting values. setting to
disables coloring while inspecting values. setting to
makes color support depend on the value of the  property
and the value returned by  on the respective stream. this
option can not be used, if  is set as well.
specifies options that are passed along to
set group indentation.
creates a new  with one or two writable stream instances.  is a
writable stream to print log or info output.  is used for warning or
error output. if  is not provided,  is used for .
the global  is a special  whose output is sent to
and . it is equivalent to calling:
the value tested for being truthy.
all arguments besides  are used as error message.
writes a message if  is  or omitted. it only
writes a message and does not otherwise affect execution. the output always
starts with . if provided,  is formatted using
if  is , nothing happens.
when  is a tty, calling  will attempt to clear the
tty. when  is not a tty, this method does nothing.
the specific operation of  can vary across operating systems
and terminal types. for most linux operating systems,
operates similarly to the  shell command. on windows,
will clear only the output in the current terminal viewport for the node.js
binary.
the display label for the counter. default: .
maintains an internal counter specific to  and outputs to  the
number of times  has been called with the given .
resets the internal counter specific to .
the  function is an alias for .
if  then the object's non-enumerable and symbol
properties will be shown too. default: .
tells  how many times to recurse while
formatting the object. this is useful for inspecting large complicated
objects. to make it recurse indefinitely, pass . default: .
if , then the output will be styled with ansi color
codes. colors are customizable;
see . default: .
uses  on  and prints the resulting string to .
this function bypasses any custom  function defined on .
this method calls  passing it the arguments received.
this method does not produce any xml formatting.
added in: v0.1.100
prints to  with newline. multiple arguments can be passed, with the
first used as the primary message and all additional used as substitution
values similar to  (the arguments are all passed to
if formatting elements (e.g. ) are not found in the first string then
is called on each argument and the resulting string
values are concatenated. see  for more information.
increases indentation of subsequent lines by spaces for
length.
if one or more s are provided, those are printed first without the
additional indentation.
decreases indentation of subsequent lines by spaces for
alternate properties for constructing the table.
try to construct a table with the columns of the properties of
(or use ) and rows of  and log it. falls back to just
logging the argument if it can't be parsed as tabular.
added in: v0.1.104
starts a timer that can be used to compute the duration of an operation. timers
are identified by a unique . use the same  when calling
to stop the timer and output the elapsed time in
suitable time units to . for example, if the elapsed
time is 3869ms,  displays "3.869s".
stops a timer that was previously started by calling  and
prints the result to :
for a timer that was previously started by calling , prints
the elapsed time and other  arguments to :
prints to  the string , followed by the
formatted message and stack trace to the current position in the code.
inspector only methods
the following methods are exposed by the v8 engine in the general api but do
not display anything unless used in conjunction with the
( flag).
this method does not display anything unless used in the inspector. the
method starts a javascript cpu profile with an optional
label until  is called. the profile is then added to
the profile panel of the inspector.
this method does not display anything unless used in the inspector. stops the
current javascript cpu profiling session if one has been started and prints
the report to the profiles panel of the inspector. see
for an example.
if this method is called without a label, the most recently started profile is
stopped.
method adds an event with the label  to the
timeline panel of the inspector.
corepack
added in: v16.9.0, v14.19.0
is an experimental tool to help with
managing versions of your package managers. it exposes binary proxies for
each  that, when called, will identify whatever
package manager is configured for the current project, transparently install
it if needed, and finally run it without requiring explicit user interactions.
this feature simplifies two core workflows:
it eases new contributor onboarding, since they won't have to follow
system-specific installation processes anymore just to have the package
manager you want them to.
it allows you to ensure that everyone in your team will use exactly the
package manager version you intend them to, without them having to
manually synchronize it each time you need to make an update.
workflows
enabling the feature
due to its experimental status, corepack currently needs to be explicitly
enabled to have any effect. to do that, run , which
will set up the symlinks in your environment next to the  binary
(and overwrite the existing symlinks if necessary).
from this point forward, any call to the  will work
without further setup. should you experience a problem, run
to remove the proxies from your system (and consider
opening an issue on the  to let us know).
configuring a package
the corepack proxies will find the closest  file in your
current directory hierarchy to extract its  property.
if the value corresponds to a , corepack will make
sure that all calls to the relevant binaries are run against the requested
version, downloading it on demand if needed, and aborting if it cannot be
successfully retrieved.
upgrading the global versions
when running outside of an existing project (for example when running
), corepack will by default use predefined versions roughly
corresponding to the latest stable releases from each tool. those versions can
be overridden by running the  command along with the
package manager version you wish to set:
alternately, a tag or range may be used:
offline workflow
many production environments don't have network access. since corepack
usually downloads the package manager releases straight from their registries,
it can conflict with such environments. to avoid that happening, call the
command while you still have network access (typically at
the same time you're preparing your deploy image). this will ensure that the
required package managers are available even without network access.
the  command has . consult the detailed
for more information.
supported package managers
the following binaries are provided through corepack:
common questions
how does corepack interact with npm?
while corepack could support npm like any other package manager, its
shims aren't enabled by default. this has a few consequences:
it's always possible to run a  command within a project configured to
be used with another package manager, since corepack cannot intercept it.
while  is a valid option in the  property, the
lack of shim will cause the global npm to be used.
running  doesn't work
npm prevents accidentally overriding the corepack binaries when doing a global
install. to avoid this problem, consider one of the following options:
don't run this command; corepack will provide the package manager
binaries anyway and will ensure that the requested versions are always
available, so installing the package managers explicitly isn't needed.
add the  flag to ; this will tell npm that it's fine to
override binaries, but you'll erase the corepack ones in the process. (run
to add them back.)
crypto
the  module provides cryptographic functionality that includes a
set of wrappers for openssl's hash, hmac, cipher, decipher, sign, and verify
functions.
determining if crypto support is unavailable
it is possible for node.js to be built without including support for the
module. in such cases, attempting to  from  or
calling  will result in an error being thrown.
when using commonjs, the error thrown can be caught using try/catch:
when using the lexical esm  keyword, the error can only be
caught if a handler for  is registered
before any attempt to load the module is made (using, for instance,
a preload module).
when using esm, if there is a chance that the code may be run on a build
of node.js where crypto support is not enabled, consider using the
function instead of the lexical  keyword:
added in: v0.11.8
spkac is a certificate signing request mechanism originally implemented by
netscape and was specified formally as part of .
is deprecated since  and new projects
should not use this element anymore.
the  module provides the  class for working with spkac
data. the most common usage is handling output generated by the html5
element. node.js uses  internally.
|  |  |  |
the  of the  string.
returns:  the challenge component of the  data structure, which
includes a public key and a challenge.
returns:  the public key component of the  data structure,
which includes a public key and a challenge.
returns:   if the given  data structure is valid,
otherwise.
as a legacy interface, it is possible to create new instances of
the  class as illustrated in the examples below.
instances of the  class can be created using the  keyword
or by calling  as a function:
added in: v0.1.94
instances of the  class are used to encrypt data. the class can be
used in one of two ways:
as a  that is both readable and writable, where plain unencrypted
data is written to produce encrypted data on the readable side, or
using the  and  methods to produce
the encrypted data.
the  or  methods are
used to create  instances.  objects are not to be created
directly using the  keyword.
example: using  objects as streams:
example: using  and piped streams:
example: using the  and  methods:
the  of the return value.
returns:  |  any remaining enciphered contents.
if  is specified, a string is
returned. if an  is not provided, a  is returned.
once the  method has been called, the  object can no
longer be used to encrypt data. attempts to call  more than
once will result in an error being thrown.
added in: v1.0.0
returns:  when using an authenticated encryption mode (, ,
, and  are currently supported), the
method returns a
containing the authentication tag that has been computed from
the given data.
the  method should only be called after encryption has
been completed using the  method.
if the  option was set during the  instance's creation,
this function will return exactly  bytes.
the string encoding to use when  is a string.
returns:  for method chaining.
when using an authenticated encryption mode (, , , and
currently supported), the  method sets the value used for the
additional authenticated data (aad) input parameter.
the  option is optional for  and . when using ,
the  option must be specified and its value must match the
length of the plaintext in bytes. see .
the  method must be called before .
when using block encryption algorithms, the  class will automatically
add padding to the input data to the appropriate block size. to disable the
default padding call .
when  is , the length of the entire input data must be a
multiple of the cipher's block size or  will throw an error.
disabling automatic padding is useful for non-standard padding, for instance
using  instead of pkcs padding.
the  method must be called before
the  of the data.
returns:  |
updates the cipher with . if the  argument is given,
argument is a string using the specified encoding. if the
argument is not given,  must be a , , or
. if  is a , , or , then
is ignored.
the  specifies the output format of the enciphered
data. if the
is specified, a string using the specified encoding is returned. if no
is provided, a  is returned.
the  method can be called multiple times with new data until
is called. calling  after
will result in an error being thrown.
instances of the  class are used to decrypt data. the class can be
as a  that is both readable and writable, where plain encrypted
data is written to produce unencrypted data on the readable side, or
using the  and  methods to
produce the unencrypted data.
returns:  |  any remaining deciphered contents.
once the  method has been called, the  object can
no longer be used to decrypt data. attempts to call  more
than once will result in an error being thrown.
string encoding to use when  is a string.
the  argument is optional for . when using , the
option must be specified and its value must match the length
of the ciphertext in bytes. see .
when passing a string as the , please consider
currently supported), the  method is used to pass in the
received authentication tag. if no tag is provided, or if the cipher text
has been tampered with,  will throw, indicating that the
cipher text should be discarded due to failed authentication. if the tag length
is invalid according to  or does not match the value of the
option,  will throw an error.
for  mode or before  for  and  modes and
when passing a string as the authentication tag, please consider
when data has been encrypted without standard block padding, calling
will disable automatic padding to prevent
from checking for and removing padding.
turning auto padding off will only work if the input data's length is a
multiple of the ciphers block size.
updates the decipher with . if the  argument is given,
argument is not given,  must be a . if  is a
then  is ignored.
added in: v0.5.0
the  class is a utility for creating diffie-hellman key
exchanges.
instances of the  class can be created using the
the  of an  string.
computes the shared secret using  as the other
party's public key and returns the computed shared secret. the supplied
key is interpreted using the specified , and secret is
encoded using specified .
if the  is not
provided,  is expected to be a ,
if  is given a string is returned; otherwise, a
generates private and public diffie-hellman key values, and returns
the public key in the specified . this key should be
transferred to the other party.
if  is provided a string is returned; otherwise a
returns the diffie-hellman generator in the specified .
if  is provided a string is
returned; otherwise a  is returned.
returns the diffie-hellman prime in the specified .
returns the diffie-hellman private key in the specified .
if  is provided a
string is returned; otherwise a  is returned.
returns the diffie-hellman public key in the specified .
sets the diffie-hellman private key. if the  argument is provided,
is expected
to be a string. if no  is provided,  is expected
to be a , , or .
sets the diffie-hellman public key. if the  argument is provided,
added in: v0.11.12
a bit field containing any warnings and/or errors resulting from a check
performed during initialization of the  object.
the following values are valid for this property (as defined in  module):
added in: v0.7.5
the  class takes a well-known modp group as its argument.
it works the same as , except that it does not allow changing
its keys after creation. in other words, it does not implement
or  methods.
the following groups are supported:
(2048 bits,  section 3)
(3072 bits,  section 4)
(4096 bits,  section 5)
(6144 bits,  section 6)
(8192 bits,  section 7)
the following groups are still supported but deprecated (see ):
(768 bits,  section 6.1)
(1024 bits,  section 6.2)
(1536 bits,  section 2)
these deprecated groups might be removed in future versions of node.js.
the  class is a utility for creating elliptic curve diffie-hellman (ecdh)
key exchanges.
converts the ec diffie-hellman public key specified by  and  to the
format specified by . the  argument specifies point encoding
and can be ,  or . the supplied key is
interpreted using the specified , and the returned key is encoded
using the specified .
use  to obtain a list of available curve names.
on recent openssl releases,  will also display
the name and description of each available elliptic curve.
if  is not specified the point will be returned in
format.
if the  is not provided,  is expected to be a ,
example (uncompressing a key):
key is interpreted using specified , and the returned secret
is encoded using the specified .
provided,  is expected to be a , , or
if  is given a string will be returned; otherwise a
will throw an
error when
lies outside of the elliptic curve. since  is
usually supplied from a remote user over an insecure network,
be sure to handle this exception accordingly.
generates private and public ec diffie-hellman key values, and returns
the public key in the specified  and . this key should be
the  argument specifies point encoding and can be  or
. if  is not specified, the point will be returned in
returns:  |  the ec diffie-hellman in the specified .
if  is specified, a string is returned; otherwise a  is
returned.
returns:  |  the ec diffie-hellman public key in the specified
. if  is not specified the point will be returned in
sets the ec diffie-hellman private key.
if  is provided,  is expected
to be a string; otherwise  is expected to be a ,
if  is not valid for the curve specified when the  object was
created, an error is thrown. upon setting the private key, the associated
public point (key) is also generated and set in the  object.
added in: v0.11.14deprecated since: v5.2.0
sets the ec diffie-hellman public key.
if  is provided  is expected to
be a string; otherwise a , , or  is expected.
there is not normally a reason to call this method because
only requires a private key and the other party's public key to compute the
shared secret. typically either  or
will be called. the  method
attempts to generate the public point/key associated with the private key being
set.
example (obtaining a shared secret):
added in: v0.1.92
the  class is a utility for creating hash digests of data. it can be
as a  that is both readable and writable, where data is written
to produce a computed hash digest on the readable side, or
using the  and  methods to produce the
computed hash.
the  method is used to create  instances.
objects are not to be created directly using the  keyword.
creates a new  object that contains a deep copy of the internal state
of the current  object.
the optional  argument controls stream behavior. for xof hash
functions such as , the  option can be used to
specify the desired output length in bytes.
an error is thrown when an attempt is made to copy the  object after
its  method has been called.
calculates the digest of all of the data passed to be hashed (using the
method).
if  is provided a string will be returned; otherwise
a  is returned.
the  object can not be used again after  method has been
called. multiple calls will cause an error to be thrown.
updates the hash content with the given , the encoding of which
is given in .
if  is not provided, and the  is a string, an
encoding of  is enforced. if  is a , , or
, then  is ignored.
this can be called many times with new data as it is streamed.
the  class is a utility for creating cryptographic hmac digests. it can
be used in one of two ways:
to produce a computed hmac digest on the readable side, or
computed hmac digest.
calculates the hmac digest of all of the data passed using .
if  is
provided a string is returned; otherwise a  is returned;
the  object can not be used again after  has been
called. multiple calls to  will result in an error being thrown.
updates the  content with the given , the encoding of which
node.js uses a  class to represent a symmetric or asymmetric key,
and each kind of key exposes different functions. the
,  and
methods are used to create
instances.  objects are not to be created directly using the
keyword.
most applications should consider using the new  api instead of
passing keys as strings or s due to improved security features.
instances can be passed to other threads via .
the receiver obtains a cloned , and the  does not need to
be listed in the  argument.
added in: v15.0.0
example: converting a  instance to a :
:  key size in bits (rsa, dsa).
:  public exponent (rsa).
:  name of the message digest (rsa-pss).
:  name of the message digest used by
mgf1 (rsa-pss).
:  minimal salt length in bytes (rsa-pss).
:  size of  in bits (dsa).
:  name of the curve (ec).
this property exists only on asymmetric keys. depending on the type of the key,
this object contains information about the key. none of the information obtained
through this property can be used to uniquely identify a key or to compromise
the security of the key.
for rsa-pss keys, if the key material contains a  sequence,
the , , and  properties will be
other key details might be exposed via this api using additional attributes.
for asymmetric keys, this property represents the type of the key. supported key
types are:
(oid 1.2.840.113549.1.1.1)
(oid 1.2.840.113549.1.1.10)
(oid 1.2.840.10040.4.1)
(oid 1.2.840.10045.2.1)
(oid 1.3.101.110)
(oid 1.3.101.111)
(oid 1.3.101.112)
(oid 1.3.101.113)
(oid 1.2.840.113549.1.3.1)
this property is  for unrecognized  types and symmetric
keys.
returns:  |  |
for symmetric keys, the following encoding options can be used:
:  must be  (default) or .
for public keys, the following encoding options can be used:
:  must be one of  (rsa only) or .
:  must be , , or .
for private keys, the following encoding options can be used:
:  must be one of  (rsa only),  or
(ec only).
:  if specified, the private key will be encrypted with
the given  and  using pkcs#5 v2.0 password based
encryption.
:  |  the passphrase to use for encryption, see
the result type depends on the selected encoding format, when pem the
result is a string, when der it will be a buffer containing the data
encoded as der, when  it will be an object.
when  encoding format was selected, all other encoding options are
ignored.
pkcs#1, sec1, and pkcs#8 type keys can be encrypted by using a combination of
the  and  options. the pkcs#8  can be used with any
to encrypt any key algorithm (rsa, ec, or dh) by specifying a
. pkcs#1 and sec1 can only be encrypted by specifying a
when the pem  is used. for maximum compatibility, use pkcs#8 for
encrypted private keys. since pkcs#8 defines its own
encryption mechanism, pem-level encryption is not supported when encrypting
a pkcs#8 key. see  for pkcs#8 encryption and  for
pkcs#1 and sec1 encryption.
added in: v17.7.0
:  a  with which to
returns  or  depending on whether the keys have exactly the same
type, value, and parameters. this method is not
added in: v11.6.0
for secret keys, this property represents the size of the key in bytes. this
property is  for asymmetric keys.
depending on the type of this , this property is either
for secret (symmetric) keys,  for public (asymmetric) keys
or  for private (asymmetric) keys.
the  class is a utility for generating signatures. it can be used in one
of two ways:
as a writable , where data to be signed is written and the
method is used to generate and return the signature, or
signature.
the  method is used to create  instances. the
argument is the string name of the hash function to use.  objects are not
to be created directly using the  keyword.
example: using  and  objects as streams:
|  |  |  |  |  |  |
calculates the signature on all the data passed through using either
if  is not a , this function behaves as if
had been passed to . if it is an
object, the following additional properties can be passed:
for dsa and ecdsa, this option specifies the
format of the generated signature. it can be one of the following:
(default): der-encoded asn.1 signature structure encoding .
: signature format  as proposed in ieee-p1363.
optional padding value for rsa, one of the following:
(default)
will use mgf1 with the same hash function
used to sign the message as specified in section 3.1 of , unless
an mgf1 hash function has been specified as part of the key in compliance with
section 3.3 of .
salt length for when padding is
. the special value
sets the salt length to the digest
size,  (default) sets it to the
maximum permissible value.
the  object can not be again used after  method has been
the  class is a utility for verifying signatures. it can be used in one
as a writable  where written data is used to validate against the
supplied signature, or
using the  and  methods to verify
the signature.
see  for examples.
returns:   or  depending on the validity of the
signature for the data and public key.
verifies the provided data using the given  and .
format of the signature. it can be one of the following:
used to verify the message as specified in section 3.1 of , unless
size,  (default) causes it to be
determined automatically.
the  argument is the previously calculated signature for the data, in
if a  is specified, the  is expected to be a
string; otherwise  is expected to be a ,
called. multiple calls to  will result in an error being
thrown.
because public keys can be derived from private keys, a private key may
be passed instead of a public key.
encapsulates an x509 certificate and provides read-only access to
its information.
|  |  |  a pem or der encoded
x509 certificate.
type:  will be  if this is a certificate authority (ca)
certificate.
, , or .
returns:  |  returns  if the certificate matches,
if it does not.
checks whether the certificate matches the given email address.
if the  option is undefined or set to , the certificate
subject is only considered if the subject alternative name extension either does
not exist or does not contain any email addresses.
if the  option is set to  and if the subject alternative
name extension either does not exist or does not contain a matching email
address, the certificate subject is considered.
if the  option is set to , the certificate subject is never
considered, even if the certificate contains no subject alternative names.
returns:  |  returns a subject name that matches ,
or  if no subject name matches .
checks whether the certificate matches the given host name.
if the certificate matches the given host name, the matching subject name is
returned. the returned name might be an exact match (e.g., )
or it might contain wildcards (e.g., ). because host name
comparisons are case-insensitive, the returned subject name might also differ
from the given  in capitalization.
not exist or does not contain any dns names. this behavior is consistent with
("http over tls").
name extension either does not exist or does not contain a matching dns name,
the certificate subject is considered.
checks whether the certificate matches the given ip address (ipv4 or ipv6).
only   subject alternative names are considered, and they
must match the given  address exactly. other subject alternative names as
well as the subject field of the certificate are ignored.
checks whether this certificate was issued by the given .
a private key.
checks whether the public key for this certificate is consistent with
the given private key.
the sha-1 fingerprint of this certificate.
because sha-1 is cryptographically broken and because the security of sha-1 is
significantly worse than that of algorithms that are commonly used to sign
certificates, consider using  instead.
the sha-256 fingerprint of this certificate.
the sha-512 fingerprint of this certificate.
because computing the sha-256 fingerprint is usually faster and because it is
only half the size of the sha-512 fingerprint,  may be
a better choice. while sha-512 presumably provides a higher level of security in
general, the security of sha-256 matches that of most algorithms that are
commonly used to sign certificates.
a textual representation of the certificate's authority information access
extension.
this is a line feed separated list of access descriptions. each line begins with
the access method and the kind of the access location, followed by a colon and
the value associated with the access location.
after the prefix denoting the access method and the kind of the access location,
the remainder of each line might be enclosed in quotes to indicate that the
value is a json string literal. for backward compatibility, node.js only uses
json string literals within this property when necessary to avoid ambiguity.
third-party code should be prepared to handle both possible entry formats.
the issuer identification included in this certificate.
added in: v15.9.0
the issuer certificate or  if the issuer certificate is not
an array detailing the key usages for this certificate.
the public key  for this certificate.
a  containing the der encoding of this certificate.
the serial number of this certificate.
serial numbers are assigned by certificate authorities and do not uniquely
identify certificates. consider using  as a unique
identifier instead.
the complete subject of this certificate.
the subject alternative name specified for this certificate.
this is a comma-separated list of subject alternative names. each entry begins
with a string identifying the kind of the subject alternative name followed by
a colon and the value associated with the entry.
earlier versions of node.js incorrectly assumed that it is safe to split this
property at the two-character sequence  (see ). however,
both malicious and legitimate certificates can contain subject alternative names
that include this sequence when represented as a string.
after the prefix denoting the type of the entry, the remainder of each entry
might be enclosed in quotes to indicate that the value is a json string literal.
for backward compatibility, node.js only uses json string literals within this
property when necessary to avoid ambiguity. third-party code should be prepared
to handle both possible entry formats.
there is no standard json encoding for x509 certificates. the
method returns a string containing the pem encoded
returns information about this certificate using the legacy
encoding.
returns the pem-encoded certificate.
the date/time from which this certificate is considered valid.
the date/time until which this certificate is considered valid.
a public key.
verifies that this certificate was signed by the given public key.
does not perform any other validation checks on the certificate.
module methods and properties
an object containing commonly used constants for crypto and security related
operations. the specific constants currently defined are described in
added in: v0.9.3deprecated since: v10.0.0
the default encoding to use for functions that can take either strings
or . the default value is , which makes methods
default to  objects.
the  mechanism is provided for backward compatibility
with legacy programs that expect  to be the default encoding.
new applications should expect the default to be .
this property is deprecated.
added in: v6.0.0deprecated since: v10.0.0
property for checking and controlling whether a fips compliant crypto provider
is currently in use. setting to true requires a fips build of node.js.
this property is deprecated. please use  and
|  |  |  |  |
a possible prime encoded as a sequence of big endian octets of arbitrary
the number of miller-rabin probabilistic primality
iterations to perform. when the value is  (zero), a number of checks
is used that yields a false positive rate of at most 2-64 for
random input. care must be used when selecting a number of checks. refer
to the openssl documentation for the  function
options for more details. default:
set to an  object if an error occurred during check.
if the candidate is a prime with an error
probability less than .
checks the primality of the .
added in: v15.8.0
returns:   if the candidate is a prime with an error
creates and returns a  object that uses the given  and
the  argument controls stream behavior and is optional except when a
cipher in ccm or ocb mode (e.g. ) is used. in that case, the
option is required and specifies the length of the
authentication tag in bytes, see . in gcm mode, the
option is not required but can be used to set the length of the authentication
tag that will be returned by  and defaults to 16 bytes.
for , the  option defaults to 16 bytes.
the  is dependent on openssl, examples are , etc. on
recent openssl releases,  will
display the available cipher algorithms.
the  is used to derive the cipher key and initialization vector (iv).
the value must be either a  encoded string, a , a
, or a .
this function is semantically insecure for all
supported ciphers and fatally flawed for ciphers in counter mode (such as ctr,
gcm, or ccm).
the implementation of  derives keys using the openssl
function  with the digest algorithm set to md5, one
iteration, and no salt. the lack of salt allows dictionary attacks as the same
password always creates the same key. the low iteration count and
non-cryptographically secure hash algorithm allow passwords to be tested very
rapidly.
in line with openssl's recommendation to use a more modern algorithm instead of
it is recommended that developers derive a key and iv on
their own using  and to use
to create the  object. users should not use ciphers with counter mode
(e.g. ctr, gcm, or ccm) in . a warning is emitted when
they are used in order to avoid the risk of iv reuse that causes
vulnerabilities. for the case when iv is reused in gcm, see  for details.
|  |  |  |  |  |
creates and returns a  object, with the given ,  and
initialization vector ().
the  is the raw key used by the  and  is an
. both arguments must be  encoded strings,
, , or s. the  may optionally be
a  of type . if the cipher does not need
an initialization vector,  may be .
when passing strings for  or , please consider
initialization vectors should be unpredictable and unique; ideally, they will be
cryptographically random. they do not have to be secret: ivs are typically just
added to ciphertext messages unencrypted. it may sound contradictory that
something has to be unpredictable and unique, but does not have to be secret;
remember that an attacker must not be able to predict ahead of time what a
given iv will be.
(key).
authentication tag in bytes, see .
to create the  object.
creates and returns a  object that uses the given ,
and initialization vector ().
option is not required but can be used to restrict accepted authentication tags
to those with the specified length.
remember that an attacker must not be able to predict ahead of time what a given
iv will be.
creates a  key exchange object using the supplied  and an
optional specific .
the  argument can be a number, string, or . if
is not specified, the value  is used.
if  is specified,  is expected to be a string; otherwise
a , , or  is expected.
if  is specified,  is expected to be a string;
otherwise a number, , , or  is expected.
creates a  key exchange object and generates a prime of
bits using an optional specific numeric .
if  is not specified, the value  is used.
added in: v0.9.3
an alias for
creates an elliptic curve diffie-hellman () key exchange object using a
predefined curve specified by the  string. use
to obtain a list of available curve names. on recent
openssl releases,  will also display the name
and description of each available elliptic curve.
creates and returns a  object that can be used to generate hash digests
using the given . optional  argument controls stream
behavior. for xof hash functions such as , the  option
can be used to specify the desired output length in bytes.
the  is dependent on the available algorithms supported by the
version of openssl on the platform. examples are , , etc.
on recent releases of openssl,  will
display the available digest algorithms.
example: generating the sha256 sum of a file
creates and returns an  object that uses the given  and .
optional  argument controls stream behavior.
the  is the hmac key used to generate the cryptographic hmac hash. if it is
a , its type must be .
example: generating the sha256 hmac of a file
:  |  |  |  |  |  the key
material, either in pem, der, or jwk format.
:  must be , , or '.
:  must be ,  or . this option is
required only if the  is  and ignored otherwise.
:  |  the passphrase to use for decryption.
:  the string encoding to use when  is a string.
creates and returns a new key object containing a private key. if  is a
string or ,  is assumed to be ; otherwise,
must be an object with the properties described above.
if the private key is encrypted, a  must be specified. the length
of the passphrase is limited to 1024 bytes.
:  must be  or . this option is
creates and returns a new key object containing a public key. if  is a
string or ,  is assumed to be ; if  is a
with type , the public key is derived from the given private key;
otherwise,  must be an object with the properties described above.
if the format is , the  may also be an x.509 certificate.
because public keys can be derived from private keys, a private key may be
passed instead of a public key. in that case, this function behaves as if
had been called, except that the type of the
returned  will be  and that the private key cannot be
extracted from the returned . similarly, if a  with type
is given, a new  with type  will be returned
and it will be impossible to extract the private key from the returned object.
the string encoding when  is a string.
creates and returns a new key object containing a secret key for symmetric
encryption or .
creates and returns a  object that uses the given . use
to obtain the names of the available digest algorithms.
optional  argument controls the  behavior.
in some cases, a  instance can be created using the name of a signature
algorithm, such as , instead of a digest algorithm. this will use
the corresponding digest algorithm. this does not work for all signature
algorithms, such as , so it is best to always use digest
algorithm names.
creates and returns a  object that uses the given algorithm.
use  to obtain an array of names of the available
signing algorithms. optional  argument controls the
behavior.
computes the diffie-hellman secret based on a  and a .
both keys must have the same , which must be one of
(for diffie-hellman),  (for ecdh), , or  (for ecdh-es).
:  the intended use of the generated secret key. currently
accepted values are  and .
:  the bit length of the key to generate. this must be a
value greater than 0.
if  is , the minimum is 8, and the maximum length is
231-1. if the value is not a multiple of 8, the generated
key will be truncated to .
if  is , the length must be one of , , or .
asynchronously generates a new random secret key of the given . the
will determine which validations will be performed on the .
:  must be , , , , ,
, , , or .
:  public exponent (rsa). default: .
:  name of the curve to use (ec).
:  the prime parameter (dh).
:  prime length in bits (dh).
:  custom generator (dh). default: .
:  diffie-hellman group name (dh). see
:  see .
:  |  |
generates a new asymmetric key pair of the given . rsa, rsa-pss, dsa, ec,
ed25519, ed448, x25519, x448, and dh are currently supported.
if a  or  was specified, this function
behaves as if  had been called on its result. otherwise,
the respective part of the key is returned as a .
it is recommended to encode public keys as  and private keys as
with encryption for long-term storage:
on completion,  will be called with  set to  and
/  representing the generated key pair.
a  for an  with  and  properties.
when encoding public keys, it is recommended to use . when encoding
private keys, it is recommended to use  with a strong passphrase,
and to keep the passphrase confidential.
the return value  represents the generated key pair.
when pem encoding was selected, the respective key will be a string, otherwise
it will be a buffer containing the data encoded as der.
:  the bit length of the key to generate.
synchronously generates a new random secret key of the given . the
the size (in bits) of the prime to generate.
when , the generated prime is returned
as a .
generates a pseudorandom prime of  bits.
if  is , the prime will be a safe prime -- that is,
will also be a prime.
the  and  parameters can be used to enforce additional
requirements, e.g., for diffie-hellman:
if  and  are both set, the prime will satisfy the
condition that .
if only  is set and  is not , the prime will
satisfy the condition that .
if only  is set and  is set to , the prime
will instead satisfy the condition that . this is necessary
because  for  would contradict the condition
enforced by .
is ignored if  is not given.
both  and  must be encoded as big-endian sequences
if given as an , , , , or
by default, the prime is encoded as a big-endian sequence of octets
in an . if the  option is , then a
is provided.
:  |  the name or nid of the cipher to query.
:  a test key length.
:  a test iv length.
the name of the cipher
the nid of the cipher
the block size of the cipher in bytes. this property
is omitted when  is .
the expected or default initialization vector length in
bytes. this property is omitted if the cipher does not use an initialization
vector.
the expected or default key length in bytes.
the cipher mode. one of , , , ,
, , , , , , .
returns information about a given cipher.
some ciphers accept variable length keys and initialization vectors. by default,
the  method will return the default values for these
ciphers. to test if a given key length or iv length is acceptable for given
cipher, use the  and  options. if the given values are
unacceptable,  will be returned.
returns:  an array with the names of the supported cipher
algorithms.
added in: v2.3.0
returns:  an array with the names of the supported elliptic curves.
creates a predefined  key exchange object. the
supported groups are listed in the documentation for .
the returned object mimics the interface of objects created by
, but will not allow changing
the keys (with , for example). the
advantage of using this method is that the parties do not have to
generate nor exchange a group modulus beforehand, saving both processor
and communication time.
returns:   if and only if a fips compliant crypto provider is
currently in use,  otherwise. a future semver-major release may change
the return type of this api to a .
returns:  an array of the names of the supported hash algorithms,
such as . hash algorithms are also called "digest" algorithms.
added in: v17.4.0
returns:  |  |  |  returns .
a convenient alias for . this
implementation is not compliant with the web crypto spec, to write
web-compatible code use  instead.
the digest algorithm to use.
|  |  |  |  |  the input
keying material. must be provided but can be zero-length.
|  |  |  |  the salt value. must
be provided but can be zero-length.
|  |  |  |  additional info value.
must be provided but can be zero-length, and cannot be more than 1024 bytes.
the length of the key to generate. must be greater than 0.
the maximum allowable value is  times the number of bytes produced by
the selected digest function (e.g.  generates 64-byte hashes, making
the maximum hkdf output 16320 bytes).
hkdf is a simple key derivation function defined in rfc 5869. the given ,
and  are used with the  to derive a key of  bytes.
the supplied  function is called with two arguments:  and
. if an errors occurs while deriving the key,  will be set;
otherwise  will be . the successfully generated  will
be passed to the callback as an . an error will be thrown if any
of the input arguments specify invalid values or types.
provides a synchronous hkdf key derivation function as defined in rfc 5869. the
given ,  and  are used with the  to derive a key of
bytes.
the successfully generated  will be returned as an .
an error will be thrown if any of the input arguments specify invalid values or
types, or if the derived key cannot be generated.
provides an asynchronous password-based key derivation function 2 (pbkdf2)
implementation. a selected hmac digest algorithm specified by  is
applied to derive a key of the requested byte length () from the
,  and .
. if an error occurs while deriving the key,  will be set;
otherwise  will be . by default, the successfully generated
will be passed to the callback as a . an error will be
thrown if any of the input arguments specify invalid values or types.
the  argument must be a number set as high as possible. the
higher the number of iterations, the more secure the derived key will be,
but will take a longer amount of time to complete.
the  should be as unique as possible. it is recommended that a salt is
random and at least 16 bytes long. see  for details.
the  property can be used to change the way the
is passed to the callback. this property, however, has been
deprecated and use should be avoided.
an array of supported digest functions can be retrieved using
this api uses libuv's threadpool, which can have surprising and
negative performance implications for some applications; see the
documentation for more information.
provides a synchronous password-based key derivation function 2 (pbkdf2)
if an error occurs an  will be thrown, otherwise the derived key will be
returned as a .
the  property may be used to change the way the
is returned. this property, however, is deprecated and use
should be avoided.
the hash function to use for oaep padding and mgf1.
|  |  |  |  the label to
use for oaep padding. if not specified, no label is used.
an optional padding value defined in
, which may be: ,
, or
returns:  a new  with the decrypted content.
decrypts  with .  was previously encrypted using
the corresponding public key, for example using .
object, the  property can be passed. otherwise, this function uses
a pem encoded private key.
|  |  |  |  an optional
passphrase for the private key.
, which may be:  or
the string encoding to use when , ,
or  are strings.
returns:  a new  with the encrypted content.
encrypts  with . the returned data can be decrypted using
decrypts  with . was previously encrypted using
the corresponding private key, for example using .
because rsa public keys can be derived from private keys, a private key may
a pem encoded public or private key, , or .
, or  are strings.
encrypts the content of  with  and returns a new
with encrypted content. the returned data can be decrypted using
the number of bytes to generate.  the  must
not be larger than .
returns:  if the  function is not provided.
generates cryptographically strong pseudorandom data. the  argument
is a number indicating the number of bytes to generate.
if a  function is provided, the bytes are generated asynchronously
and the  function is invoked with two arguments:  and .
if an error occurs,  will be an  object; otherwise it is . the
argument is a  containing the generated bytes.
if the  function is not provided, the random bytes are generated
synchronously and returned as a . an error will be thrown if
there is a problem generating the bytes.
the  method will not complete until there is
sufficient entropy available.
this should normally never take longer than a few milliseconds. the only time
when generating the random bytes may conceivably block for a longer period of
time is right after boot, when the whole system is still low on entropy.
the asynchronous version of  is carried out in a single
threadpool request. to minimize threadpool task length variation, partition
large  requests when doing so as part of fulfilling a client
request.
|  |  |  must be supplied. the
size of the provided  must not be larger than .
default: . the  must
returns:  |  |  |  the object passed as
synchronous version of .
any ,  or  instance may be passed as
this function is similar to  but requires the first
argument to be a  that will be filled. it also
requires that a callback is passed in.
if the  function is not provided, an error will be thrown.
any , , or  instance may be passed as
while this includes instances of  and , this
function should not be used to generate random floating-point numbers. the
result may contain , , and , and even if the array
contains finite numbers only, they are not drawn from a uniform random
distribution and have no meaningful lower or upper bounds.
start of random range (inclusive). default: .
end of random range (exclusive).
return a random integer  such that .  this
implementation avoids .
the range () must be less than 248.  and  must
be .
if the  function is not provided, the random integer is
generated synchronously.
added in: v15.6.0, v14.17.0
by default, to improve performance,
node.js generates and caches enough
random data to generate up to 128 random uuids. to generate a uuid
without using the cache, set  to .
generates a random  version 4 uuid. the uuid is generated using a
cryptographic pseudorandom number generator.
cpu/memory cost parameter. must be a power of two greater
than one. default: .
block size parameter. default: .
parallelization parameter. default: .
alias for . only one of both may be specified.
memory upper bound. it is an error when (approximately)
provides an asynchronous  implementation. scrypt is a password-based
key derivation function that is designed to be expensive computationally and
memory-wise in order to make brute-force attacks unrewarding.
the  function is called with two arguments:  and .
is an exception object when key derivation fails, otherwise  is
.  is passed to the callback as a .
an exception is thrown when any of the input arguments specify invalid values
or types.
provides a synchronous  implementation. scrypt is a password-based
an exception is thrown when key derivation fails, otherwise the derived key is
the total allocated secure heap size as specified
using the  command-line flag.
the minimum allocation from the secure heap as
specified using the  command-line flag.
the total number of bytes currently allocated from
the secure heap.
the calculated ratio of  to
allocated bytes.
added in: v0.11.11
load and set the  for some or all openssl functions (selected by flags).
could be either an id or a path to the engine's shared library.
the optional  argument uses  by default. the
is a bit field taking one of or a mix of the following flags (defined in
to enable fips mode.
enables the fips compliant crypto provider in a fips-enabled node.js build.
throws an error if fips mode is not available.
calculates and returns the signature for  using the given private key and
algorithm. if  is  or , then the algorithm is
dependent upon the key type (especially ed25519 and ed448).
if  is not a , this function behaves as if  had been
passed to . if it is an object, the following
additional properties can be passed:
used to sign the message as specified in section 3.1 of .
if the  function is provided this function uses libuv's threadpool.
a convenient alias for .
this function compares the underlying bytes that represent the given
, , or  instances using a constant-time
algorithm.
this function does not leak timing information that
would allow an attacker to guess one of the values. this is suitable for
comparing hmac digests or secret values like authentication cookies or
and  must both be s, s, or s, and they
must have the same byte length. an error is thrown if  and  have
different byte lengths.
if at least one of  and  is a  with more than one byte per
entry, such as , the result will be computed using the platform
byte order.
when both of the inputs are s or
s, this function might return unexpected results due to ieee 754
encoding of floating-point numbers. in particular, neither  nor
implies that the byte representations of two floating-point
numbers  and  are equal.
use of  does not guarantee that the surrounding code
is timing-safe. care should be taken to ensure that the surrounding code does
not introduce timing vulnerabilities.
signature for the data and public key if the  function is not
provided.
verifies the given signature for  using the given key and algorithm. if
is  or , then the algorithm is dependent upon the
key type (especially ed25519 and ed448).
the  argument is the previously calculated signature for the .
because public keys can be derived from private keys, a private key or a public
key may be passed for .
type:  an implementation of the web crypto api standard.
see the  for details.
notes
using strings as inputs to cryptographic apis
for historical reasons, many cryptographic apis provided by node.js accept
strings as inputs where the underlying cryptographic algorithm works on byte
sequences. these instances include plaintexts, ciphertexts, symmetric keys,
initialization vectors, passphrases, salts, authentication tags,
and additional authenticated data.
when passing strings to cryptographic apis, consider the following factors.
not all byte sequences are valid utf-8 strings. therefore, when a byte
sequence of length  is derived from a string, its entropy is generally
lower than the entropy of a random or pseudorandom  byte sequence.
for example, no utf-8 string will result in the byte sequence . secret
keys should almost exclusively be random or pseudorandom byte sequences.
similarly, when converting random or pseudorandom byte sequences to utf-8
strings, subsequences that do not represent valid code points may be replaced
by the unicode replacement character (). the byte representation of
the resulting unicode string may, therefore, not be equal to the byte sequence
that the string was created from.
the outputs of ciphers, hash functions, signature algorithms, and key
derivation functions are pseudorandom byte sequences and should not be
used as unicode strings.
when strings are obtained from user input, some unicode characters can be
represented in multiple equivalent ways that result in different byte
sequences. for example, when passing a user passphrase to a key derivation
function, such as pbkdf2 or scrypt, the result of the key derivation function
depends on whether the string uses composed or decomposed characters. node.js
does not normalize character representations. developers should consider using
on user inputs before passing them to
cryptographic apis.
legacy streams api (prior to node.js 0.10)
the crypto module was added to node.js before there was the concept of a
unified stream api, and before there were  objects for handling
binary data. as such, the many of the  defined classes have methods not
typically found on other node.js classes that implement the
api (e.g. , , or ). also, many methods accepted
and returned  encoded strings by default rather than s. this
default was changed after node.js v0.8 to use  objects by default
support for weak or compromised algorithms
the  module still supports some algorithms which are already
compromised and are not currently recommended for use. the api also allows
the use of ciphers and hashes with a small key size that are too weak for safe
use.
users should take full responsibility for selecting the crypto
algorithm and key size according to their security requirements.
based on the recommendations of :
md5 and sha-1 are no longer acceptable where collision resistance is
required such as digital signatures.
the key used with rsa, dsa, and dh algorithms is recommended to have
at least 2048 bits and that of the curve of ecdsa and ecdh at least
224 bits, to be safe to use for several years.
the dh groups of ,  and  have a key size
smaller than 2048 bits and are not recommended.
see the reference for other recommendations and details.
some algorithms that have known weaknesses and are of little relevance in
practice are only available through the , which is not
enabled by default.
ccm mode
ccm is one of the supported . applications which use this
mode must adhere to certain restrictions when using the cipher api:
the authentication tag length must be specified during cipher creation by
setting the  option and must be one of 4, 6, 8, 10, 12, 14 or
16 bytes.
the length of the initialization vector (nonce)  must be between 7 and 13
bytes ().
the length of the plaintext is limited to  bytes.
when decrypting, the authentication tag must be set via  before
otherwise, decryption will fail and  will throw an error in
compliance with section 2.6 of .
using stream methods such as ,  or  in ccm
mode might fail as ccm cannot handle more than one chunk of data per instance.
when passing additional authenticated data (aad), the length of the actual
message in bytes must be passed to  via the
option.
many crypto libraries include the authentication tag in the ciphertext,
which means that they produce ciphertexts of the length
. node.js does not include the authentication
tag, so the ciphertext length is always .
this is not necessary if no aad is used.
as ccm processes the whole message at once,  must be called exactly
once.
even though calling  is sufficient to encrypt/decrypt the message,
applications must call  to compute or verify the
authentication tag.
crypto constants
the following constants exported by  apply to various uses of
the , , and  modules and are generally
specific to openssl.
openssl options
openssl engine constants
other openssl constants
node.js crypto constants
debugger
node.js includes a command-line debugging utility. the node.js debugger client
is not a full-featured debugger, but simple stepping and inspection are
possible.
to use it, start node.js with the  argument followed by the path to the
script to debug.
the debugger automatically breaks on the first executable line. to instead
run until the first breakpoint (specified by a  statement), set
the  environment variable to .
the  command allows code to be evaluated remotely. the  command
steps to the next line. type  to see what other commands are available.
pressing  without typing a command will repeat the previous debugger
watchers
it is possible to watch expression and variable values while debugging. on
every breakpoint, each expression from the watchers list will be evaluated
in the current context and displayed immediately before the breakpoint's
source code listing.
to begin watching an expression, type . the command
will print the active watchers. to remove a watcher, type
command reference
stepping
, : continue execution
, : step next
, : step in
, : step out
: pause running code (like pause button in developer tools)
breakpoints
, : set breakpoint on current line
, : set breakpoint on specific line
, : set breakpoint on a first statement in
function's body
, : set breakpoint on first line of
, : set conditional
breakpoint on first line of  that only breaks when
evaluates to
, : clear breakpoint in
on line 1
it is also possible to set a breakpoint in a file (module) that
is not loaded yet:
it is also possible to set a conditional breakpoint that only breaks when a
given expression evaluates to :
information
, : print backtrace of current execution frame
: list scripts source code with 5 line context (5 lines before and
after)
: add expression to watch list
: remove expression from watch list
: list all watchers and their values (automatically listed on each
breakpoint)
: open debugger's repl for evaluation in debugging script's context
, : execute an expression in debugging script's context and
print its value
execution control
: run script (automatically runs on debugger's start)
: restart script
: kill script
various
: list all loaded scripts
: display v8's version
advanced usage
v8 inspector integration for node.js
v8 inspector integration allows attaching chrome devtools to node.js
instances for debugging and profiling. it uses the
v8 inspector can be enabled by passing the  flag when starting a
node.js application. it is also possible to supply a custom port with that flag,
e.g.  will accept devtools connections on port 9222.
to break on the first line of the application code, pass the
flag instead of .
(in the example above, the uuid dc9010dd-f8b8-4ac5-a510-c1a114ec7d29
at the end of the url is generated on the fly, it varies in different
debugging sessions.)
if the chrome browser is older than 66.0.3345.0,
use  instead of  in the above url.
chrome devtools doesn't support debugging  yet.
can be used to debug them.
deprecated apis
node.js apis might be deprecated for any of the following reasons:
use of the api is unsafe.
an improved alternative api is available.
breaking changes to the api are expected in a future major release.
node.js uses three kinds of deprecations:
documentation-only
runtime
end-of-life
a documentation-only deprecation is one that is expressed only within the
node.js api docs. these generate no side-effects while running node.js.
some documentation-only deprecations trigger a runtime warning when launched
with  flag (or its alternative,
environment variable), similarly to runtime
deprecations below. documentation-only deprecations that support that flag
are explicitly labeled as such in the
a runtime deprecation will, by default, generate a process warning that will
be printed to  the first time the deprecated api is used. when the
command-line flag is used, a runtime deprecation will
cause an error to be thrown.
an end-of-life deprecation is used when functionality is or will soon be removed
from node.js.
revoking deprecations
occasionally, the deprecation of an api might be reversed. in such situations,
this document will be updated with information relevant to the decision.
however, the deprecation identifier will not be modified.
list of deprecated apis
dep0001:
type: end-of-life
has been removed. use
dep0002:
the  module is deprecated. please use a userland alternative.
dep0003:
the  has been removed. use
dep0004:
the  property was removed.
dep0005:  constructor
type: runtime (supports )
the  function and  constructor are deprecated due to
api usability issues that can lead to accidental security issues.
as an alternative, use one of the following methods of constructing
objects:
: create a  with
initialized memory.
uninitialized memory.
: create a  with uninitialized
memory.
: create a  with a copy of
create a  that wraps the given .
: create a  that copies .
: create a
that copies .
without , runtime warnings occur only for code not in
. this means there will not be deprecation warnings for
usage in dependencies. with , a runtime
warning results no matter where the  usage occurs.
dep0006:
within the  module's , , and
methods, the  option is deprecated. the
option should be used instead.
dep0007: replace   with
in an earlier version of the node.js , a boolean property with the name
was added to the  object. the intent of this property was to
provide an indication of how and why the  instance exited. in node.js
6.0.0, the old property was deprecated and replaced with a new
property. the old property name did not
precisely describe the actual semantics and was unnecessarily emotion-laden.
dep0008:
type: documentation-only
the  module is deprecated. when requiring access to constants
relevant to specific node.js builtin modules, developers should instead refer
to the  property exposed by the relevant module. for instance,
dep0009:  without digest
use of the  api without specifying a digest was deprecated
in node.js 6.0 because the method defaulted to using the non-recommended
digest. previously, a deprecation warning was printed. starting in
node.js 8.0.0, calling  or  with
set to  will throw a .
beginning in node.js v11.0.0, calling these functions with  set to
would print a deprecation warning to align with the behavior when
is .
now, however, passing either  or  will throw a .
dep0010:
the  api was removed. please use
dep0011:
the  class was removed. please use
dep0012:
has been removed. recover from failed i/o actions
explicitly via error event handlers set on the domain instead.
dep0013:  asynchronous function without callback
calling an asynchronous function without a callback throws a
in node.js 10.0.0 onwards. see .
dep0014:  legacy string interface
the  legacy  interface is deprecated. use the
api as mentioned in the documentation instead.
dep0015:  legacy string interface
dep0016: /
the  and  aliases for the  property were deprecated
in node.js 6.0.0 and have since been removed.
dep0017:
was a non-standard extension and has been removed.
dep0018: unhandled promise rejections
unhandled promise rejections are deprecated. by default, promise rejections
that are not handled terminate the node.js process with a non-zero exit
code. to change the way node.js treats unhandled rejections, use the
dep0019:  resolved outside directory
in certain cases,  could resolve outside the package directory.
this behavior has been removed.
dep0020:
the  property was deprecated in node.js v0.9.7 and has
been removed. please use the  method instead.
dep0021:
the  method was deprecated and removed. please use
dep0022:
the  api was deprecated in node.js 7.0.0 and has since been
removed. please use  instead.
dep0023:
the  method is deprecated. please use the
method instead.
dep0024:
the  api has been removed.
dep0025:
type: runtime
the  module is deprecated. please use the  module instead.
dep0026:
has been removed. please use  instead.
dep0027:
dep0028:
dep0029:
dep0030:
the  class is deprecated. please use
dep0031:
the  method is now deprecated as its inclusion in the
api is not useful.
dep0032:  module
the  module is deprecated and should not be used.
dep0033:
the  api is
deprecated. please use  instead.
dep0034:
the  api is deprecated. please use
or  instead.
dep0035:
the  api is deprecated.
dep0036:
dep0037:
type: deprecation revoked
the  api was deprecated. the
deprecation was revoked because the requisite supporting apis were added in
libuv.
dep0038:
the  api was deprecated. the deprecation was
revoked because the requisite supporting apis were added in libuv.
dep0039:
the  property is deprecated.
dep0040:  module
type: documentation-only (supports )
the  module is deprecated. please use a userland alternative
dep0041:  environment variable
the  environment variable was removed. please use
dep0042:
dep0043:
dep0044:
dep0045:
dep0046:
dep0047:
dep0048:
dep0049:
dep0050:
dep0051:
dep0052:
dep0053:
dep0054:
dep0055:
dep0056:
dep0057:
dep0058:
dep0059:
dep0060:
dep0061:
the  class was never intended to be a publicly accessible
api and has been removed. no alternative api is available. please use a userland
alternative.
dep0062:
activates the legacy v8 debugger interface, which was removed as
of v8 5.8. it is replaced by inspector which is activated with
dep0063:
the  module  api is
the  method was never documented as an
officially supported api.
dep0064:
the  api was deprecated in documentation in node.js
0.11.3. users should use  instead.
dep0065:  and
the  module's  constant, used for  option,
has been removed. its behavior has been functionally identical to that of
since node.js 6.0.0, when v8 5.0 was imported. please use
the  environment variable is used to set the underlying
of an interactive  session. its value, , is also
dep0066:
the  module  and
properties are deprecated. use one of
the public methods (e.g. ,
) for working with outgoing headers.
the  and
properties were never documented as
officially supported properties.
dep0067:
deprecated.
the  property was never documented as
an officially supported api.
dep0068:
corresponds to the legacy cli debugger which has been replaced with
a v8-inspector based cli debugger available through .
dep0069:
debugcontext has been removed in v8 and is not available in node.js 10+.
debugcontext was an experimental api.
dep0070:
was renamed to  for
clarity.
this change was made while  was an experimental api.
dep0071:
dep0072:
was renamed to
for clarity.
dep0073: several internal properties of
accessing several internal, undocumented properties of  instances
with inappropriate names is deprecated.
as the original api was undocumented and not generally useful for non-internal
code, no replacement api is provided.
dep0074:
the  property was deprecated in favor of
dep0075:
was removed from userland visibility.
dep0076:
was a trivial parsing helper that was made public by
mistake. while it was supposed to parse certificate subject and issuer strings,
it never handled multi-value relative distinguished names correctly.
earlier versions of this document suggested using  as an
alternative to . however,  also does
not handle all certificate subjects correctly and should not be used.
dep0077:
is deprecated.
the  function was never documented as an officially
supported api.
dep0078:
dep0079: custom inspection function on objects via
using a property named  on an object to specify a custom inspection
function for  is deprecated. use
instead. for backward compatibility with node.js prior to version 6.4.0, both
can be specified.
dep0080:
the internal  was not intended for public use. however,
userland modules have found it useful. the internal api is deprecated
and replaced with an identical, public  method.
dep0081:  using a file descriptor
usage with a file descriptor is
deprecated. please use  or  to work with
file descriptors.
dep0082:
is only necessary for the internal mechanics of
the  itself. do not use this function.
dep0083: disabling ecdh by setting  to
type: end-of-life.
the  option to  and  could
be set to  to disable ecdh entirely on the server only. this mode was
deprecated in preparation for migrating to openssl 1.1.0 and consistency with
the client and is now unsupported. use the  parameter instead.
dep0084: requiring bundled internal dependencies
since node.js versions 4.4.0 and 5.2.0, several modules only intended for
internal usage were mistakenly exposed to user code through . these
modules were:
(from 7.6.0)
the  modules do not have any exports, and if not imported in a specific
order would in fact throw errors. as such there are virtually no legitimate use
cases for importing them through .
on the other hand,  can be installed locally through a package
manager, as it is published on the npm registry under the same name. no source
code modification is necessary if that is done.
dep0085: asynchooks sensitive api
the asynchooks sensitive api was never documented and had various minor issues.
use the  api instead. see
dep0086: remove
doesn't emit the  or  event and can thus
cause a lot of issues. see .
dep0089:
importing assert directly was not recommended as the exposed functions use
loose equality checks. the deprecation was revoked because use of the
module is not discouraged, and the deprecation caused developer
confusion.
dep0090: invalid gcm authentication tag lengths
node.js used to support all gcm authentication tag lengths which are accepted by
openssl when calling . beginning with node.js
v11.0.0, only authentication tag lengths of 128, 120, 112, 104, 96, 64, and 32
bits are allowed. authentication tags of other lengths are invalid per
dep0091:
dep0092: top-level  bound to
assigning properties to the top-level  as an alternative
to  is deprecated. developers should use
dep0093:  is deprecated and replaced
the  property is deprecated. please use
and  instead.
dep0094: using  with more than one argument
using  with more than one argument is deprecated. use
with only one argument or use a different  module
dep0095:
is deprecated. please use the publicly documented
dep0096:
dep0097:  with  property
users of  that add the  property to carry context,
should start using the  variant of  or
, or the high-level  class.
dep0098: asynchooks embedder  and  apis
the embedded api provided by asynchooks exposes  and
methods which are very easy to use incorrectly which can lead
to unrecoverable errors.
use  api instead which provides a much
safer, and more convenient, alternative. see
dep0099: async context-unaware  c++ apis
type: compile-time
certain versions of  apis available to native addons are
deprecated. please use the versions of the api that accept an
dep0100:
is deprecated. please use the  module instead.
this was never a documented feature.
dep0101:
the  compile-time option has been removed.
dep0102: using  in  operations
using the  argument has no functionality anymore. all input is
verified regardless of the value of . skipping the verification
could lead to hard-to-find errors and crashes.
dep0103:  typechecks
using  in general should be avoided. the type checking
methods in particular can be replaced by using .
this deprecation has been superseded by the deprecation of the
api ().
dep0104:  string coercion
when assigning a non-string property to , the assigned value is
implicitly converted to a string. this behavior is deprecated if the assigned
value is not a string, boolean, or number. in the future, such assignment might
result in a thrown error. please convert the property to a string before
assigning it to .
dep0105:
has never been documented and was an alias for
. this api has been removed, and it is recommended to use
dep0106:  and
using  and  must be
avoided as they use a weak key derivation function (md5 with no salt) and static
initialization vectors. it is recommended to derive a key using
or  with random salts and to use
and  to obtain the
and  objects respectively.
dep0107:
this was an undocumented helper function not intended for use outside node.js
core and obsoleted by the removal of npn (next protocol negotiation) support.
dep0108:
deprecated alias for . this original name was chosen
because it also made sense to interpret the value as the number of bytes
read by the engine, but is inconsistent with other streams in node.js that
expose values under these names.
dep0109: , , and  support for invalid urls
some previously supported (but strictly invalid) urls were accepted through the
, , ,
, and  apis because those were
accepted by the legacy  api. the mentioned apis now use the whatwg
url parser that requires strictly valid urls. passing an invalid url is
deprecated and support will be removed in the future.
dep0110:  cached data
the  option is deprecated. use
dep0111:
is for use by node.js internal code only.
dep0112:  private apis
the  module previously contained several apis that were never meant
to accessed outside of node.js core: ,
dep0113: ,
and  are no longer available. they
were never documented and would throw when called.
dep0114:
the  function was not designed to be used by modules outside
of node.js core and was removed.
dep0115: , ,
in recent versions of node.js, there is no difference between
and . the latter is
deprecated along with the undocumented aliases  and
in favor of  and might be removed in a
future release.
dep0116: legacy url api
the  is deprecated. this includes ,
, , and the . please
use the  instead.
dep0117: native crypto handles
previous versions of node.js exposed handles to internal native objects through
the  property of the , , ,
, , , , , and  classes.
the  property has been removed because improper use of the native
object can lead to crashing the application.
dep0118:  support for a falsy host name
previous versions of node.js supported  with a falsy host name
like  due to backward compatibility.
this behavior is undocumented and is thought to be unused in real world apps.
it will become an error in future versions of node.js.
dep0119:  private api
is deprecated. please use
dep0120: windows performance counter support
windows performance counter support has been removed from node.js. the
undocumented ,
functions have been deprecated.
dep0121:
the undocumented  function was originally
intended for debugging and performance tuning when using the
and  modules on windows. the function is not
generally useful and is being removed. see discussion here:
dep0122:
please use  instead.
dep0123: setting the tls servername to an ip address
setting the tls servername to an ip address is not permitted by
. this will be ignored in a future version.
dep0124: using
this property is a reference to the instance itself.
dep0125:
the  module is deprecated.
dep0126:
the previously undocumented  is deprecated.
please use the publicly documented  instead.
if re-referencing the timeout is necessary,  can be used
with no performance impact since node.js 10.
dep0127:
the previously undocumented and "private"  is deprecated.
if unreferencing the timeout is necessary,  can be used
dep0128: modules with an invalid  entry and an  file
modules that have an invalid  entry (e.g., ) and
also have an  file in the top level directory will resolve the
file. that is deprecated and is going to throw an error in future
node.js versions.
dep0129:
the  property of child process objects returned by  and
similar functions is not intended for public use. use
dep0130:
dep0131: legacy http parser
the legacy http parser, used by default in versions of node.js prior to 12.0.0,
is deprecated and has been removed in v13.0.0. prior to v13.0.0, the
command-line flag could be used to revert to using the
legacy parser.
dep0132:  with callback
passing a callback to  is deprecated. use the returned
instead, or a listener to the worker's  event.
dep0133:
prefer  over  and
over .
dep0134:
dep0135:  and  are internal
and  are undocumented internal
apis that do not make sense to use in userland. file streams should always be
opened through their corresponding factory methods
and ) or by passing a file descriptor in options.
dep0136:
indicates whether  has been
called, not whether  has been emitted and the underlying data
is flushed.
use  or
accordingly instead to avoid the ambiguity.
to maintain existing behavior  should be replaced with
dep0137: closing fs.filehandle on garbage collection
allowing a  object to be closed on garbage collection is
deprecated. in the future, doing so might result in a thrown error that will
terminate the process.
please ensure that all  objects are explicitly closed using
when the  is no longer needed:
dep0138:
is a commonjs-only feature while  global
object is shared with non-commonjs environment. its use within ecmascript
modules is unsupported.
it is deprecated in favor of , because it serves the same
purpose and is only available on commonjs environment.
dep0139:  with no arguments
calling  with no argument causes the process-wide umask to be
written twice. this introduces a race condition between threads, and is a
potential security vulnerability. there is no safe, cross-platform alternative
dep0140: use  instead of
use  instead of .
dep0141:  and
the  module exported the input and output stream twice. use
instead of  and  instead of .
dep0142:
the  module exports a  property that contains an array
of built-in modules. it was incomplete so far and instead it's better to rely
upon .
dep0143:
will be removed in future versions where it is
no longer required due to simplification of the implementation.
dep0144:
a commonjs module can access the first module that required it using
. this feature is deprecated because it does not work
consistently in the presence of ecmascript modules and because it gives an
inaccurate representation of the commonjs module graph.
some modules use it to check if they are the entry point of the current process.
instead, it is recommended to compare  and :
when looking for the commonjs modules that have required the current one,
and  can be used:
dep0145:
is just an alias for .
dep0146:
the  is deprecated. use
dep0147:
in future versions of node.js,  option will be ignored for
, , and .
use ,
dep0148: folder mappings in  (trailing )
using a trailing  to define subpath folder mappings in the
or  fields is deprecated. use
dep0149:
type: documentation-only.
prefer  over .
dep0150: changing the value of
the  property provides access to node.js compile-time settings.
however, the property is mutable and therefore subject to tampering. the ability
to change the value will be removed in a future version of node.js.
dep0151: main index lookup and extension searching
previously,  and extension searching lookups would apply to
main entry point resolution, even when resolving es modules.
with this deprecation, all es module main entry point resolutions require
an explicit  with the exact file extension.
dep0152: extension performanceentry properties
the , , and   object types have
additional properties assigned to them that provide additional information.
these properties are now available within the standard  property
of the  object. the existing accessors have been
deprecated and should no longer be used.
dep0153:  and  options type coercion
using a non-nullish non-integer value for  option, a non-nullish
non-number value for  option, a non-nullish non-boolean value for
option, or a non-nullish non-boolean value for  option in
and  throws an
dep0154: rsa-pss generate key pair options
the  and  options are replaced with
dep0155: trailing slashes in pattern specifier resolutions
the remapping of specifiers ending in  like  is deprecated
for package  and  pattern resolutions.
dep0156:  property and ,  event in
move to  api instead, as the ,
, and  are all stream-based.
check  instead of the  property, and listen for
instead of ,  event.
the  property and  event are only useful for detecting
calls. for closing a request early, use the stream
then check the  property and  event
should have the same effect. the receiving end should also check the
value on  to get whether
it was an aborted or graceful destroy.
dep0157: thenable support in streams
an undocumented feature of node.js streams was to support thenables in
implementation methods. this is now deprecated, use callbacks instead and avoid
use of async function for streams implementation methods.
this feature caused users to encounter unexpected problems where the user
implements the function in callback style but uses e.g. an async method which
would cause an error since mixing promise and callback semantics is not valid.
dep0158:
this method was deprecated because it is not compatible with
, which is a superclass of .
use  which does the same thing instead.
dep0159:
this error code was removed due to adding more confusion to
the errors used for value type validation.
dep0160:
type: runtime.
this event was deprecated because it did not work with v8 promise combinators
which diminished its usefulness.
dep0161:  and
functions are not intended for public use and can be removed in future
releases.
use  to get a list of types of active
resources and not the actual references.
dep0162: ,  coercion to string
implicit coercion of objects with own  property, passed as second
parameter in , , ,
, and  is deprecated.
convert them to primitive strings.
dep0163: ,
these methods were deprecated because they can be used in a way which does not
hold the channel reference alive long enough to receive the events.
which does the same
thing instead.
dep0164: ,  coercion to integer
values other than , , integer numbers, and integer strings
(e.g., ) are deprecated as value for the  parameter in
and as value to assign to .
dep0165:
the  flag is deprecated.
dep0166: double slashes in imports and exports targets
package imports and exports targets mapping into paths including a double slash
(of "/" or "\") are deprecated and will fail with a resolution validation
error in a future release. this same deprecation also applies to pattern matches
starting or ending in a slash.
dep0167: weak  instances (, , )
the well-known modp groups , , and  are deprecated because
they are not secure against practical attacks. see  for
details.
these groups might be removed in future versions of node.js. applications that
rely on these groups should evaluate using stronger modp groups instead.
dep0168: unhandled exception in node-api callbacks
the implicit suppression of uncaught exceptions in node-api callbacks is now
set the flag  to force node.js
to emit an  event if the exception is not handled in
node-api callbacks.
diagnostics channel
the  module provides an api to create named channels
to report arbitrary message data for diagnostics purposes.
it is intended that a module writer wanting to report diagnostics messages
will create one or many top-level channels to report messages through.
channels may also be acquired at runtime but it is not encouraged
due to the additional overhead of doing so. channels may be exported for
convenience, but as long as the name is known it can be acquired anywhere.
if you intend for your module to produce diagnostics data for others to
consume it is recommended that you include documentation of what named
channels are used along with the shape of the message data. channel names
should generally include the module name to avoid collisions with data from
other modules.
public api
|  the channel name
returns:  if there are active subscribers
check if there are active subscribers to the named channel. this is helpful if
the message you want to send might be expensive to prepare.
this api is optional but helpful when trying to publish messages from very
performance-sensitive code.
returns:  the named channel object
this is the primary entry-point for anyone wanting to publish to a named
channel. it produces a channel object which is optimized to reduce overhead at
publish time as much as possible.
the handler to receive channel messages
the message data
|  the name of the channel
register a message handler to subscribe to this channel. this message handler
will be run synchronously whenever a message is published to the channel. any
errors thrown in the message handler will trigger an .
the previous subscribed handler to remove
returns:   if the handler was found,  otherwise.
remove a message handler previously registered to this channel with
the class  represents an individual named channel within the data
pipeline. it is used to track subscribers and to publish messages when there
are subscribers present. it exists as a separate object to avoid channel
lookups at publish time, enabling very fast publish speeds and allowing
for heavy use while incurring very minimal cost. channels are created with
, constructing a channel directly
with  is not supported.
check if there are active subscribers to this channel. this is helpful if
the message to send to the channel subscribers
publish a message to any subscribers to the channel. this will trigger
message handlers synchronously so they will execute within the same context.
added in: v15.1.0, v14.17.0deprecated since: v18.7.0
built-in channels
http
emitted when client starts a request.
emitted when client receives a response.
emitted when server receives a request.
emitted when server sends a response.
emitted when a new tcp or pipe client socket is created.
emitted when a new tcp or pipe connection is received.
emitted when a new udp socket is created.
the  module enables name resolution. for example, use it to look up ip
addresses of host names.
although named for the , it does not always use the
dns protocol for lookups.  uses the operating system
facilities to perform name resolution. it may not need to perform any network
communication. to perform name resolution the way other applications on the same
system do, use .
all other functions in the  module connect to an actual dns server to
perform name resolution. they will always use the network to perform dns
queries. these functions do not use the same set of configuration files used by
(e.g. ). use these functions to always perform
dns queries, bypassing other name-resolution facilities.
see the  for more information.
an independent resolver for dns requests.
creating a new resolver uses the default server settings. setting
the servers used for a resolver using
does not affect
other resolvers:
the following methods from the  module are available:
create a new resolver.
query timeout in milliseconds, or  to use the
default timeout.
the number of tries the resolver will try contacting
each name server before giving up. default:
cancel all outstanding dns queries made by this resolver. the corresponding
callbacks will be called with an error with code .
a string representation of an ipv4 address.
a string representation of an ipv6 address.
the resolver instance will send its requests from the specified ip address.
this allows programs to specify outbound interfaces when used on multi-homed
if a v4 or v6 address is not specified, it is set to the default and the
operating system will choose a local address automatically.
the resolver will use the v4 local address when making requests to ipv4 dns
servers, and the v6 local address when making requests to ipv6 dns servers.
the  of resolution requests has no impact on the local address used.
returns an array of ip address strings, formatted according to ,
that are currently configured for dns resolution. a string will include a port
section if a custom port is used.
|  the record family. must be , , or . for
backward compatibility reasons, and  are interpreted as
and  respectively. the value  indicates that ipv4 and ipv6 addresses
are both returned. default: .
one or more . multiple
flags may be passed by bitwise ing their values.
when , the callback returns all resolved addresses in
an array. otherwise, returns a single address. default: .
when , the callback receives ipv4 and ipv6
addresses in the order the dns resolver returned them. when ,
ipv4 addresses are placed before ipv6 addresses.
default:  (addresses are not reordered). default value is
configurable using  or
a string representation of an ipv4 or ipv6 address.
or , denoting the family of , or  if
the address is not an ipv4 or ipv6 address.  is a likely indicator of a
bug in the name resolution service used by the operating system.
resolves a host name (e.g. ) into the first found a (ipv4) or
aaaa (ipv6) record. all  properties are optional. if  is an
integer, then it must be  or  – if  is  or not provided, then
ipv4 and ipv6 addresses are both returned if found.
with the  option set to , the arguments for  change to
, with  being an array of objects with the
properties  and .
on error,  is an  object, where  is the error code.
keep in mind that  will be set to  not only when
the host name does not exist but also when the lookup fails in other ways
such as no available file descriptors.
does not necessarily have anything to do with the dns protocol.
the implementation uses an operating system facility that can associate names
with addresses and vice versa. this implementation can have subtle but
important consequences on the behavior of any node.js program. please take some
time to consult the  before using
if this method is invoked as its ed version, and
is not set to , it returns a  for an  with  and
supported getaddrinfo flags
the following flags can be passed as hints to .
: limits returned address types to the types of non-loopback
addresses configured on the system. for example, ipv4 addresses are only
returned if the current system has at least one ipv4 address configured.
: if the ipv6 family was specified, but no ipv6 addresses were
found, then return ipv4 mapped ipv6 addresses. it is not supported
on some operating systems (e.g. freebsd 10.1).
: if  is specified, return resolved ipv6 addresses as
well as ipv4 mapped ipv6 addresses.
e.g.
resolves the given  and  into a host name and service using
the operating system's underlying  implementation.
if  is not a valid ip address, a  will be thrown.
the  will be coerced to a number. if it is not a legal port, a
will be thrown.
on an error,  is an  object, where  is the error code.
if this method is invoked as its ed version, it returns a
for an  with  and  properties.
host name to resolve.
resource record type. default: .
uses the dns protocol to resolve a host name (e.g. ) into an array
of the resource records. the  function has arguments
. when successful,  will be an array of resource
records. the type and structure of individual results varies based on :
on error,  is an  object, where  is one of the
retrieves the time-to-live value (ttl) of each record.
when , the callback receives an array of
objects rather than an array of strings,
with the ttl expressed in seconds.
uses the dns protocol to resolve a ipv4 addresses ( records) for the
. the  argument passed to the  function
will contain an array of ipv4 addresses (e.g.
retrieve the time-to-live value (ttl) of each record.
objects rather than an array of
strings, with the ttl expressed in seconds.
uses the dns protocol to resolve ipv6 addresses ( records) for the
will contain an array of ipv6 addresses.
uses the dns protocol to resolve all records (also known as  or  query).
the  argument passed to the  function will be an array containing
various types of records. each object has a property  that indicates the
type of the current record. and depending on the , additional properties
will be present on the object:
here is an example of the  object passed to the callback:
dns server operators may choose not to respond to
queries. it may be better to call individual methods like ,
, and so on. for more details, see .
uses the dns protocol to resolve  records for the . the
argument passed to the  function
will contain an array of canonical name records available for the
(e.g. ).
will contain an array of certification authority authorization records
available for the  (e.g. ).
uses the dns protocol to resolve mail exchange records ( records) for the
. the  argument passed to the  function will
contain an array of objects containing both a  and
property (e.g. ).
uses the dns protocol to resolve regular expression-based records (
records) for the . the  argument passed to the
function will contain an array of objects with the following properties:
uses the dns protocol to resolve name server records ( records) for the
contain an array of name server records available for
uses the dns protocol to resolve pointer records ( records) for the
be an array of strings containing the reply records.
uses the dns protocol to resolve a start of authority record ( record) for
the . the  argument passed to the  function will
be an object with the following properties:
uses the dns protocol to resolve service records ( records) for the
be an array of objects with the following properties:
uses the dns protocol to resolve text queries ( records) for the
. the  argument passed to the  function is a
two-dimensional array of the text records available for  (e.g.
). each sub-array contains txt chunks of
one record. depending on the use case, these could be either joined together or
treated separately.
added in: v0.1.16
performs a reverse dns query that resolves an ipv4 or ipv6 address to an
array of host names.
on error,  is an  object, where  is
one of the .
added in: v16.4.0, v14.18.0
must be  or .
priority than . when using ,
from the main thread won't affect the default
dns orders in workers.
array of  formatted addresses
sets the ip address and port of servers to be used when performing dns
resolution. the  argument is an array of  formatted
addresses. if the port is the iana default dns port (53) it can be omitted.
an error will be thrown if an invalid address is provided.
the  method must not be called while a dns query is in
progress.
the  method affects only ,
and  (and specifically not
this method works much like
that is, if attempting to resolve with the first server provided results in a
error, the  method will not attempt to resolve with
subsequent servers provided. fallback dns servers will only be used if the
earlier ones time out or result in some other error.
dns promises api
the  api provides an alternative set of asynchronous dns methods
that return  objects rather than using callbacks. the api is accessible
via  or .
the following methods from the  api are available:
added in: v15.3.0, v14.17.0
promises will be rejected with an error with the code .
the record family. must be , , or . the value
indicates that ipv4 and ipv6 addresses are both returned. default:
when , the  is resolved with all addresses in
when , the  is resolved with ipv4 and
ipv6 addresses in the order the dns resolver returned them. when ,
default: currently  (addresses are reordered) but this is
expected to change in the not too distant future. default value is
. new code should use .
integer, then it must be  or  – if  is not provided, then ipv4
and ipv6 addresses are both returned if found.
with the  option set to , the  is resolved with
being an array of objects with the properties  and .
on error, the  is rejected with an  object, where
is the error code.
does not necessarily have anything to do with the dns
protocol. the implementation uses an operating system facility that can
associate names with addresses and vice versa. this implementation can have
subtle but important consequences on the behavior of any node.js program. please
take some time to consult the  before
using .
of the resource records. when successful, the  is resolved with an
array of resource records. the type and structure of individual results vary
based on :
is one of the .
when , the  is resolved with an array of
uses the dns protocol to resolve ipv4 addresses ( records) for the
. on success, the  is resolved with an array of ipv4
addresses (e.g. ).
. on success, the  is resolved with an array of ipv6
addresses.
on success, the  is resolved with an array containing various types of
records. each object has a property  that indicates the type of the
current record. and depending on the , additional properties will be
present on the object:
here is an example of the result object:
added in: v15.0.0, v14.17.0
uses the dns protocol to resolve  records for the . on success,
the  is resolved with an array of objects containing available
certification authority authorization records available for the
the  is resolved with an array of canonical name records available for
the  (e.g. ).
. on success, the  is resolved with an array of objects
containing both a  and  property (e.g.
records) for the . on success, the  is resolved with an array
of objects with the following properties:
. on success, the  is resolved with an array of name server
records available for  (e.g.
. on success, the  is resolved with an array of strings
containing the reply records.
the . on success, the  is resolved with an object with the
following properties:
. on success, the  is resolved with an array of objects with
. on success, the  is resolved with a two-dimensional array
of the text records available for  (e.g.
the default is  and  have
higher priority than . when using ,
from the main thread won't affect the
default dns orders in workers.
error codes
each dns query can return one of the following error codes:
: dns server returned an answer with no data.
: dns server claims query was misformatted.
: dns server returned general failure.
: domain name not found.
: dns server does not implement the requested operation.
: dns server refused query.
: misformatted dns query.
: misformatted host name.
: unsupported address family.
: misformatted dns reply.
: could not contact dns servers.
: timeout while contacting dns servers.
: end of file.
: error reading file.
: out of memory.
: channel is being destroyed.
: misformatted string.
: illegal flags specified.
: given host name is not numeric.
: illegal hints flags specified.
: c-ares library initialization not yet performed.
: error loading .
: could not find  function.
: dns query cancelled.
the  api also exports the above error codes, e.g., .
implementation considerations
although  and the various
functions have the same goal of associating a network name with a network
address (or vice versa), their behavior is quite different. these differences
can have subtle but significant consequences on the behavior of node.js
programs.
under the hood,  uses the same operating system facilities
as most other programs. for instance,  will almost always
resolve a given name the same way as the  command. on most posix-like
operating systems, the behavior of the  function can be
modified by changing settings in  and/or ,
but changing these files will change the behavior of all other
programs running on the same operating system.
though the call to  will be asynchronous from javascript's
perspective, it is implemented as a synchronous call to  that runs
on libuv's threadpool. this can have surprising negative performance
implications for some applications, see the
various networking apis will call  internally to resolve
host names. if that is an issue, consider resolving the host name to an address
using  and using the address instead of a host name. also, some
networking apis (such as  and )
allow the default resolver, , to be replaced.
these functions are implemented quite differently than . they
do not use  and they always perform a dns query on the
network. this network communication is always done asynchronously and does not
use libuv's threadpool.
as a result, these functions cannot have the same negative impact on other
processing that happens on libuv's threadpool that  can have.
they do not use the same set of configuration files that
uses. for instance, they do not use the configuration from .
domain
this module is pending deprecation. once a replacement api has been
finalized, this module will be fully deprecated. most developers should
not have cause to use this module. users who absolutely must have
the functionality that domains provide may rely on it for the time being
but should expect to have to migrate to a different solution
in the future.
domains provide a way to handle multiple different io operations as a
single group. if any of the event emitters or callbacks registered to a
domain emit an  event, or throw an error, then the domain object
will be notified, rather than losing the context of the error in the
handler, or causing the program to
exit immediately with an error code.
warning: don't ignore errors!
domain error handlers are not a substitute for closing down a
process when an error occurs.
by the very nature of how  works in javascript, there is almost
never any way to safely "pick up where it left off", without leaking
references, or creating some other sort of undefined brittle state.
the safest way to respond to a thrown error is to shut down the
process. of course, in a normal web server, there may be many
open connections, and it is not reasonable to abruptly shut those down
because an error was triggered by someone else.
the better approach is to send an error response to the request that
triggered the error, while letting the others finish in their normal
time, and stop listening for new requests in that worker.
in this way,  usage goes hand-in-hand with the cluster module,
since the primary process can fork a new worker when a worker
encounters an error. for node.js programs that scale to multiple
machines, the terminating proxy or service registry can take note of
the failure, and react accordingly.
for example, this is not a good idea:
by using the context of a domain, and the resilience of separating our
program into multiple worker processes, we can react more
appropriately, and handle errors with much greater safety.
additions to  objects
any time an  object is routed through a domain, a few extra fields
are added to it.
the domain that first handled the error.
the event emitter that emitted an  event
with the error object.
the callback function which was bound to the
domain, and passed an error as its first argument.
a boolean indicating whether the error was
thrown, emitted, or passed to a bound callback function.
implicit binding
if domains are in use, then all new  objects (including
stream objects, requests, responses, etc.) will be implicitly bound to
the active domain at the time of their creation.
additionally, callbacks passed to lowlevel event loop requests (such as
to , or other callback-taking methods) will automatically be
bound to the active domain. if they throw, then the domain will catch
in order to prevent excessive memory usage,  objects themselves
are not implicitly added as children of the active domain. if they
were, then it would be too easy to prevent request and response objects
from being properly garbage collected.
to nest  objects as children of a parent  they must be
explicitly added.
implicit binding routes thrown errors and  events to the
's  event, but does not register the  on the
implicit binding only takes care of thrown errors and  events.
explicit binding
sometimes, the domain in use is not the one that ought to be used for a
specific event emitter. or, the event emitter could have been created
in the context of one domain, but ought to instead be bound to some
other domain.
for example, there could be one domain in use for an http server, but
perhaps we would like to have a separate domain to use for each request.
that is possible via explicit binding.
the  class encapsulates the functionality of routing errors and
uncaught exceptions to the active  object.
to handle the errors that it catches, listen to its  event.
an array of timers and event emitters that have been explicitly added
to the domain.
|  emitter or timer to be added to the domain
explicitly adds an emitter to the domain. if any event handlers called by
the emitter throw an error, or if the emitter emits an  event, it
will be routed to the domain's  event, just like with implicit
binding.
this also works with timers that are returned from  and
. if their callback function throws, it will be caught by
the domain  handler.
if the timer or  was already bound to a domain, it is removed
from that one, and bound to this one instead.
the callback function
returns:  the bound function
the returned function will be a wrapper around the supplied callback
function. when the returned function is called, any errors that are
thrown will be routed to the domain's  event.
the  method is plumbing used by the , , and
methods to set the active domain. it sets  and
to the domain, and implicitly pushes the domain onto the domain
stack managed by the domain module (see  for details on the
domain stack). the call to  delimits the beginning of a chain of
asynchronous calls and i/o operations bound to a domain.
calling  changes only the active domain, and does not alter the domain
itself.  and  can be called an arbitrary number of times on a
single domain.
the  method exits the current domain, popping it off the domain stack.
any time execution is going to switch to the context of a different chain of
asynchronous calls, it's important to ensure that the current domain is exited.
the call to  delimits either the end of or an interruption to the chain
of asynchronous calls and i/o operations bound to a domain.
if there are multiple, nested domains bound to the current execution context,
will exit any domains nested within this domain.
returns:  the intercepted function
this method is almost identical to . however, in
addition to catching thrown errors, it will also intercept
objects sent as the first argument to the function.
in this way, the common  pattern can be replaced
with a single error handler in a single place.
|  emitter or timer to be removed from the domain
the opposite of . removes domain handling from the
specified emitter.
run the supplied function in the context of the domain, implicitly
binding all event emitters, timers, and lowlevel requests that are
created in that context. optionally, arguments can be passed to
the function.
this is the most basic way to use a domain.
in this example, the  handler will be triggered, rather
than crashing the program.
domains and promises
as of node.js 8.0.0, the handlers of promises are run inside the domain in
which the call to  or  itself was made:
a callback may be bound to a specific domain using :
domains will not interfere with the error handling mechanisms for
promises. in other words, no  event will be emitted for unhandled
rejections.
errors
applications running in node.js will generally experience four categories of
errors:
standard javascript errors such as , , ,
system errors triggered by underlying operating system constraints such
as attempting to open a file that does not exist or attempting to send data
over a closed socket.
user-specified errors triggered by application code.
s are a special class of error that can be triggered when
node.js detects an exceptional logic violation that should never occur. these
are raised typically by the  module.
all javascript and system errors raised by node.js inherit from, or are
instances of, the standard javascript  class and are guaranteed
to provide at least the properties available on that class.
error propagation and interception
node.js supports several mechanisms for propagating and handling errors that
occur while an application is running. how these errors are reported and
handled depends entirely on the type of  and the style of the api that is
all javascript errors are handled as exceptions that immediately generate
and throw an error using the standard javascript  mechanism. these
are handled using the  provided by the
javascript language.
any use of the javascript  mechanism will raise an exception that
must be handled using  or the node.js process will exit
immediately.
with few exceptions, synchronous apis (any blocking method that does not
accept a  function, such as ), will use
to report errors.
errors that occur within asynchronous apis may be reported in multiple ways:
most asynchronous methods that accept a  function will accept an
object passed as the first argument to that function. if that first
argument is not  and is an instance of , then an error occurred
that should be handled.
when an asynchronous method is called on an object that is an
, errors can be routed to that object's  event.
a handful of typically asynchronous methods in the node.js api may still
use the  mechanism to raise exceptions that must be handled using
. there is no comprehensive list of such methods; please
refer to the documentation of each method to determine the appropriate
error handling mechanism required.
the use of the  event mechanism is most common for
and  apis, which themselves represent a series of
asynchronous operations over time (as opposed to a single operation that may
pass or fail).
for all  objects, if an  event handler is not
provided, the error will be thrown, causing the node.js process to report an
uncaught exception and crash unless either: the  module is
used appropriately or a handler has been registered for the
event.
errors generated in this way cannot be intercepted using  as
they are thrown after the calling code has already exited.
developers must refer to the documentation for each method to determine
exactly how errors raised by those methods are propagated.
error-first callbacks
most asynchronous methods exposed by the node.js core api follow an idiomatic
pattern referred to as an error-first callback. with this pattern, a callback
function is passed to the method as an argument. when the operation either
completes or an error is raised, the callback function is called with the
object (if any) passed as the first argument. if no error was raised,
the first argument will be passed as .
the javascript  mechanism cannot be used to intercept errors
generated by asynchronous apis. a common mistake for beginners is to try to
use  inside an error-first callback:
this will not work because the callback function passed to  is
called asynchronously. by the time the callback has been called, the
surrounding code, including the  block, will have already exited.
throwing an error inside the callback can crash the node.js process in most
cases. if  are enabled, or a handler has been registered with
, such errors can be intercepted.
a generic javascript  object that does not denote any specific
circumstance of why the error occurred.  objects capture a "stack trace"
detailing the point in the code at which the  was instantiated, and may
provide a text description of the error.
all errors generated by node.js, including all system and javascript errors,
will either be instances of, or inherit from, the  class.
the error that caused the newly created error.
creates a new  object and sets the  property to the
provided text message. if an object is passed as , the text message
is generated by calling . if the  option is provided,
it is assigned to the  property. the  property will
represent the point in the code at which  was called. stack traces
are dependent on . stack traces extend only to either
(a) the beginning of synchronous code execution, or (b) the number of frames
given by the property , whichever is smaller.
creates a  property on , which when accessed returns
a string representing the location in the code at which
was called.
the first line of the trace will be prefixed with
the optional  argument accepts a function. if given, all frames
above , including , will be omitted from the
generated stack trace.
the  argument is useful for hiding implementation
details of error generation from the user. for instance:
the  property specifies the number of stack frames
collected by a stack trace (whether generated by  or
the default value is  but may be set to any valid javascript number. changes
will affect any stack trace captured after the value has been changed.
if set to a non-number value, or set to a negative number, stack traces will
not capture any frames.
added in: v16.9.0
if present, the  property is the underlying cause of the .
it is used when catching an error and throwing a new one with a different
message or code in order to still have access to the original error.
the  property is typically set by calling
. it is not set by the constructor if the
option is not provided.
this property allows errors to be chained. when serializing  objects,
recursively serializes  if it is set.
the  property is a string label that identifies the kind of error.
is the most stable way to identify an error. it will only change
between major versions of node.js. in contrast,  strings may
change between any versions of node.js. see  for details
about specific codes.
the  property is the string description of the error as set by
calling . the  passed to the constructor will also
appear in the first line of the stack trace of the , however changing
this property after the  object is created may not change the first
line of the stack trace (for example, when  is read before this
property is changed).
the  property is a string describing the point in the code at which
the  was instantiated.
the first line is formatted as , and
is followed by a series of stack frames (each line beginning with "at ").
each frame describes a call site within the code that lead to the error being
generated. v8 attempts to display a name for each function (by variable name,
function name, or object method name), but occasionally it will not be able to
find a suitable name. if v8 cannot determine a name for the function, only
location information will be displayed for that frame. otherwise, the
determined function name will be displayed with location information appended
in parentheses.
frames are only generated for javascript functions. if, for example, execution
synchronously passes through a c++ addon function called  which
itself calls a javascript function, the frame representing the  call
will not be present in the stack traces:
the location information will be one of:
, if the frame represents a call internal to v8 (as in ).
, if the frame represents a call internal
to node.js.
, if the frame represents a call in
a user program (using commonjs module system), or its dependencies.
, if the frame
represents a call in a user program (using es module system), or
its dependencies.
the string representing the stack trace is lazily generated when the
property is accessed.
the number of frames captured by the stack trace is bounded by the smaller of
or the number of available frames on the current event
loop tick.
indicates the failure of an assertion. for details, see
indicates that a provided argument was not within the set or range of
acceptable values for a function; whether that is a numeric range, or
outside the set of options for a given function parameter.
node.js will generate and throw  instances immediately as a form
of argument validation.
indicates that an attempt is being made to access a variable that is not
defined. such errors commonly indicate typos in code, or an otherwise broken
program.
while client code may generate and propagate these errors, in practice, only v8
will do so.
unless an application is dynamically generating and running code,
instances indicate a bug in the code or its dependencies.
indicates that a program is not valid javascript. these errors may only be
generated and propagated as a result of code evaluation. code evaluation may
happen as a result of , , , or . these errors
are almost always indicative of a broken program.
instances are unrecoverable in the context that created them –
they may only be caught by other contexts.
node.js generates system errors when exceptions occur within its runtime
environment. these usually occur when an application violates an operating
system constraint. for example, a system error will occur if an application
attempts to read a file that does not exist.
if present, the address to which a network connection
failed
the string error code
if present, the file path destination when reporting a file
system error
the system-provided error number
if present, extra details about the error condition
a system-provided human-readable description of the error
if present, the file path when reporting a file system error
if present, the network connection port that is not available
the name of the system call that triggered the error
if present,  is a string describing the address to which a
network connection failed.
the  property is a string representing the error code.
if present,  is the file path destination when reporting a file
system error.
the  property is a negative number which corresponds
to the error code defined in .
on windows the error number provided by the system will be normalized by libuv.
to get the string representation of the error code, use
if present,  is an object with details about the error condition.
is a system-provided human-readable description of the error.
if present,  is a string containing a relevant invalid pathname.
if present,  is the network connection port that is not available.
the  property is a string describing the  that failed.
common system errors
this is a list of system errors commonly-encountered when writing a node.js
program. for a comprehensive list, see the .
(permission denied): an attempt was made to access a file in a way
forbidden by its file access permissions.
(address already in use): an attempt to bind a server
(, , or ) to a local address failed due to
another server on the local system already occupying that address.
(connection refused): no connection could be made because the
target machine actively refused it. this usually results from trying to
connect to a service that is inactive on the foreign host.
(connection reset by peer): a connection was forcibly closed by
a peer. this normally results from a loss of the connection on the remote
socket due to a timeout or reboot. commonly encountered via the
and  modules.
(file exists): an existing file was the target of an operation that
required that the target not exist.
(is a directory): an operation expected a file, but the given
pathname was a directory.
(too many open files in system): maximum number of
allowable on the system has been reached, and
requests for another descriptor cannot be fulfilled until at least one
has been closed. this is encountered when opening many files at once in
parallel, especially on systems (in particular, macos) where there is a low
file descriptor limit for processes. to remedy a low limit, run
in the same shell that will run the node.js process.
(no such file or directory): commonly raised by  operations
to indicate that a component of the specified pathname does not exist. no
entity (file or directory) could be found by the given path.
(not a directory): a component of the given pathname existed, but
was not a directory as expected. commonly raised by .
(directory not empty): a directory with entries was the target
of an operation that requires an empty directory, usually .
(dns lookup failed): indicates a dns failure of either
or . this is not a standard posix error.
(operation not permitted): an attempt was made to perform an
operation that requires elevated privileges.
(broken pipe): a write on a pipe, socket, or fifo for which there is
no process to read the data. commonly encountered at the  and
layers, indicative that the remote side of the stream being
written to has been closed.
(operation timed out): a connect or send request failed because
the connected party did not properly respond after a period of time. usually
encountered by  or . often a sign that a
was not properly called.
extends
indicates that a provided argument is not an allowable type. for example,
passing a function to a parameter which expects a string would be a .
exceptions vs. errors
a javascript exception is a value that is thrown as a result of an invalid
operation or as the target of a  statement. while it is not required
that these values are instances of  or classes which inherit from
, all exceptions thrown by node.js or the javascript runtime will be
some exceptions are unrecoverable at the javascript layer. such exceptions
will always cause the node.js process to crash. examples include
checks or  calls in the c++ layer.
openssl errors
errors originating in  or  are of class , and in addition to
the standard  and  properties, may have some additional
openssl-specific properties.
an array of errors that can give context to where in the openssl library an
error originates from.
the openssl function the error originates in.
the openssl library the error originates in.
a human-readable string describing the reason for the error.
node.js error codes
used when an operation has been aborted (typically using an ).
apis not using s typically do not raise an error with this code.
this code does not use the regular  convention node.js errors use in
order to be compatible with the web platform's .
a function argument is being used in a way that suggests that the function
signature may be misunderstood. this is thrown by the  module when
the  parameter in  matches the error
message thrown by  because that usage suggests that the user believes
is the expected message rather than the message the
will display if  does not throw.
an iterable argument (i.e. a value that works with  loops) was
required, but not provided to a node.js api.
a special type of error that can be triggered whenever node.js detects an
exceptional logic violation that should never occur. these are raised typically
by the  module.
an attempt was made to use  in an environment that
does not support snapshots, such as the repl, or when using .
an attempt was made to register something that is not a function as an
the type of an asynchronous resource was invalid. users are also able
to define their own types if using the public embedder api.
data passed to a brotli stream was not successfully compressed.
an invalid parameter key was passed during construction of a brotli stream.
an attempt was made to create a node.js  instance from addon or embedder
code, while in a js engine context that is not associated with a node.js
instance. the data passed to the  method will have been released
by the time the method returns.
when encountering this error, a possible alternative to creating a
instance is to create a normal , which only differs in the
prototype of the resulting object. s are generally accepted in all
node.js core apis where s are; they are available in all contexts.
an operation outside the bounds of a  was attempted.
an attempt has been made to create a  larger than the maximum allowed
size.
node.js was unable to watch for the  signal.
a child process was closed before the parent received a reply.
used when a child process is being forked without specifying an ipc channel.
used when the main process is trying to read data from the child process's
stderr/stdout, and the data's length is longer than the  option.
there was an attempt to use a  instance in a closed
state, usually after  has been called.
was instantiated without  stream, or  has a
non-writable  or  stream.
a class constructor was called that is not callable.
a constructor for a class was called without .
the vm context passed into the api is not yet initialized. this could happen
when an error occurs (and is caught) during the creation of the
context, for example, when the allocation fails or the maximum call stack
size is reached when the context is created.
a client certificate engine was requested that is not supported by the version
of openssl being used.
an invalid value for the  argument was passed to the
class  method.
an invalid value for the  argument has been passed to the
class  method. it means that the public
key lies outside of the elliptic curve.
an invalid crypto engine identifier was passed to
the  command-line argument was used but there was an attempt
to enable or disable fips mode in the  module.
an attempt was made to enable or disable fips mode, but fips mode was not
was called multiple times. the  method must
be called no more than one time per instance of a  object.
failed for any reason. this should rarely, if ever, happen.
the given crypto keys are incompatible with the attempted operation.
the selected public or private key encoding is incompatible with other options.
initialization of the crypto subsystem failed.
an invalid authentication tag was provided.
an invalid counter was provided for a counter-mode cipher.
an invalid elliptic-curve was provided.
an invalid  was specified.
an invalid initialization vector was provided.
an invalid json web key was provided.
the given crypto key object's type is invalid for the attempted operation.
an invalid key length was provided.
an invalid key pair was provided.
an invalid key type was provided.
an invalid message length was provided.
invalid scrypt algorithm parameters were provided.
a crypto method was used on an object that was in an invalid state. for
instance, calling  before calling .
an invalid authentication tag length was provided.
initialization of an asynchronous crypto operation failed.
key's elliptic curve is not registered for use in the
key's asymmetric key type is not registered for use in the
a crypto operation failed for an otherwise unspecified reason.
the pbkdf2 algorithm failed for unspecified reasons. openssl does not provide
more details and therefore neither does node.js.
one or more  or  parameters are
outside their legal range.
node.js was compiled without  support. not possible with the official
release binaries but can happen with custom builds, including distro builds.
a signing  was not provided to the  method.
was called with , , or
arguments of different lengths.
an unknown cipher was specified.
an unknown diffie-hellman group name was given. see
for a list of valid group names.
an attempt to invoke an unsupported crypto operation was made.
added in: v16.4.0, v14.17.4
an error occurred with the .
the  timed out waiting for the required host/port to be free.
loading native addons has been disabled using .
a call to  failed.
the  was previously closed.
added in: v14.3.0
a synchronous read or close call was attempted on an  which has
ongoing asynchronous operations.
failed to set the dns server.
the  module was not usable since it could not establish the
required error handling hooks, because
had been called at an
earlier point in time.
could not be called
because the  module has been loaded at an earlier point in time.
the stack trace is extended to include the point in time at which the
module had been loaded.
because it had already been called before.
data provided to  api was invalid according to the encoding
encoding provided to  api was not one of the
cannot be used with esm input.
thrown when an attempt is made to recursively dispatch an event on .
the js execution context is not associated with a node.js environment.
this may occur when node.js is used as an embedded library and some hooks
for the js engine are not set up properly.
a  that was callbackified via  was rejected with a
falsy value.
added in: v14.0.0
used when a feature that is not available
to the current platform which is running node.js is used.
an attempt was made to copy a directory to a non-directory (file, symlink,
etc.) using .
an attempt was made to copy over a file that already existed with
, with the  and  set to .
when using ,  or  pointed to an invalid path.
response body size doesn't match with the specified content-length header value.
an attempt was made to copy a named pipe with .
an attempt was made to copy a non-directory (file, symlink, etc.) to a directory
an attempt was made to copy to a socket with .
when using , a symlink in  pointed to a subdirectory
an attempt was made to copy to an unknown file type with .
path is a directory.
an attempt has been made to read a file whose size is larger than the maximum
allowed size for a .
an invalid symlink type was passed to the  or
an attempt was made to add more headers after the headers had already been sent.
an invalid http header value was specified.
status code was outside the regular status code range (100-999).
the client has not sent the entire request within the allowed time.
changing the socket encoding is not allowed per .
the  header was set even though the transfer encoding does not support
that.
http/2 altsvc frames require a valid origin.
http/2 altsvc frames are limited to a maximum of 16,382 payload bytes.
for http/2 requests using the  method, the  pseudo-header
is required.
for http/2 requests using the  method, the  pseudo-header is
forbidden.
a non-specific http/2 error has occurred.
new http/2 streams may not be opened after the  has received a
frame from the connected peer.
multiple values were provided for an http/2 header field that was required to
have only a single value.
an additional headers was specified after an http/2 response was initiated.
an attempt was made to send multiple response headers.
informational http status codes () may not be set as the response status
code on http/2 responses.
http/1 connection specific headers are forbidden to be used in http/2
requests and responses.
an invalid http/2 header value was specified.
an invalid http informational status code has been specified. informational
status codes must be an integer between  and  (inclusive).
http/2  frames require a valid origin.
input  and  instances passed to the
api must have a length that is a multiple of
six.
only valid http/2 pseudoheaders (, , , ,
and ) may be used.
an action was performed on an  object that had already been
destroyed.
an invalid value has been specified for an http/2 setting.
an operation was performed on a stream that had already been destroyed.
whenever an http/2  frame is sent to a connected peer, the peer is
required to send an acknowledgment that it has received and applied the new
. by default, a maximum number of unacknowledged  frames may
be sent at any given time. this error code is used when that limit has been
reached.
an attempt was made to initiate a new push stream from within a push stream.
nested push streams are not permitted.
out of memory when using the  api.
an attempt was made to directly manipulate (read, write, pause, resume, etc.) a
socket attached to an .
http/2  frames are limited to a length of 16382 bytes.
the number of streams created on a single http/2 session reached the maximum
limit.
a message payload was specified for an http response code for which a payload is
an http/2 ping was canceled.
http/2 ping payloads must be exactly 8 bytes in length.
an http/2 pseudo-header has been used inappropriately. pseudo-headers are header
key names that begin with the  prefix.
an attempt was made to create a push stream, which had been disabled by the
client.
an attempt was made to use the  api to
send a directory.
send something other than a regular file, but  or  options were
the  closed with a non-zero error code.
the  settings canceled.
an attempt was made to connect a  object to a  or
that had already been bound to another  object.
an attempt was made to use the  property of an  that
has already been closed.
use of the  informational status code is forbidden in http/2.
an invalid http status code has been specified. status codes must be an integer
between  and  (inclusive).
an  was destroyed before any data was transmitted to the connected
peer.
a non-zero error code was been specified in an  frame.
when setting the priority for an http/2 stream, the stream may be marked as
a dependency for a parent stream. this error code is used when an attempt is
made to mark a stream and dependent of itself.
the limit of acceptable invalid http/2 protocol frames sent by the peer,
as specified through the  option, has been exceeded.
trailing headers have already been sent on the .
the  method cannot be called until after the
event is emitted on an  object. the
event will only be emitted if the  option
is set for the .
was passed a url that uses any protocol other than  or
an attempt was made to construct an object using a non-public constructor.
added in: v17.1.0, v16.14.0
an import assertion has failed, preventing the specified module to be imported.
an import assertion is missing, preventing the specified module to be imported.
an import assertion is not supported by this version of node.js.
an option pair is incompatible with each other and cannot be used at the same
time.
the  flag was used to attempt to execute a file. this flag can
only be used with input via , , or .
while using the  module, an attempt was made to activate the
inspector when it already started to listen on a port. use
before activating it on a different address.
while using the  module, an attempt was made to connect when the
inspector was already connected.
while using the  module, an attempt was made to use the
inspector after the session had already closed.
an error occurred while issuing a command via the  module.
the  is not active when  is called.
the  module is not available for use.
inspector before it was connected.
an api was called on the main thread that can only be used from
the worker thread.
there was a bug in node.js or incorrect usage of node.js internals.
to fix the error, open an issue at .
the provided address family is not understood by the node.js api.
an argument of the wrong type was passed to a node.js api.
an invalid or unsupported value was passed for a given argument.
an invalid  or  was passed using . an id
less than -1 should never happen.
a swap was performed on a  but its size was not compatible with the
invalid characters were detected in headers.
a cursor on a given stream cannot be moved to a specified row without a
specified column.
a file descriptor ('fd') was not valid (e.g. it was a negative value).
a file descriptor ('fd') type was not valid.
a node.js api that consumes  urls (such as certain functions in the
module) encountered a file url with an incompatible host. this
situation can only occur on unix-like systems where only  or an empty
host is supported.
module) encountered a file url with an incompatible path. the exact
semantics for determining whether a path can be used is platform-dependent.
an attempt was made to send an unsupported "handle" over an ipc communication
channel to a child process. see  and
an invalid http token was supplied.
an ip address is not valid.
an attempt was made to load a module that does not exist or was otherwise not
valid.
the imported module string is an invalid url, package name, or package subpath
specifier.
an error occurred while setting an invalid attribute on the property of
an object.
an invalid  file failed parsing.
the   field contains an invalid target mapping
value for the attempted module resolution.
while using the performance timing api (), a performance mark is
invalid.
an invalid  was passed to .
both  and  options were set in the  config,
which is not supported.
the input may not be used in the . the conditions under which this
error is used are described in the  documentation.
thrown in case a function option does not provide a valid value for one of its
returned object properties on execution.
thrown in case a function option does not provide an expected value
type for one of its returned object properties on execution.
thrown in case a function option does not return an expected value
type on execution, such as when a function is expected to return a promise.
indicates that an operation cannot be completed due to an invalid state.
for instance, an object may have already been destroyed, or may be
performing another operation.
a , , , or  was provided as stdio input to
an asynchronous fork. see the documentation for the  module
a node.js api function was called with an incompatible  value.
an invalid transfer object was passed to .
an element in the  provided to the
did not
represent a  tuple – that is, if an element is not iterable, or
does not consist of exactly two elements.
an invalid uri was passed.
an invalid url was passed to the   or the legacy  to be parsed.
the thrown error object typically has an additional property  that
contains the url that failed to parse.
an attempt was made to use a url of an incompatible scheme (protocol) for a
specific purpose. it is only used in the  support in the
module (which only accepts urls with  scheme), but may be used
in other node.js apis as well in the future.
an attempt was made to use an ipc communication channel that was already closed.
an attempt was made to disconnect an ipc communication channel that was already
disconnected. see the documentation for the  module
an attempt was made to create a child node.js process using more than one ipc
communication channel. see the documentation for the  module
an attempt was made to open an ipc communication channel with a synchronously
forked node.js process. see the documentation for the  module
added in: v18.6.0
an esm loader hook returned without calling  and without explicitly
signaling a short circuit.
an attempt was made to load a resource, but the resource did not match the
integrity defined by the policy manifest. see the documentation for
manifests for more information.
an attempt was made to load a resource, but the resource was not listed as a
dependency from the location that attempted to load it. see the documentation
for  manifests for more information.
an attempt was made to load a policy manifest, but the manifest had multiple
entries for a resource which did not match each other. update the manifest
entries to match in order to resolve this error. see the documentation for
a policy manifest resource had an invalid value for one of its fields. update
the manifest entry to match in order to resolve this error. see the
documentation for  manifests for more information.
a policy manifest resource had an invalid value for one of its dependency
mappings. update the manifest entry to match to resolve this error. see the
an attempt was made to load a policy manifest, but the manifest was unable to
be parsed. see the documentation for  manifests for more information.
an attempt was made to read from a policy manifest, but the manifest
initialization has not yet taken place. this is likely a bug in node.js.
a policy manifest was loaded, but had an unknown value for its "onerror"
behavior. see the documentation for  manifests for more information.
an attempt was made to allocate memory (usually in the c++ layer) but it
failed.
added in: v14.5.0, v12.19.0
a message posted to a  could not be deserialized in the target
. not all node.js objects can be successfully instantiated in
any context at this time, and attempting to transfer them using
can fail on the receiving side in that case.
a method is required but not implemented.
a required argument of a node.js api was not passed. this is only used for
strict compliance with the api specification (which in some cases may accept
but not ). in most native node.js apis,
and  are treated identically, and the
error code may be used instead.
for apis that accept options objects, some options might be mandatory. this code
is thrown if a required option is missing.
an attempt was made to read an encrypted key without specifying a passphrase.
the v8 platform used by this instance of node.js does not support creating
workers. this is caused by lack of embedder support for workers. in particular,
this error will not occur with standard builds of node.js.
an object that needs to be explicitly listed in the  argument
is in the object passed to a  call, but is not provided
in the  for that call. usually, this is a .
in node.js versions prior to v15.0.0, the error code being used here was
. however, the set of
transferable object types has been expanded to cover more types than
a module file could not be resolved by the ecmascript modules loader while
attempting an  operation or when loading the program entry point.
a callback was called more than once.
a callback is almost always meant to only be called once as the query
can either be fulfilled or rejected but not both at the same time. the latter
would be possible by calling a callback more than once.
while using , a constructor passed was not a function.
while calling , a given  was outside the bounds
of the dataview or  was larger than a length of given .
while calling , the provided  was not a
multiple of the element size.
while calling ,  was larger than the length of given .
an error occurred while invoking the javascript portion of the thread-safe
an error occurred while attempting to retrieve the javascript
on the main thread, values are removed from the queue associated with the
thread-safe function in an idle loop. this error indicates that an error
has occurred when attempting to start the loop.
once no more items are left in the queue, the idle loop must be suspended. this
error indicates that the idle loop has failed to stop.
an attempt was made to use operations that can only be used when building
v8 startup snapshot even though node.js isn't building one.
an attempt was made to use crypto features while node.js was not compiled with
openssl crypto support.
an attempt was made to use features that require , but node.js was not
compiled with icu support.
a non-context-aware native addon was loaded in a process that disallows them.
a given value is out of the accepted range.
the   field does not define the given internal
package specifier mapping.
the   field does not export the requested subpath.
because exports are encapsulated, private internal modules that are not exported
cannot be imported through the package resolution, unless using an absolute url.
when  set to , thrown by  if a
value is provided for an option of type , or if a
value is provided for an option of type .
thrown by , when a positional argument is provided and
is set to .
when  set to , thrown by  if an argument
is not configured in .
an invalid timestamp value was provided for a performance mark or measure.
invalid options were provided for a performance measure.
accessing  has been forbidden using
.  and
should be used to get and set the prototype of an
an attempt was made to  an .
script execution was interrupted by  (for
example, ctrl+c was pressed.)
script execution timed out, possibly due to bugs in the script being executed.
the  method was called while a  was already
listening. this applies to all instances of , including http, https,
and http/2  instances.
the  method was called when a  was not
running. this applies to all instances of , including http, https,
an attempt was made to bind a socket that has already been bound.
an invalid (negative) size was passed for either the  or
options in .
an api function expecting a port >= 0 and < 65536 received an invalid value.
an api function expecting a socket type ( or ) received an invalid
while using , the size of the receive or send
could not be determined.
an attempt was made to operate on an already closed socket.
a  call was made on an already connected socket.
a  or  call was made on a
disconnected socket.
a call was made and the udp subsystem was not running.
a string was provided for a subresource integrity check, but was unable to be
parsed. check the format of integrity attributes by looking at the
a stream method was called that cannot complete because the stream was
an attempt was made to call  on a  stream.
destroyed using .
an attempt was made to call  with a  chunk.
an error returned by  and , when a stream
or a pipeline ends non gracefully with no explicit error.
an attempt was made to call  after a (eof) had been
pushed to the stream.
an attempt was made to call  after the  event was
prevents an abort if a string decoder was set on the socket or if the decoder
is in .
an attempt was made to call  after  has been
an attempt has been made to create a string longer than the maximum allowed
an artificial error object used to capture the call stack for diagnostic
reports.
an unspecified or non-specific system error has occurred within the node.js
process. the error object will have an  object property with
additional details.
this error represents a failed test. additional information about the failure
is available via the  property. the  property specifies
what the test was doing when the failure occurred.
this error is thrown by  if a user-supplied
property violates encoding rules. certificate objects produced
by node.js itself always comply with encoding rules and will never cause
this error.
while using tls, the host name/ip of the peer did not match any of the
in its certificate.
while using tls, the parameter offered for the diffie-hellman ()
key-agreement protocol is too small. by default, the key length must be greater
than or equal to 1024 bits to avoid vulnerabilities, even though it is strongly
recommended to use 2048 bits or larger for stronger security.
a tls/ssl handshake timed out. in this case, the server must also abort the
connection.
added in: v13.3.0
the context must be a .
the specified   method is invalid. it is  either unknown, or
disabled because it is insecure.
valid tls protocol versions are , , or .
the tls socket must be connected and securely established. ensure the 'secure'
event is emitted before continuing.
attempting to set a tls protocol  or  conflicts with an
attempt to set the  explicitly. use one mechanism or the other.
failed to set psk identity hint. hint may be too long.
an attempt was made to renegotiate tls on a socket instance with tls disabled.
while using tls, the  method was called without providing
a host name in the first parameter.
an excessive amount of tls renegotiations is detected, which is a potential
vector for denial-of-service attacks.
an attempt was made to issue server name indication from a tls server-side
socket, which is only valid from a client.
the  method requires at least one trace event
category.
the  module could not be loaded because node.js was compiled
with the  flag.
a  stream finished while it was still transforming.
a  stream finished with data still in the write buffer.
the initialization of a tty failed due to a system error.
function was called within a  handler that shouldn't be
called within  handler.
was called twice,
without first resetting the callback to .
this error is designed to prevent accidentally overwriting a callback registered
from another module.
a string that contained unescaped characters was received.
an unhandled error occurred (for instance, when an  event is emitted
by an  but an  handler is not registered).
used to identify a specific kind of internal node.js error that should not
typically be triggered by user code. instances of this error point to an
internal bug within the node.js binary itself.
a unix group or user identifier that does not exist was passed.
an invalid or unknown encoding option was passed to an api.
an attempt was made to load a module with an unknown or unsupported file
an attempt was made to load a module with an unknown or unsupported format.
an invalid or unknown process signal was passed to an api expecting a valid
signal (such as ).
a directory url is unsupported. instead,
and  in
the  field of the  file.
with url schemes other than  and  is unsupported.
an attempt was made to use something that was already closed.
while using the performance timing api (), no valid performance
entry types are found.
a dynamic import callback was not specified.
the module attempted to be linked is not eligible for linking, because of one of
the following reasons:
it has already been linked ( is )
it is being linked ( is )
linking has failed for this module ( is )
the  option passed to a module constructor is invalid.
cached data cannot be created for modules which have already been evaluated.
the module being returned from the linker function is from a different context
than the parent module. linked modules must share the same context.
the module was unable to be linked due to a failure.
the fulfilled value of a linking promise is not a  object.
the current module's status does not allow for this operation. the specific
meaning of the error depends on the specific function.
the wasi instance has already started.
the wasi instance has not been started.
the  that has been passed to  or to
is not a valid webassembly response.
the  initialization failed.
the  option passed to the  constructor contains
invalid flags.
an operation failed because the  instance is not currently running.
the  instance terminated because it reached its memory limit.
the path for the main script of a worker is neither an absolute path
nor a relative path starting with  or .
all attempts at serializing an uncaught exception from a worker thread failed.
the requested functionality is not supported in worker threads.
creation of a  object failed due to incorrect configuration.
too much http header data was received. in order to protect against malicious or
malconfigured clients, if more than 8 kib of http header data is received then
http parsing will abort without a request or response object being created, and
an  with this code will be emitted.
server is sending both a  header and .
allows the server to maintain an http persistent
connection for dynamically generated content.
in this case, the  http header cannot be used.
use  or .
a module file could not be resolved by the commonjs modules loader while
attempting a  operation or when loading the program entry point.
legacy node.js error codes
- deprecated. these error codes are either inconsistent, or have
been removed.
the value passed to  contained an object that is not supported
for transferring.
added in: v9.0.0removed in: v12.12.0
the utf-16 encoding was used with . while the
method does allow an  argument to be passed in,
causing the method to return a string rather than a , the utf-16
encoding (e.g.  or ) is not supported.
added in: v9.0.0removed in: v10.0.0
used when a failure occurs sending an individual frame on the http/2
session.
used when an http/2 headers object is expected.
used when a required header is missing in an http/2 message.
http/2 informational headers must only be sent prior to calling the
used when an action has been performed on an http/2 stream that has already
been closed.
used when an invalid character is found in an http response status message
(reason phrase).
added in: v10.0.0removed in: v11.0.0
a given index was out of the accepted range (e.g. negative offsets).
added in: v8.0.0removed in: v15.0.0
an invalid or unexpected value was passed in an options object.
added in: v9.0.0removed in: v15.0.0
an invalid or unknown file encoding was passed.
removed in: v15.0.0
this error code was replaced by
in node.js v15.0.0, because it is no longer accurate as other types of
transferable objects also exist now.
used by the  when  is not an object.
response was received but was invalid when importing a module over the network.
a network module attempted to load another module that it is not allowed to
load. likely this restriction is for security reasons.
a node.js api was called in an unsupported manner, such as
an operation failed. this is typically used to signal the general failure
of an asynchronous operation.
used generically to identify that an operation caused an out of memory
condition.
the  module was unable to parse data from the repl history file.
added in: v9.0.0removed in: v14.0.0
data could not be sent on a socket.
an attempt was made to close the  stream. by design, node.js
does not allow  or  streams to be closed by user code.
used when an attempt is made to use a readable stream that has not implemented
used when a tls renegotiation request has failed in a non-specific way.
added in: v10.5.0removed in: v14.0.0
a  whose memory is not managed by the javascript engine
or by node.js was encountered during serialization. such a
cannot be serialized.
this can only happen when native addons create s in
"externalized" mode, or put existing  into externalized mode.
added in: v8.0.0removed in: v11.7.0
an attempt was made to launch a node.js process with an unknown  file
type. this error is usually an indication of a bug within node.js itself,
although it is possible for user code to trigger it.
an attempt was made to launch a node.js process with an unknown  or
file type. this error is usually an indication of a bug within node.js
itself, although it is possible for user code to trigger it.
the v8  api was used but the full icu data set is not installed.
used when a given value is out of the accepted range.
the module must be successfully linked before instantiation.
added in: v10.0.0removed in: v18.1.0
the linker function returned a module for which linking has failed.
added in: v11.0.0removed in: v16.9.0
the pathname used for the main script of a worker has an
unknown file extension.
used when an attempt is made to use a  object after it has already been
the native call from  could not be processed.
much of the node.js core api is built around an idiomatic asynchronous
event-driven architecture in which certain kinds of objects (called "emitters")
emit named events that cause  objects ("listeners") to be called.
for instance: a  object emits an event each time a peer
connects to it; a  emits an event when the file is opened;
a  emits an event whenever data is available to be read.
all objects that emit events are instances of the  class. these
objects expose an  function that allows one or more
functions to be attached to named events emitted by the object. typically,
event names are camel-cased strings but any valid javascript property key
can be used.
when the  object emits an event, all of the functions attached
to that specific event are called synchronously. any values returned by the
called listeners are ignored and discarded.
the following example shows a simple  instance with a single
listener. the  method is used to register listeners, while
the  method is used to trigger the event.
passing arguments and  to listeners
the  method allows an arbitrary set of arguments to be
passed to the listener functions. keep in mind that when
an ordinary listener function is called, the standard  keyword
is intentionally set to reference the  instance to which the
listener is attached.
it is possible to use es6 arrow functions as listeners, however, when doing so,
the  keyword will no longer reference the  instance:
asynchronous vs. synchronous
the  calls all listeners synchronously in the order in which
they were registered. this ensures the proper sequencing of
events and helps avoid race conditions and logic errors. when appropriate,
listener functions can switch to an asynchronous mode of operation using
the  or  methods:
handling events only once
when a listener is registered using the  method, that
listener is invoked every time the named event is emitted.
using the  method, it is possible to register a listener
that is called at most once for a particular event. once the event is emitted,
the listener is unregistered and then called.
error events
when an error occurs within an  instance, the typical action is
for an  event to be emitted. these are treated as special cases
within node.js.
if an  does not have at least one listener registered for the
event, and an  event is emitted, the error is thrown, a
stack trace is printed, and the node.js process exits.
to guard against crashing the node.js process the  module can be
used. (note, however, that the  module is deprecated.)
as a best practice, listeners should always be added for the  events.
it is possible to monitor  events without consuming the emitted error
by installing a listener using the symbol .
capture rejections of promises
using  functions with event handlers is problematic, because it
can lead to an unhandled rejection in case of a thrown exception:
the  option in the  constructor or the global
setting change this behavior, installing a
handler on the . this handler routes the exception
asynchronously to the  method
if there is one, or to  event handler if there is none.
setting  will change the default for all
new instances of .
the  events that are generated by the  behavior
do not have a catch handler to avoid infinite error loops: the
recommendation is to not use  functions as  event handlers.
the  class is defined and exposed by the  module:
all s emit the event  when new listeners are
added and  when existing listeners are removed.
it supports the following option:
it enables
added in: v0.1.26
|  the name of the event being listened for
the event handler function
the  instance will emit its own  event before
a listener is added to its internal array of listeners.
listeners registered for the  event are passed the event
name and a reference to the listener being added.
the fact that the event is triggered before adding the listener has a subtle
but important side effect: any additional listeners registered to the same
within the  callback are inserted before the
listener that is in the process of being added.
|  the event name
the  event is emitted after the  is removed.
alias for .
synchronously calls each of the listeners registered for the event named
, in the order they were registered, passing the supplied arguments
to each.
returns  if the event had listeners,  otherwise.
returns an array listing the events for which the emitter has registered
listeners. the values in the array are strings or s.
returns the current max listener value for the  which is either
set by  or defaults to
added in: v3.2.0
returns the number of listeners listening to the event named .
returns a copy of the array of listeners for the event named .
|  the name of the event.
adds the  function to the end of the listeners array for the
event named . no checks are made to see if the  has
already been added. multiple calls passing the same combination of
and  will result in the  being added, and called, multiple
times.
returns a reference to the , so that calls can be chained.
by default, event listeners are invoked in the order they are added. the
method can be used as an alternative to add the
event listener to the beginning of the listeners array.
adds a one-time  function for the event named . the
next time  is triggered, this listener is removed and then invoked.
adds the  function to the beginning of the listeners array for the
adds a one-time  function for the event named  to the
beginning of the listeners array. the next time  is triggered, this
listener is removed, and then invoked.
removes all listeners, or those of the specified .
it is bad practice to remove listeners added elsewhere in the code,
particularly when the  instance was created by some other
component or module (e.g. sockets or file streams).
removes the specified  from the listener array for the event named
will remove, at most, one instance of a listener from the
listener array. if any single listener has been added multiple times to the
listener array for the specified , then  must be
called multiple times to remove each instance.
once an event is emitted, all listeners attached to it at the
time of emitting are called in order. this implies that any
or  calls after emitting and
before the last listener finishes execution will not remove them from
in progress. subsequent events behave as expected.
because listeners are managed using an internal array, calling this will
change the position indices of any listener registered after the listener
being removed. this will not impact the order in which listeners are called,
but it means that any copies of the listener array as returned by
the  method will need to be recreated.
when a single function has been added as a handler multiple times for a single
event (as in the example below),  will remove the most
recently added instance. in the example the
listener is removed:
added in: v0.3.5
by default s will print a warning if more than  listeners are
added for a particular event. this is a useful default that helps finding
memory leaks. the  method allows the limit to be
modified for this specific  instance. the value can be set to
(or ) to indicate an unlimited number of listeners.
added in: v9.4.0
returns a copy of the array of listeners for the event named ,
including any wrappers (such as those created by ).
error
the  method is called in case a
promise rejection happens when emitting an event and
is enabled on the emitter.
it is possible to use  in
place of .
by default, a maximum of  listeners can be registered for any single
event. this limit can be changed for individual  instances
using the  method. to change the default
for all  instances, the
property can be used. if this value is not a positive number, a
take caution when setting the  because the
change affects all  instances, including those created before
the change is made. however, calling  still has
precedence over .
this is not a hard limit. the  instance will allow
more listeners to be added but will output a trace warning to stderr indicating
that a "possible eventemitter memory leak" has been detected. for any single
, the  and
methods can be used to temporarily avoid this warning:
the  command-line flag can be used to display the
stack trace for such warnings.
the emitted warning can be inspected with  and will
have the additional , , and  properties, referring to
the event emitter instance, the event's name and the number of attached
listeners, respectively.
its  property is set to .
this symbol shall be used to install a listener for only monitoring
events. listeners installed using this symbol are called before the regular
listeners are called.
installing a listener using this symbol does not change the behavior once an
event is emitted. therefore, the process will still crash if no
regular  listener is installed.
added in: v15.2.0, v14.17.0
for s this behaves exactly the same as calling  on
the emitter.
for s this is the only way to get the event listeners for the
event target. this is useful for debugging and diagnostic purposes.
can be used to cancel waiting for the event.
creates a  that is fulfilled when the  emits the given
event or that is rejected if the  emits  while waiting.
the  will resolve with an array of all the arguments emitted to the
given event.
this method is intentionally generic and works with the web platform
interface, which has no special
event semantics and does not listen to the  event.
the special handling of the  event is only used when
is used to wait for another event. if  is used to wait for the
' event itself, then it is treated as any other kind of event without
special handling:
an  can be used to cancel waiting for the event:
awaiting multiple events emitted on
there is an edge case worth noting when using the  function
to await multiple events emitted on in the same batch of
operations, or whenever multiple events are emitted synchronously. specifically,
because the  queue is drained before the  microtask
queue, and because  emits all events synchronously, it is possible
for  to miss an event.
to catch both events, create each of the promises before awaiting either
of them, then it becomes possible to use , ,
or :
value:
change the default  option on all new  objects.
see how to write a custom .
added in: v0.9.12deprecated since: v3.2.0
the emitter to query
a class method that returns the number of listeners for the given
registered on the given .
added in: v13.6.0, v12.16.0
can be used to cancel awaiting events.
returns:  that iterates  events emitted by the
returns an  that iterates  events. it will throw
if the  emits . it removes all listeners when
exiting the loop. the  returned by each iteration is an array
composed of the emitted event arguments.
an  can be used to cancel waiting on events:
added in: v15.4.0
a non-negative number. the maximum number of listeners per
|  zero or more
or  instances. if none are specified,  is set as the default
max for all newly created  and  objects.
added in: v17.4.0, v16.14.0
integrates  with  for s that
require manual async tracking. specifically, all events emitted by instances
of  will run within its .
the  class has the same methods and takes the
same options as  and  themselves.
the type of async event. default::
type:  the unique  assigned to the resource.
type: the underlying .
the returned  object has an additional  property
that provides a reference to this .
type:  the same  that is passed to the
and  api
the  and  objects are a node.js-specific implementation
of the  that are exposed by some node.js core apis.
node.js  vs. dom
there are two key differences between the node.js  and the
whereas dom  instances may be hierarchical, there is no
concept of hierarchy and event propagation in node.js. that is, an event
dispatched to an  does not propagate through a hierarchy of
nested target objects that may each have their own set of handlers for the
in the node.js , if an event listener is an async function
or returns a , and the returned  rejects, the rejection
is automatically captured and handled the same way as a listener that
throws synchronously (see  for details).
the  object implements a modified subset of the
api that allows it to closely emulate an  in
certain situations. a  is not an instance of
and cannot be used in place of an  in most cases.
unlike , any given  can be registered at most once
per event . attempts to register a  multiple times are
the  does not emulate the full  api.
specifically the , ,
, , , and
apis are not emulated. the  and
events will also not be emitted.
the  does not implement any special default behavior
for events with type .
the  supports  objects as well as
functions as handlers for all event types.
event listener
event listeners registered for an event  may either be javascript
functions or objects with a  property whose value is a function.
in either case, the handler function is invoked with the  argument
passed to the  function.
async functions may be used as event listeners. if an async handler function
rejects, the rejection is captured and handled as described in
an error thrown by one handler function does not prevent the other handlers
from being invoked.
the return value of a handler function is ignored.
handlers are always invoked in the order they were added.
handler functions may mutate the  object.
when a registered event listener throws (or returns a promise that rejects),
by default the error is treated as an uncaught exception on
. this means uncaught exceptions in s will
terminate the node.js process by default.
throwing within an event listener will not stop the other registered handlers
the  does not implement any special default handling for
type events like .
currently errors are first forwarded to the  event
before reaching . this behavior is
deprecated and will change in a future release to align  with
other node.js apis. any code relying on the  event should
be aligned with the new behavior.
the  object is an adaptation of the . instances
are created internally by node.js.
type:  always returns .
this is not used in node.js and is provided purely for completeness.
alias for . this is not used in node.js and is
provided purely for completeness.
type:  true if the event was created with the  option.
returns an array containing the current  as the only entry or
empty if the event is not being dispatched. this is not used in
node.js and is provided purely for completeness.
type:  the  dispatching the event.
is  if  is  and  has been
type:  returns  while an event is not being dispatched,  while
it is being dispatched.
the   event is emitted with  set to . the
value is  in all other cases.
sets the  property to  if  is .
type:  true if the event has not been canceled.
stops the invocation of event listeners after the current one completes.
the millisecond timestamp when the  object was created.
the event type identifier.
when , the listener is automatically removed
when it is first invoked. default: .
when , serves as a hint that the listener will
not call the  object's  method.
not directly used by node.js. added for api
completeness. default: .
the listener will be removed when the given
abortsignal object's  method is called.
adds a new handler for the  event. any given  is added
only once per  and per  option value.
if the  option is , the  is removed after the
next time a  event is dispatched.
the  option is not used by node.js in any functional way other than
tracking registered event listeners per the  specification.
specifically, the  option is used as part of the key when registering
a . any individual  may be added once with
, and once with .
returns:   if either event's  attribute value is
false or its  method was not invoked, otherwise .
dispatches the  to the list of handlers for .
the registered event listeners is synchronously invoked in the order they
were registered.
removes the  from the list of handlers for event .
- experimental.
the  object is an adaptation of the .
instances are created internally by node.js.
type:  returns custom data passed when initializing.
read-only.
the  is a node.js-specific extension to
that emulates a subset of the  api.
returns:  this
node.js-specific extension to the  class that emulates the
equivalent  api. the only difference between  and
is that  will return a reference to the
node.js-specific extension to the  class that returns an array
of event  names for which event listeners are registered.
node.js-specific extension to the  class that returns the number
of event listeners registered for the .
node.js-specific alias for .
node.js-specific extension to the  class that adds a
listener for the given event . this is equivalent to calling
with the  option set to .
node.js-specific extension to the  class. if  is specified,
removes all registered listeners for , otherwise removes all registered
listeners.
node.js-specific extension to the  class that removes the
for the given . the only difference between
and  is that  will return a reference
to the .
file system
the  module enables interacting with the file system in a
way modeled on standard posix functions.
to use the promise-based apis:
to use the callback and sync apis:
all file system operations have synchronous, callback, and promise-based
forms, and are accessible using both commonjs syntax and es6 modules (esm).
promise example
promise-based operations return a promise that is fulfilled when the
asynchronous operation is complete.
callback example
the callback form takes a completion callback function as its last
argument and invokes the operation asynchronously. the arguments passed to
the completion callback depend on the method, but the first argument is always
reserved for an exception. if the operation is completed successfully, then
the first argument is  or .
the callback-based versions of the  module apis are preferable over
the use of the promise apis when maximal performance (both in terms of
execution time and memory allocation) is required.
synchronous example
the synchronous apis block the node.js event loop and further javascript
execution until the operation is complete. exceptions are thrown immediately
and can be handled using , or can be allowed to bubble up.
promises api
the  api provides asynchronous file system methods that return
promises.
the promise apis use the underlying node.js threadpool to perform file
system operations off the event loop thread. these operations are not
synchronized or threadsafe. care must be taken when performing multiple
concurrent modifications on the same file or data corruption may occur.
a  object is an object wrapper for a numeric file descriptor.
instances of the  object are created by the
all  objects are s.
if a  is not closed using the  method, it will
try to automatically close the file descriptor and emit a process warning,
helping to prevent memory leaks. please do not rely on this behavior because
it can be unreliable and the file may not be closed. instead, always explicitly
close s. node.js may change this behavior in the future.
the  event is emitted when the  has been closed and can no
longer be used.
returns:  fulfills with  upon success.
alias of .
when operating on file handles, the mode cannot be changed from what it was set
to with . therefore, this is equivalent to
the file mode bit mask.
modifies the permissions on the file. see .
the file's new owner's user id.
the file's new group's group id.
changes the ownership of the file. a wrapper for .
closes the file handle after waiting for any pending operation on the handle to
added in: v16.11.0
unlike the 16 kib default  for a , the stream
returned by this method has a default  of 64 kib.
can include  and  values to read a range of bytes from
the file instead of the entire file. both  and  are inclusive and
start counting at 0, allowed values are in the
[0, ] range. if  is
omitted or ,  reads sequentially from
the current file position. the  can be any one of those accepted by
if the  points to a character device that only supports blocking
reads (such as keyboard or sound card), read operations do not finish until data
is available. this can prevent the process from exiting and the stream from
closing naturally.
by default, the stream will emit a  event after it has been
destroyed.  set the  option to  to change this behavior.
if  is false, then the file descriptor won't be closed, even if
there's an error. it is the application's responsibility to close it and make
sure there's no file descriptor leak. if  is set to true (default
behavior), on  or  the file descriptor will be closed
automatically.
an example to read the last 10 bytes of a file which is 100 bytes long:
may also include a  option to allow writing data at some
position past the beginning of the file, allowed values are in the
[0, ] range. modifying a file rather than
replacing it may require the   option to be set to  rather than
the default . the  can be any one of those accepted by .
if  is set to true (default behavior) on  or
the file descriptor will be closed automatically. if  is false,
then the file descriptor won't be closed, even if there's an error.
it is the application's responsibility to close it and make sure there's no
file descriptor leak.
forces all currently queued i/o operations associated with the file to the
operating system's synchronized i/o completion state. refer to the posix
documentation for details.
unlike  this method does not flush modified metadata.
the numeric file descriptor managed by the  object.
|  |  a buffer that will be filled with the
file data read.
the location in the buffer at which to start filling.
the number of bytes to read.
|  the location where to begin reading data from the
file. if , data will be read from the current file position, and
the position will be updated. if  is an integer, the current
file position will remain unchanged.
returns:  fulfills upon success with an object with two properties:
the number of bytes read
|  |  a reference to the passed in
reads data from the file and stores that in the given buffer.
if the file is not modified concurrently, the end-of-file is reached when the
number of bytes read is zero.
file data read. default:
the number of bytes to read. default:
file position will remain unchanged. default::
added in: v18.2.0
the location where to begin reading data from the
returns a  that may be used to read the files data.
an error will be thrown if this method is called more than once or is called
after the  is closed or closing.
while the  will read the file to completion, it will not
close the  automatically. user code must still call the
allows aborting an in-progress readfile
returns:  fulfills upon a successful read with the contents of the
file. if no encoding is specified (using ), the data is
returned as a  object. otherwise, the data will be a string.
asynchronously reads the entire contents of a file.
if  is a string, then it specifies the .
the  has to support reading.
if one or more  calls are made on a file handle and then a
call is made, the data will be read from the current
position till the end of the file. it doesn't always read from the beginning
of the file.
convenience method to create a  interface and stream over the file.
see  for the options.
added in: v13.13.0, v12.17.0
|  the offset from the beginning of the file where
the data should be read from. if  is not a , the data will
be read from the current position. default:
returns:  fulfills upon success an object containing two properties:
|  |  property containing
a reference to the  input.
read from a file and write to an array of s
whether the numeric values in the returned
object should be . default: .
returns:  fulfills with an  for the file.
request that all data for the open file descriptor is flushed to the storage
device. the specific implementation is operating system and device specific.
refer to the posix  documentation for more detail.
truncates the file.
if the file was larger than  bytes, only the first  bytes will be
retained in the file.
the following example retains only the first four bytes of the file:
if the file previously was shorter than  bytes, it is extended, and the
extended part is filled with null bytes ():
if  is negative then  will be used.
change the file system timestamps of the object referenced by the
then resolves the promise with no arguments upon success.
the start position from within  where the data
to write begins.
the number of bytes from  to write. default:
|  the offset from the beginning of the file where the
data from  should be written. if  is not a ,
the data will be written at the current position. see the posix
documentation for more detail. default:
write  to the file.
the promise is resolved with an object containing two properties:
the number of bytes written
|  |  a reference to the
written.
it is unsafe to use  multiple times on the same file
without waiting for the promise to be resolved (or rejected). for this
scenario, use .
on linux, positional writes do not work when the file is opened in append mode.
the kernel ignores the position argument and always appends the data to
the end of the file.
similar to the above  function, this version takes an
optional  object. if no  object is specified, it will
default with the above values.
data from  should be written. if  is not a  the
data will be written at the current position. see the posix
the expected string encoding. default:
write  to the file. if  is not a string, the promise is
rejected with an error.
a reference to the  written.
|  the expected character encoding when  is a
string. default:
asynchronously writes data to a file, replacing the file if it already exists.
can be a string, a buffer, an , or an  object.
the promise is resolved with no arguments upon success.
the  has to support writing.
without waiting for the promise to be resolved (or rejected).
call is made, the data will be written from the
current position till the end of the file. it doesn't always write from the
beginning of the file.
added in: v12.9.0
the data will be written at the current position. default:
write an array of s to the file.
the promise is resolved with an object containing a two properties:
input.
it is unsafe to call  multiple times on the same file without waiting
for the promise to be resolved (or rejected).
on linux, positional writes don't work when the file is opened in append mode.
tests a user's permissions for the file or directory specified by .
the  argument is an optional integer that specifies the accessibility
checks to be performed.  should be either the value
or a mask consisting of the bitwise or of any of ,
, and  (e.g.
). check  for
possible values of .
if the accessibility check is successful, the promise is resolved with no
value. if any of the accessibility checks fail, the promise is rejected
with an  object. the following example checks if the file
can be read and written by the current process.
using  to check for the accessibility of a file before
calling  is not recommended. doing so introduces a race
condition, since other processes may change the file's state between the two
calls. instead, user code should open/read/write the file directly and handle
the error raised if the file is not accessible.
|  |  |  filename or
asynchronously append data to a file, creating the file if it does not yet
exist.  can be a string or a .
the  option only affects the newly created file. see
the  may be specified as a  that has been opened
for appending (using ).
changes the permissions of a file.
changes the ownership of a file.
|  |  source filename to copy
|  |  destination filename of the copy operation
optional modifiers that specify the behavior of the copy
operation. it is possible to create a mask consisting of the bitwise or of
two or more values (e.g.
: the copy operation will fail if
already exists.
: the copy operation will attempt to create
a copy-on-write reflink. if the platform does not support copy-on-write,
then a fallback copy mechanism is used.
: the copy operation will attempt to
create a copy-on-write reflink. if the platform does not support
copy-on-write, then the operation will fail.
asynchronously copies  to . by default,  is overwritten if it
no guarantees are made about the atomicity of the copy operation. if an
error occurs after the destination file has been opened for writing, an attempt
will be made to remove the destination.
|  source path to copy.
|  destination path to copy to.
dereference symlinks. default: .
when  is , and the destination
exists, throw an error. default: .
function to filter copied files/directories. return
to copy the item,  to ignore it. can also return a
that resolves to  or  default: .
overwrite existing file or directory. the copy
operation will ignore errors if you set this to false and the destination
exists. use the  option to change this behavior.
when  timestamps from  will
be preserved. default: .
copy directories recursively default:
when , path resolution for symlinks will
be skipped. default:
asynchronously copies the entire directory structure from  to ,
including subdirectories and files.
when copying a directory to another directory, globs are not supported and
behavior is similar to .
deprecated since: v10.0.0
changes the permissions on a symbolic link.
this method is only implemented on macos.
returns:   fulfills with  upon success.
changes the ownership on a symbolic link.
changes the access and modification times of a file in the same way as
, with the difference that if the path refers to a
symbolic link, then the link is not dereferenced: instead, the timestamps of
the symbolic link itself are changed.
creates a new link from the  to the . see the posix
documentation for more detail.
returns:   fulfills with the  object for the given
symbolic link .
equivalent to  unless  refers to a symbolic link,
in which case the link itself is stat-ed, not the file that it refers to.
refer to the posix  document for more detail.
|  not supported on windows. default: .
returns:  upon success, fulfills with  if
is , or the first directory path created if  is .
asynchronously creates a directory.
the optional  argument can be an integer specifying  (permission
and sticky bits), or an object with a  property and a
property indicating whether parent directories should be created. calling
when  is a directory that exists results in a
rejection only when  is false.
returns:   fulfills with a string containing the filesystem path
of the newly created temporary directory.
creates a unique temporary directory. a unique directory name is generated by
appending six random characters to the end of the provided . due to
platform inconsistencies, avoid trailing  characters in . some
platforms, notably the bsds, can return more than six random characters, and
replace trailing  characters in  with random characters.
the optional  argument can be a string specifying an encoding, or an
object with an  property specifying the character encoding to use.
the  method will append the six randomly selected
characters directly to the  string. for instance, given a directory
, if the intention is to create a temporary directory within , the
must end with a trailing platform-specific path separator
|  see .
|  sets the file mode (permission and sticky bits)
if the file is created. default:  (readable and writable)
returns:  fulfills with a  object.
opens a .
some characters () are reserved under windows as documented
by . under ntfs, if the filename contains
a colon, node.js will open a file system stream, as described by
number of directory entries that are buffered
internally when reading from the directory. higher values lead to better
performance but higher memory usage. default:
returns:   fulfills with an .
asynchronously open a directory for iterative scanning. see the posix
creates an , which contains all further functions for reading from
and cleaning up the directory.
the  option sets the encoding for the  while opening the
directory and subsequent read operations.
example using async iteration:
when using the async iterator, the  object will be automatically
closed after the iterator exits.
returns:   fulfills with an array of the names of the files in
the directory excluding  and .
reads the contents of a directory.
object with an  property specifying the character encoding to use for
the filenames. if the  is set to , the filenames returned
will be passed as  objects.
if  is set to , the resolved array will contain
returns:   fulfills with the contents of the file.
if no encoding is specified (using ), the data is returned
as a  object. otherwise, the data will be a string.
if  is a string, then it specifies the encoding.
when the  is a directory, the behavior of  is
platform-specific. on macos, linux, and windows, the promise will be rejected
with an error. on freebsd, a representation of the directory's contents will be
an example of reading a  file located in the same directory of the
running code:
it is possible to abort an ongoing  using an . if a
request is aborted the promise returned is rejected with an :
aborting an ongoing request does not abort individual operating
system requests but rather the internal buffering  performs.
any specified  has to support reading.
returns:  fulfills with the  upon success.
reads the contents of the symbolic link referred to by . see the posix
documentation for more detail. the promise is resolved with the
upon success.
the link path returned. if the  is set to , the link path
returned will be passed as a  object.
returns:   fulfills with the resolved path upon success.
determines the actual location of  using the same semantics as the
only paths that can be converted to utf8 strings are supported.
the path. if the  is set to , the path returned will be
passed as a  object.
on linux, when node.js is linked against musl libc, the procfs file system must
be mounted on  in order for this function to work. glibc does not have
this restriction.
renames  to .
if an , , , , or
error is encountered, node.js retries the operation with a linear
backoff wait of  milliseconds longer on each try. this option
represents the number of retries. this option is ignored if the
option is not . default: .
if , perform a recursive directory removal. in
recursive mode, operations are retried on failure. default: .
the amount of time in milliseconds to wait between
retries. this option is ignored if the  option is not .
removes the directory identified by .
using  on a file (not a directory) results in the
promise being rejected with an  error on windows and an
error on posix.
to get a behavior similar to the  unix command, use
with options .
added in: v14.14.0
when , exceptions will be ignored if  does
not exist. default: .
error is encountered, node.js will retry the operation with a linear
recursive mode operations are retried on failure. default: .
removes files and directories (modeled on the standard posix  utility).
returns:   fulfills with the  object for the
given .
creates a symbolic link.
the  argument is only used on windows platforms and can be one of ,
, or . windows junction points require the destination path
to be absolute. when using , the  argument will
automatically be normalized to absolute path.
truncates (shortens or extends the length) of the content at  to
if  refers to a symbolic link, then the link is removed without affecting
the file or directory to which that link refers. if the  refers to a file
path that is not a symbolic link, the file is deleted. see the posix
change the file system timestamps of the object referenced by .
the  and  arguments follow these rules:
values can be either numbers representing unix epoch time, s, or a
numeric string like .
if the value can not be converted to a number, or is , , or
, an  will be thrown.
added in: v15.9.0, v14.18.0
indicates whether the process should continue to run
as long as files are being watched. default: .
indicates whether all subdirectories should be
watched, or only the current directory. this applies when a directory is
specified, and only on supported platforms (see ). default:
specifies the character encoding to be used for the
filename passed to the listener. default: .
an  used to signal when the watcher
should stop.
returns:  of objects with the properties:
the type of change
|  the name of the file changed.
returns an async iterator that watches for changes on , where
is either a file or a directory.
on most platforms,  is emitted whenever a filename appears or
disappears in the directory.
all the  for  also apply to .
allows aborting an in-progress writefile
the  option is ignored if  is a buffer.
any specified  has to support writing.
without waiting for the promise to be settled.
similarly to  -  is a convenience
method that performs multiple  calls internally to write the buffer
passed to it. for performance sensitive code consider using
it is possible to use an  to cancel an .
cancelation is "best effort", and some amount of data is likely still
to be written.
returns an object containing commonly used constants for file system
operations. the object is the same as . see
callback api
the callback apis perform all operations asynchronously, without blocking the
event loop, then invoke a callback function upon completion or error.
the callback apis use the underlying node.js threadpool to perform file
the final argument, , is a callback function that is invoked with
a possible error argument. if any of the accessibility checks fail, the error
argument will be an  object. the following examples check if
exists, and if it is readable or writable.
do not use  to check for the accessibility of a file before calling
, , or . doing
so introduces a race condition, since other processes may change the file's
state between the two calls. instead, user code should open/read/write the
file directly and handle the error raised if the file is not accessible.
write (not recommended)
write (recommended)
read (not recommended)
read (recommended)
the "not recommended" examples above check for accessibility and then use the
file; the "recommended" examples are better because they use the file directly
and handle the error, if any.
in general, check for the accessibility of a file only if the file will not be
used directly, for example when its accessibility is a signal from another
on windows, access-control policies (acls) on a directory may limit access to
a file or directory. the  function, however, does not check the
acl and therefore may report that a path is accessible even if the acl restricts
the user from reading or writing to it.
|  |  |  filename or file descriptor
if  is a string, then it specifies the encoding:
the  may be specified as a numeric file descriptor that has been opened
for appending (using  or ). the file descriptor will
not be closed automatically.
asynchronously changes the permissions of a file. no arguments other than a
possible exception are given to the completion callback.
see the posix  documentation for more detail.
file modes
the  argument used in both the  and
methods is a numeric bitmask created using a logical or of the following
constants:
an easier method of constructing the  is to use a sequence of three
octal digits (e.g. ). the left-most digit ( in the example), specifies
the permissions for the file owner. the middle digit ( in the example),
specifies permissions for the group. the right-most digit ( in the example),
specifies the permissions for others.
for example, the octal value  means:
the owner may read, write, and execute the file.
the group may read and write the file.
others may read and execute the file.
when using raw numbers where file modes are expected, any value larger than
may result in platform-specific behaviors that are not supported to work
consistently. therefore constants like , , or  are
not exposed in .
caveats: on windows only the write permission can be changed, and the
distinction among the permissions of group, owner, or others is not
implemented.
asynchronously changes owner and group of a file. no arguments other than a
closes the file descriptor. no arguments other than a possible exception are
given to the completion callback.
calling  on any file descriptor () that is currently in use
through any other  operation may lead to undefined behavior.
modifiers for copy operation. default: .
already exists. no arguments other than a possible exception are given to the
callback function. node.js makes no guarantees about the atomicity of the copy
operation. if an error occurs after the destination file has been opened for
writing, node.js will attempt to remove the destination.
is an optional integer that specifies the behavior
of the copy operation. it is possible to create a mask consisting of the bitwise
or of two or more values (e.g.
: the copy operation will fail if  already
: the copy operation will attempt to create a
copy-on-write reflink. if the platform does not support copy-on-write, then a
fallback copy mechanism is used.
see . default:
[0, ] range. if  is specified and  is
omitted or ,  reads sequentially from the
current file position. the  can be any one of those accepted by
if  is specified,  will ignore the  argument and will use
the specified file descriptor. this means that no  event will be
emitted.  should be blocking; non-blocking s should be passed to
if  points to a character device that only supports blocking reads
(such as keyboard or sound card), read operations do not finish until data is
available. this can prevent the process from exiting and the stream from
by providing the  option, it is possible to override the corresponding
implementations for , , and . when providing the  option,
an override for  is required. if no  is provided, an override for
is also required. if  is , an override for  is
also required.
sets the file mode (permission and sticky bits), but only if the
file was created.
replacing it may require the  option to be set to  rather than the
default . the  can be any one of those accepted by .
by providing the  option it is possible to override the corresponding
implementations for , , , and . overriding
without  can reduce performance as some optimizations ()
will be disabled. when providing the  option, overrides for at least one of
and  are required. if no  option is supplied, an override
for  is also required. if  is , an override for
is also required.
like , if  is specified,  will ignore the
argument and will use the specified file descriptor. this means that no
event will be emitted.  should be blocking; non-blocking s
should be passed to .
- deprecated: use  or  instead.
test whether or not the given path exists by checking with the file system.
then call the  argument with either true or false:
the parameters for this callback are not consistent with other node.js
callbacks. normally, the first parameter to a node.js callback is an
parameter, optionally followed by other parameters. the  callback
has only one boolean parameter. this is one reason  is recommended
instead of .
using  to check for the existence of a file before calling
, , or  is not recommended. doing
file directly and handle the error raised if the file does not exist.
the "not recommended" examples above check for existence and then use the
in general, check for the existence of a file only if the file won't be
used directly, for example when its existence is a signal from another
sets the permissions on the file. no arguments other than a possible exception
are given to the completion callback.
sets the owner of the file. no arguments other than a possible exception are
documentation for details. no arguments other than a possible
exception are given to the completion callback.
invokes the callback with the  for the file descriptor.
refer to the posix  documentation for more detail. no arguments other
than a possible exception are given to the completion callback.
truncates the file descriptor. no arguments other than a possible exception are
if the file referred to by the file descriptor was larger than  bytes, only
the first  bytes will be retained in the file.
for example, the following program retains only the first four bytes of the
file:
change the file system timestamps of the object referenced by the supplied file
descriptor. see .
changes the permissions on a symbolic link. no arguments other than a possible
set the owner of the symbolic link. no arguments other than a possible
, with the difference that if the path refers to a symbolic
link, then the link is not dereferenced: instead, the timestamps of the
symbolic link itself are changed.
no arguments other than a possible exception are given to the completion
documentation for more detail. no arguments other than a possible
retrieves the  for the symbolic link referred to by the path.
the callback gets two arguments  where  is a
object.  is identical to , except that if  is a symbolic
link, then the link itself is stat-ed, not the file that it refers to.
see the posix  documentation for more details.
|  present only if a directory is created with
set to .
the callback is given a possible exception and, if  is , the
first directory path created, .
can still be  when  is , if no directory was
when  is a directory that exists results in an error only
when  is false.
on windows, using  on the root directory even with recursion will
result in an error:
creates a unique temporary directory.
generates six random characters to be appended behind a required
to create a unique temporary directory. due to platform
inconsistencies, avoid trailing  characters in . some platforms,
notably the bsds, can return more than six random characters, and replace
trailing  characters in  with random characters.
the created directory path is passed as a string to the callback's second
the  method will append the six randomly selected characters
directly to the  string. for instance, given a directory , if the
intention is to create a temporary directory within , the
|  default:  (readable and writable)
asynchronous file open. see the posix  documentation for more details.
sets the file mode (permission and sticky bits), but only if the file was
created. on windows, only the write permission can be manipulated; see
the callback gets two arguments .
functions based on  exhibit this behavior as well:
, , etc.
asynchronously open a directory. see the posix  documentation for
|  |  the buffer that the data will be
written to.
the position in  to write the data to.
|  |  specifies where to begin reading from in the
file. if  is  or , data will be read from the current
file position, and the file position will be updated. if  is an
integer, the file position will be unchanged.
read data from the file specified by .
the callback is given the three arguments, .
a promise for an  with  and  properties.
|  |  default:
similar to the  function, this version takes an optional
object. if no  object is specified, it will default with the
above values.
reads the contents of a directory. the callback gets two arguments
where  is an array of the names of the files in the directory excluding
the filenames passed to the callback. if the  is set to ,
the filenames returned will be passed as  objects.
if  is set to , the  array will contain
the callback is passed two arguments , where  is the
contents of the file.
if no encoding is specified, then the raw buffer is returned.
when the path is a directory, the behavior of  and
is platform-specific. on macos, linux, and windows, an
error will be returned. on freebsd, a representation of the directory's contents
it is possible to abort an ongoing request using an . if a
request is aborted the callback is called with an :
the  function buffers the entire file. to minimize memory costs,
when possible prefer streaming via .
file descriptors
any specified file descriptor has to support reading.
if a file descriptor is specified as the , it will not be closed
the reading will begin at the current position. for example, if the file
already had ' and six bytes are read with the file descriptor,
the call to  with the same file descriptor, would give
, rather than .
performance considerations
the  method asynchronously reads the contents of a file into
memory one chunk at a time, allowing the event loop to turn between each chunk.
this allows the read operation to have less impact on other activity that may
be using the underlying libuv thread pool but means that it will take longer
to read a complete file into memory.
the additional read overhead can vary broadly on different systems and depends
on the type of file being read. if the file type is not a regular file (a pipe
for instance) and node.js is unable to determine an actual file size, each read
operation will load on 64 kib of data. for regular files, each read will process
512 kib of data.
for applications that require as-fast-as-possible reading of file contents, it
is better to use  directly and for application code to manage
reading the full contents of the file itself.
the node.js github issue  provides more information and a detailed
analysis on the performance of  for multiple file sizes in
different node.js versions.
reads the contents of the symbolic link referred to by . the callback gets
two arguments .
the link path passed to the callback. if the  is set to ,
the link path returned will be passed as a  object.
read from a file specified by  and write to an array of s
is the offset from the beginning of the file from where data
should be read. if , the data will be read
from the current position.
the callback will be given three arguments: , , and
.  is how many bytes were read from the file.
asynchronously computes the canonical pathname by resolving , , and
symbolic links.
a canonical pathname is not necessarily unique. hard links and bind mounts can
expose a file system entity through many pathnames.
this function behaves like , with some exceptions:
no case conversion is performed on case-insensitive file systems.
the maximum number of symbolic links is platform-independent and generally
(much) higher than what the native  implementation supports.
the  gets two arguments . may use
to resolve relative paths.
the path passed to the callback. if the  is set to ,
the path returned will be passed as a  object.
if  resolves to a socket or a pipe, the function will return a system
dependent name for that object.
asynchronous .
the  gets two arguments .
asynchronously rename file at  to the pathname provided
as . in the case that  already exists, it will
be overwritten. if there is a directory at , an error will
be raised instead. no arguments other than a possible exception are
see also: .
asynchronous . no arguments other than a possible exception are given
to the completion callback.
using  on a file (not a directory) results in an  error on
windows and an  error on posix.
if , perform a recursive removal. in
asynchronously removes files and directories (modeled on the standard posix
utility). no arguments other than a possible exception are given to the
completion callback.
asynchronous . the callback gets two arguments  where
is an  object.
in case of an error, the  will be one of .
, , or  is not recommended.
instead, user code should open/read/write the file directly and handle the
error raised if the file is not available.
to check if a file exists without manipulating it afterwards,
is recommended.
for example, given the following directory structure:
the next program will check for the stats of the given paths:
the resulting output will resemble:
creates the link called  pointing to . no arguments other than a
the  argument is only available on windows and ignored on other platforms.
it can be set to , , or . if the  argument is
not a string, node.js will autodetect  type and use  or .
if the  does not exist,  will be used. windows junction points
require the destination path to be absolute. when using , the
argument will automatically be normalized to absolute path.
relative targets are relative to the link's parent directory.
the above example creates a symbolic link  which points to  in the
same directory:
truncates the file. no arguments other than a possible exception are
given to the completion callback. a file descriptor can also be passed as the
first argument. in this case,  is called.
passing a file descriptor is deprecated and may result in an error being thrown
asynchronously removes a file or symbolic link. no arguments other than a
will not work on a directory, empty or otherwise. to remove a
directory, use .
added in: v0.1.31
optional, a listener previously attached using
stop watching for changes on . if  is specified, only that
particular listener is removed. otherwise, all listeners are removed,
effectively stopping watching of .
calling  with a filename that is not being watched is a
no-op, not an error.
using  is more efficient than  and
.  should be used instead of
and  when possible.
values can be either numbers representing unix epoch time in seconds,
s, or a numeric string like .
allows closing the watcher with an abortsignal.
watch for changes on , where  is either a file or a
directory.
the second argument is optional. if  is provided as a string, it
specifies the . otherwise  should be passed as an object.
the listener callback gets two arguments .
is either  or , and  is the name of the file
which triggered the event.
the listener callback is attached to the  event fired by
, but it is not the same thing as the  value of
if a  is passed, aborting the corresponding abortcontroller will close
the returned .
the  api is not 100% consistent across platforms, and is
unavailable in some situations.
the recursive option is only supported on macos and windows.
on windows, no events will be emitted if the watched directory is moved or
renamed. an  error is reported when the watched directory is deleted.
availability
this feature depends on the underlying operating system providing a way
to be notified of filesystem changes.
on linux systems, this uses .
on bsd systems, this uses .
on macos, this uses  for files and  for
directories.
on sunos systems (including solaris and smartos), this uses .
on windows systems, this feature depends on .
on aix systems, this feature depends on , which must be enabled.
on ibm i systems, this feature is not supported.
if the underlying functionality is not available for some reason, then
will not be able to function and may throw an exception.
for example, watching files or directories can be unreliable, and in some
cases impossible, on network file systems (nfs, smb, etc) or host file systems
when using virtualization software such as vagrant or docker.
it is still possible to use , which uses stat polling, but
this method is slower and less reliable.
inodes
on linux and macos systems,  resolves the path to an  and
watches the inode. if the watched path is deleted and recreated, it is assigned
a new inode. the watch will emit an event for the delete but will continue
watching the original inode. events for the new inode will not be emitted.
this is expected behavior.
aix files retain the same inode for the lifetime of a file. saving and closing a
watched file on aix will result in two notifications (one for adding new
content, and one for truncation).
filename argument
providing  argument in the callback is only supported on linux,
macos, windows, and aix. even on supported platforms,  is not always
guaranteed to be provided. therefore, don't assume that  argument is
always provided in the callback, and have some fallback logic if it is .
watch for changes on . the callback  will be called each
time the file is accessed.
the  argument may be omitted. if provided, it should be an object. the
object may contain a boolean named  that indicates
whether the process should continue to run as long as files are being watched.
the  object may specify an  property indicating how often the
target should be polled in milliseconds.
the  gets two arguments the current stat object and the previous
stat object:
these stat objects are instances of . if the  option is ,
the numeric values in these objects are specified as s.
to be notified when the file was modified, not just accessed, it is necessary
to compare  and .
when an  operation results in an  error, it
will invoke the listener once, with all the fields zeroed (or, for dates, the
unix epoch). if the file is created later on, the listener will be called
again, with the latest stat objects. this is a change in functionality since
v0.10.
.  should be used instead of  and
when possible.
when a file being watched by  disappears and reappears,
then the contents of  in the second callback event (the file's
reappearance) will be the same as the contents of  in the first
callback event (its disappearance).
this happens when:
the file is deleted, followed by a restore
the file is renamed and then renamed a second time back to its original name
write  to the file specified by .
determines the part of the buffer to be written, and  is
an integer specifying the number of bytes to write.
refers to the offset from the beginning of the file where this data
should be written. if , the data will be written
at the current position. see .
the callback will be given three arguments  where
specifies how many bytes were written from .
it is unsafe to use  multiple times on the same file without waiting
for the callback. for this scenario,  is
write  to the file specified by . if  is not a string, or an
object with an own  function property, then an exception is thrown.
should be written. if  the data will be written at
the current position. see .
is the expected string encoding.
the callback will receive the arguments  where
specifies how many bytes the passed string required to be written. bytes
written is not necessarily the same as string characters written. see
on windows, if the file descriptor is connected to the console (e.g.
or ) a string containing non-ascii characters will not be rendered
properly by default, regardless of the encoding used.
it is possible to configure the console to render utf-8 properly by changing the
active codepage with the  command. see the  docs for more
when  is a filename, asynchronously writes data to the file, replacing the
file if it already exists.  can be a string or a buffer.
when  is a file descriptor, the behavior is similar to calling
directly (which is recommended). see the notes below on using
a file descriptor.
it is unsafe to use  multiple times on the same file without
waiting for the callback. for this scenario,  is
similarly to  -  is a convenience method that
performs multiple  calls internally to write the buffer passed to it.
for performance sensitive code consider using .
using  with file descriptors
when  is a file descriptor, the behavior is almost identical to directly
calling  like:
the difference from directly calling  is that under some unusual
conditions,  might write only part of the buffer and need to be
retried to write the remaining data, whereas  retries until
the data is entirely written (or an error occurs).
the implications of this are a common source of confusion. in
the file descriptor case, the file is not replaced! the data is not necessarily
written to the beginning of the file, and the file's original data may remain
before and/or after the newly written data.
for example, if  is called twice in a row, first to write the
string , then to write the string , the file would contain
, and might contain some of the file's original data (depending
on the size of the original file, and the position of the file descriptor). if
a file name had been used instead of a descriptor, the file would be guaranteed
to contain only .
write an array of s to the file specified by  using
is the offset from the beginning of the file where this data
at the current position.
.  is how many bytes were written from .
if this method is ed, it returns a promise for an
with  and  properties.
waiting for the callback. for this scenario, use .
synchronous api
the synchronous apis perform all operations synchronously, blocking the
event loop until the operation completes or fails.
synchronously tests a user's permissions for the file or directory specified
by . the  argument is an optional integer that specifies the
accessibility checks to be performed.  should be either the value
or a mask consisting of the bitwise or of any of
, , and  (e.g.
if any of the accessibility checks fail, an  will be thrown. otherwise,
the method will return .
synchronously append data to a file, creating the file if it does not yet
for detailed information, see the documentation of the asynchronous version of
this api: .
synchronously changes owner and group of a file. returns .
this is the synchronous version of .
closes the file descriptor. returns .
synchronously copies  to . by default,  is overwritten if it
already exists. returns . node.js makes no guarantees about the
atomicity of the copy operation. if an error occurs after the destination file
has been opened for writing, node.js will attempt to remove the destination.
to copy the item,  to ignore it. default:
synchronously copies the entire directory structure from  to ,
returns  if the path exists,  otherwise.
is deprecated, but  is not. the
parameter to  accepts parameters that are inconsistent with other
node.js callbacks.  does not use a callback.
added in: v0.4.7
sets the permissions on the file. returns .
sets the owner of the file. returns .
added in: v0.1.96
documentation for details. returns .
retrieves the  for the file descriptor.
refer to the posix  documentation for more detail. returns .
added in: v0.8.6
truncates the file descriptor. returns .
synchronous version of . returns .
deprecated since: v0.4.7
changes the permissions on a symbolic link. returns .
set the owner for the path. returns .
change the file system timestamps of the symbolic link referenced by .
returns , or throws an exception when parameters are incorrect or
the operation fails. this is the synchronous version of .
documentation for more detail. returns .
whether an exception will be thrown
if no file system entry exists, rather than returning .
retrieves the  for the symbolic link referred to by .
synchronously creates a directory. returns , or if  is
, the first directory path created.
returns the created directory path.
synchronously open a directory. see .
|  default: .
returns an integer representing the file descriptor.
reads the contents of the directory.
the filenames returned. if the  is set to ,
if  is set to , the result will contain
returns the contents of the .
if the  option is specified then this function returns a
string. otherwise it returns a buffer.
similar to , when the path is a directory, the behavior of
is platform-specific.
returns the symbolic link's string value.
the link path returned. if the  is set to ,
returns the number of .
similar to the above  function, this version takes an optional  object.
if no  object is specified, it will default with the above values.
returns:  the number of bytes read.
returns the resolved pathname.
added in: v9.2.0
synchronous .
the path returned. if the  is set to ,
renames the file from  to . returns .
synchronous . returns .
using  on a file (not a directory) results in an  error
on windows and an  error on posix.
synchronously removes files and directories (modeled on the standard posix
utility). returns .
retrieves the  for the path.
truncates the file. returns . a file descriptor can also be
passed as the first argument. in this case,  is called.
returns:  the number of bytes written.
common objects
the common objects are shared by all of the file system api variants
(promise, callback, and synchronous).
a class representing a directory stream.
created by , , or
asynchronously close the directory's underlying resource handle.
subsequent reads will result in errors.
a promise is returned that will be resolved after the resource has been
the  will be called after the resource handle has been closed.
synchronously close the directory's underlying resource handle.
the read-only path of this directory as was provided to ,
returns:  containing  |
asynchronously read the next directory entry via  as an
a promise is returned that will be resolved with an , or
if there are no more directory entries to read.
directory entries returned by this function are in no particular order as
provided by the operating system's underlying directory mechanisms.
entries added or removed while iterating over the directory might not be
included in the iteration results.
after the read is completed, the  will be called with an
, or  if there are no more directory entries to read.
synchronously read the next directory entry as an . see the
posix  documentation for more detail.
if there are no more directory entries to read,  will be returned.
returns:  of
asynchronously iterates over the directory until all entries have
been read. refer to the posix  documentation for more detail.
entries returned by the async iterator are always an .
the  case from  is handled internally.
see  for an example.
directory entries returned by this iterator are in no particular order as
added in: v10.10.0
a representation of a directory entry, which can be a file or a subdirectory
within the directory, as returned by reading from an . the
directory entry is a combination of the file name and file type pairs.
additionally, when  or  is called with
the  option set to , the resulting array is filled with
objects, rather than strings or s.
returns  if the  object describes a block device.
returns  if the  object describes a character device.
returns  if the  object describes a file system
returns  if the  object describes a first-in-first-out
(fifo) pipe.
returns  if the  object describes a regular file.
returns  if the  object describes a socket.
returns  if the  object describes a symbolic link.
the file name that this  object refers to. the type of this
value is determined by the  passed to  or
added in: v0.5.8
a successful call to  method will return a new
all  objects emit a  event whenever a specific watched
file is modified.
the type of change event that has occurred
|  the filename that changed (if relevant/available)
emitted when something changes in a watched directory or file.
see more details in .
the  argument may not be provided depending on operating system
support. if  is provided, it will be provided as a  if
is called with its  option set to , otherwise
will be a utf-8 string.
emitted when the watcher stops watching for changes. the closed
object is no longer usable in the event handler.
emitted when an error occurs while watching the file. the errored
stop watching for changes on the given . once stopped, the
object is no longer usable.
added in: v14.3.0, v12.20.0
when called, requests that the node.js event loop not exit so long as the
is active. calling  multiple times will have
no effect.
by default, all  objects are "ref'ed", making it normally
unnecessary to call  unless  had been
called previously.
when called, the active  object will not require the node.js
event loop to remain active. if there is no other activity keeping the
event loop running, the process may exit before the  object's
callback is invoked. calling  multiple times will have
added in: v0.1.93
instances of  are created and returned using the
emitted when the 's underlying file descriptor has been closed.
integer file descriptor used by the .
emitted when the 's file descriptor has been opened.
added in: v9.11.0
emitted when the  is ready to be used.
fires immediately after .
added in: v6.4.0
the number of bytes that have been read so far.
the path to the file the stream is reading from as specified in the first
argument to . if  is passed as a string, then
will be a string. if  is passed as a , then
will be a . if  is specified, then
added in: v11.2.0, v10.16.0
this property is  if the underlying file has not been opened yet,
i.e. before the  event is emitted.
a  object provides information about a file.
objects returned from , , , and
their synchronous counterparts are of this type.
if  in the  passed to those methods is true, the numeric values
will be  instead of , and the object will contain additional
nanosecond-precision properties suffixed with .
version:
added in: v0.1.10
returns  if the  object describes a file system directory.
if the  object was obtained from , this method will
always return . this is because  returns information
about a symbolic link itself and not the path it resolves to.
returns  if the  object describes a first-in-first-out (fifo)
pipe.
this method is only valid when using .
the numeric identifier of the device containing the file.
the file system specific "inode" number for the file.
a bit-field describing the file type and mode.
the number of hard-links that exist for the file.
the numeric user identifier of the user that owns the file (posix).
the numeric group identifier of the group that owns the file (posix).
a numeric device identifier if the file represents a device.
the size of the file in bytes.
if the underlying file system does not support getting the size of the file,
this will be .
the file system block size for i/o operations.
the number of blocks allocated for this file.
the timestamp indicating the last time this file was accessed expressed in
milliseconds since the posix epoch.
the timestamp indicating the last time this file was modified expressed in
the timestamp indicating the last time the file status was changed expressed
in milliseconds since the posix epoch.
the timestamp indicating the creation time of this file expressed in
added in: v12.10.0
only present when  is passed into the method that generates
the object.
nanoseconds since the posix epoch.
in nanoseconds since the posix epoch.
added in: v0.11.13
the timestamp indicating the last time this file was accessed.
the timestamp indicating the last time this file was modified.
the timestamp indicating the last time the file status was changed.
the timestamp indicating the creation time of this file.
stat time values
the , , ,  properties are
numeric values that hold the corresponding times in milliseconds. their
precision is platform specific. when  is passed into the
method that generates the object, the properties will be ,
otherwise they will be .
that hold the corresponding times in nanoseconds. they are
the object. their precision is platform specific.
, , , and  are
object alternate representations of the various times. the
and number values are not connected. assigning a new number value, or
mutating the  value, will not be reflected in the corresponding alternate
representation.
the times in the stat object have the following semantics:
"access time": time when file data last accessed. changed
by the , , and  system calls.
"modified time": time when file data last modified.
changed by the , , and  system calls.
"change time": time when file status was last changed
(inode data modification). changed by the , ,
, , , , ,
, and  system calls.
"birth time": time of file creation. set once when the
file is created. on filesystems where birthtime is not available,
this field may instead hold either the  or
(ie, unix epoch timestamp ). this value may be greater
than  or  in this case. on darwin and other freebsd variants,
also set if the  is explicitly set to an earlier value than the current
using the  system call.
prior to node.js 0.12, the  held the  on windows systems. as
of 0.12,  is not "creation time", and on unix systems, it never was.
emitted when the 's file is opened.
the number of bytes written so far. does not include data that is still queued
for writing.
added in: v0.9.4
closes . optionally accepts a
callback that will be executed once the
is closed.
the path to the file the stream is writing to as specified in the first
will be a .
added in: v11.2.0
operations.
fs constants
the following constants are exported by  and .
not every constant will be available on every operating system;
this is especially important for windows, where many of the posix specific
definitions are not available.
for portable applications it is recommended to check for their presence
before use.
to use more than one constant, use the bitwise or  operator.
file access constants
the following constants are meant for use as the  parameter passed to
the definitions are also available on windows.
file copy constants
the following constants are meant for use with .
file open constants
on windows, only , , , , ,
, , and  are available.
file type constants
the following constants are meant for use with the  object's
property for determining a file's type.
on windows, only , , , , and ,
are available.
file mode constants
property for determining the access permissions for a file.
on windows, only  and  are available.
ordering of callback and promise-based operations
because they are executed asynchronously by the underlying thread pool,
there is no guaranteed ordering when using either the callback or
promise-based methods.
for example, the following is prone to error because the
operation might complete before the  operation:
it is important to correctly order the operations by awaiting the results
of one before invoking the other:
or, when using the callback apis, move the  call into the callback
of the  operation:
file paths
most  operations accept file paths that may be specified in the form of
a string, a , or a  object using the  protocol.
string paths
string paths are interpreted as utf-8 character sequences identifying
the absolute or relative filename. relative paths will be resolved relative
to the current working directory as determined by calling .
example using an absolute path on posix:
example using a relative path on posix (relative to ):
file url paths
for most  module functions, the  or  argument may be
passed as a  object using the  protocol.
urls are always absolute paths.
platform-specific considerations
on windows,  s with a host name convert to unc paths, while
s with drive letters convert to local absolute paths.  s
with no host name and no drive letter will result in an error:
s with drive letters must use  as a separator just after
the drive letter. using another separator will result in an error.
on all other platforms,  s with a host name are unsupported and
will result in an error:
a   having encoded slash characters will result in an error on all
platforms:
on windows,  s having encoded backslash will result in an error:
buffer paths
paths specified using a  are useful primarily on certain posix
operating systems that treat file paths as opaque byte sequences. on such
systems, it is possible for a single file path to contain sub-sequences that
use multiple character encodings. as with string paths,  paths may
be relative or absolute:
per-drive working directories on windows
on windows, node.js follows the concept of per-drive working directory. this
behavior can be observed when using a drive path without a backslash. for
example  can potentially return a different result than
. for more information, see
on posix systems, for every process, the kernel maintains a table of currently
open files and resources. each open file is assigned a simple numeric
identifier called a file descriptor. at the system-level, all file system
operations use these file descriptors to identify and track each specific
file. windows systems use a different but conceptually similar mechanism for
tracking resources. to simplify things for users, node.js abstracts away the
differences between operating systems and assigns all open files a numeric file
descriptor.
the callback-based , and synchronous  methods open a
file and allocate a new file descriptor. once allocated, the file descriptor may
be used to read data from, write data to, or request information about the file.
operating systems limit the number of file descriptors that may be open
at any given time so it is critical to close the descriptor when operations
are completed. failure to do so will result in a memory leak that will
eventually cause an application to crash.
the promise-based apis use a  object in place of the numeric
file descriptor. these objects are better managed by the system to ensure
that resources are not leaked. however, it is still required that they are
closed when operations are completed:
threadpool usage
all callback and promise-based file system apis (with the exception of
) use libuv's threadpool. this can have surprising and negative
performance implications for some applications. see the
file system flags
the following flags are available wherever the  option takes a
: open file for appending.
the file is created if it does not exist.
: like  but fails if the path exists.
: open file for reading and appending.
: open file for appending in synchronous mode.
: open file for reading and appending in synchronous mode.
: open file for reading.
an exception occurs if the file does not exist.
: open file for reading and writing.
: open file for reading and writing in synchronous mode. instructs
the operating system to bypass the local file system cache.
this is primarily useful for opening files on nfs mounts as it allows
skipping the potentially stale local cache. it has a very real impact on
i/o performance so using this flag is not recommended unless it is needed.
this doesn't turn  or  into a synchronous
blocking call. if synchronous operation is desired, something like
should be used.
: open file for writing.
the file is created (if it does not exist) or truncated (if it exists).
can also be a number as documented by ; commonly used constants
are available from . on windows, flags are translated to
their equivalent ones where applicable, e.g.  to ,
or  to , as accepted by .
the exclusive flag  ( flag in ) causes the operation to
return an error if the path already exists. on posix, if the path is a symbolic
link, using  returns an error even if the link is to a path that does
not exist. the exclusive flag might not work with network file systems.
modifying a file rather than replacing it may require the  option to be
set to  rather than the default .
the behavior of some flags are platform-specific. as such, opening a directory
on macos and linux with the  flag, as in the example below, will return an
error. in contrast, on windows and freebsd, a file descriptor or a
on windows, opening an existing hidden file using the  flag (either
through , , or ) will fail with
. existing hidden files can be opened for writing with the  flag.
a call to  or  can be used to reset
the file contents.
global objects
these objects are available in all modules. the following variables may appear
to be global but are not. they exist only in the scope of modules, see the
the objects listed here are specific to node.js. there are
that are part of the javascript language itself, which are also globally
accessible.
a utility class used to signal cancelation in selected -based apis.
the api is based on the web api .
an optional reason, retrievable on the 's
triggers the abort signal, causing the  to emit
the  event.
the  is used to notify observers when the
method is called.
returns a new already aborted .
added in: v17.3.0, v16.14.0
the number of milliseconds to wait before triggering
the abortsignal.
returns a new  which will be aborted in  milliseconds.
the  event is emitted when the  method
is called. the callback is invoked with a single object argument with a
single  property set to :
the  with which the  is associated will only
ever trigger the  event once. we recommended that code check
that the  attribute is  before adding an
event listener.
any event listeners attached to the  should use the
option (or, if using the  apis to attach a
listener, use the  method) to ensure that the event listener is
removed as soon as the  event is handled. failure to do so may
result in memory leaks.
type:  true after the  has been aborted.
an optional callback function that may be set by user code to be notified
when the  function has been called.
an optional reason specified when the  was triggered.
added in: v17.3.0
if  is , throws .
added in: v0.1.103
used to handle binary data. see the .
a browser-compatible implementation of .
this variable may appear to be global but is not. see .
global alias for .
is described in the  section.
added in: v0.0.1
used to print to stdout and stderr. see the  section.
- experimental. enable this api with the
cli flag.
a browser-compatible implementation of . this global is available
only if the node.js binary was compiled with including support for the
a browser-compatible implementation of the .
a browser-compatible implementation of the  class. see
- experimental. disable this api with the
a browser-compatible implementation of the  function.
class
added in: v0.1.27
the global namespace object.
in browsers, the top-level scope is the global scope. this means that
within the browser  will define a new global variable. in
node.js this is different. the top-level scope is not the global scope;
inside a node.js module will be local to that module.
the  class. see  for more details.
the  object.
added in: v0.1.7
the process object. see the  section.
added in: v11.0.0
function to be queued.
the  method queues a microtask to invoke . if
throws an exception, the
event will be emitted.
the microtask queue is managed by v8 and may be used in a similar manner to
the  queue, which is managed by node.js. the
queue is always processed before the microtask queue
within each turn of the node.js event loop.
the whatwg  method.
the whatwg  class. see  for more details.
the whatwg  class. see the  section.
the object that acts as the namespace for all w3c
related functionality. see the
for usage and compatibility.
to use the http server and client one must .
the http interfaces in node.js are designed to support many features
of the protocol which have been traditionally difficult to use.
in particular, large, possibly chunk-encoded, messages. the interface is
careful to never buffer entire requests or responses, so the
user is able to stream data.
http message headers are represented by an object like this:
keys are lowercased. values are not modified.
in order to support the full spectrum of possible http applications, the node.js
http api is very low-level. it deals with stream handling and message
parsing only. it parses a message into headers and body but it does not
parse the actual headers or the body.
see  for details on how duplicate headers are handled.
the raw headers as they were received are retained in the
property, which is an array of . for
example, the previous message header object might have a
list like the following:
added in: v0.3.4
an  is responsible for managing connection persistence
and reuse for http clients. it maintains a queue of pending requests
for a given host and port, reusing a single socket connection for each
until the queue is empty, at which time the socket is either destroyed
or put into a pool where it is kept to be used again for requests to the
same host and port. whether it is destroyed or pooled depends on the
pooled connections have tcp keep-alive enabled for them, but servers may
still close idle connections, in which case they will be removed from the
pool and a new connection will be made when a new http request is made for
that host and port. servers may also refuse to allow multiple requests
over the same connection, in which case the connection will have to be
remade for every request and cannot be pooled. the  will still make
the requests to that server, but each one will occur over a new connection.
when a connection is closed by the client or the server, it is removed
from the pool. any unused sockets in the pool will be unrefed so as not
to keep the node.js process running when there are no outstanding requests.
(see ).
it is good practice, to  an  instance when it is no
longer in use, because unused sockets consume os resources.
sockets are removed from an agent when the socket emits either
a  event or an  event. when intending to keep one
http request open for a long time without keeping it in the agent, something
like the following may be done:
an agent may also be used for an individual request. by providing
as an option to the  or
functions, a one-time use  with default options will be used
for the client connection.
set of configurable options to set on the agent.
can have the following fields:
keep sockets around even when there are no
outstanding requests, so they can be used for future requests without
having to reestablish a tcp connection. not to be confused with the
value of the  header. the
header is always sent when using an agent except when the
header is explicitly specified or when the  and
options are respectively set to  and , in which case
will be used. default: .
when using the  option, specifies
for tcp keep-alive packets. ignored when the
option is  or . default: .
maximum number of sockets to allow per host.
if the same host opens multiple concurrent connections, each request
will use new socket until the  value is reached.
if the host attempts to open more connections than ,
the additional requests will enter into a pending request queue, and
will enter active connection state when an existing connection terminates.
this makes sure there are at most  active connections at
any point in time, from a given host.
maximum number of sockets allowed for
all hosts in total. each request will use a new socket
until the maximum is reached.
maximum number of sockets per host to leave open
in a free state. only relevant if  is set to .
scheduling strategy to apply when picking
the next free socket to use. it can be  or .
the main difference between the two scheduling strategies is that
selects the most recently used socket, while  selects
the least recently used socket.
in case of a low rate of request per second, the  scheduling
will lower the risk of picking a socket that might have been closed
by the server due to inactivity.
in case of a high rate of request per second,
the  scheduling will maximize the number of open sockets,
while the  scheduling will keep it as low as possible.
socket timeout in milliseconds.
this will set the timeout when the socket is created.
in  are also supported.
the default  that is used by  has all
of these values set to their respective defaults.
to configure any of them, a custom  instance must be created.
added in: v0.11.4
options containing connection details. check
for the format of the options
callback function that receives the created socket
produces a socket/stream to be used for http requests.
by default, this function is the same as . however,
custom agents may override this method in case greater flexibility is desired.
a socket/stream can be supplied in one of two ways: by returning the
socket/stream from this function, or by passing the socket/stream to .
this method is guaranteed to return an instance of the  class,
a subclass of , unless the user specifies a socket
type other than .
has a signature of .
called when  is detached from a request and could be persisted by the
. default behavior is to:
this method can be overridden by a particular  subclass. if this
method returns a falsy value, the socket will be destroyed instead of persisting
it for use with the next request.
the  argument can be an instance of , a subclass of
called when  is attached to  after being persisted because of
the keep-alive options. default behavior is to:
this method can be overridden by a particular  subclass.
destroy any sockets that are currently in use by the agent.
it is usually not necessary to do this. however, if using an
agent with  enabled, then it is best to explicitly shut down
the agent when it is no longer needed. otherwise,
sockets might stay open for quite a long time before the server
terminates them.
an object which contains arrays of sockets currently awaiting use by
the agent when  is enabled. do not modify.
sockets in the  list will be automatically destroyed and
removed from the array on .
a set of options providing information for name generation
a domain name or ip address of the server to issue the
request to
port of remote server
local interface to bind for network connections
when issuing the request
must be 4 or 6 if this doesn't equal .
get a unique name for a set of request options, to determine whether a
connection can be reused. for an http agent, this returns
or . for an https agent,
the name includes the ca, cert, ciphers, and other https/tls-specific options
that determine socket reusability.
added in: v0.11.7
by default set to 256. for agents with  enabled, this
sets the maximum number of sockets that will be left open in the free
added in: v0.3.6
by default set to . determines how many concurrent sockets the agent
can have open per origin. origin is the returned value of .
can have open. unlike , this parameter applies across all origins.
an object which contains queues of requests that have not yet been assigned to
sockets. do not modify.
an object which contains arrays of sockets currently in use by the
agent. do not modify.
added in: v0.1.17
this object is created internally and returned from . it
represents an in-progress request whose header has already been queued. the
header is still mutable using the ,
,  api. the actual header will
be sent along with the first data chunk or when calling .
to get the response, add a listener for  to the request object.
will be emitted from the request object when the response
headers have been received. the  event is executed with one
argument which is an instance of .
during the  event, one can add listeners to the
response object; particularly to listen for the  event.
if no  handler is added, then the response will be
entirely discarded. however, if a  event handler is added,
then the data from the response object must be consumed, either by
calling  whenever there is a  event, or
by adding a  handler, or by calling the  method.
until the data is consumed, the  event will not fire. also, until
the data is read it will consume memory that can eventually lead to a
'process out of memory' error.
for backward compatibility,  will only emit  if there is an
listener registered.
set  header to limit the response body size. mismatching the
header value will result in an [][] being thrown,
identified by  .
value should be in bytes, not characters. use
to determine the length of the body in bytes.
added in: v1.4.1deprecated since: v17.0.0, v16.12.0
- deprecated. listen for the  event instead.
emitted when the request has been aborted by the client. this event is only
emitted on the first call to .
indicates that the request is completed, or its underlying connection was
terminated prematurely (before the response completion).
emitted each time a server responds to a request with a  method. if
this event is not being listened for, clients receiving a  method will
have their connections closed.
this event is guaranteed to be passed an instance of the  class,
a client and server pair demonstrating how to listen for the  event:
added in: v0.3.2
emitted when the server sends a '100 continue' http response, usually because
the request contained 'expect: 100-continue'. this is an instruction that
the client should send the request body.
emitted when the request has been sent. more specifically, this event is emitted
when the last segment of the response headers and body have been handed off to
the operating system for transmission over the network. it does not imply that
the server has received anything yet.
emitted when the server sends a 1xx intermediate response (excluding 101
upgrade). the listeners of this event will receive an object containing the
http version, status code, status message, key-value headers object,
and array with the raw header names followed by their respective values.
101 upgrade statuses do not fire this event due to their break from the
traditional http request/response chain, such as web sockets, in-place tls
upgrades, or http 2.0. to be notified of 101 upgrade notices, listen for the
event instead.
added in: v0.1.0
emitted when a response is received to this request. this event is emitted only
added in: v0.5.3
added in: v0.7.8
emitted when the underlying socket times out from inactivity. this only notifies
that the socket has been idle. the request must be destroyed manually.
emitted each time a server responds to a request with an upgrade. if this
event is not being listened for and the response status code is 101 switching
protocols, clients receiving an upgrade header will have their connections
a client server pair demonstrating how to listen for the  event.
added in: v0.3.8deprecated since: v14.1.0, v13.14.0
marks the request as aborting. calling this will cause remaining data
in the response to be dropped and the socket to be destroyed.
- deprecated. check  instead.
the  property will be  if the request has
been aborted.
added in: v0.3.0deprecated since: v13.0.0
- deprecated. use .
finishes sending the request. if any parts of the body are
unsent, it will flush them to the stream. if the request is
chunked, this will send the terminating .
if  is specified, it is equivalent to calling
followed by .
if  is specified, it will be called when the request stream
is finished.
optional, an error to emit with  event.
destroy the request. optionally emit an  event,
and emit a  event. calling this will cause remaining data
see  for further details.
added in: v14.1.0, v13.14.0
is  after  has been called.
added in: v0.0.1deprecated since: v13.4.0, v12.16.0
the  property will be  if
has been called.  will automatically be called if the
request was initiated via .
flushes the request headers.
for efficiency reasons, node.js normally buffers the request headers until
is called or the first chunk of request data is written. it
then tries to pack the request headers and data into a single tcp packet.
that's usually desired (it saves a tcp round-trip), but not when the first
data is not sent until possibly much later.  bypasses
the optimization and kickstarts the request.
reads out a header on the request. the name is case-insensitive.
the type of the return value depends on the arguments provided to
returns an array containing the unique names of the current outgoing headers.
all header names are lowercase.
returns a shallow copy of the current outgoing headers. since a shallow copy
is used, array values may be mutated without additional calls to various
header-related http module methods. the keys of the returned object are the
header names and the values are the respective header values. all header names
are lowercase.
the object returned by the  method does not
prototypically inherit from the javascript . this means that typical
methods such as , , and others
are not defined and will not work.
returns an array containing the unique names of the current outgoing raw
headers. header names are returned with their exact casing being set.
returns  if the header identified by  is currently set in the
outgoing headers. the header name matching is case-insensitive.
limits maximum response headers count. if set to 0, no limit will be applied.
added in: v0.4.0
the request path.
added in: v0.1.97
the request method.
the request host.
the request protocol.
removes a header that's already defined into headers object.
whether the request is send through a reused socket.
when sending request through a keep-alive enabled agent, the underlying socket
might be reused. but if server closes connection at unfortunate time, client
may run into a 'econnreset' error.
by marking a request whether it reused socket or not, we can do
automatic error retry base on it.
sets a single header value for headers object. if this header already exists in
the to-be-sent headers, its value will be replaced. use an array of strings
here to send multiple headers with the same name. non-string values will be
stored without modification. therefore,  may return
non-string values. however, the non-string values will be converted to strings
for network transmission.
when the value is a string an exception will be thrown if it contains
characters outside the  encoding.
if you need to pass utf-8 characters in the value please encode the value
using the  standard.
once a socket is assigned to this request and is connected
will be called.
milliseconds before a request times out.
optional function to be called when a timeout occurs.
same as binding to the  event.
reference to the underlying socket. usually users will not want to access
this property. in particular, the socket will not emit  events
because of how the protocol parser attaches to the socket.
this property is guaranteed to be an instance of the  class,
a subclass of , unless the user specified a socket
is  after  has been called. this property
does not indicate whether the data has been flushed, for this use
is  if all data has been flushed to the underlying system, immediately
before the  event is emitted.
added in: v0.1.29
sends a chunk of the body. this method can be called multiple times. if no
is set, data will automatically be encoded in http chunked
transfer encoding, so that server knows when the data ends. the
header is added. calling
is necessary to finish sending the request.
the  argument is optional and only applies when  is a string.
defaults to .
the  argument is optional and will be called when this chunk of data
is flushed, but only if the chunk is non-empty.
returns  if the entire data was flushed successfully to the kernel
buffer. returns  if all or part of the data was queued in user memory.
will be emitted when the buffer is free again.
when  function is called with empty string or buffer, it does
nothing and waits for more input.
emitted each time a request with an http  is received.
if this event is not listened for, the server will automatically respond
with a  as appropriate.
handling this event involves calling  if the
client should continue to send the request body, or generating an appropriate
http response (e.g. 400 bad request) if the client should not continue to send
the request body.
when this event is emitted and handled, the  event will
not be emitted.
added in: v5.5.0
emitted each time a request with an http  header is received, where the
value is not . if this event is not listened for, the server will
automatically respond with a  as appropriate.
if a client connection emits an  event, it will be forwarded here.
listener of this event is responsible for closing/destroying the underlying
socket. for example, one may wish to more gracefully close the socket with a
custom http response instead of abruptly severing the connection.
default behavior is to try close the socket with a http '400 bad request',
or a http '431 request header fields too large' in the case of a
error. if the socket is not writable or headers
of the current attached  has been sent, it is
immediately destroyed.
is the  object that the error originated from.
when the  event occurs, there is no  or
object, so any http response sent, including response headers and payload,
must be written directly to the  object. care must be taken to
ensure the response is a properly formatted http response message.
is an instance of  with two extra columns:
: the bytes count of request packet that node.js may have parsed
correctly;
: the raw packet of current request.
in some cases, the client has already received the response and/or the socket
has already been destroyed, like in case of  errors. before
trying to send data to the socket, it is better to check that it is still
writable.
added in: v0.1.4
emitted when the server closes.
arguments for the http request, as it is in
the  event
network socket between the server and client
the first packet of the tunneling stream (may be empty)
emitted each time a client requests an http  method. if this event is
not listened for, then clients requesting a  method will have their
connections closed.
after this event is emitted, the request's socket will not have a
event listener, meaning it will need to be bound in order to handle data
sent to the server on that socket.
this event is emitted when a new tcp stream is established.  is
typically an object of type . usually users will not want to
access this event. in particular, the socket will not emit  events
because of how the protocol parser attaches to the socket. the  can
also be accessed at .
this event can also be explicitly emitted by users to inject connections
into the http server. in that case, any  stream can be passed.
if  is called here, the timeout will be replaced with
when the socket has served a request (if
is non-zero).
when the number of requests on a socket reaches the threshold of
, the server will drop new requests
and emit  event instead, then send  to client.
emitted each time there is a request. there may be multiple requests
per connection (in the case of http keep-alive connections).
the first packet of the upgraded stream (may be empty)
emitted each time a client requests an http upgrade. listening to this event
is optional and clients cannot insist on a protocol change.
stops the server from accepting new connections. see .
closes all connections connected to this server.
closes all connections connected to this server which are not sending a request
or waiting for a response.
added in: v11.3.0, v10.14.0
limit the amount of time the parser will wait to receive the complete http
headers.
if the timeout expires, the server responds with status 408 without
forwarding the request to the request listener and then closes the connection.
it must be set to a non-zero value (e.g. 120 seconds) to protect against
potential denial-of-service attacks in case the server is deployed without a
reverse proxy in front.
starts the http server listening for connections.
this method is identical to  from .
added in: v5.7.0
indicates whether or not the server is listening for connections.
limits maximum incoming headers count. if set to 0, no limit will be applied.
sets the timeout value in milliseconds for receiving the entire request from
the client.
default: 0 (no timeout)
sets the timeout value for sockets, and emits a  event on
the server object, passing the socket as an argument, if a timeout
occurs.
if there is a  event listener on the server object, then it
will be called with the timed-out socket as an argument.
by default, the server does not timeout sockets. however, if a callback
is assigned to the server's  event, timeouts must be handled
explicitly.
requests per socket. default: 0 (no limit)
the maximum number of requests socket can handle
before closing keep alive connection.
a value of  will disable the limit.
when the limit is reached it will set the  header value to ,
but will not actually close the connection, subsequent requests sent
after the limit is reached will get  as a response.
timeout in milliseconds. default: 0 (no timeout)
the number of milliseconds of inactivity before a socket is presumed
to have timed out.
a value of  will disable the timeout behavior on incoming connections.
the socket timeout logic is set up on connection, so changing this
value only affects new connections to the server, not any existing connections.
timeout in milliseconds. default:  (5 seconds).
the number of milliseconds of inactivity a server needs to wait for additional
incoming data, after it has finished writing the last response, before a socket
will be destroyed. if the server receives new data before the keep-alive
timeout has fired, it will reset the regular inactivity timeout, i.e.,
a value of  will disable the keep-alive timeout behavior on incoming
a value of  makes the http server behave similarly to node.js versions prior
to 8.0.0, which did not have a keep-alive timeout.
the socket timeout logic is set up on connection, so changing this value only
affects new connections to the server, not any existing connections.
this object is created internally by an http server, not by the user. it is
passed as the second parameter to the  event.
added in: v0.6.7
indicates that the response is completed, or its underlying connection was
emitted when the response has been sent. more specifically, this event is
emitted when the last segment of the response headers and body have been
handed off to the operating system for transmission over the network. it
does not imply that the client has received anything yet.
this method adds http trailing headers (a header but at the end of the
message) to the response.
trailers will only be emitted if chunked encoding is used for the
response; if it is not (e.g. if the request was http/1.0), they will
be silently discarded.
http requires the  header to be sent in order to
emit trailers, with a list of the header fields in its value. e.g.,
attempting to set a header field name or value that contains invalid characters
will result in a  being thrown.
this method signals to the server that all of the response headers and body
have been sent; that server should consider this message complete.
the method, , must be called on each response.
if  is specified, it is similar in effect to calling
if  is specified, it will be called when the response stream
added in: v0.0.2deprecated since: v13.4.0, v12.16.0
has been called.
flushes the response headers. see also: .
reads out a header that's already been queued but not sent to the client.
the name is case-insensitive. the type of the return value depends
on the arguments provided to .
boolean (read-only). true if headers were sent, false otherwise.
removes a header that's queued for implicit sending.
added in: v15.7.0
a reference to the original http  object.
when true, the date header will be automatically generated and sent in
the response if it is not already present in the headers. defaults to true.
this should only be disabled for testing; http requires the date header
in responses.
returns the response object.
sets a single header value for implicit headers. if this header already exists
in the to-be-sent headers, its value will be replaced. use an array of strings
for network transmission. the same response object is returned to the caller,
to enable call chaining.
when headers have been set with , they will be merged
with any headers passed to , with the headers passed
to  given precedence.
if  method is called and this method has not been
called, it will directly write the supplied header values onto the network
channel without caching internally, and the  on the
header will not yield the expected result. if progressive population of headers
is desired with potential future retrieval and modification, use
sets the socket's timeout value to . if a callback is
provided, then it is added as a listener on the  event on
the response object.
if no  listener is added to the request, the response, or
the server, then sockets are destroyed when they time out. if a handler is
assigned to the request, the response, or the server's  events,
timed out sockets must be handled explicitly.
because of how the protocol parser attaches to the socket. after
, the property is nulled.
when using implicit headers (not calling  explicitly),
this property controls the status code that will be sent to the client when
the headers get flushed.
after response header was sent to the client, this property indicates the
status code which was sent out.
this property controls the status message that will be sent to the client when
the headers get flushed. if this is left as  then the standard
message for the status code will be used.
status message which was sent out.
if this method is called and  has not been called,
it will switch to implicit header mode and flush the implicit headers.
this sends a chunk of the response body. this method may
be called multiple times to provide successive parts of the body.
in the  module, the response body is omitted when the
request is a head request. similarly, the  and  responses
must not include a message body.
can be a string or a buffer. if  is a string,
the second parameter specifies how to encode it into a byte stream.
will be called when this chunk of data is flushed.
this is the raw http body and has nothing to do with higher-level multi-part
body encodings that may be used.
the first time  is called, it will send the buffered
header information and the first chunk of the body to the client. the second
time  is called, node.js assumes data will be streamed,
and sends the new data separately. that is, the response is buffered up to the
first chunk of the body.
sends an http/1.1 100 continue message to the client, indicating that
the request body should be sent. see the  event on
sends an http/1.1 103 early hints message to the client with a link header,
indicating that the user agent can preload/preconnect the linked resources.
the  is an object containing the values of headers to be sent with
early hints message. the optional  argument will be called when
the response message has been written.
sends a response header to the request. the status code is a 3-digit http
status code, like . the last argument, , are the response headers.
optionally one can give a human-readable  as the second
may be an  where the keys and values are in the same list.
it is not a list of tuples. so, the even-numbered offsets are key values,
and the odd-numbered offsets are the associated values. the array is in the same
format as .
this method must only be called once on a message and it must
be called before  is called.
if  or  are called before calling
this, the implicit/mutable headers will be calculated and call this function.
it will directly write the supplied header values onto the network channel
without caching internally, and the  on the header
will not yield the expected result. if progressive population of headers is
desired with potential future retrieval and modification, use
is read in bytes, not characters. use
to determine the length of the body in bytes. node.js
will check whether  and the length of the body which has
been transmitted are equal or not.
will result in a [][] being thrown.
sends a http/1.1 102 processing message to the client, indicating that
the request body should be sent.
an  object is created by  or
and passed as the first argument to the
and  event respectively. it may be used to access response
status, headers, and data.
different from its  value which is a subclass of , the
itself extends  and is created separately to
parse and emit the incoming http headers and payload, as the underlying socket
may be reused multiple times in case of keep-alive.
added in: v0.3.8deprecated since: v17.0.0, v16.12.0
- deprecated. listen for  event instead.
emitted when the request has been aborted.
emitted when the request has been completed.
added in: v10.1.0deprecated since: v17.0.0, v16.12.0
- deprecated. check  from .
the  property will be  if a complete http message has
been received and successfully parsed.
this property is particularly useful as a means of determining if a client or
server fully transmitted a message before a connection was terminated:
added in: v0.1.90deprecated since: v16.0.0
calls  on the socket that received the . if
is provided, an  event is emitted on the socket and  is passed
as an argument to any listeners on the event.
the request/response headers object.
key-value pairs of header names and values. header names are lower-cased.
duplicates in raw headers are handled in the following ways, depending on the
header name:
duplicates of , , , ,
, , , , , ,
, , or  are discarded.
is always an array. duplicates are added to the array.
for duplicate  headers, the values are joined together with .
for all other headers, the values are joined together with .
similar to , but there is no join logic and the values are
always arrays of strings, even for headers received just once.
added in: v0.1.1
in case of server request, the http version sent by the client. in the case of
client response, the http version of the connected-to server.
probably either  or .
also  is the first integer and
is the second.
only valid for request obtained from .
the request method as a string. read only. examples: , .
added in: v0.11.6
the raw request/response headers list exactly as they were received.
the keys and values are in the same list. it is not a
list of tuples. so, the even-numbered offsets are key values, and the
odd-numbered offsets are the associated values.
header names are not lowercased, and duplicates are not merged.
the raw request/response trailer keys and values exactly as they were
received. only populated at the  event.
calls .
the  object associated with the connection.
with https support, use  to obtain the
client's authentication details.
type other than  or internally nulled.
only valid for response obtained from .
the 3-digit http response status code. e.g. .
added in: v0.11.10
the http response status message (reason phrase). e.g.  or .
the request/response trailers object. only populated at the  event.
only populated at the  event.
request url string. this contains only the url that is present in the actual
http request. take the following request:
to parse the url into its parts:
when  is  and  is
this class serves as the parent class of
and . it is an abstract outgoing message from
the perspective of the participants of an http transaction.
emitted when the buffer of the message is free again.
emitted when the transmission is finished successfully.
emitted after  is called.
when the event is emitted, all data has been processed but not necessarily
completely flushed.
adds http trailers (headers but at the end of the message) to the message.
trailers will only be emitted if the message is chunked encoded. if not,
the trailers will be silently discarded.
http requires the  header to be sent to emit trailers,
with a list of header field names in its value, e.g.
header name
|  header value
append a single header value for the header object.
if the value is an array, this is equivalent of calling this method multiple
if there were no previous value for the header, this is equivalent of calling
depending of the value of  when the client request or the
server were created, this will end up in the header being sent multiple times or
a single time with values joined using .
added in: v0.3.0deprecated since: v15.12.0, v14.17.1
optional, an error to emit with  event
destroys the message. once a socket is associated with the message
and is connected, that socket will be destroyed as well.
optional, default:
finishes the outgoing message. if any parts of the body are unsent, it will
flush them to the underlying system. if the message is chunked, it will
send the terminating chunk , and send the trailers (if any).
, followed by
if  is provided, it will be called when the message is finished
(equivalent to a listener of the  event).
flushes the message headers.
for efficiency reason, node.js normally buffers the message headers
until  is called or the first chunk of message data
is written. it then tries to pack the headers and data into a single tcp
packet.
it is usually desired (it saves a tcp round-trip), but not when the first
data is not sent until possibly much later.
bypasses the optimization and kickstarts the message.
name of header
returns  |
gets the value of the http header with the given name. if that header is not
set, the returned value will be .
returns
all names are lowercase.
returns a shallow copy of the current outgoing headers. since a shallow
copy is used, array values may be mutated without additional calls to
various header-related http module methods. the keys of the returned
object are the header names and the values are the respective header
values. all header names are lowercase.
the object returned by the  method does
not prototypically inherit from the javascript . this means that
typical  methods such as , ,
and others are not defined and will not work.
outgoing headers. the header name is case-insensitive.
read-only.  if the headers were sent, otherwise .
overrides the  method inherited from the legacy  class
which is the parent class of .
calling this method will throw an  because  is a
write-only stream.
removes a header that is queued for implicit sending.
header value
sets a single header value. if the header already exists in the to-be-sent
headers, its value will be replaced. use an array of strings to send multiple
headers with the same name.
optional function to be called when a timeout
occurs. same as binding to the  event.
once a socket is associated with the message and is connected,
will be called with  as the first parameter.
reference to the underlying socket. usually, users will not want to access
this property.
after calling , this property will be nulled.
the number of times  has been called.
is  if  has been called. this property does
not indicate whether the data has been flushed. for that purpose, use
is  if all data has been flushed to the underlying system.
the  of the underlying socket if assigned. otherwise, the default
buffer level when  starts returning false ().
the number of buffered bytes.
always .
sends a chunk of the body. this method can be called multiple times.
the  argument is only relevant when  is a string. defaults to
buffer. returns  if all or part of the data was queued in the user
memory. the  event will be emitted when the buffer is free again.
a list of the http methods that are supported by the parser.
added in: v0.1.22
a collection of all the standard http response status codes, and the
short description of each. for example, .
specifies the
class to be used. useful for extending the original .
specifies the  class
to be used. useful for extending the original . default:
: sets the timeout value in milliseconds for receiving
the entire request from the client.
the complete http headers from the client.
: the number of milliseconds of inactivity a server
needs to wait for additional incoming data, after it has finished writing
the last response, before a socket will be destroyed.
: sets the interval value in milliseconds to
check for request and headers timeout in incomplete requests.
use an insecure http parser that accepts
invalid http headers when . using the insecure parser should be
avoided. see  for more information.
optionally overrides the value of
for requests received by this server, i.e.
the maximum length of request headers in bytes.
default: 16384 (16 kib).
if set to , it disables the use of nagle's
algorithm immediately after a new incoming connection is received.
if set to , it enables keep-alive functionality
on the socket immediately after a new incoming connection is received,
similarly on what is done in [][].
if set to a positive number, it sets the
initial delay before the first keepalive probe is sent on an idle socket.
a list of response headers that should be sent only
once. if the header's value is an array, the items will be joined
returns a new instance of .
the  is a function which is automatically
added to the  event.
accepts the same  as
, with the  always set to .
properties that are inherited from the prototype are ignored.
since most requests are get requests without bodies, node.js provides this
convenience method. the only difference between this method and
is that it sets the method to get and calls
automatically. the callback must take care to consume the response
data for reasons stated in  section.
the  is invoked with a single argument that is an instance of
json fetching example:
global instance of  which is used as the default for all http client
requests.
added in: v11.6.0, v10.15.0
read-only property specifying the maximum allowed size of http headers in bytes.
defaults to 16 kib. configurable using the  cli
this can be overridden for servers and client requests by passing the
|  controls  behavior. possible
values:
(default): use  for this host and port.
object: explicitly use the passed in .
: causes a new  with default values to be used.
basic authentication () to compute an
authorization header.
a function that produces a socket/stream to
use for the request when the  option is not used. this can be used to
avoid creating a custom  class just to override the default
function. see  for more
details. any  stream is a valid return value.
default port for the protocol. default:
if an  is used, else .
ip address family to use when resolving  or
. valid values are  or . when unspecified, both ip v4 and
v6 will be used.
an object containing request headers.
optional .
request to. default: .
alias for . to support ,
will be used if both  and  are specified.
local interface to bind for network connections.
local port to connect from.
custom lookup function. default: .
(the maximum length of response headers in
bytes) for responses received from the server.
a string specifying the http request method. default:
request path. should include query string if any.
e.g. . an exception is thrown when the request path
contains illegal characters. currently, only spaces are rejected but that
may change in the future. default: .
port of remote server. default:  if set,
else .
protocol to use. default: .
: specifies whether or not to automatically add the
header. defaults to .
: an abortsignal that may be used to abort an ongoing
unix domain socket. cannot be used if one of
or  is specified, as those specify a tcp socket.
: a number specifying the socket timeout in milliseconds.
this will set the timeout before the socket is connected.
a list of request headers that should be sent
only once. if the header's value is an array, the items will be joined
node.js maintains several connections per server to make http requests.
this function allows one to transparently issue requests.
can be a string or a  object. if  is a
string, it is automatically parsed with . if it is a
object, it will be automatically converted to an ordinary  object.
if both  and  are specified, the objects are merged, with the
properties taking precedence.
the optional  parameter will be added as a one-time listener for
returns an instance of the
class. the  instance is a writable stream. if one needs to
upload a file with a post request, then write to the  object.
in the example  was called. with  one
must always call  to signify the end of the request -
even if there is no data being written to the request body.
if any error is encountered during the request (be that with dns resolution,
tcp level errors, or actual http parse errors) an  event is emitted
on the returned request object. as with all  events, if no listeners
are registered the error will be thrown.
there are a few special headers that should be noted.
sending a 'connection: keep-alive' will notify node.js that the connection to
the server should be persisted until the next request.
sending a 'content-length' header will disable the default chunked encoding.
sending an 'expect' header will immediately send the request headers.
usually, when sending 'expect: 100-continue', both a timeout and a listener
for the  event should be set. see rfc 2616 section 8.2.3 for more
information.
sending an authorization header will override using the  option
to compute basic authentication.
example using a  as :
in a successful request, the following events will be emitted in the following
order:
any number of times, on the  object
( will not be emitted at all if the response body is empty, for
instance, in most redirects)
on the  object
in the case of a connection error, the following events will be emitted:
in the case of a premature connection close before the response is received,
the following events will be emitted in the following order:
with an error with message  and code
in the case of a premature connection close after the response is received,
(connection closed here)
on the  object with an error with message
and code .
if  is called before a socket is assigned, the following
events will be emitted in the following order:
( called here)
if  is called before the connection succeeds, the following
if  is called after the response is received, the following
setting the  option or using the  function will
not abort the request or do anything besides add a  event.
passing an  and then calling  on the corresponding
will behave the same way as calling  on the
request itself.
performs the low-level validations on the provided  that are done when
is called.
passing illegal value as  will result in a  being thrown,
identified by .
it is not necessary to use this method before passing headers to an http request
or response. the http module will automatically validate such headers.
passing illegal value as  will result in a  being thrown.
undefined value error is identified by .
invalid value character error is identified by .
set the maximum number of idle http parsers. default: .
http/2
the  module provides an implementation of the  protocol.
core api
the core api provides a low-level interface designed specifically around
support for http/2 protocol features. it is specifically not designed for
compatibility with the existing  module api. however,
the  is.
the  core api is much more symmetric between client and server than the
api. for instance, most events, like ,  and
, can be emitted either by client-side code or server-side code.
server-side example
the following illustrates a simple http/2 server using the core api.
since there are no browsers known that support
, the use of
is necessary when communicating
with browser clients.
to generate the certificate and key for this example, run:
client-side example
the following illustrates an http/2 client:
instances of the  class represent an active communications
session between an http/2 client and server. instances of this class are not
intended to be constructed directly by user code.
each  instance will exhibit slightly different behaviors
depending on whether it is operating as a server or a client. the
property can be used to determine the mode in which an
is operating. on the server side, user code should rarely
have occasion to work with the  object directly, with most
actions typically taken through interactions with either the  or
user code will not create  instances directly. server-side
instances are created by the  instance when a
new http/2 connection is received. client-side  instances are
created using the  method.
and sockets
every  instance is associated with exactly one  or
when it is created. when either the  or the
are destroyed, both will be destroyed.
because of the specific serialization and processing requirements imposed
by the http/2 protocol, it is not recommended for user code to read data from
or write data to a  instance bound to a . doing so can
put the http/2 session into an indeterminate state causing the session and
the socket to become unusable.
once a  has been bound to an , user code should rely
solely on the api of the .
the  event is emitted once the  has been destroyed. its
listener does not expect any arguments.
the  event is emitted once the  has been successfully
connected to the remote peer and communication may begin.
user code will typically not listen for this event directly.
the  event is emitted when an error occurs during the processing of
an .
the frame type.
the error code.
the stream id (or  if the frame isn't associated with a
stream).
the  event is emitted when an error occurs while attempting to
send a frame on the session. if the frame that could not be sent is associated
with a specific , an attempt to emit a  event on the
is made.
if the  event is associated with a stream, the stream will be
closed and destroyed immediately following the  event. if the
event is not associated with a stream, the  will be shut down
immediately following the  event.
the http/2 error code specified in the  frame.
the id of the last stream the remote peer successfully
processed (or  if no id is specified).
if additional opaque data was included in the
frame, a  instance will be passed containing that data.
the  event is emitted when a  frame is received.
the  instance will be shut down automatically when the
event is emitted.
a copy of the  frame received.
the  event is emitted when an acknowledgment  frame
has been received.
when using  to submit new settings, the modified
settings do not take effect until the  event is emitted.
the  frame 8-byte payload
the  event is emitted whenever a  frame is received from the
connected peer.
the  event is emitted when a new  frame is received
from the connected peer.
a reference to the stream
an object describing the headers
the associated numeric flags
an array containing the raw header names followed by
their respective values.
the  event is emitted when a new  is created.
on the server side, user code will typically not listen for this event directly,
and would instead register a handler for the  event emitted by the
or  instances returned by  and
, respectively, as in the example below:
even though http/2 streams and network sockets are not in a 1:1 correspondence,
a network error will destroy each individual stream and must be handled on the
stream level, as shown above.
after the  method is used to set the timeout period
for this , the  event is emitted if there is no
activity on the  after the configured number of milliseconds.
its listener does not expect any arguments.
value will be  if the  is not yet connected to a
socket,  if the  is not connected to a , or
will return the value of the connected 's own
gracefully closes the , allowing any existing streams to
complete on their own and preventing new  instances from being
created. once closed,  might be called if there
are no open  instances.
if specified, the  function is registered as a handler for the
will be  if this  instance has been closed, otherwise
will be  if this  instance is still connecting, will be set
to  before emitting  event and/or calling the
an  object if the  is being destroyed
due to an error.
the http/2 error code to send in the final  frame.
if unspecified, and  is not undefined, the default is ,
otherwise defaults to .
immediately terminates the  and the associated  or
once destroyed, the  will emit the  event. if
is not undefined, an  event will be emitted immediately before the
if there are any remaining open  associated with the
, those will also be destroyed.
will be  if this  instance has been destroyed and must no
longer be used, otherwise .
value is  if the  session socket has not yet been
connected,  if the  is connected with a ,
and  if the  is connected to any other kind of socket
or stream.
an http/2 error code
the numeric id of the last processed
|  |  a  or
instance containing additional data to be carried within the  frame.
transmits a  frame to the connected peer without shutting down the
a prototype-less object describing the current local settings of this
. the local settings are local to this  instance.
if the  is connected to a , the  property
will return an  of origins for which the  may be
considered authoritative.
the  property is only available when using a secure tls connection.
indicates whether the  is currently waiting for acknowledgment of
a sent  frame. will be  after calling the
method. will be  once all sent
frames have been acknowledged.
|  |  optional ping payload.
sends a  frame to the connected http/2 peer. a  function must
be provided. the method will return  if the  was sent,
the maximum number of outstanding (unacknowledged) pings is determined by the
configuration option. the default maximum is 10.
if provided, the  must be a , , or
containing 8 bytes of data that will be transmitted with the  and
returned with the ping acknowledgment.
the callback will be invoked with three arguments: an error argument that will
be  if the  was successfully acknowledged, a  argument
that reports the number of milliseconds elapsed since the ping was sent and the
acknowledgment was received, and a  containing the 8-byte
payload.
if the  argument is not specified, the default payload will be the
64-bit timestamp (little endian) marking the start of the  duration.
calls  on this
instance's underlying .
a prototype-less object describing the current remote settings of this
. the remote settings are set by the connected http/2 peer.
added in: v15.3.0, v14.18.0
sets the local endpoint's window size.
the  is the total window size to set, not
the delta.
used to set a callback function that is called when there is no activity on
the  after  milliseconds. the given  is
registered as a listener on the  event.
returns a  object that acts as a  (or ) but
limits available methods to ones safe to use with http/2.
, , , , , , and  will throw
an error with code . see
method will be called on this .
all other interactions will be routed directly to the socket.
provides miscellaneous information about the current state of the
the current local (receive)
flow control window size for the .
the current number of bytes
that have been received since the last flow control .
the numeric identifier to be used the
next time a new  is created by this .
the number of bytes that the remote peer can
send without receiving a .
the numeric id of the
for which a  or  frame was most recently received.
the number of bytes that this
may send without receiving a .
the number of frames currently within the
outbound queue for this .
the current size in bytes of the
outbound header compression state table.
inbound header compression state table.
an object describing the current status of this .
callback that is called once the session is connected or
right away if the session is already connected.
the updated  object.
updates the current local settings for this  and sends a new
frame to the connected http/2 peer.
once called, the  property will be
while the session is waiting for the remote peer to acknowledge the new
settings.
the new settings will not become effective until the  acknowledgment
is received and the  event is emitted. it is possible to send
multiple  frames while acknowledgment is still pending.
the  will be equal to
if this  instance is a
server, and  if the instance is a
a description of the alternative service configuration as
defined by .
|  |  |  either a url string specifying
the origin (or an  with an  property) or the numeric
identifier of an active  as given by the
submits an  frame (as defined by ) to the connected client.
sending an  frame with a specific stream id indicates that the alternate
service is associated with the origin of the given .
the  and origin string must contain only ascii bytes and are
strictly interpreted as a sequence of ascii bytes. the special value
may be passed to clear any previously set alternative service for a given
domain.
when a string is passed for the  argument, it will be parsed as
a url and the origin will be derived. for instance, the origin for the
http url  is the ascii string
. an error will be thrown if either the given string
cannot be parsed as a url or if a valid origin cannot be derived.
a  object, or any object with an  property, may be passed as
, in which case the value of the  property will be
used. the value of the  property must be a properly serialized
ascii origin.
specifying alternative services
the format of the  parameter is strictly defined by  as an
ascii string containing a comma-delimited list of "alternative" protocols
associated with a specific host and port.
for example, the value  indicates that the http/2
protocol is available on the host  on tcp/ip port 81. the
host and port must be contained within the quote () characters.
multiple alternatives may be specified, for instance: .
the protocol identifier ( in the examples) may be any valid
the syntax of these values is not validated by the node.js implementation and
are passed through as provided by the user or received from the peer.
|  |  one or more url strings passed as
separate arguments.
submits an  frame (as defined by ) to the connected client
to advertise the set of origins for which the server is capable of providing
authoritative responses.
when a string is passed as an , it will be parsed as a url and the
origin will be derived. for instance, the origin for the http url
is the ascii string
an , in which case the value of the  property will be
alternatively, the  option may be used when creating a new http/2
server using the  method:
the  event is emitted whenever an  frame is received by
the client. the event is emitted with the  value, origin, and stream
id. if no  is provided in the  frame,  will
be an empty string.
the client. the event is emitted with an array of  strings. the
will be updated to include the received
origins.
the  event is only emitted when using a secure tls connection.
if the  writable side should
be closed initially, such as when sending a  request that should not
expect a payload body.
when  and  identifies a parent stream,
the created stream is made the sole direct dependency of the parent, with
all other existing dependents made a dependent of the newly created stream.
specifies the numeric identifier of a stream the newly
created stream is dependent on.
specifies the relative dependency of a stream in relation
to other streams with the same . the value is a number between
and  (inclusive).
when , the  will emit the
event after the final  frame has been sent.
an abortsignal that may be used to abort an ongoing
for http/2 client  instances only, the
creates and returns an  instance that can be used to send an
http/2 request to the connected server.
when a  is first created, the socket may not yet be
connected. if  is called during this time, the
actual request will be deferred until the socket is ready to go.
if the  is closed before the actual request be executed, an
this method is only available if  is equal to
when the  option is set, the  event
is emitted immediately after queuing the last chunk of payload data to be sent.
the  method can then be called to send trailing
headers to the peer.
when  is set, the  will not automatically
close when the final  frame is transmitted. user code must call either
or  to close the
when  is set with an  and then  on the
corresponding  is called, the request will emit an
event with an  error.
the  and  pseudo-headers are not specified within ,
they respectively default to:
each instance of the  class represents a bidirectional http/2
communications stream over an  instance. any single
may have up to 231-1  instances over its lifetime.
user code will not construct  instances directly. rather, these
are created, managed, and provided to user code through the
instance. on the server,  instances are created either in response
to an incoming http request (and handed off to user code via the
event), or in response to a call to the  method.
on the client,  instances are created and returned when either the
method is called, or in response to an incoming
the  class is a base for the  and
classes, each of which is used specifically by either
the server or client side, respectively.
all  instances are  streams. the  side of the
is used to send data to the connected peer, while the  side
is used to receive data sent by the connected peer.
the default text character encoding for an  is utf-8. when using an
to send text, use the  header to set the character
lifecycle
creation
on the server side, instances of  are created either
when:
a new http/2  frame with a previously unused stream id is received;
the  method is called.
on the client side, instances of  are created when the
on the client, the  instance returned by
may not be immediately ready for use if the parent  has not yet
been fully established. in such cases, operations called on the
will be buffered until the  event is emitted. user code should rarely,
if ever, need to handle the  event directly. the ready status of an
can be determined by checking the value of . if
the value is , the stream is not yet ready for use.
destruction
all  instances are destroyed either when:
an  frame for the stream is received by the connected peer,
and (for client streams only) pending data has been read.
the  method is called, and (for client streams only)
pending data has been read.
the  or  methods are called.
when an  instance is destroyed, an attempt will be made to send an
frame to the connected peer.
when the  instance is destroyed, the  event will
be emitted. because  is an instance of , the
event will also be emitted if the stream data is currently flowing.
the  event may also be emitted if  was called
with an  passed as the first argument.
after the  has been destroyed, the
property will be  and the  property will specify the
error code. the  instance is no longer usable once
the  event is emitted whenever a  instance is
abnormally aborted in mid-communication.
the  event will only be emitted if the  writable side
has not been ended.
the  event is emitted when the  is destroyed. once
this event is emitted, the  instance is no longer usable.
the http/2 error code used when closing the stream can be retrieved using
the  property. if the code is any value other than
(), an  event will have also been emitted.
send a frame. when invoked, the handler function will receive an integer
argument identifying the frame type, and an integer argument identifying the
error code. the  instance will be destroyed immediately after the
the  event is emitted when the  has been opened, has
been assigned an , and can be used. the listener does not expect any
arguments.
the  event is emitted after no activity is received for this
within the number of milliseconds set using
the  event is emitted when a block of headers associated with
trailing header fields is received. the listener callback is passed the
and flags associated with the headers.
this event might not be emitted if  is called
before trailers are received and the incoming data is not being read or
listened for.
the  event is emitted when the  has queued the
final  frame to be sent on a frame and the  is ready to send
trailing headers. when initiating a request or response, the
option must be set for this event to be emitted.
set to  if the  instance was aborted abnormally. when set,
the  event will have been emitted.
this property shows the number of characters currently buffered to be written.
see  for details.
unsigned 32-bit integer identifying the error code.
default:  ().
an optional function registered to listen for the
closes the  instance by sending an  frame to the
connected http/2 peer.
set to  if the  instance has been closed.
set to  if the  instance has been destroyed and is no longer
usable.
added in: v10.11.0
set to  if the  flag was set in the request or response
headers frame received, indicating that no additional data should be received
and the readable side of the  will be closed.
the numeric stream identifier of this  instance. set to
if the stream identifier has not yet been assigned.
set to  if the  instance has not yet been assigned a
numeric stream identifier.
this stream is made the sole direct dependency of the parent, with
all other existing dependents made a dependent of this stream. default:
specifies the numeric identifier of a stream this stream
is dependent on.
when , changes the priority locally without
sending a  frame to the connected peer.
updates the priority for this  instance.
set to the   reported when the  is
destroyed after either receiving an  frame from the connected peer,
calling , or . will be
if the  has not been closed.
added in: v9.5.0
an object containing the outbound headers sent for this .
an array of objects containing the outbound informational (additional) headers
sent for this .
an object containing the outbound trailers sent for this .
a reference to the  instance that owns this . the
value will be  after the  instance is destroyed.
the number of bytes the connected peer may send
for this  without receiving a .
a flag indicating the low-level current state of the
as determined by .
if this  has been closed locally.
if this  has been closed
remotely.
the sum weight of all
instances that depend on this  as specified using
frames.
the priority weight of this .
a current state of this .
sends a trailing  frame to the connected http/2 peer. this method
will cause the  to be immediately closed and must only be
called after the  event has been emitted. when sending a
request or sending a response, the  option must be set
in order to keep the  open after the final  frame so that
trailers can be sent.
the http/1 specification forbids trailers from containing http/2 pseudo-header
fields (e.g. , , etc).
the  class is an extension of  that is
used exclusively on http/2 clients.  instances on the client
provide events such as  and  that are only relevant on
emitted when the server sends a  status, usually because
the request contained . this is an instruction that
the  event is emitted when an additional block of headers is received
for a stream, such as when a block of  informational headers is received.
the listener callback is passed the  and flags
associated with the headers.
the  event is emitted when response headers for a server push stream
are received. the listener callback is passed the  and
flags associated with the headers.
the  event is emitted when a response  frame has been
received for this stream from the connected http/2 server. the listener is
invoked with two arguments: an  containing the received
, and flags associated with the headers.
used exclusively on http/2 servers.  instances on the server
provide additional methods such as  and
that are only relevant on the server.
sends an additional informational  frame to the connected http/2 peer.
true if headers were sent, false otherwise (read-only).
read-only property mapped to the  flag of the remote
client's most recent  frame. will be  if the remote peer
accepts push streams,  otherwise. settings are the same for every
in the same .
callback that is called once the push stream has been
initiated.
the returned  object.
headers object the  was
initiated with.
initiates a push stream. the callback is invoked with the new
instance created for the push stream passed as the second argument, or an
passed as the first argument.
setting the weight of a push stream is not allowed in the  frame. pass
a  value to  with the  option set to
to enable server-side bandwidth balancing between concurrent streams.
calling  from within a pushed stream is not permitted
and will throw an error.
set to  to indicate that the response will not
include payload data.
initiates a response. when the  option is set, the
event will be emitted immediately after queuing the last chunk
of payload data to be sent. the  method can then be
used to sent trailing header fields to the peer.
|  a readable file descriptor.
the offset position at which to begin reading.
the amount of data from the fd to send.
initiates a response whose data is read from the given file descriptor. no
validation is performed on the given file descriptor. if an error occurs while
attempting to read data using the file descriptor, the  will be
closed using an  frame using the standard  code.
when used, the  object's  interface will be closed
the optional  function may be specified to give user code
an opportunity to set additional content headers based on the  details
of the given fd. if the  function is provided, the
method will perform an  call to
collect details on the provided file descriptor.
the  and  options may be used to limit the response to a
specific range subset. this can be used, for instance, to support http range
the file descriptor or  is not closed when the stream is closed,
so it will need to be closed manually once it is no longer needed.
using the same file descriptor concurrently for multiple streams
is not supported and may result in data loss. re-using a file descriptor
after a stream has finished is supported.
will be emitted immediately after queuing the last chunk of payload data to be
sent. the  method can then be used to sent trailing
header fields to the peer.
callback function invoked in the case of an
error before send.
sends a regular file as the response. the  must specify a regular file
or an  event will be emitted on the  object.
of the given file:
if an error occurs while attempting to read the file data, the
will be closed using an  frame using the standard
code. if the  callback is defined, then it will be called. otherwise
the stream will be destroyed.
example using a file path:
the  function may also be used to cancel the send operation
by returning . for instance, a conditional request may check the stat
results to determine if the file has been modified to return an appropriate
response:
the  header field will be automatically set.
the  function may also be used to handle all the errors
that could happen before the delivery of the file is initiated. the
default behavior is to destroy the stream.
instances of  are created using the
function. the  class is not exported directly by the
if a  listener is registered or  is
supplied a callback function, the  event is emitted each time
a request with an http  is received. if this event is
not listened for, the server will automatically respond with a status
as appropriate.
access this event.
per session. see the .
the  event is emitted when a new  is created by the
the  event is emitted when an  event is emitted by
an  object associated with the .
the  event is emitted when a  event has been emitted by
an  associated with the server.
see also .
the  event is emitted when there is no activity on the server for
a given number of milliseconds set using .
stops the server from establishing new sessions. this does not prevent new
request streams from being created due to the persistent nature of http/2
sessions. to gracefully shut down the server, call  on
all active sessions.
if  is provided, it is not invoked until all active sessions have been
closed, although the server has already stopped allowing new sessions. see
used to set the timeout value for http2 server requests,
and sets a callback function that is called when there is no activity
on the  after  milliseconds.
the given callback is registered as a listener on the  event.
in case if  is not a function, a new
error will be thrown.
used to update the server with the provided settings.
throws  for invalid  values.
throws  for invalid  argument.
function. the  class is not
exported directly by the  module.
if a  listener is registered or
is supplied a callback function, the  event is emitted each
time a request with an http  is received. if this event
is not listened for, the server will automatically respond with a status
this event is emitted when a new tcp stream is established, before the tls
handshake begins.  is typically an object of type .
usually users will not want to access this event.
default: 2 minutes.
the  event is emitted when a connecting client fails to
negotiate an allowed protocol (i.e. http/2 or http/1.1). the event handler
receives the socket for handling. if no listener is registered for this event,
the connection is terminated. a timeout may be specified using the
option passed to .
default:  (2 minutes)
used to set the timeout value for http2 secure server requests,
sets the maximum dynamic table size
for deflating header fields. default: .
sets the maximum number of settings entries per
frame. the minimum value allowed is . default: .
sets the maximum memory that the
is permitted to use. the value is expressed in terms of number of megabytes,
e.g.  equal 1 megabyte. the minimum value allowed is .
this is a credit based limit, existing s may cause this
limit to be exceeded, but new  instances will be rejected
while this limit is exceeded. the current number of  sessions,
the current memory use of the header compression tables, current data
queued to be sent, and unacknowledged  and  frames are all
counted towards the current limit. default: .
sets the maximum number of header entries.
this is similar to  or
in the  module. the minimum value
is . default: .
sets the maximum number of outstanding,
unacknowledged pings. default: .
sets the maximum allowed size for a
serialized, compressed block of headers. attempts to send headers that
exceed this limit will result in a  event being emitted
and the stream being closed and destroyed.
while this sets the maximum allowed size to the entire block of headers,
(the internal http2 library) has a limit of
for each decompressed key/value pair.
the strategy used for determining the amount of
padding to use for  and  frames. default:
. value may be one of:
: no padding is applied.
: the maximum amount of padding,
determined by the internal implementation, is applied.
: attempts to apply enough
padding to ensure that the total frame length, including the 9-byte
header, is a multiple of 8. for each frame, there is a maximum allowed
number of padding bytes that is determined by current flow control state
and settings. if this maximum is less than the calculated amount needed to
ensure alignment, the maximum is used and the total frame length is not
necessarily aligned at 8 bytes.
sets the maximum number of concurrent
streams for the remote peer as if a  frame had been received. will
be overridden if the remote peer sets its own value for
sets the maximum number of invalid
frames that will be tolerated before the session is closed.
sets the maximum number of rejected
upon creation streams that will be tolerated before the session is closed.
each rejection is associated with an
error that should tell the peer to not open any more streams, continuing
to open streams is therefore regarded as a sign of a misbehaving peer.
the initial settings to send to the
remote peer upon connection.
class to used for http/1 fallback. useful for extending
the original . default: .
class to used for http/1 fallback. useful for extending the original
class to use.
useful for extending the original .
specifies a timeout in milliseconds that
a server should wait when an  is emitted. if the
socket has not been destroyed by that time the server will destroy it.
...: any  option can be provided.
returns a  instance that creates and manages
incoming client connections that do not support
http/2 will be downgraded to http/1.x when set to .
see the  event. see .
e.g.  equal 1 megabyte. the minimum value allowed is . this is a
credit based limit, existing s may cause this
strategy used for determining the amount of
padding to ensure that the total frame length, including the
9-byte header, is a multiple of 8. for each frame, there is a maximum
allowed number of padding bytes that is determined by current flow control
state and settings. if this maximum is less than the calculated amount
needed to ensure alignment, the maximum is used and the total frame length
is not necessarily aligned at 8 bytes.
...: any  options can be provided. for
servers, the identity options ( or /) are usually required.
an array of origin strings to send within an
frame immediately following creation of a new server .
a server should wait when an  event is emitted. if
the socket has not been destroyed by that time the server will destroy it.
|  the remote http/2 server to connect to. this must
be in the form of a minimal, valid url with the  or
prefix, host name, and ip port (if a non-default port is used). userinfo
(user id and password), path, querystring, and fragment details in the
url will be ignored.
sets the maximum number of reserved push
streams the client will accept at any given time. once the current number of
currently reserved push streams exceeds reaches this limit, new push streams
sent by the server will be automatically rejected. the minimum allowed value
is 0. the maximum allowed value is 232-1. a negative value sets
this option to the maximum allowed value. default: .
the protocol to connect with, if not set in the
. value may be either  or . default:
an optional callback that receives the
instance passed to  and the  object, and returns any
stream that is to be used as the connection for this session.
...: any  or  options can be provided.
will be registered as a one-time listener of the
returns a  instance.
error codes for  and
returns an object containing the default settings for an
instance. this method returns a new object instance every time it is called
so instances returned may be safely modified for use.
returns a  instance containing serialized representation of the given
http/2 settings as specified in the  specification. this is intended
for use with the  header field.
|  the packed settings.
returns a  containing the deserialized settings from
the given  as generated by .
this symbol can be set as a property on the http/2 headers object with an array
value in order to provide a list of headers considered sensitive.
headers object
headers are represented as own-properties on javascript objects. the property
keys will be serialized to lower-case. property values should be strings (if
they are not they will be coerced to strings) or an  of strings (in order
to send more than one value per header field).
header objects passed to callback functions will have a  prototype. this
means that normal javascript object methods such as
and  will
not work.
for incoming headers:
the  header is converted to .
duplicates of , , , , ,
, , , ,
, , , , , , ,
, , ,, ,
,  or  are
discarded.
for duplicate  headers, the values are joined together with '; '.
for all other headers, the values are joined together with ', '.
sensitive headers
http2 headers can be marked as sensitive, which means that the http/2
header compression algorithm will never index them. this can make sense for
header values with low entropy and that may be considered valuable to an
attacker, for example  or . to achieve this, add
the header name to the  property as an array:
for some headers, such as  and short  headers,
this flag is set automatically.
this property is also set for received headers. it will contain the names of
all headers marked as sensitive, including ones marked that way automatically.
settings object
the , ,
apis either return or receive as input an
object that defines configuration settings for an  object.
these objects are ordinary javascript objects containing the following
specifies the maximum number of bytes used for
header compression. the minimum allowed value is 0. the maximum allowed value
is 232-1. default: .
specifies  if http/2 push streams are to be
permitted on the  instances. default: .
specifies the sender's initial window size in
bytes for stream-level flow control. the minimum allowed value is 0. the
maximum allowed value is 232-1. default: .
specifies the size in bytes of the largest frame
payload. the minimum allowed value is 16,384. the maximum allowed value is
224-1. default: .
specifies the maximum number of concurrent
streams permitted on an . there is no default value which
implies, at least theoretically, 232-1 streams may be open
concurrently at any given time in an . the minimum value
is 0. the maximum allowed value is 232-1. default:
specifies the maximum size (uncompressed octets)
of header list that will be accepted. the minimum allowed value is 0. the
specifies  if the "extended connect
protocol" defined by  is to be enabled. this setting is only
meaningful if sent by the server. once the  setting
has been enabled for a given , it cannot be disabled.
all additional properties on the settings object are ignored.
there are several types of error conditions that may arise when using the
validation errors occur when an incorrect argument, option, or setting value is
passed in. these will always be reported by a synchronous .
state errors occur when an action is attempted at an incorrect time (for
instance, attempting to send data on a stream after it has closed). these will
be reported using either a synchronous  or via an  event on
the ,  or http/2 server objects, depending on where
and when the error occurs.
internal errors occur when an http/2 session fails unexpectedly. these will be
reported via an  event on the  or http/2 server objects.
protocol errors occur when various http/2 protocol constraints are violated.
these will be reported using either a synchronous  or via an
event on the ,  or http/2 server objects, depending
on where and when the error occurs.
invalid character handling in header names and values
the http/2 implementation applies stricter handling of invalid characters in
http header names and values than the http/1 implementation.
header field names are case-insensitive and are transmitted over the wire
strictly as lower-case strings. the api provided by node.js allows header
names to be set as mixed-case strings (e.g. ) but will convert
those to lower-case (e.g. ) upon transmission.
header field-names must only contain one or more of the following ascii
characters: -, -, -, , , , , , , , ,
, , , ,  (backtick), , and .
using invalid characters within an http header field name will cause the
stream to be closed with a protocol error being reported.
header field values are handled with more leniency but should not contain
new-line or carriage return characters and should be limited to us-ascii
characters, per the requirements of the http specification.
push streams on the client
to receive pushed streams on the client, set a listener for the
event on the :
supporting the  method
the  method is used to allow an http/2 server to be used as a proxy
for tcp/ip connections.
a simple tcp server:
an http/2 connect proxy:
an http/2 connect client:
the extended  protocol
defines an "extended connect protocol" extension to http/2 that
may be used to bootstrap the use of an  using the
method as a tunnel for other communication protocols (such as websockets).
the use of the extended connect protocol is enabled by http/2 servers by using
the  setting:
once the client receives the  frame from the server indicating that
the extended connect may be used, it may send  requests that use the
http/2 pseudo-header:
compatibility api
the compatibility api has the goal of providing a similar developer experience
of http/1 when using http/2, making it possible to develop applications
that support both  and http/2. this api targets only the
public api of the . however many modules use internal
methods or state, and those are not supported as it is a completely
different implementation.
the following example creates an http/2 server using the compatibility
api:
in order to create a mixed  and http/2 server, refer to the
upgrading from non-tls http/1 servers is not supported.
the http/2 compatibility api is composed of  and
. they aim at api compatibility with http/1, but
they do not hide the differences between the protocols. as an example,
the status message for http codes is ignored.
alpn negotiation
alpn negotiation allows supporting both  and http/2 over
the same socket. the  and  objects can be either http/1 or
http/2, and an application must restrict itself to the public api of
, and detect if it is possible to use the more advanced
features of http/2.
the following example creates a server that supports both protocols:
the  event works identically on both  and
http/2.
a  object is created by  or
event. it may be used to access a request status, headers, and
data.
the  event will only be emitted if the  writable
side has not been ended.
indicates that the underlying  was closed.
just like , this event occurs only once per response.
added in: v10.1.0
the request authority pseudo header field. because http/2 allows requests
to set either  or , this value is derived from
if present. otherwise, it is derived from
been completed, aborted, or destroyed.
added in: v8.4.0deprecated since: v13.0.0
calls  on the  that received
the . if  is provided, an  event
is emitted and  is passed as an argument to any listeners on the event.
it does nothing if the stream was already destroyed.
in http/2, the request path, host name, protocol, and method are represented as
special headers prefixed with the  character (e.g. ). these special
headers will be included in the  object. care must be taken not
to inadvertently modify these special headers or errors may occur. for instance,
removing all headers from the request will cause errors to occur:
client response, the http version of the connected-to server. returns
the request method as a string. read-only. examples: , .
the request scheme pseudo header field indicating the scheme
portion of the target url.
sets the 's timeout value to . if a callback is
the server, then s are destroyed when they time out. if a
handler is assigned to the request, the response, or the server's
events, timed out sockets must be handled explicitly.
applies getters, setters, and methods based on http/2 logic.
, , and  properties will be retrieved from and
set on .
, , ,  and  methods will be called on
method will be called on .
, , , and  will throw an error with code
. see  for
all other interactions will be routed directly to the socket. with tls support,
use  to obtain the client's
authentication details.
the  object backing the request.
http request. if the request is:
then  will be:
to parse the url into its parts,  can be used:
indicates that the underlying  was terminated before
was called or able to flush.
handed off to the http/2 multiplexing for transmission over the network. it
after this event, no more events will be emitted on the response object.
called once  is finished,
or either when the attempt to create the pushed  has failed or
has been rejected, or the state of  is closed prior to
calling the  method
the newly-created
call  with the given headers, and wrap the
given  on a newly created  as the callback
parameter if successful. when  is closed, the callback is
called with an error .
added in: v8.4.0deprecated since: v13.4.0, v12.16.0
boolean value that indicates whether the response has completed. starts
as . after  executes, the value will be .
reads out a header that has already been queued but not sent to the client.
the name is case-insensitive.
removes a header that has been queued for implicit sending.
a reference to the original http2  object.
here to send multiple headers with the same name.
status message is not supported by http/2 (rfc 7540 8.1.2.4). it returns
an empty string.
the  object backing the response.
by default the  is .  will be called when this chunk
of data is flushed.
sends a status  to the client, indicating that the request body
should be sent. see the  event on  and
sends a status  to the client with a link header,
the  can be a string or an array of strings containing the values
of the  header.
for compatibility with , a human-readable  may be
passed as the second argument. however, because the  has no
meaning within http/2, the argument will have no effect and a process warning
will be emitted.
is given in bytes not characters. the
api may be used to determine the number of bytes in a
given encoding. on outbound messages, node.js does not check if content-length
and the length of the body being transmitted are equal or not. however, when
receiving messages, node.js will automatically reject messages when the
does not match the actual payload size.
this method may be called at most one time on a message before
collecting http/2 performance metrics
the  api can be used to collect basic performance
metrics for each  and  instance.
the  property of the  will be equal to .
the  property of the  will be equal to either
if  is equal to , the  will contain the
following additional properties:
the number of  frame bytes received for this
the number of  frame bytes sent for this
the identifier of the associated
the number of milliseconds elapsed between the
and the reception of the first  frame.
the number of milliseconds elapsed between
the   and sending of the first  frame.
and the reception of the first header.
the number of bytes received for this .
the number of bytes sent for this .
the number of http/2 frames received by the
the number of http/2 frames sent by the .
the maximum number of streams concurrently
open during the lifetime of the .
the number of milliseconds elapsed since the transmission
of a  frame and the reception of its acknowledgment. only present if
a  frame has been sent on the .
the average duration (in milliseconds) for
all  instances.
the number of  instances processed by
either  or  to identify the type of
note on  and
http/2 requires requests to have either the  pseudo-header
or the  header. prefer  when constructing an http/2
request directly, and  when converting from http/1 (in proxies,
for instance).
the compatibility api falls back to  if  is not
present. see  for more information. however,
if you don't use the compatibility api (or use  directly),
you need to implement any fall-back behavior yourself.
https
https is the http protocol over tls/ssl. in node.js this is implemented as a
separate module.
an  object for https similar to . see
can have the same fields as for , and
maximum number of tls cached sessions.
use  to disable tls session caching. default: .
the value of
to be sent to the server. use
empty string  to disable sending the extension.
default: host name of the target server, unless the target server
is specified using an ip address, in which case the default is  (no
extension).
see  for information about tls session reuse.
line of ascii text, in nss  format.
the  instance on which it was
generated.
the  event is emitted when key material is generated or received by a
connection managed by this agent (typically before handshake has completed, but
not necessarily). this keying material can be stored for debugging, as it
allows captured tls traffic to be decrypted. it may be emitted multiple times
for each socket.
a typical use case is to append received lines to a common text file, which is
later used by software (such as wireshark) to decrypt the traffic:
see  in the  module.
added in: v11.3.0
starts the https server listening for encrypted connections.
added in: v14.11.0
default:  (5 seconds)
accepts  from ,
a listener to be added to the  event.
|  |  accepts the same  as
like  but for https.
can be an object, a string, or a  object. if  is a
global instance of  for all https client requests.
|  |  accepts all  from
, with some differences in default values:
makes a request to a secure web server.
the following additional  from  are also accepted:
example using options from :
alternatively, opt out of connection pooling by not using an .
example pinning on certificate fingerprint, or the public key (similar to
outputs for example:
inspector
the  module provides an api for interacting with the v8
inspector.
deactivate the inspector. blocks until there are no active connections.
an object to send messages to the remote inspector console.
the inspector console does not have api parity with node.js
console.
port to listen on for inspector connections. optional.
default: what was specified on the cli.
host to listen on for inspector connections. optional.
block until a client has connected. optional.
activate inspector on host and port. equivalent to
, but can be done programmatically after node has
started.
if wait is , will block until a client has connected to the inspect port
and flow control has been passed to the debugger client.
see the  regarding the
return the url of the active inspector, or  if there is none.
blocks until a client (existing or connected later) has sent
an exception will be thrown if there is no active inspector.
the  is used for dispatching messages to the v8 inspector
back-end and receiving message responses and notifications.
create a new instance of the  class. the inspector session
needs to be connected through  before the messages
can be dispatched to the inspector backend.
the notification message object
emitted when any notification from the v8 inspector is received.
it is also possible to subscribe only to notifications with specific method:
event: ;
emitted when an inspector notification is received that has its method field set
to the  value.
the following snippet installs a listener on the
event, and prints the reason for program suspension whenever program
execution is suspended (through breakpoints, for example):
connects a session to the inspector back-end.
added in: v12.11.0
connects a session to the main thread inspector back-end. an exception will
be thrown if this api was not called on a worker thread.
immediately close the session. all pending message callbacks will be called
with an error.  will need to be called to be able to send
messages again. reconnected session will lose all inspector state, such as
enabled agents or configured breakpoints.
posts a message to the inspector back-end.  will be notified when
a response is received.  is a function that accepts two optional
arguments: error and message-specific result.
the latest version of the v8 inspector protocol is published on the
node.js inspector supports all the chrome devtools protocol domains declared
by v8. chrome devtools protocol domain provides an interface for interacting
with one of the runtime agents used to inspect the application state and listen
to the run-time events.
example usage
apart from the debugger, various v8 profilers are available through the devtools
protocol.
cpu profiler
here's an example showing how to use the :
heap profiler
internationalization support
node.js has many features that make it easier to write internationalized
programs. some of them are:
locale-sensitive or unicode-aware functions in the :
all functionality described in the  (aka ecma-402):
locale-sensitive methods like  and
the 's  (idns) support
more accurate  line editing
node.js and the underlying v8 engine use
to implement these features
in native c/c++ code. the full icu data set is provided by node.js by default.
however, due to the size of the icu data file, several
options are provided for customizing the icu data set either when
building or running node.js.
options for building node.js
to control how icu is used in node.js, four  options are available
during compilation. additional details on how to compile node.js are documented
an overview of available node.js and javascript features for each
option:
the "(not locale-aware)" designation denotes that the function carries out its
operation just like the non- version of the function, if one
exists. for example, under  mode, 's
operation is identical to that of .
disable all internationalization features ()
if this option is chosen, icu is disabled and most internationalization
features mentioned above will be unavailable in the resulting  binary.
build with a pre-installed icu ()
node.js can link against an icu build already installed on the system. in fact,
most linux distributions already come with icu installed, and this option would
make it possible to reuse the same set of data used by other components in the
functionalities that only require the icu library itself, such as
and the , are fully
supported under . features that require icu locale data in
addition, such as  may be fully or partially
supported, depending on the completeness of the icu data installed on the
system.
embed a limited set of icu data ()
this option makes the resulting binary link against the icu library statically,
and includes a subset of icu data (typically only the english locale) within
the  executable.
supported under . features that require icu locale data in addition,
such as , generally only work with the english locale:
this mode provides a balance between features and binary size.
providing icu data at runtime
if the  option is used, one can still provide additional locale data
at runtime so that the js methods would work for all icu locales. assuming the
data file is stored at , it can be made available to icu
through either:
the  environment variable:
the  cli parameter:
(if both are specified, the  cli parameter takes precedence.)
icu is able to automatically find and load a variety of data formats, but the
data must be appropriate for the icu version, and the file correctly named.
the most common name for the data file is , where  denotes
the intended icu version, and  or  indicates the system's endianness.
check  article in the icu user guide for other supported formats
and more details on icu data in general.
the  npm module can greatly simplify icu data installation by
detecting the icu version of the running  executable and downloading the
appropriate data file. after installing the module through ,
the data file will be available at . this path can be
then passed either to  or  as shown above to
enable full  support.
embed the entire icu ()
this option makes the resulting binary link against icu statically and include
a full set of icu data. a binary created this way has no further external
dependencies and supports all locales, but might be rather large. this is
the default behavior if no  flag is passed. the official binaries
are also built in this mode.
detecting internationalization support
to verify that icu is enabled at all (, , or
), simply checking the existence of  should suffice:
alternatively, checking for , a property defined only
when icu is enabled, works too:
to check for support for a non-english locale (i.e.  or
),  can be a good distinguishing factor:
for more verbose tests for  support, the following resources may be found
to be helpful:
: generally used to check whether node.js with  support is
built correctly.
: ecmascript's official conformance test suite includes a section
dedicated to ecma-402.
modules: commonjs modules
commonjs modules are the original way to package javascript code for node.js.
node.js also supports the  standard used by browsers
and other javascript runtimes.
in node.js, each file is treated as a separate module. for
example, consider a file named :
on the first line,  loads the module  that is in the same
directory as .
here are the contents of :
the module  has exported the functions  and
. functions and objects are added to the root of a module
by specifying additional properties on the special  object.
variables local to the module will be private, because the module is wrapped
in a function by node.js (see ).
in this example, the variable  is private to .
the  property can be assigned a new value (such as a function
or object).
below,  makes use of the  module, which exports a square class:
the  module is defined in :
the commonjs module system is implemented in the .
enabling
node.js has two module systems: commonjs modules and .
by default, node.js will treat the following as commonjs modules:
files with a  extension;
files with a  extension when the nearest parent  file
contains a top-level field  with a value of .
doesn't contain a top-level field . package authors should include
the  field, even in packages where all sources are commonjs. being
explicit about the  of the package will make things easier for build
tools and loaders to determine how the files in the package should be
interpreted.
files with an extension that is not , , , , or
(when the nearest parent  file contains a top-level field
with a value of , those files will be recognized as
commonjs modules only if they are being d, not when used as the
command-line entry point of the program).
calling  always use the commonjs module loader. calling
always use the ecmascript module loader.
accessing the main module
when a file is run directly from node.js,  is set to its
. that means that it is possible to determine whether a file has been
run directly by testing .
for a file , this will be  if run via , but
if run by .
when the entry point is not a commonjs module,  is ,
and the main module is out of reach.
package manager tips
the semantics of the node.js  function were designed to be general
enough to support reasonable directory structures. package manager programs
such as , , and  will hopefully find it possible to build
native packages from node.js modules without modification.
below we give a suggested directory structure that could work:
let's say that we wanted to have the folder at
hold the contents of a
specific version of a package.
packages can depend on one another. in order to install package , it
may be necessary to install a specific version of package . the
package may itself have dependencies, and in some cases, these may even collide
or form cyclic dependencies.
because node.js looks up the  of any modules it loads (that is, it
resolves symlinks) and then ,
this situation can be resolved with the following architecture:
: contents of the  package, version 1.2.3.
: contents of the  package that  depends
: symbolic link to
: symbolic links to the packages that
depends on.
thus, even if a cycle is encountered, or if there are dependency
conflicts, every module will be able to get a version of its dependency
that it can use.
when the code in the  package does , it will get the
version that is symlinked into .
then, when the code in the  package calls , it'll get
the version that is symlinked into
furthermore, to make the module lookup process even more optimal, rather
than putting packages directly in , we could put them in
. then node.js will not bother
looking for missing dependencies in  or .
in order to make modules available to the node.js repl, it might be useful to
also add the  folder to the  environment
variable. since the module lookups using  folders are all
relative, and based on the real path of the files making the calls to
, the packages themselves can be anywhere.
the  extension
due to the synchronous nature of , it is not possible to use it to
load ecmascript module files. attempting to do so will throw a
error. use  instead.
the  extension is reserved for  which cannot be
loaded via . see  section for more info
regarding which files are parsed as ecmascript modules.
all together
to get the exact filename that will be loaded when  is called, use
the  function.
putting together all of the above, here is the high-level algorithm
in pseudocode of what  does:
caching
modules are cached after the first time they are loaded. this means (among other
things) that every call to  will get exactly the same object
returned, if it would resolve to the same file.
provided  is not modified, multiple calls to
will not cause the module code to be executed multiple times. this is an
important feature. with it, "partially done" objects can be returned, thus
allowing transitive dependencies to be loaded even when they would cause cycles.
to have a module execute code multiple times, export a function, and call that
module caching caveats
modules are cached based on their resolved filename. since modules may resolve
to a different filename based on the location of the calling module (loading
from  folders), it is not a guarantee that  will
always return the exact same object, if it would resolve to different files.
additionally, on case-insensitive file systems or operating systems, different
resolved filenames can point to the same file, but the cache will still treat
them as different modules and will reload the file multiple times. for example,
and  return two different objects,
irrespective of whether or not  and  are the same file.
core modules
node.js has several modules compiled into the binary. these modules are
described in greater detail elsewhere in this documentation.
the core modules are defined within the node.js source and are located in the
folder.
core modules can be identified using the  prefix, in which case
it bypasses the  cache. for instance,  will
always return the built in http module, even if there is  entry
by that name.
some core modules are always preferentially loaded if their identifier is
passed to . for instance,  will always
return the built-in http module, even if there is a file by that name. the list
of core modules that can be loaded without using the  prefix is exposed
cycles
when there are circular  calls, a module might not have finished
executing when it is returned.
consider this situation:
when  loads , then  in turn loads . at that
point,  tries to load . in order to prevent an infinite
loop, an unfinished copy of the  exports object is returned to the
module.  then finishes loading, and its  object is
provided to the  module.
by the time  has loaded both modules, they're both finished.
the output of this program would thus be:
careful planning is required to allow cyclic module dependencies to work
correctly within an application.
file modules
if the exact filename is not found, then node.js will attempt to load the
required filename with the added extensions: , , and finally
. when loading a file that has a different extension (e.g. ), its
full name must be passed to , including its file extension (e.g.
files are parsed as json text files,  files are interpreted as
compiled addon modules loaded with . files using any other
extension (or no extension at all) are parsed as javascript text files. refer to
the  section to understand what parse goal will be
used.
a required module prefixed with  is an absolute path to the file. for
example,  will load the file at
a required module prefixed with  is relative to the file calling
. that is,  must be in the same directory as  for
to find it.
without a leading , , or  to indicate a file, the module must
either be a core module or is loaded from a  folder.
if the given path does not exist,  will throw a
folders as modules
- legacy: use  or  instead.
there are three ways in which a folder may be passed to  as
an argument.
the first is to create a  file in the root of the folder,
which specifies a  module. an example  file might
look like this:
if this was in a folder at , then
would attempt to load
if there is no  file present in the directory, or if the
entry is missing or cannot be resolved, then node.js
will attempt to load an  or  file out of that
directory. for example, if there was no  file in the previous
example, then  would attempt to load:
if these attempts fail, then node.js will report the entire module as missing
with the default error:
in all three above cases, an  call would result in a
error. using package  or
can provide the same containment organization benefits as
folders as modules, and work for both  and .
loading from  folders
if the module identifier passed to  is not a
module, and does not begin with , , or
, then node.js starts at the directory of the current module, and
adds , and attempts to load the module from that location.
node.js will not append  to a path already ending in
if it is not found there, then it moves to the parent directory, and so
on, until the root of the file system is reached.
for example, if the file at  called
, then node.js would look in the following locations, in
this order:
this allows programs to localize their dependencies, so that they do not
clash.
it is possible to require specific files or sub modules distributed with a
module by including a path suffix after the module name. for instance
would resolve
relative to where  is located. the suffixed path follows the
same module resolution semantics.
loading from the global folders
if the  environment variable is set to a colon-delimited list
of absolute paths, then node.js will search those paths for modules if they
are not found elsewhere.
on windows,  is delimited by semicolons () instead of colons.
was originally created to support loading modules from
varying paths before the current  algorithm was defined.
is still supported, but is less necessary now that the node.js
ecosystem has settled on a convention for locating dependent modules.
sometimes deployments that rely on  show surprising behavior
when people are unaware that  must be set. sometimes a
module's dependencies change, causing a different version (or even a
different module) to be loaded as the  is searched.
additionally, node.js will search in the following list of global_folders:
where  is the user's home directory, and  is the node.js
configured .
these are mostly for historic reasons.
it is strongly encouraged to place dependencies in the local
folder. these will be loaded faster, and more reliably.
the module wrapper
before a module's code is executed, node.js will wrap it with a function
wrapper that looks like the following:
by doing this, node.js achieves a few things:
it keeps top-level variables (defined with , , or ) scoped to
the module rather than the global object.
it helps to provide some global-looking variables that are actually specific
to the module, such as:
the  and  objects that the implementor can use to export
values from the module.
the convenience variables  and , containing the
module's absolute filename and directory path.
the module scope
the directory name of the current module. this is the same as the
example: running  from
the file name of the current module. this is the current module file's absolute
path with symlinks resolved.
for a main program this is not necessarily the same as the file name used in the
command line.
see  for the directory name of the current module.
running  from
given two modules:  and , where  is a dependency of
and there is a directory structure of:
references to  within  will return
while references to  within
will return .
added in: v0.1.12
a reference to the  that is shorter to type.
see the section about the  for details on when to use
and when to use .
a reference to the current module, see the section about the
. in particular,  is used for defining what
a module exports and makes available through .
added in: v0.1.13
module name or path
returns:  exported module content
used to import modules, , and local files. modules can be imported
from . local modules and json files can be imported using
a relative path (e.g. , , , ) that will be
resolved against the directory named by  (if defined) or
the current working directory. the relative paths of posix style are resolved
in an os independent fashion, meaning that the examples above will work on
windows in the same way they would on unix systems.
modules are cached in this object when they are required. by deleting a key
value from this object, the next  will reload the module.
this does not apply to , for which reloading will result in an
adding or replacing entries is also possible. this cache is checked before
built-in modules and if a name matching a built-in module is added to the cache,
only -prefixed require calls are going to receive the built-in module.
use with care!
added in: v0.3.0deprecated since: v0.10.6
instruct  on how to handle certain file extensions.
process files with the extension  as :
deprecated. in the past, this list has been used to load non-javascript
modules into node.js by compiling them on-demand. however, in practice, there
are much better ways to do this, such as loading modules via some other node.js
program, or compiling them to javascript ahead of time.
avoid using . use could cause subtle bugs and resolving the
extensions gets slower with each registered extension.
the  object representing the entry script loaded when the node.js
process launched, or  if the entry point of the program is not a
commonjs module.
in  script:
the module path to resolve.
paths to resolve module location from. if present, these
paths are used instead of the default resolution paths, with the exception
of  like , which are
always included. each of these paths is used as a starting point for
the module resolution algorithm, meaning that the  hierarchy
is checked from this location.
use the internal  machinery to look up the location of a module,
but rather than loading the module, just return the resolved filename.
if the module can not be found, a  error is thrown.
added in: v8.9.0
the module path whose lookup paths are being retrieved.
returns an array containing the paths searched during resolution of  or
if the  string references a core module, for example  or
the  object
in each module, the  free variable is a reference to the object
representing the current module. for convenience,  is
also accessible via the  module-global.  is not actually
a global but rather local to each module.
the module objects required for the first time by this one.
the  object is created by the  system. sometimes this is
not acceptable; many want their module to be an instance of some class. to do
this, assign the desired export object to . assigning
the desired object to  will simply rebind the local  variable,
which is probably not what is desired.
for example, suppose we were making a module called :
then in another file we could do:
assignment to  must be done immediately. it cannot be
done in any callbacks. this does not work:
shortcut
the  variable is available within a module's file-level scope, and is
assigned the value of  before the module is evaluated.
it allows a shortcut, so that  can be written more
succinctly as . however, be aware that like any variable, if a
new value is assigned to , it is no longer bound to :
when the  property is being completely replaced by a new
object, it is common to also reassign :
to illustrate the behavior, imagine this hypothetical implementation of
, which is quite similar to what is actually done by :
the fully resolved filename of the module.
the identifier for the module. typically this is the fully resolved
filename.
added in: v15.4.0, v14.17.0
type:   if the module is running during the node.js preload
phase.
whether or not the module is done loading, or is in the process of
loading.
added in: v0.1.16deprecated since: v14.6.0, v12.19.0
- deprecated: please use  and
the module that first required this one, or  if the current module is the
entry point of the current process, or  if the module was loaded by
something that is not a commonjs module (e.g.: repl or ).
added in: v11.14.0
the directory name of the module. this is usually the same as the
the search paths for the module.
added in: v0.5.1
the  method provides a way to load a module as if
was called from the original module.
in order to do this, it is necessary to get a reference to the  object.
since  returns the , and the  is typically
only available within a specific module's code, it must be explicitly exported
in order to be used.
this section was moved to
source map v3 support
modules: ecmascript modules
ecmascript modules are  to package javascript
code for reuse. modules are defined using a variety of  and
statements.
the following example of an es module exports a function:
the following example of an es module imports the function from :
node.js fully supports ecmascript modules as they are currently specified and
provides interoperability between them and its original module format,
node.js has two module systems:  modules and ecmascript modules.
authors can tell node.js to use the ecmascript modules loader
via the  file extension, the   field, or the
flag. outside of those cases, node.js will use the commonjs
module loader. see  for more details.
packages
this section was moved to .
specifiers
the specifier of an  statement is the string after the  keyword,
e.g.  in . specifiers are also
used in  statements, and as the argument to an
expression.
there are three types of specifiers:
relative specifiers like  or . they refer
to a path relative to the location of the importing file. the file extension
is always necessary for these.
bare specifiers like  or . they can
refer to the main entry point of a package by the package name, or a
specific feature module within a package prefixed by the package name as per
the examples respectively. including the file extension is only necessary
for packages without an  field.
absolute specifiers like . they refer
directly and explicitly to a full path.
bare specifier resolutions are handled by the . all other specifier resolutions are always only resolved with
the standard relative  resolution semantics.
like in commonjs, module files within packages can be accessed by appending a
path to the package name unless the package's  contains an
field, in which case files within packages can only be accessed
via the paths defined in .
for details on these package resolution rules that apply to bare specifiers in
the node.js module resolution, see the .
mandatory file extensions
a file extension must be provided when using the  keyword to resolve
relative or absolute specifiers. directory indexes (e.g. )
must also be fully specified.
this behavior matches how  behaves in browser environments, assuming a
typically configured server.
urls
es modules are resolved and cached as urls. this means that special characters
must be , such as  with  and  with .
, , and  url schemes are supported. a specifier like
is not supported natively in node.js unless using
modules are loaded multiple times if the  specifier used to resolve
them has a different query or fragment.
the volume root may be referenced via , , or . given the
differences between  and path resolution (such as percent encoding
details), it is recommended to use  when importing a path.
imports
are supported for importing with the following mime types:
for es modules
for json
for wasm
urls only resolve  for builtin modules
and . resolving
does not work because  is not a
. for example, attempting to load
from  fails to resolve because there
is no concept of relative resolution for  urls.
urls are supported as an alternative means to load node.js builtin
modules. this url scheme allows for builtin modules to be referenced by valid
absolute url strings.
import assertions
the  adds an inline syntax for module import
statements to pass on more information alongside the module specifier.
node.js supports the following  values, for which the assertion is
mandatory:
builtin modules
provide named exports of their public api. a
default export is also provided which is the value of the commonjs exports.
the default export can be used for, among other things, modifying the named
exports. named exports of builtin modules are updated only by calling
expressions
is supported in both commonjs and es modules. in commonjs
modules it can be used to load es modules.
the  meta property is an  that contains the following
the absolute  url of the module.
this is defined exactly the same as it is in browsers providing the url of the
current module file.
this enables useful patterns such as relative file loading:
this feature is only available with the
command flag enabled.
the module specifier to resolve relative to .
|  the absolute parent module url to resolve from. if none
is specified, the value of  is used as the default.
provides a module-relative resolution function scoped to each module, returning
the url string.
also accepts a second argument which is the parent module
from which to resolve from:
this function is asynchronous because the es module resolver in node.js is
allowed to be asynchronous.
interoperability with commonjs
statements
an  statement can reference an es module or a commonjs module.
statements are permitted only in es modules, but dynamic
expressions are supported in commonjs for loading es modules.
when importing , the
object is provided as the default export. named exports may be
available, provided by static analysis as a convenience for better ecosystem
compatibility.
the commonjs module  always treats the files it references as commonjs.
using  to load an es module is not supported because es modules have
asynchronous execution. instead, use  to load an es module
from a commonjs module.
commonjs namespaces
commonjs modules consist of a  object which can be of any type.
when importing a commonjs module, it can be reliably imported using the es
module default import or its corresponding sugar syntax:
the ecmascript module namespace representation of a commonjs module is always
a namespace with a  export key pointing to the commonjs
this module namespace exotic object can be directly observed either when using
or a dynamic import:
for better compatibility with existing usage in the js ecosystem, node.js
in addition attempts to determine the commonjs named exports of every imported
commonjs module to provide them as separate es module exports using a static
analysis process.
for example, consider a commonjs module written:
the preceding module supports named imports in es modules:
as can be seen from the last example of the module namespace exotic object being
logged, the  export is copied off of the  object and set
directly on the es module namespace when the module is imported.
live binding updates or new exports added to  are not detected
for these named exports.
the detection of named exports is based on common syntax patterns but does not
always correctly detect named exports. in these cases, using the default
import form described above can be a better option.
named exports detection covers many common export patterns, reexport patterns
and build tool and transpiler outputs. see  for the exact
semantics implemented.
differences between es modules and commonjs
no , , or
in most cases, the es module  can be used to load commonjs modules.
if needed, a  function can be constructed within an es module using
no  or
these commonjs variables are not available in es modules.
and  use cases can be replicated via
no native module loading
native modules are not currently supported with es module imports.
they can instead be loaded with  or
relative resolution can be handled via .
for a complete  replacement, there is a flagged experimental
alternatively  can be used.
is not part of resolving  specifiers. please use symlinks
if this behavior is desired.
is not used by . the expectation is that loader
hooks can provide this workflow in the future.
is not used by  as the es module loader has its own
separate cache.
json modules
json files can be referenced by :
the  syntax is mandatory; see .
the imported json only exposes a  export. there is no support for named
exports. a cache entry is created in the commonjs cache to avoid duplication.
the same object is returned in commonjs if the json module has already been
imported from the same path.
wasm modules
importing webassembly modules is supported under the
flag, allowing any  files to be
imported as normal modules while also supporting their module imports.
this integration is in line with the
for example, an  containing:
executed under:
would provide the exports interface for the instantiation of .
top-level
added in: v14.8.0
the  keyword may be used in the top level body of an ecmascript module.
assuming an  with
and a  with
if a top level  expression never resolves, the  process will exit
with a  .
https and http imports
importing network based modules using  and  is supported under
the  flag. this allows web browser-like imports
to work in node.js with a few differences due to application stability and
security concerns that are different when running in a privileged environment
instead of a browser sandbox.
imports are limited to http/1
automatic protocol negotiation for http/2 and http/3 is not yet supported.
http is limited to loopback addresses
is vulnerable to man-in-the-middle attacks and is not allowed to be
used for addresses outside of the ipv4 address  ( to
) and the ipv6 address . support for  is intended
to be used for local development.
authentication is never sent to the destination server.
, , and  headers are not sent to the
server. avoid including user info in parts of imported urls. a security model
for safely using these on the server is being worked on.
cors is never checked on the destination server
cors is designed to allow a server to limit the consumers of an api to a
specific set of hosts. this is not supported as it does not make sense for a
server-based implementation.
cannot load non-network dependencies
these modules cannot access other modules that are not over  or .
to still access local modules while avoiding the security concern, pass in
references to the local dependencies:
network-based loading is not enabled by default
for now, the  flag is required to enable loading
resources over  or . in the future, a different mechanism will be
used to enforce this. opt-in is required to prevent transitive dependencies
inadvertently using potentially mutable state that could affect reliability
of node.js applications.
loaders
this api is currently being redesigned and will still change.
to customize the default module resolution, loader hooks can optionally be
provided via a  argument to node.js.
when hooks are used they apply to the entry point and all  calls. they
won't apply to  calls; those still follow  rules.
loaders follow the pattern of :
these are called in the following sequence:  calls
which calls .
hooks are part of a chain, even if that chain consists of only one custom
(user-provided) hook and the default hook, which is always present. hook
functions nest: each one must always return a plain object, and chaining happens
as a result of each function calling , which is a reference
to the subsequent loader’s hook.
a hook that returns a value lacking a required property triggers an exception.
a hook that returns without calling  and without returning
also triggers an exception. these errors are to help
prevent unintentional breaks in the chain.
the loaders api is being redesigned. this hook may disappear or its
signature may change. do not rely on the api described below.
export conditions of the relevant
|  the module importing this one, or undefined
if this is the node.js entry point
the subsequent  hook in the chain, or the
node.js default  hook after the last user-supplied  hook
|  |  a hint to the load hook (it might be
ignored)
|  a signal that this hook intends to
terminate the chain of  hooks. default:
the absolute url to which this input resolves
the  hook chain is responsible for resolving file url for a given
module specifier and parent url, and optionally its format (such as )
as a hint to the  hook. if a format is specified, the  hook is
ultimately responsible for providing the final  value (and it is free to
ignore the hint provided by ); if  provides a , a
custom  hook is required even if only to pass the value to the node.js
default  hook.
the module specifier is the string in an  statement or
the parent url is the url of the module that imported this one, or
if this is the main entry point for the application.
the  property in  is an array of conditions for
that apply to this resolution
request. they can be used for looking up conditional mappings elsewhere or to
modify the list when calling the default resolution logic.
the current  are always in
the  array passed into the hook. to guarantee default
node.js module specifier resolution behavior when calling , the
array passed to it must include all elements of the
array originally passed into the  hook.
in a previous version of this api, this was split across 3 separate, now
deprecated, hooks (, , and ).
the url returned by the  chain
|  |  the format optionally supplied by the
hook chain
|  |  the source for node.js to evaluate
the  hook provides a way to define a custom method of determining how
a url should be interpreted, retrieved, and parsed. it is also in charge of
validating the import assertion.
the final value of  must be one of the following:
the value of  is ignored for type  because currently it is
not possible to replace the value of a node.js builtin (core) module. the value
of  is ignored for type  because the commonjs module loader
does not provide a mechanism for the es module loader to override the
. this limitation might be
overcome in the future.
caveat: the esm  hook and namespaced exports from commonjs modules
are incompatible. attempting to use them together will result in an empty
object from the import. this may be addressed in the future.
these types all correspond to classes defined in ecmascript.
the specific  object is a .
if the source value of a text-based format (i.e., , )
is not a string, it is converted to a string using .
the  hook provides a way to define a custom method for retrieving the
source code of an es module specifier. this would allow a loader to potentially
avoid reading files from disk. it could also be used to map an unrecognized
format to a supported one, for example  to .
in a more advanced scenario, this can also be used to transform an unsupported
source to a supported one (see  below).
in a previous version of this api, this hook was named
information to assist the preload code
returns:  code to run before application startup
sometimes it might be necessary to run some code inside of the same global
scope that the application runs in. this hook allows the return of a string
that is run as a sloppy-mode script on startup.
similar to how commonjs wrappers work, the code runs in an implicit function
scope. the only argument is a -like function that can be used to load
builtins like "fs": .
if the code needs more advanced  features, it has to construct
its own  using  .
in order to allow communication between the application and the loader, another
argument is provided to the preload code: . this is available as a
parameter to the loader hook and inside of the source text returned by the hook.
some care must be taken in order to properly call  and
to prevent a process from being in a state where it won't
close normally.
the various loader hooks can be used together to accomplish wide-ranging
customizations of the node.js code loading and evaluation behaviors.
https loader
in current node.js, specifiers starting with  are experimental (see
the loader below registers hooks to enable rudimentary support for such
specifiers. while this may seem like a significant improvement to node.js core
functionality, there are substantial downsides to actually using this loader:
performance is much slower than loading files from disk, there is no caching,
and there is no security.
with the preceding loader, running
prints the current version of coffeescript per the module at the url in
transpiler loader
sources that are in formats node.js doesn't understand can be converted into
javascript using the . before that hook gets called,
however, a  needs to tell node.js not to
throw an error on unknown file types.
this is less performant than transpiling source files before running
node.js; a transpiler loader should only be used for development and testing
purposes.
causes  to be turned into javascript after its source code is
loaded from disk but before node.js executes it; and so on for any ,
or  files referenced via  statements of any
loaded file.
resolution algorithm
features
the resolver has the following properties:
fileurl-based resolution as is used by es modules
support for builtin module loading
relative and absolute url resolution
no default extensions
no folder mains
bare specifier package resolution lookup through node_modules
resolver algorithm
the algorithm to load an es module specifier is given through the
esm_resolve method below. it returns the resolved url for a
module specifier relative to a parenturl.
the algorithm to determine the module format of a resolved url is
provided by esm_format, which returns the unique module
format for any file. the "module" format is returned for an ecmascript
module, while the "commonjs" format is used to indicate loading through the
legacy commonjs loader. additional formats such as "addon" can be extended in
future updates.
in the following algorithms, all subroutine errors are propagated as errors
of these top-level routines unless stated otherwise.
defaultconditions is the conditional environment name array,
the resolver can throw the following errors:
invalid module specifier: module specifier is an invalid url, package name
or package subpath specifier.
invalid package configuration: package.json configuration is invalid or
contains an invalid configuration.
invalid package target: package exports or imports define a target module
for the package that is an invalid type or string target.
package path not exported: package exports do not define or permit a target
subpath in the package for the given module.
package import not defined: package imports do not define the specifier.
module not found: the package or module requested does not exist.
unsupported directory import: the resolved path corresponds to a directory,
which is not a supported target for module imports.
resolver algorithm specification
esm_resolve(specifier, parenturl)
let resolved be undefined.
if specifier is a valid url, then
set resolved to the result of parsing and reserializing
specifier as a url.
otherwise, if specifier starts with "/", "./", or "../", then
set resolved to the url resolution of specifier relative to
parenturl.
otherwise, if specifier starts with "#", then
set resolved to the result of
package_imports_resolve(specifier,
parenturl, defaultconditions).
otherwise,
note: specifier is now a bare specifier.
set resolved the result of
package_resolve(specifier, parenturl).
let format be undefined.
if resolved is a "file:" url, then
if resolved contains any percent encodings of "/" or "\" ("%2f"
and "%5c" respectively), then
throw an invalid module specifier error.
if the file at resolved is a directory, then
throw an unsupported directory import error.
if the file at resolved does not exist, then
throw a module not found error.
set resolved to the real path of resolved, maintaining the
same url querystring and fragment components.
set format to the result of esm_file_format(resolved).
set format the module format of the content type associated with the
url resolved.
load resolved as module format, format.
package_resolve(packagespecifier, parenturl)
let packagename be undefined.
if packagespecifier is an empty string, then
if packagespecifier is a node.js builtin module name, then
return the string "node:" concatenated with packagespecifier.
if packagespecifier does not start with "@", then
set packagename to the substring of packagespecifier until the first
"/" separator or the end of the string.
if packagespecifier does not contain a "/" separator, then
set packagename to the substring of packagespecifier
until the second "/" separator or the end of the string.
if packagename starts with "." or contains "\" or "%", then
let packagesubpath be "." concatenated with the substring of
packagespecifier from the position at the length of packagename.
if packagesubpath ends in "/", then
let selfurl be the result of
package_self_resolve(packagename, packagesubpath, parenturl).
if selfurl is not undefined, return selfurl.
while parenturl is not the file system root,
let packageurl be the url resolution of "node_modules/"
concatenated with packagespecifier, relative to parenturl.
set parenturl to the parent folder url of parenturl.
if the folder at packageurl does not exist, then
continue the next loop iteration.
let pjson be the result of read_package_json(packageurl).
if pjson is not null and pjson.exports is not null or
undefined, then
return the result of package_exports_resolve(packageurl,
packagesubpath, pjson.exports, defaultconditions).
otherwise, if packagesubpath is equal to ".", then
if pjson.main is a string, then
return the url resolution of main in packageurl.
return the url resolution of packagesubpath in packageurl.
package_self_resolve(packagename, packagesubpath, parenturl)
let packageurl be the result of lookup_package_scope(parenturl).
if packageurl is null, then
return undefined.
if pjson is null or if pjson.exports is null or
if pjson.name is equal to packagename, then
otherwise, return undefined.
package_exports_resolve(packageurl, subpath, exports, conditions)
if exports is an object with both a key starting with "." and a key not
starting with ".", throw an invalid package configuration error.
if subpath is equal to ".", then
let mainexport be undefined.
if exports is a string or array, or an object containing no keys
starting with ".", then
set mainexport to exports.
otherwise if exports is an object containing a "." property, then
set mainexport to exports["."].
if mainexport is not undefined, then
let resolved be the result of package_target_resolve(
packageurl, mainexport, null, false, conditions).
if resolved is not null or undefined, return resolved.
otherwise, if exports is an object and all keys of exports start with
".", then
let matchkey be the string "./" concatenated with subpath.
let resolved be the result of package_imports_exports_resolve(
matchkey, exports, packageurl, false, conditions).
throw a package path not exported error.
package_imports_resolve(specifier, parenturl, conditions)
assert: specifier begins with "#".
if specifier is exactly equal to "#" or starts with "#/", then
if packageurl is not null, then
if pjson.imports is a non-null object, then
let resolved be the result of
package_imports_exports_resolve(
specifier, pjson.imports, packageurl, true, conditions).
throw a package import not defined error.
package_imports_exports_resolve(matchkey, matchobj, packageurl,
isimports, conditions)
if matchkey is a key of matchobj and does not contain "*", then
let target be the value of matchobj[matchkey].
return the result of package_target_resolve(packageurl,
target, null, isimports, conditions).
let expansionkeys be the list of keys of matchobj containing only a
single "*", sorted by the sorting function pattern_key_compare
which orders in descending order of specificity.
for each key expansionkey in expansionkeys, do
let patternbase be the substring of expansionkey up to but excluding
the first "*" character.
if matchkey starts with but is not equal to patternbase, then
let patterntrailer be the substring of expansionkey from the
index after the first "*" character.
if patterntrailer has zero length, or if matchkey ends with
patterntrailer and the length of matchkey is greater than or
equal to the length of expansionkey, then
let target be the value of matchobj[expansionkey].
let patternmatch be the substring of matchkey starting at the
index of the length of patternbase up to the length of
matchkey minus the length of patterntrailer.
target, patternmatch, isimports, conditions).
return null.
pattern_key_compare(keya, keyb)
assert: keya ends with "/" or contains only a single "*".
assert: keyb ends with "/" or contains only a single "*".
let baselengtha be the index of "*" in keya plus one, if keya
contains "*", or the length of keya otherwise.
let baselengthb be the index of "*" in keyb plus one, if keyb
contains "*", or the length of keyb otherwise.
if baselengtha is greater than baselengthb, return -1.
if baselengthb is greater than baselengtha, return 1.
if keya does not contain "*", return 1.
if keyb does not contain "*", return -1.
if the length of keya is greater than the length of keyb, return -1.
if the length of keyb is greater than the length of keya, return 1.
return 0.
package_target_resolve(packageurl, target, patternmatch,
if target is a string, then
if target does not start with "./", then
if isimports is false, or if target starts with "../" or
"/", or if target is a valid url, then
throw an invalid package target error.
if patternmatch is a string, then
return package_resolve(target with every instance of "*"
replaced by patternmatch, packageurl + "/").
return package_resolve(target, packageurl + "/").
if target split on "/" or "\" contains any "", ".", "..",
or "node_modules" segments after the first "." segment, case
insensitive and including percent encoded variants, throw an invalid
package target error.
let resolvedtarget be the url resolution of the concatenation of
packageurl and target.
assert: resolvedtarget is contained in packageurl.
if patternmatch is null, then
return resolvedtarget.
if patternmatch split on "/" or "\" contains any "", ".",
"..", or "node_modules" segments, case insensitive and including
percent encoded variants, throw an invalid module specifier error.
return the url resolution of resolvedtarget with every instance of
"*" replaced with patternmatch.
otherwise, if target is a non-null object, then
if exports contains any index property keys, as defined in ecma-262
, throw an invalid package configuration error.
for each property p of target, in object insertion order as,
if p equals "default" or conditions contains an entry for p,
then
let targetvalue be the value of the p property in target.
packageurl, targetvalue, patternmatch, isimports,
conditions).
if resolved is equal to undefined, continue the loop.
return resolved.
otherwise, if target is an array, then
if _target.length is zero, return null.
for each item targetvalue in target, do
conditions), continuing the loop on any invalid package target
if resolved is undefined, continue the loop.
return or throw the last fallback resolution null return or error.
otherwise, if target is null, return null.
otherwise throw an invalid package target error.
esm_file_format(url)
assert: url corresponds to an existing file.
if url ends in ".mjs", then
return "module".
if url ends in ".cjs", then
return "commonjs".
if url ends in ".json", then
return "json".
let packageurl be the result of lookup_package_scope(url).
if pjson?.type exists and is "module", then
if url ends in ".js", then
throw an unsupported file extension error.
lookup_package_scope(url)
let scopeurl be url.
while scopeurl is not the file system root,
set scopeurl to the parent url of scopeurl.
if scopeurl ends in a "node_modules" path segment, return null.
let pjsonurl be the resolution of "package.json" within
scopeurl.
if the file at pjsonurl exists, then
return scopeurl.
read_package_json(packageurl)
let pjsonurl be the resolution of "package.json" within packageurl.
if the file at pjsonurl does not exist, then
if the file at packageurl does not parse as valid json, then
throw an invalid package configuration error.
return the parsed json source of the file at pjsonurl.
customizing esm specifier resolution algorithm
do not rely on this flag. we plan to remove it once the
has advanced to the point that equivalent functionality can
be achieved via custom loaders.
the current specifier resolution does not support all default behavior of
the commonjs loader. one of the behavior differences is automatic resolution
of file extensions and the ability to import directories that have an index
file.
the  flag can be used to customize
the extension resolution algorithm. the default mode is , which
requires the full path to a module be provided to the loader. to enable the
automatic extension resolution and importing from directories that include an
index file use the  mode.
modules:  api
added in: v0.3.7
provides general utility methods when interacting with instances of
, the  variable often seen in  modules. accessed
added in: v9.3.0, v8.10.0, v6.13.0
a list of the names of all modules provided by node.js. can be used to verify
if a module is maintained by a third party or not.
in this context isn't the same object that's provided
by the . to access it, require the  module:
|  filename to be used to construct the require
function. must be a file url object, file url string, or absolute path
returns:  require function
name of the module
returns:  returns true if the module is builtin else returns false
the  method updates all the live bindings for
builtin  to match the properties of the  exports. it
does not add or remove exported names from the .
added in: v13.7.0, v12.17.0
helpers for interacting with the source map cache. this cache is
populated when source map parsing is enabled and
are found in a modules' footer.
to enable source map parsing, node.js must be run with the flag
, or with code coverage enabled by setting
returns:  |  returns  if a source
map is found,  otherwise.
is the resolved path for the file for which a corresponding source map
should be fetched.
creates a new  instance.
is an object with keys matching the :
getter for the payload used to construct the  instance.
given a line number and column number in the generated source file, returns
an object representing the position in the original file. the object returned
consists of the following keys:
generatedline:
generatedcolumn:
originalsource:
originalline:
originalcolumn:
name:
modules: packages
a package is a folder tree described by a  file. the package
consists of the folder containing the  file and all subfolders
until the next folder containing another  file, or a folder
this page provides guidance for package authors writing  files
along with a reference for the  fields defined by node.js.
determining module system
node.js will treat the following as  when passed to  as the
initial input, or when referenced by  statements or
expressions:
files with an  extension.
contains a top-level  field with a value of .
strings passed in as an argument to , or piped to  via ,
with the flag .
node.js will treat as  all other forms of input, such as  files
where the nearest parent  file contains no top-level
field, or string input without the flag . this behavior is to
preserve backward compatibility. however, now that node.js supports both
commonjs and es modules, it is best to be explicit whenever possible. node.js
will treat the following as commonjs when passed to  as the initial input,
or when referenced by  statements,  expressions, or
files with a  extension.
strings passed in as an argument to  or , or piped to
via , with the flag .
package authors should include the  field, even in packages where
all sources are commonjs. being explicit about the  of the package will
future-proof the package in case the default type of node.js ever changes, and
it will also make things easier for build tools and loaders to determine how the
files in the package should be interpreted.
modules loaders
node.js has two systems for resolving a specifier and loading modules.
there is the commonjs module loader:
it is fully synchronous.
it is responsible for handling  calls.
it is monkey patchable.
it supports .
when resolving a specifier, if no exact match is found, it will try to add
extensions (, , and finally ) and then attempt to resolve
it treats  as json text files.
files are interpreted as compiled addon modules loaded with
it treats all files that lack  or  extensions as javascript
text files.
it cannot be used to load ecmascript modules (although it is possible to
). when used to load a
javascript text file that is not an ecmascript module, it loads it as a
there is the ecmascript module loader:
it is asynchronous.
it is responsible for handling  statements and  expressions.
it is not monkey patchable, can be customized using .
it does not support folders as modules, directory indexes (e.g.
) must be fully specified.
it does no extension searching. a file extension must be provided
when the specifier is a relative or absolute file url.
it can load json modules, but an import assertion is required.
it accepts only , , and  extensions for javascript text
files.
it can be used to load javascript commonjs modules. such modules
are passed through the  to try to identify named exports,
which are available if they can be determined through static analysis.
imported commonjs modules have their urls converted to absolute
paths and are then loaded via the commonjs module loader.
and file extensions
within a package, the   field defines how
node.js should interpret  files. if a  file does not have a
field,  files are treated as .
a   value of  tells node.js to interpret
files within that package as using  syntax.
the  field applies not only to initial entry points ()
but also to files referenced by  statements and  expressions.
files ending with  are always loaded as  regardless of
the nearest parent .
files ending with  are always loaded as  regardless of the
nearest parent .
the  and  extensions can be used to mix types within the same
package:
within a  package, node.js can be instructed to
interpret a particular file as  by naming it with a
extension (since both  and  files are treated as es modules within
a  package).
interpret a particular file as an  by naming it with an
extension (since both  and  files are treated as commonjs within a
package).
flag
strings passed in as an argument to  (or ), or piped to  via
, are treated as  when the  flag
is set.
for completeness there is also , for explicitly running
string input as commonjs. this is the default behavior if  is
unspecified.
determining package manager
while all node.js projects are expected to be installable by all package
managers once published, their development teams are often required to use one
specific package manager. to make this process easier, node.js ships with a
tool called  that aims to make all package managers transparently
available in your environment - provided you have node.js installed.
by default corepack won't enforce any specific package manager and will use
the generic "last known good" versions associated with each node.js release,
but you can improve this experience by setting the  field
in your project's .
package entry points
in a package's  file, two fields can define entry points for a
package:  and . both fields apply to both es module
and commonjs module entry points.
the  field is supported in all versions of node.js, but its
capabilities are limited: it only defines the main entry point of the package.
the  provides a modern alternative to  allowing
multiple entry points to be defined, conditional entry resolution support
between environments, and preventing any other entry points besides those
defined in . this encapsulation allows module authors to
clearly define the public interface for their package.
for new packages targeting the currently supported versions of node.js, the
field is recommended. for packages supporting node.js 10 and
below, the  field is required. if both  and
are defined, the  field takes precedence over
in supported versions of node.js.
can be used within  to define different
package entry points per environment, including whether the package is
referenced via  or via . for more information about supporting
both commonjs and es modules in a single package please consult
existing packages introducing the  field will prevent consumers
of the package from using any entry points that are not defined, including the
(e.g. . this will
likely be a breaking change.
to make the introduction of  non-breaking, ensure that every
previously supported entry point is exported. it is best to explicitly specify
entry points so that the package's public api is well-defined. for example,
a project that previously exported , ,
, and the  could use the following :
alternatively a project could choose to export entire folders both with and
without extensioned subpaths using export patterns:
with the above providing backwards-compatibility for any minor package versions,
a future major change for the package can then properly restrict the exports
to only the specific feature exports exposed:
main entry point export
when writing a new package, it is recommended to use the  field:
when the  field is defined, all subpaths of the package are
encapsulated and no longer available to importers. for example,
throws an
this encapsulation of exports provides more reliable guarantees
about package interfaces for tools and when handling semver upgrades for a
package. it is not a strong encapsulation since a direct require of any
absolute subpath of the package such as
will still load .
all currently supported versions of node.js and modern build tools support the
field. for projects using an older version of node.js or a related
build tool, compatibility can be achieved by including the  field
alongside  pointing to the same module:
subpath exports
when using the  field, custom subpaths can be defined along
with the main entry point by treating the main entry point as the
subpath:
now only the defined subpath in  can be imported by a consumer:
while other subpaths will error:
extensions in subpaths
package authors should provide either extensioned () or
extensionless () subpaths in their exports. this ensures
that there is only one subpath for each exported module so that all dependents
import the same consistent specifier, keeping the package contract clear for
consumers and simplifying package subpath completions.
traditionally, packages tended to use the extensionless style, which has the
benefits of readability and of masking the true path of the file within the
package.
with  now providing a standard for package resolution in browsers
and other javascript runtimes, using the extensionless style can result in
bloated import map definitions. explicit file extensions can avoid this issue by
enabling the import map to utilize a  to map multiple
subpaths where possible instead of a separate map entry per package subpath
export. this also mirrors the requirement of using
in relative and absolute import specifiers.
exports sugar
if the  export is the only export, the  field provides sugar
for this case being the direct  field value.
can be written:
subpath imports
added in: v14.6.0, v12.19.0
in addition to the  field, there is a package  field
to create private mappings that only apply to import specifiers from within the
package itself.
entries in the  field must always start with  to ensure they are
disambiguated from external package specifiers.
for example, the imports field can be used to gain the benefits of conditional
exports for internal modules:
where  does not get the resolution of the external package
(including its exports in turn), and instead gets the local
file  relative to the package in other environments.
unlike the  field, the  field permits mapping to external
packages.
the resolution rules for the imports field are otherwise analogous to the
exports field.
subpath patterns
for packages with a small number of exports or imports, we recommend
explicitly listing each exports subpath entry. but for packages that have
large numbers of subpaths, this might cause  bloat and
maintenance issues.
for these use cases, subpath export patterns can be used instead:
maps expose nested subpaths as it is a string replacement syntax
only.
all instances of  on the right hand side will then be replaced with this
value, including if it contains any  separators.
this is a direct static matching and replacement without any special handling
for file extensions. including the  on both sides of the mapping
restricts the exposed package exports to only js files.
the property of exports being statically enumerable is maintained with exports
patterns since the individual exports for a package can be determined by
treating the right hand side target pattern as a  glob against the list of
files within the package. because  paths are forbidden in exports
targets, this expansion is dependent on only the files of the package itself.
to exclude private subfolders from patterns,  targets can be used:
conditional exports
conditional exports provide a way to map to different paths depending on
certain conditions. they are supported for both commonjs and es module imports.
for example, a package that wants to provide different es module exports for
and  can be written:
node.js implements the following conditions, listed in order from most
specific to least specific as conditions should be defined:
- similar to  and matches for any node.js environment.
this condition can be used to provide an entry point which uses native c++
addons as opposed to an entry point which is more universal and doesn't rely
on native addons. this condition can be disabled via the
- matches for any node.js environment. can be a commonjs or es
module file. in most cases explicitly calling out the node.js platform is
not necessary.
- matches when the package is loaded via  or
, or via any top-level import or resolve operation by the
ecmascript module loader. applies regardless of the module format of the
target file. always mutually exclusive with .
- matches when the package is loaded via . the
referenced file should be loadable with  although the condition
matches regardless of the module format of the target file. expected
formats include commonjs, json, and native addons but not es modules as
doesn't support them. always mutually exclusive with
- the generic fallback that always matches. can be a commonjs
or es module file. this condition should always come last.
within the  object, key order is significant. during condition
matching, earlier entries have higher priority and take precedence over later
entries. the general rule is that conditions should be from most specific to
least specific in object order.
using the  and  conditions can lead to some hazards,
which are further explained in .
the  condition can be used to provide an entry point which
uses native c++ addons. however, this condition can be disabled via the
. when using , it's recommended to treat
as an enhancement that provides a more universal entry point, e.g.
using webassembly instead of a native addon.
conditional exports can also be extended to exports subpaths, for example:
defines a package where  and
could provide different implementations between
node.js and other js environments.
when using environment branches, always include a  condition where
possible. providing a  condition ensures that any unknown js
environments are able to use this universal implementation, which helps avoid
these js environments from having to pretend to be existing environments in
order to support packages with conditional exports. for this reason, using
and  condition branches is usually preferable to using
and  condition branches.
nested conditions
in addition to direct mappings, node.js also supports nested condition objects.
for example, to define a package that only has dual mode entry points for
use in node.js but not the browser:
conditions continue to be matched in order as with flat conditions. if
a nested condition does not have any mapping it will continue checking
the remaining conditions of the parent condition. in this way nested
conditions behave analogously to nested javascript  statements.
resolving user conditions
when running node.js, custom user conditions can be added with the
which would then resolve the  condition in package imports and
exports, while resolving the existing , , ,
, and  conditions as appropriate.
any number of custom conditions can be set with repeat flags.
community conditions definitions
condition strings other than the , , ,
and  conditions
are ignored by default.
other platforms may implement other conditions and user conditions can be
enabled in node.js via the .
since custom package conditions require clear definitions to ensure correct
usage, a list of common known package conditions and their strict definitions
is provided below to assist with ecosystem coordination.
- can be used by typing systems to resolve the typing file for
the given export. this condition should always be included first.
- indicates a variation for the deno platform.
- any web browser environment.
- can be used to define a development-only environment
entry point, for example to provide additional debugging context such as
better error messages when running in a development mode. must always be
mutually exclusive with .
- can be used to define a production environment entry
point. must always be mutually exclusive with .
new conditions definitions may be added to this list by creating a pull request
to the . the requirements for listing
a new condition definition here are that:
the definition should be clear and unambiguous for all implementers.
the use case for why the condition is needed should be clearly justified.
there should exist sufficient existing implementation usage.
the condition name should not conflict with another condition definition or
condition in wide usage.
the listing of the condition definition should provide a coordination
benefit to the ecosystem that wouldn't otherwise be possible. for example,
this would not necessarily be the case for company-specific or
application-specific conditions.
the above definitions may be moved to a dedicated conditions registry in due
course.
self-referencing a package using its name
within a package, the values defined in the package's
field can be referenced via the package's name.
for example, assuming the  is:
then any module in that package can reference an export in the package itself:
self-referencing is available only if  has , and
will allow importing only what that  (in the )
allows. so the code below, given the previous package, will generate a runtime
error:
self-referencing is also available when using , both in an es module,
and in a commonjs one. for example, this code will also work:
finally, self-referencing also works with scoped packages. for example, this
code will also work:
dual commonjs/es module packages
prior to the introduction of support for es modules in node.js, it was a common
pattern for package authors to include both commonjs and es module javascript
sources in their package, with   specifying the
commonjs entry point and   specifying the es module
entry point.
this enabled node.js to run the commonjs entry point while build tools such as
bundlers used the es module entry point, since node.js ignored (and still
ignores) the top-level  field.
node.js can now run es module entry points, and a package can contain both
commonjs and es module entry points (either via separate specifiers such as
and , or both at the same specifier via ). unlike in the scenario where  is only used by bundlers,
or es module files are transpiled into commonjs on the fly before evaluation by
node.js, the files referenced by the es module entry point are evaluated as es
modules.
dual package hazard
when an application is using a package that provides both commonjs and es module
sources, there is a risk of certain bugs if both versions of the package get
loaded. this potential comes from the fact that the  created by
is not the same as the
created by  (or an alternative main path like
). this is the “dual package hazard,” where two versions of the
same package can be loaded within the same runtime environment. while it is
unlikely that an application or package would intentionally load both versions
directly, it is common for an application to load one version while a dependency
of the application loads the other version. this hazard can happen because
node.js supports intermixing commonjs and es modules, and can lead to unexpected
if the package main export is a constructor, an  comparison of
instances created by the two versions returns , and if the export is an
object, properties added to one (like ) are not present on
the other. this differs from how  and  statements work in
all-commonjs or all-es module environments, respectively, and therefore is
surprising to users. it also differs from the behavior users are familiar with
when using transpilation via tools like  or .
writing dual packages while avoiding or minimizing hazards
first, the hazard described in the previous section occurs when a package
contains both commonjs and es module sources and both sources are provided for
use in node.js, either via separate main entry points or exported paths. a
package might instead be written where any version of node.js receives only
commonjs sources, and any separate es module sources the package might contain
are intended only for other environments such as browsers. such a package
would be usable by any version of node.js, since  can refer to commonjs
files; but it would not provide any of the advantages of using es module syntax.
a package might also switch from commonjs to es module syntax in a  version bump. this has the disadvantage that the
newest version of the package would only be usable in es module-supporting
versions of node.js.
every pattern has tradeoffs, but there are two broad approaches that satisfy the
following conditions:
the package is usable via both  and .
the package is usable in both current node.js and older versions of node.js
that lack support for es modules.
the package main entry point, e.g.  can be used by both  to
resolve to a commonjs file and by  to resolve to an es module file.
(and likewise for exported paths, e.g. .)
the package provides named exports, e.g.  rather
than .
the package is potentially usable in other es module environments such as
browsers.
the hazards described in the previous section are avoided or minimized.
approach #1: use an es module wrapper
write the package in commonjs or transpile es module sources into commonjs, and
create an es module wrapper file that defines the named exports. using
, the es module wrapper is used for  and the
commonjs entry point for .
the preceding example uses explicit extensions  and .
if your files use the  extension,  will cause such files
to be treated as es modules, just as  would cause them
to be treated as commonjs.
in this example, the  from  is the same
singleton as the  from . therefore
returns  when comparing the two s and the divergent specifier hazard
is avoided.
if the module is not simply a list of named exports, but rather contains a
unique function or object export like ,
or if support in the wrapper for the  pattern is desired,
then the wrapper would instead be written to export the default optionally
along with any named exports as well:
this approach is appropriate for any of the following use cases:
the package is currently written in commonjs and the author would prefer not
to refactor it into es module syntax, but wishes to provide named exports for
es module consumers.
the package has other packages that depend on it, and the end user might
install both this package and those other packages. for example a
package is used directly in an application, and a  package
adds a few more functions to . because the wrapper exports
underlying commonjs files, it doesn't matter if  is written in
commonjs or es module syntax; it will work either way.
the package stores internal state, and the package author would prefer not to
refactor the package to isolate its state management. see the next section.
a variant of this approach not requiring conditional exports for consumers could
be to add an export, e.g. , to point to an all-es module-syntax
version of the package. this could be used via  by users
who are certain that the commonjs version will not be loaded anywhere in the
application, such as by dependencies; or if the commonjs version can be loaded
but doesn't affect the es module version (for example, because the package is
stateless):
approach #2: isolate state
a  file can define the separate commonjs and es module entry
points directly:
this can be done if both the commonjs and es module versions of the package are
equivalent, for example because one is the transpiled output of the other; and
the package's management of state is carefully isolated (or the package is
stateless).
the reason that state is an issue is because both the commonjs and es module
versions of the package might get used within an application; for example, the
user's application code could  the es module version while a dependency
s the commonjs version. if that were to occur, two copies of the
package would be loaded in memory and therefore two separate states would be
present. this would likely cause hard-to-troubleshoot bugs.
aside from writing a stateless package (if javascript's  were a package,
for example, it would be stateless as all of its methods are static), there are
some ways to isolate state so that it's shared between the potentially loaded
commonjs and es module instances of the package:
if possible, contain all state within an instantiated object. javascript's
, for example, needs to be instantiated to contain state; if it were a
package, it would be used like this:
the  keyword isn't required; a package's function can return a new
object, or modify a passed-in object, to keep the state external to the
isolate the state in one or more commonjs files that are shared between the
commonjs and es module versions of the package. for example, if the commonjs
and es module entry points are  and , respectively:
even if  is used via both  and  in an application (for
example, via  in application code and via  by a dependency)
each reference of  will contain the same state; and modifying that
state from either module system will apply to both.
any plugins that attach to the package's singleton would need to separately
attach to both the commonjs and es module singletons.
the package is currently written in es module syntax and the package author
wants that version to be used wherever such syntax is supported.
the package is stateless or its state can be isolated without too much
difficulty.
the package is unlikely to have other public packages that depend on it, or if
it does, the package is stateless or has state that need not be shared between
dependencies or with the overall application.
even with isolated state, there is still the cost of possible extra code
execution between the commonjs and es module versions of a package.
as with the previous approach, a variant of this approach not requiring
conditional exports for consumers could be to add an export, e.g.
, to point to an all-es module-syntax version of the package:
node.js  field definitions
this section describes the fields used by the node.js runtime. other tools (such
as ) use
additional fields which are ignored by node.js and not documented here.
the following fields in  files are used in node.js:
- relevant when using named imports within a package. also used
by package managers as the name of the package.
- the default module when loading the package, if exports is not
specified, and in versions of node.js prior to the introduction of exports.
- the package manager recommended when contributing to
the package. leveraged by the  shims.
- the package type determining whether to load  files as
commonjs or es modules.
- package exports and conditional exports. when present,
limits which submodules can be loaded from within the package.
- package imports, for use by modules within the package
the  field defines your package's name. publishing to the
npm registry requires a name that satisfies
the  field can be used in addition to the  field to
a package using its name.
the  field defines the entry point of a package when imported by name
via a  lookup.  its value is a path.
when a package has an  field, this will take precedence over the
field when importing the package by name.
it also defines the script that is used when the .
the  field defines which package manager is expected to be
used when working on the current project. it can set to any of the
, and will ensure that your teams use the exact
same package manager versions without having to install anything else than
node.js.
this field is currently experimental and needs to be opted-in; check the
page for details about the procedure.
the  field defines the module format that node.js uses for all
files that have that  file as their nearest parent.
files ending with  are loaded as es modules when the nearest parent
file contains a top-level field  with a value of
the nearest parent  is defined as the first  found
when searching in the current folder, that folder's parent, and so on up
until a node_modules folder or the volume root is reached.
if the nearest parent  lacks a  field, or contains
,  files are treated as . if the volume
root is reached and no  is found,  files are treated as
statements of  files are treated as es modules if the nearest
parent  contains .
regardless of the value of the  field,  files are always treated
as es modules and  files are always treated as commonjs.
type:  |  |
the  field allows defining the  of a package when
imported by name loaded either via a  lookup or a
to its own name. it is supported in node.js 12+ as an
alternative to the  that can support defining
and  while encapsulating internal unexported modules.
can also be used within  to define different
referenced via  or via .
all paths defined in the  must be relative file urls starting with
entries in the imports field must be strings starting with .
package imports permit mapping to external packages.
this field defines  for the current package.
the  module provides an asynchronous network api for creating stream-based
tcp or  servers () and clients
ipc support
the  module supports ipc with named pipes on windows, and unix domain
sockets on other operating systems.
identifying paths for ipc connections
take a  parameter to identify ipc endpoints.
on unix, the local domain is also known as the unix domain. the path is a
filesystem pathname. it gets truncated to an os-dependent length of
. typical values are 107 bytes on linux and
103 bytes on macos. if a node.js api abstraction creates the unix domain socket,
it will unlink the unix domain socket as well. for example,
may create a unix domain socket and
will unlink it. but if a user creates the unix domain
socket outside of these abstractions, the user will need to remove it. the same
applies when a node.js api creates a unix domain socket but the program then
crashes. in short, a unix domain socket will be visible in the filesystem and
will persist until unlinked.
on windows, the local domain is implemented using a named pipe. the path must
refer to an entry in  or . any characters are permitted,
but the latter may do some processing of pipe names, such as resolving
sequences. despite how it might look, the pipe namespace is flat. pipes will
not persist. they are removed when the last reference to them is closed.
unlike unix domain sockets, windows will close and remove the pipe when the
owning process exits.
javascript string escaping requires paths to be specified with extra backslash
escaping such as:
the  object can be used with some network apis to specify rules for
disabling inbound or outbound access to specific ip addresses, ip ranges, or
ip subnets.
|  an ipv4 or ipv6 address.
either  or . default: .
adds a rule to block the given ip address.
|  the starting ipv4 or ipv6 address in the
range.
|  the ending ipv4 or ipv6 address in the range.
adds a rule to block a range of ip addresses from  (inclusive) to
(inclusive).
|  the network ipv4 or ipv6 address.
the number of cidr prefix bits. for ipv4, this
must be a value between  and . for ipv6, this must be between
adds a rule to block a range of ip addresses specified as a subnet mask.
|  the ip address to check
returns  if the given ip address matches any of the rules added to the
the list of rules added to the blocklist.
added in: v15.14.0, v14.18.0
the network address as either an ipv4 or ipv6 string.
default:  if  is ;  if  is
one of either  or .
an ipv6 flow-label used only if  is .
an ip port.
type
type  either  or .
this class is used to create a tcp or  server.
automatically set as a listener for the
is an  with the following events:
emitted when the server closes. if connections exist, this
event is not emitted until all connections are ended.
the connection object
emitted when a new connection is made.  is an instance of
emitted when an error occurs. unlike , the
event will not be emitted directly following this event unless
is manually called. see the example in discussion of
emitted when the server has been bound after calling .
when the number of connections reaches the threshold of ,
the server will drop new connections and emit  event instead. if it is a
tcp server, the argument is as follows, otherwise the argument is .
the argument passed to event listener.
local address.
local port.
local family.
remote address.
remote port.
remote ip family.  or .
returns the bound , the address  name, and  of the server
as reported by the operating system if listening on an ip socket
(useful to find which port was assigned when getting an os-assigned address):
for a server listening on a pipe or unix domain socket, the name is returned
as a string.
returns  before the  event has been
emitted or after calling .
called when the server is closed.
stops the server from accepting new connections and keeps existing
connections. this function is asynchronous, the server is finally closed
when all connections are ended and the server emits a  event.
the optional  will be called once the  event occurs. unlike
that event, it will be called with an  as its only argument if the server
was not open when it was closed.
added in: v0.9.7
asynchronously get the number of concurrent connections on the server. works
when sockets were sent to forks.
callback should take two arguments  and .
start a server listening for connections. a  can be a tcp or
an  server depending on what it listens to.
possible signatures:
for  servers
for tcp servers
this function is asynchronous. when the server starts listening, the
event will be emitted. the last parameter
will be added as a listener for the  event.
all  methods can take a  parameter to specify the maximum
length of the queue of pending connections. the actual length will be determined
by the os through sysctl settings such as  and
on linux. the default value of this parameter is 511 (not 512).
all  are set to  (see  for
details).
the  method can be called again if and only if there was an
error during the first  call or  has been
called. otherwise, an  error will be thrown.
one of the most common errors raised when listening is .
this happens when another server is already listening on the requested
//. one way to handle this would be to retry
after a certain amount of time:
common parameter of  functions
start a server listening for connections on a given  that has
already been bound to a port, a unix domain socket, or a windows named pipe.
the  object can be either a server, a socket (anything with an
underlying  member), or an object with an  member that is a
valid file descriptor.
listening on a file descriptor is not supported on windows.
required. supports the following properties:
will be ignored if  is specified. see
common parameter of
for ipc servers makes the pipe readable
for all users. default: .
for ipc servers makes the pipe writable
for tcp servers, setting  to  will
disable dual-stack support, i.e., binding to host  won't make
be bound. default: .
an abortsignal that may be used to close a listening server.
if  is specified, it behaves the same as
otherwise, if  is specified, it behaves the same as
if none of them is specified, an error will be thrown.
if  is  (default), then cluster workers will use the same
underlying handle, allowing connection handling duties to be shared. when
is , the handle is not shared, and attempted port sharing
results in an error. an example which listens on an exclusive port is
shown below.
when  is  and the underlying handle is shared, it is
possible that several workers query a handle with different backlogs.
in this case, the first  passed to the master process will be used.
starting an ipc server as root may cause the server path to be inaccessible for
unprivileged users. using  and  will make the server
accessible for all users.
is similar to calling  on the server:
path the server should listen to. see
common parameter of  functions.
start an  server listening for connections on the given .
start a tcp server listening for connections on the given  and .
if  is omitted or is 0, the operating system will assign an arbitrary
unused port, which can be retrieved by using
after the  event has been emitted.
if  is omitted, the server will accept connections on the
() when ipv6 is available, or the
() otherwise.
in most operating systems, listening to the  ()
may cause the  to also listen on the
added in: v0.2.0
set this property to reject connections when the server's connection count gets
high.
it is not recommended to use this option once a socket has been sent to a child
opposite of , calling  on a previously ed server will
not let the program exit if it's the only server left (the default behavior).
if the server is ed calling  again will have no effect.
calling  on a server will allow the program to exit if this is the only
active server in the event system. if the server is already ed calling
again will have no effect.
this class is an abstraction of a tcp socket or a streaming  endpoint
(uses named pipes on windows, and unix domain sockets otherwise). it is also
a  can be created by the user and used directly to interact with
a server. for example, it is returned by ,
so the user can use it to talk to the server.
it can also be created by node.js and passed to the user when a connection
is received. for example, it is passed to the listeners of a
event emitted on a , so the user can use
it to interact with the client.
available options are:
if specified, wrap around an existing socket with
the given file descriptor, otherwise a new socket will be created.
if set to , then the socket will
automatically end the writable side when the readable side ends. see
and the  event for details. default:
allow reads on the socket when an  is passed,
otherwise ignored. default: .
allow writes on the socket when an  is passed,
an abort signal that may be used to destroy the
socket.
creates a new socket object.
the newly created socket can be either a tcp socket or a streaming
endpoint, depending on what it  to.
if the socket had a transmission error.
emitted once the socket is fully closed. the argument  is a boolean
which says if the socket was closed due to a transmission error.
emitted when a socket connection is successfully established.
emitted when data is received. the argument  will be a  or
. encoding of data is set by .
the data will be lost if there is no listener when a
emits a  event.
emitted when the write buffer becomes empty. can be used to throttle uploads.
see also: the return values of .
emitted when the other end of the socket signals the end of transmission, thus
ending the readable side of the socket.
by default ( is ) the socket will send an end of
transmission packet back and destroy its file descriptor once it has written out
its pending write queue. however, if  is set to , the
socket will not automatically  its writable side,
allowing the user to write arbitrary amounts of data. the user must call
explicitly to close the connection (i.e. sending a
fin packet back).
emitted when an error occurs. the  event will be called directly
following this event.
emitted after resolving the host name but before connecting.
not applicable to unix sockets.
|  the error object. see .
the ip address.
|  the address type. see .
the host name.
emitted when a socket is ready to be used.
triggered immediately after .
emitted if the socket times out from inactivity. this is only to notify that
the socket has been idle. the user must manually close the connection.
returns the bound , the address  name and  of the
socket as reported by the operating system:
added in: v0.3.8deprecated since: v14.6.0
this property shows the number of characters buffered for writing. the buffer
may contain strings whose length after encoding is not yet known. so this number
is only an approximation of the number of bytes in the buffer.
has the property that  always works. this is to
help users get up and running quickly. the computer cannot always keep up
with the amount of data that is written to a socket. the network connection
simply might be too slow. node.js will internally queue up the data written to a
socket and send it out over the wire when it is possible.
the consequence of this internal buffering is that memory may grow.
users who experience large or growing  should attempt to
"throttle" the data flows in their program with
the amount of received bytes.
the amount of bytes sent.
initiate a connection on a given socket.
for  connections.
for tcp connections.
returns:  the socket itself.
this function is asynchronous. when the connection is established, the
event will be emitted. if there is a problem connecting,
instead of a  event, an  event will be emitted with
the error passed to the  listener.
the last parameter , if supplied, will be added as a listener
for the  event once.
this function should only be used for reconnecting a socket after
has been emitted or otherwise it may lead to undefined
methods. will be added as a listener for the  event once.
initiate a connection on a given socket. normally this method is not needed,
the socket should be created and opened with . use
this only when implementing a custom socket.
for tcp connections, available  are:
required. port the socket should connect to.
host the socket should connect to. default: .
local address the socket should connect from.
local port the socket should connect from.
: version of ip stack. must be , , or . the value
indicates that both ipv4 and ipv6 addresses are allowed. default: .
if set to , it disables the use of nagle's algorithm immediately
after the socket is established. default: .
if set to , it enables keep-alive functionality on the socket
immediately after the connection is established, similarly on what is done in
if set to a positive number, it sets the initial delay before
the first keepalive probe is sent on an idle socket.default: .
for  connections, available  are:
required. path the client should connect to.
see . if provided, the tcp-specific
options above are ignored.
for both types, available  include:
if specified, incoming data is stored in a single
and passed to the supplied  when data arrives on the socket.
this will cause the streaming functionality to not provide any data.
the socket will emit events like , , and
as usual. methods like  and  will also behave as
expected.
|  |  either a reusable chunk of memory to
use for storing incoming data or a function that returns such.
this function is called for every chunk of incoming
data. two arguments are passed to it: the number of bytes written to
and a reference to . return  from this function to
implicitly  the socket. this function will be executed in the
global context.
following is an example of a client using the  option:
path the client should connect to. see
initiate an  connection on the given socket.
alias to
called with  as .
port the client should connect to.
host the client should connect to.
initiate a tcp connection on the given socket.
added in: v6.1.0
if ,
called and has not yet finished. it will stay  until the socket becomes
connected, then it is set to  and the  event is emitted. note
that the
callback is a listener for the  event.
ensures that no more i/o activity happens on this socket.
destroys the stream and closes the connection.
indicates if the connection is destroyed or not. once a
connection is destroyed no further data can be transferred using it.
only used when data is . default: .
optional callback for when the socket is finished.
half-closes the socket. i.e., it sends a fin packet. it is possible the
server will still send some data.
added in: v0.9.6
the string representation of the local ip address the remote client is
connecting on. for example, in a server listening on , if a client
connects on , the value of  would be
the numeric representation of the local port. for example,  or .
the string representation of the local ip family.  or .
pauses the reading of data. that is,  events will not be emitted.
useful to throttle back an upload.
this is  if the socket is not connected yet, either because
has not yet been called or because it is still in the process of connecting
opposite of , calling  on a previously ed socket will
not let the program exit if it's the only socket left (the default behavior).
if the socket is ed calling  again will have no effect.
the string representation of the remote ip address. for example,
or . value may be  if
the socket is destroyed (for example, if the client disconnected).
the string representation of the remote ip family.  or .
the numeric representation of the remote port. for example,  or .
close the tcp connection by sending an rst packet and destroy the stream.
if this tcp socket is in connecting status, it will send an rst packet and destroy this tcp socket once it is connected.
otherwise, it will call  with an  error.
if this is not a tcp socket (for example, a pipe), calling this method will immediately throw an  error.
resumes reading after a call to .
set the encoding for the socket as a . see
enable/disable keep-alive functionality, and optionally set the initial
delay before the first keepalive probe is sent on an idle socket.
set  (in milliseconds) to set the delay between the last
data packet received and the first keepalive probe. setting  for
will leave the value unchanged from the default
(or previous) setting.
enabling the keep-alive functionality will set the following socket options:
enable/disable the use of nagle's algorithm.
when a tcp connection is created, it will have nagle's algorithm enabled.
nagle's algorithm delays data before it is sent via the network. it attempts
to optimize throughput at the expense of latency.
passing  for  or not passing an argument will disable nagle's
algorithm for the socket. passing  for  will enable nagle's
sets the socket to timeout after  milliseconds of inactivity on
the socket. by default  do not have a timeout.
when an idle timeout is triggered the socket will receive a
event but the connection will not be severed. the user must manually call
or  to end the connection.
if  is 0, then the existing idle timeout is disabled.
the optional  parameter will be added as a one-time listener for the
the socket timeout in milliseconds as set by .
it is  if a timeout has not been set.
calling  on a socket will allow the program to exit if this is the only
active socket in the event system. if the socket is already ed calling
sends data on the socket. the second parameter specifies the encoding in the
case of a string. it defaults to utf8 encoding.
will be emitted when the buffer is again free.
the optional  parameter will be executed when the data is finally
written out, which may not be immediately.
see  stream  method for more
this property represents the state of the connection as a string.
if the stream is connecting  is .
if the stream is readable and writable, it is .
if the stream is readable and not writable, it is .
if the stream is not readable and writable, it is .
aliases to
a factory function, which creates a new ,
immediately initiates connection with ,
then returns the  that starts the connection.
when the connection is established, a  event will be emitted
on the returned socket. the last parameter , if supplied,
will be added as a listener for the  event once.
the  function is an alias to this function.
required. will be passed to both the
call and the
common parameter of the
functions. if supplied, will be added as
a listener for the  event on the returned socket once.
returns:  the newly created socket used to start the connection.
for available options, see
additional options:
if set, will be used to call
after the socket is created, but before
it starts the connection.
following is an example of a client of the echo server described
in the  section:
to connect on the socket :
path the socket should connect to. will be passed to
functions, an "once" listener for the
event on the initiating socket. will be passed to
initiates an  connection.
this function creates a new  with all options set to default,
immediately initiates connection with
port the socket should connect to. will be passed to
host the socket should connect to. will be passed to
initiates a tcp connection.
automatically end the writable side when the readable side ends.
indicates whether the socket should be
paused on incoming connections. default: .
after a new incoming connection is received. default: .
immediately after a new incoming connection is received, similarly on what is done in
creates a new tcp or  server.
if  is set to , when the other end of the socket
signals the end of transmission, the server will only send back the end of
transmission when  is explicitly called. for example, in the
context of tcp, when a fin packed is received, a fin packed is sent
back only when  is explicitly called. until then the
connection is half-closed (non-readable but still writable). see
event and  (section 4.2.2.13) for more information.
if  is set to , then the socket associated with each
incoming connection will be paused, and no data will be read from its handle.
this allows connections to be passed between processes without any data being
read by the original process. to begin reading data from a paused socket, call
the server can be a tcp server or an  server, depending on what it
here is an example of a tcp echo server which listens for connections
on port 8124:
test this by using :
to listen on the socket :
use  to connect to a unix domain socket server:
returns  if  is an ipv6 address. returns  if  is an ipv4
address in  with no leading zeroes. otherwise, returns
returns  if  is an ipv4 address in  with no
leading zeroes. otherwise, returns .
returns  if  is an ipv6 address. otherwise, returns .
the  module provides operating system-related utility methods and
properties. it can be accessed using:
the operating system-specific end-of-line marker.
on posix
on windows
returns the operating system cpu architecture for which the node.js binary was
compiled. possible values are , , , ,
, , , , , and .
the return value is equivalent to .
contains commonly used operating system-specific constants for error codes,
process signals, and so on. the specific constants defined are described in
added in: v0.3.3
returns an array of objects containing information about each logical cpu core.
the properties included on each object include:
(in mhz)
the number of milliseconds the cpu has spent in user mode.
the number of milliseconds the cpu has spent in nice mode.
the number of milliseconds the cpu has spent in sys mode.
the number of milliseconds the cpu has spent in idle mode.
the number of milliseconds the cpu has spent in irq mode.
values are posix-only. on windows, the  values of all processors
are always 0.
added in: v16.3.0, v14.18.0
the platform-specific file path of the null device.
returns a string identifying the endianness of the cpu for which the node.js
binary was compiled.
possible values are  for big endian and  for little endian.
returns the amount of free system memory in bytes as an integer.
the process id to retrieve scheduling priority for.
returns the scheduling priority for the process specified by . if  is
not provided or is , the priority of the current process is returned.
returns the string path of the current user's home directory.
on posix, it uses the  environment variable if defined. otherwise it
uses the  to look up the user's home directory.
on windows, it uses the  environment variable if defined.
otherwise it uses the path to the profile directory of the current user.
returns the host name of the operating system as a string.
returns an array containing the 1, 5, and 15 minute load averages.
the load average is a measure of system activity calculated by the operating
system and expressed as a fractional number.
the load average is a unix-specific concept. on windows, the return value is
returns an object containing network interfaces that have been assigned a
network address.
each key on the returned object identifies a network interface. the associated
value is an array of objects that each describe an assigned network address.
the properties available on the assigned network address object include:
the assigned ipv4 or ipv6 address
the ipv4 or ipv6 network mask
either  or
the mac address of the network interface
if the network interface is a loopback or
similar interface that is not remotely accessible; otherwise
the numeric ipv6 scope id (only specified when
is )
the assigned ipv4 or ipv6 address with the routing prefix
in cidr notation. if the  is invalid, this property is set
to .
returns a string identifying the operating system platform for which
the node.js binary was compiled. the value is set at compile time.
possible values are , , ,,
the value  may also be returned if node.js is built on the android
operating system. .
returns the operating system as a string.
on posix systems, the operating system release is determined by calling
. on windows,  is used. see
the process id to set scheduling priority for.
the scheduling priority to assign to the process.
attempts to set the scheduling priority for the process specified by . if
is not provided or is , the process id of the current process is used.
the  input must be an integer between  (high priority) and
(low priority). due to differences between unix priority levels and windows
priority classes,  is mapped to one of six priority constants in
. when retrieving a process priority level, this range
mapping may cause the return value to be slightly different on windows. to avoid
confusion, set  to one of the priority constants.
on windows, setting priority to  requires elevated user
privileges. otherwise the set priority will be silently reduced to
returns the operating system's default directory for temporary files as a
returns the total amount of system memory in bytes as an integer.
returns the operating system name as returned by . for example, it
returns  on linux,  on macos, and  on windows.
see  for additional information
about the output of running  on various operating systems.
returns the system uptime in number of seconds.
character encoding used to interpret resulting strings.
if  is set to , the , , and
values will be  instances. default: .
returns information about the currently effective user. on posix platforms,
this is typically a subset of the password file. the returned object includes
the , , , , and . on windows, the  and
fields are , and  is .
the value of  returned by  is provided by the operating
system. this differs from the result of , which queries
environment variables for the home directory before falling back to the
operating system response.
throws a  if a user has no  or .
returns a string identifying the kernel version.
. on windows,  is used, and if it is not
available,  will be used. see
added in: v18.9.0
returns the machine type as a string, such as , , ,
, , , , , , , .
on posix systems, the machine type is determined by calling
os constants
the following constants are exported by .
not all constants will be available on every operating system.
signal constants
the following signal constants are exported by .
error constants
the following error constants are exported by .
posix error constants
windows-specific error constants
the following error codes are specific to the windows operating system.
dlopen constants
if available on the operating system, the following constants
are exported in . see  for detailed
priority constants
the following process scheduling constants are exported by
libuv constants
path
the  module provides utilities for working with file and directory
paths. it can be accessed using:
windows vs. posix
the default operation of the  module varies based on the operating
system on which a node.js application is running. specifically, when running on
a windows operating system, the  module will assume that
windows-style paths are being used.
so using  might yield different results on posix and windows:
on posix:
on windows:
to achieve consistent results when working with windows file paths on any
operating system, use :
on posix and windows:
to achieve consistent results when working with posix file paths on any
on windows node.js follows the concept of per-drive working directory.
this behavior can be observed when using a drive path without a backslash. for
example,  can potentially return a different result than
an optional suffix to remove
the  method returns the last portion of a , similar to
the unix  command. trailing  are
although windows usually treats file names, including file extensions, in a
case-insensitive manner, this function does not. for example,  and
refer to the same file, but  treats the extension as a
case-sensitive string:
a  is thrown if  is not a string or if  is given
and is not a string.
provides the platform-specific path delimiter:
for windows
for posix
for example, on posix:
the  method returns the directory name of a , similar to
the unix  command. trailing directory separators are ignored, see
a  is thrown if  is not a string.
the  method returns the extension of the , from the last
occurrence of the  (period) character to end of string in the last portion of
the . if there is no  in the last portion of the , or if
there are no  characters other than the first character of
the basename of  (see ) , an empty string is returned.
any javascript object having the following properties:
the  method returns a path string from an object. this is the
opposite of .
when providing properties to the  remember that there are
combinations where one property has priority over another:
is ignored if  is provided
and  are ignored if  exists
the  method determines if  is an absolute path.
if the given  is a zero-length string,  will be returned.
a sequence of path segments
the  method joins all given  segments together using the
platform-specific separator as a delimiter, then normalizes the resulting path.
zero-length  segments are ignored. if the joined path string is a
zero-length string then  will be returned, representing the current
working directory.
a  is thrown if any of the path segments is not a string.
added in: v0.1.23
the  method normalizes the given , resolving  and
segments.
when multiple, sequential path segment separation characters are found (e.g.
on posix and either  or  on windows), they are replaced by a single
instance of the platform-specific path segment separator ( on posix and
on windows). trailing separators are preserved.
if the  is a zero-length string,  is returned, representing the
current working directory.
since windows recognizes multiple path separators, both separators will be
replaced by instances of the windows preferred separator ():
the  method returns an object whose properties represent
significant elements of the . trailing directory separators are ignored,
the returned object will have the following properties:
the  property provides access to posix specific implementations
of the  methods.
the api is accessible via  or .
the  method returns the relative path from  to  based
on the current working directory. if  and  each resolve to the same
path (after calling  on each), a zero-length string is returned.
if a zero-length string is passed as  or , the current working
directory will be used instead of the zero-length strings.
a  is thrown if either  or  is not a string.
a sequence of paths or path segments
the  method resolves a sequence of paths or path segments into
an absolute path.
the given sequence of paths is processed from right to left, with each
subsequent  prepended until an absolute path is constructed.
for instance, given the sequence of path segments: , , ,
calling  would return
because  is not an absolute path but  is.
if, after processing all given  segments, an absolute path has not yet
been generated, the current working directory is used.
the resulting path is normalized and trailing slashes are removed unless the
path is resolved to the root directory.
zero-length  segments are ignored.
if no  segments are passed,  will return the absolute path
of the current working directory.
a  is thrown if any of the arguments is not a string.
provides the platform-specific path segment separator:
on windows, both the forward slash () and backward slash () are accepted
as path segment separators; however, the  methods only add backward
slashes ().
on windows systems only, returns an equivalent  for
the given . if  is not a string,  will be returned without
modifications.
this method is meaningful only on windows systems. on posix systems, the
method is non-operational and always returns  without modifications.
the  property provides access to windows-specific implementations
performance measurement apis
this module provides an implementation of a subset of the w3c
as well as additional apis for
node.js-specific performance measurements.
node.js supports the following :
an object that can be used to collect performance metrics from the current
node.js instance. it is similar to  in browsers.
if  is not provided, removes all  objects from the
performance timeline. if  is provided, removes only the named mark.
performance timeline. if  is provided, removes only the named measure.
if  is not provided, removes all  objects from
the resource timeline. if  is provided, removes only the named resource.
the result of a previous call to
prior to .
the  method returns an object that contains the
cumulative duration of time the event loop has been both idle and active as a
high resolution milliseconds timer. the  value is the calculated
event loop utilization (elu).
if bootstrapping has not yet finished on the main thread the properties have
the value of . the elu is immediately available on  since
bootstrap happens within the event loop.
both  and  are optional parameters.
if  is passed, then the delta between the current call's
and  times, as well as the corresponding  value are
calculated and returned (similar to ).
if  and  are both passed, then the delta is
calculated between the two arguments. this is a convenience option because,
unlike , calculating the elu is more complex than a
single subtraction.
elu is similar to cpu utilization, except that it only measures event loop
statistics and not cpu usage. it represents the percentage of time the event
loop has spent outside the event loop's event provider (e.g. ).
no other cpu idle time is taken into consideration. the following is an example
of how a mostly idle process will have a high elu.
although the cpu is mostly idle while running this script, the value of
is . this is because the call to
blocks the event loop from proceeding.
passing in a user-defined object instead of the result of a previous call to
will lead to undefined behavior. the return values
are not guaranteed to reflect any correct state of the event loop.
returns a list of  objects in chronological order with
respect to . if you are only interested in
performance entries of certain types or that have certain names, see
returns a list of  objects in chronological order
with respect to  whose  is
equal to , and optionally, whose  is equal to
with respect to  whose
is equal to .
additional optional detail to include with the mark.
an optional timestamp to be used as the mark time.
creates a new  entry in the performance timeline. a
is a subclass of  whose
is always , and whose
is always . performance marks are used
to mark specific significant moments in the performance timeline.
the created  entry is put in the global performance timeline
and can be queried with ,
, and . when the
observation is performed, the entries should be cleared from the global
performance timeline manually with .
the resource url
the initiator name, e.g: 'fetch'
the cache mode must be an empty string ('') or 'local'
this property is an extension by node.js. it is not available in web browsers.
creates a new  entry in the resource timeline. a
is always . performance resources
are used to mark moments in the resource timeline.
the created  entry is put in the global resource timeline
|  optional.
additional optional detail to include with the measure.
duration between start and end times.
|  timestamp to be used as the end time, or a string
identifying a previously recorded mark.
|  timestamp to be used as the start time, or a string
optional. must be omitted if  is an
measures the number of milliseconds elapsed since
the  argument may identify any existing  in the
performance timeline, or may identify any of the timestamp properties
provided by the  class. if the named  does
not exist, an error is thrown.
the optional  argument must identify any existing
in the performance timeline or any of the timestamp properties provided by the
class.  will be
if no parameter is passed, otherwise if the named  does not exist, an
an instance of the  class that provides performance
metrics for specific node.js operational milestones.
returns the current high resolution millisecond timestamp, where 0 represents
the start of the current  process.
sets the global performance resource timing buffer size to the specified number
of "resource" type performance entry objects.
by default the max buffer size is set to 250.
the  specifies the high resolution millisecond timestamp at
which the current  process began, measured in unix time.
a histogram object created using
that will record runtime durations in
nanoseconds.
wraps a function within a new function that measures the running time of the
wrapped function. a  must be subscribed to the
event type in order for the timing details to be accessed.
if the wrapped function returns a promise, a finally handler will be attached
to the promise and the duration will be reported once the finally handler is
added in: v16.1.0
an object which is json representation of the  object. it
is similar to  in browsers.
the  event is fired when the global performance
resource timing buffer is full. adjust resource timing buffer size with
or clear the buffer with
in the event listener to allow
more entries to be added to the performance timeline buffer.
additional detail specific to the .
the total number of milliseconds elapsed for this entry. this value will not
be meaningful for all performance entry types.
the type of the performance entry. it may be one of:
(node.js only)
(available on the web)
when  is equal to , the
property contains additional information about garbage collection operation.
the value may be one of:
the name of the performance entry.
property identifies the type of garbage collection operation that occurred.
the high resolution millisecond timestamp marking the starting time of the
performance entry.
garbage collection ('gc') details
property will be an  with two properties:
one of:
http ('http') details
property will be an  containing additional information.
if  is equal to , the
will contain the following properties: , . and the  property
will be an  containing , , , the  property
will be an  containing , , .
this could add additional memory overhead and should only be used for
diagnostic purposes, not left turned on in production by default.
http/2 ('http2') details
property will be an  containing
additional performance information.
will contain the following properties:
if  is equal to , the  will
contain the following properties:
timerify ('function') details
property will be an  listing
the input arguments to the timed function.
net ('net') details
additional information.
will contain the following properties: , .
dns ('dns') details
will contain the following properties: , , , ,
contain the following properties: , , , .
if  is equal to  or , the  will
contain the following properties: , , . the value of  is
same as the result of  or .
provides timing details for node.js itself. the constructor of this class
is not exposed to users.
the high resolution millisecond timestamp at which the node.js process
completed bootstrapping. if bootstrapping has not yet finished, the property
has the value of -1.
the high resolution millisecond timestamp at which the node.js environment was
initialized.
the high resolution millisecond timestamp of the amount of time the event loop
has been idle within the event loop's event provider (e.g. ). this
does not take cpu usage into consideration. if the event loop has not yet
started (e.g., in the first tick of the main script), the property has the
value of 0.
the high resolution millisecond timestamp at which the node.js event loop
exited. if the event loop has not yet exited, the property has the value of -1.
it can only have a value of not -1 in a handler of the  event.
started. if the event loop has not yet started (e.g., in the first tick of the
main script), the property has the value of -1.
the high resolution millisecond timestamp at which the node.js process was
the high resolution millisecond timestamp at which the v8 platform was
provides detailed network timing data regarding the loading of an application's
resources.
the constructor of this class is not exposed to users directly.
the high resolution millisecond timestamp at immediately before dispatching
the  request. if the resource is not intercepted by a worker the property
will always return 0.
the high resolution millisecond timestamp that represents the start time
of the fetch which initiates the redirect.
the high resolution millisecond timestamp that will be created immediately after
receiving the last byte of the response of the last redirect.
the high resolution millisecond timestamp immediately before the node.js starts
to fetch the resource.
the domain name lookup for the resource.
the high resolution millisecond timestamp representing the time immediately
after the node.js finished the domain name lookup for the resource.
before node.js starts to establish the connection to the server to retrieve
the resource.
after node.js finishes establishing the connection to the server to retrieve
before node.js starts the handshake process to secure the current connection.
before node.js receives the first byte of the response from the server.
after node.js receives the last byte of the resource or immediately before
the transport connection is closed, whichever comes first.
a number representing the size (in octets) of the fetched resource. the size
includes the response header fields plus the response payload body.
a number representing the size (in octets) received from the fetch
(http or cache), of the payload body, before removing any applied
content-codings.
(http or cache), of the message body, after removing any applied
returns a  that is the json representation of the
objects provide notifications when new
instances have been added to the performance timeline.
because  instances introduce their own additional
performance overhead, instances should not be left subscribed to notifications
indefinitely. users should disconnect observers as soon as they are no
longer needed.
the  is invoked when a  is
notified about new  instances. the callback receives a
instance and a reference to the
disconnects the  instance from all notifications.
a single  type. must not be given
if  is already specified.
an array of strings identifying the types of
instances the observer is interested in. if not
provided an error will be thrown.
if true, the observer callback is called with a
list global  buffered entries. if false, only
s created after the time point are sent to the
observer callback. default: .
subscribes the  instance to notifications of new
instances identified either by
the  class is used to provide access to the
instances passed to a .
the constructor of this class is not exposed to users.
with respect to .
|  the lowest discernible value. must be an integer
value greater than 0. default: .
|  the highest recordable value. must be an integer
value that is equal to or greater than two times .
the number of accuracy digits. must be a number between
and . default: .
returns a .
added in: v11.10.0
the sampling rate in milliseconds. must be greater
than zero. default: .
creates an  object that samples and reports the event loop
delay over time. the delays will be reported in nanoseconds.
using a timer to detect approximate event loop delay works because the
execution of timers is tied specifically to the lifecycle of the libuv
event loop. that is, a delay in the loop will cause a delay in the execution
of the timer, and those delays are specifically what this api is intended to
detect.
the number of samples recorded by the histogram.
the number of times the event loop delay exceeded the maximum 1 hour event
loop delay threshold.
the maximum recorded event loop delay.
the mean of the recorded event loop delays.
the minimum recorded event loop delay.
a percentile value in the range (0, 100].
returns the value at the given percentile.
returns a  object detailing the accumulated percentile distribution.
resets the collected histogram data.
the standard deviation of the recorded event loop delays.
a  that is periodically updated on a given interval.
disables the update interval timer. returns  if the timer was
stopped,  if it was already stopped.
enables the update interval timer. returns  if the timer was
started,  if it was already started.
cloning an
instances can be cloned via . on the receiving
end, the histogram is cloned as a plain  object that does not
implement the  and  methods.
adds the values from  to this histogram.
|  the amount to record in the histogram.
calculates the amount of time (in nanoseconds) that has passed since the
previous call to  and records that amount in the histogram.
measuring the duration of async operations
the following example uses the  and performance apis to measure
the actual duration of a timeout operation (including the amount of time it took
to execute the callback).
measuring how long it takes to load dependencies
the following example measures the duration of  operations to load
dependencies:
measuring how long one http round-trip takes
the following example is used to trace the time spent by http client
() and http request (). for http client,
it means the time interval between starting the request and receiving the
response, and for http request, it means the time interval between receiving
the request and sending the response:
measuring how long the  (only for tcp) takes when the connection is successful
measuring how long the dns takes when the request is successful
permissions
permissions can be used to control what system resources the
node.js process has access to or what actions the process can take
with those resources. permissions can also control what modules can
be accessed by other modules.
control which files
or urls are available to other modules during application execution.
this can be used to control what modules can be accessed by third-party
dependencies, for example.
if you find a potential security vulnerability, please refer to our
module-based permissions
policies
node.js contains experimental support for creating policies on loading code.
policies are a security feature intended to allow guarantees
about what code node.js is able to load. the use of policies assumes
safe practices for the policy files such as ensuring that policy
files cannot be overwritten by the node.js application by using
file permissions.
a best practice would be to ensure that the policy manifest is read-only for
the running node.js application and that the file cannot be changed
by the running node.js application in any way. a typical setup would be to
create the policy file as a different user id than the one running node.js
and granting read permissions to the user id running node.js.
the  flag can be used to enable features for policies
when loading modules.
once this has been set, all modules must conform to a policy manifest file
passed to the flag:
the policy manifest will be used to enforce constraints on code loaded by
to mitigate tampering with policy files on disk, an integrity for
the policy file itself may be provided via .
this allows running  and asserting the policy file contents
even if the file is changed on disk.
error behavior
when a policy check fails, node.js by default will throw an error.
it is possible to change the error behavior to one of a few possibilities
by defining an "onerror" field in a policy manifest. the following values are
available to change the behavior:
: will exit the process immediately.
no cleanup code will be allowed to run.
: will log the error at the site of the failure.
: will throw a js error at the site of the failure. this is the
integrity checks
policy files must use integrity checks with subresource integrity strings
compatible with the browser
associated with absolute urls.
when using  or  all resources involved in loading are checked
for integrity if a policy manifest has been specified. if a resource does not
match the integrity listed in the manifest, an error will be thrown.
an example policy file that would allow loading a file :
each resource listed in the policy manifest can be of one the following
formats to determine its location:
a  to a resource from the manifest such as , , or .
a complete url string to a resource such as .
when loading resources the entire url must match including search parameters
and hash fragment.  will not be used when attempting to load
and vice versa.
to generate integrity strings, a script such as
integrity can be specified as the boolean value  to accept any
body for the resource which can be useful for local development. it is not
recommended in production since it would allow unexpected alteration of
resources to be considered valid.
dependency redirection
an application may need to ship patched versions of modules or to prevent
modules from allowing all modules access to all other modules. redirection
can be used by intercepting attempts to load the modules wishing to be
replaced.
the dependencies are keyed by the requested specifier string and have values
of either , , a string pointing to a module to be resolved,
or a conditions object.
the specifier string does not perform any searching and must match exactly what
is provided to the  or  except for a canonicalization step.
therefore, multiple specifiers may be needed in the policy if it uses multiple
different strings to point to the same module (such as excluding the extension).
specifier strings are canonicalized but not resolved prior to be used for
matching in order to have some compatibility with import maps, for example if a
resource  was given the following redirection from a
policy located at :
any specifier used to load  would then be intercepted
and redirected to  instead regardless of using an
absolute or relative specifier. however, if a specifier that is not an absolute
or relative url string is used, it would not be intercepted. so, if an import
such as  was used, it would not be intercepted.
if the value of the redirection is , a "dependencies" field at the top of
the policy file will be used. if that field at the top of the policy file is
the default node searching algorithms are used to find the module.
if the value of the redirection is a string, it is resolved relative to
the manifest and then immediately used without searching.
any specifier string for which resolution is attempted and that is not listed in
the dependencies results in an error according to the policy.
redirection does not prevent access to apis through means such as direct access
to  or through  which allow access to
loading modules. policy redirection only affects specifiers to  and
. other means, such as to prevent undesired access to apis through
variables, are necessary to lock down that path of loading modules.
a boolean value of  for the dependencies map can be specified to allow a
module to load any specifier without redirection. this can be useful for local
development and may have some valid usage in production, but should be used
only with care after auditing a module to ensure its behavior is valid.
similar to  in , dependencies can also be specified to
be objects containing conditions which branch how dependencies are loaded. in
the preceding example,  is allowed when the  condition is
part of loading it.
a value of  for the resolved value causes the resolution to fail. this
can be used to ensure some kinds of dynamic access are explicitly prevented.
unknown values for the resolved module location cause failures but are
not guaranteed to be forward compatible.
example: patched dependency
redirected dependencies can provide attenuated or modified functionality as fits
the application. for example, log data about timing of function durations by
wrapping the original:
scopes
use the  field of a manifest to set configuration for many resources
at once. the  field works by matching resources by their segments.
if a scope or resource includes , unknown specifiers will
be searched for in their containing scope. the containing scope for cascading
is found by recursively reducing the resource url by removing segments for
, keeping trailing  suffixes, and removing the query and
hash fragment. this leads to the eventual reduction of the url to its origin.
if the url is non-special the scope will be located by the url's origin. if no
scope is found for the origin or in the case of opaque origins, a protocol
string can be used as a scope. if no scope is found for the url's protocol, a
final empty string  scope will be used.
note,  urls adopt their origin from the path they contain, and so a scope
of  will have no effect since no url can have an
origin of ; urls starting with
will use  for its origin and
thus  for its protocol scope. for opaque origin  urls they will
have  for their protocol scope since they do not adopt origins.
given a file located at , the following scopes would
be checked in order:
this determines the policy for all file based resources within
. this is not in the  field of the policy and
would be skipped. adding this scope to the policy would cause it to be used
prior to the  scope.
. this is in the  field of the policy and it would
determine the policy for the resource at . if the
scope has , any unsatisfied queries about the resource would
delegate to the next relevant scope for , .
this determines the policy for all file based resources within .
this is not in the  field of the policy and would be skipped. it would
not be used for  unless  is set to
cascade or is not in the  of the policy.
this determines the policy for all file based resources on the . this
is not in the  field of the policy and would be skipped. it would not
be used for  unless  is set to cascade
or is not in the  of the policy.
this determines the policy for all file based resources. it would not be used
for  unless  is set to cascade or is not
in the  of the policy.
this determines the policy for all resources. it would not be used for
unless  is set to cascade.
integrity using scopes
setting an integrity to  on a scope will set the integrity for any
resource not found in the manifest to .
resource not found in the manifest to fail matching.
not including an integrity is the same as setting the integrity to .
for integrity checks will be ignored if  is explicitly
the following example allows loading any file:
dependency redirection using scopes
the following example, would allow access to  for all resources within
the following example, would allow access to  for all  resources:
example:  emulation
given an import map:
import maps assume you can get any resource by default. this means
at the top level of the policy should be set to .
policies require this to be opt-in since it enables all resources of the
application cross linkage which doesn't make sense for many scenarios. they also
assume any given scope has access to any scope above its allowed dependencies;
all scopes emulating import maps must set .
import maps only have a single top level scope for their "imports". so for
emulating  use the  scope. for emulating  use the
in a similar manner to how  works in import maps.
caveats: policies do not use string matching for various finding of scope. they
do url traversals. this means things like  and  urls might not be
entirely interoperable between the two systems. for example import maps can
partially match a  or  url by partitioning the url on a
character, policies intentionally cannot. for  urls import map scopes do
not adopt the origin of the  url.
additionally, import maps only work on  so it may be desirable to add a
condition to all dependency mappings.
process
the  object provides information about, and control over, the current
node.js process.
process events
the  object is an instance of .
the  event is emitted when node.js empties its event loop and has
no additional work to schedule. normally, the node.js process will exit when
there is no work scheduled, but a listener registered on the
event can make asynchronous calls, and thereby cause the node.js process to
continue.
the listener callback function is invoked with the value of
passed as the only argument.
the  event is not emitted for conditions causing explicit
termination, such as calling  or uncaught exceptions.
the  should not be used as an alternative to the  event
unless the intention is to schedule additional work.
if the node.js process is spawned with an ipc channel (see the
and  documentation), the  event will be emitted when
the ipc channel is closed.
the  event is emitted when the node.js process is about to exit as a
result of either:
the  method being called explicitly;
the node.js event loop no longer having any additional work to perform.
there is no way to prevent the exiting of the event loop at this point, and once
all  listeners have finished running the node.js process will terminate.
the listener callback function is invoked with the exit code specified either
by the  property, or the  argument passed to the
listener functions must only perform synchronous operations. the node.js
process will exit immediately after calling the  event listeners
causing any additional work still queued in the event loop to be abandoned.
in the following example, for instance, the timeout will never occur:
|  |  |  |  a parsed json object
or a serializable primitive value.
|  a  or
object, or undefined.
and  documentation), the  event is emitted whenever a
message sent by a parent process using  is received by
the child process.
the message goes through serialization and parsing. the resulting message might
not be the same as what is originally sent.
process, the  argument can contain data that json is not able
added in: v10.12.0deprecated since: v17.6.0
the resolution type. one of  or .
the promise that resolved or rejected more than once.
the value with which the promise was either resolved or
rejected after the original resolve.
the  event is emitted whenever a  has been either:
resolved more than once.
rejected more than once.
rejected after resolve.
resolved after reject.
this is useful for tracking potential errors in an application while using the
constructor, as multiple resolutions are silently swallowed. however,
the occurrence of this event does not necessarily indicate an error. for
example,  can trigger a  event.
because of the unreliability of the event in cases like the
example above it has been deprecated.
added in: v1.4.1
the late handled promise.
the  event is emitted whenever a  has been rejected
and an error handler was attached to it (using , for
example) later than one turn of the node.js event loop.
the  object would have previously been emitted in an
event, but during the course of processing gained a
rejection handler.
there is no notion of a top level for a  chain at which rejections can
always be handled. being inherently asynchronous in nature, a
rejection can be handled at a future point in time, possibly much later than
the event loop turn it takes for the  event to be emitted.
another way of stating this is that, unlike in synchronous code where there is
an ever-growing list of unhandled exceptions, with promises there can be a
growing-and-shrinking list of unhandled rejections.
in synchronous code, the  event is emitted when the list of
unhandled exceptions grows.
in asynchronous code, the  event is emitted when the list
of unhandled rejections grows, and the  event is emitted
when the list of unhandled rejections shrinks.
in this example, the   will grow and shrink over time,
reflecting rejections that start unhandled and then become handled. it is
possible to record such errors in an error log, either periodically (which is
likely best for long-running application) or upon process exit (which is likely
most convenient for scripts).
the uncaught exception.
indicates if the exception originates from an unhandled
rejection or from a synchronous error. can either be  or
. the latter is used when an exception happens in a
based async context (or if a  is rejected) and
flag set to  or  (which is the
default) and the rejection is not handled, or when a rejection happens during
the command line entry point's es module static loading phase.
the  event is emitted when an uncaught javascript
exception bubbles all the way back to the event loop. by default, node.js
handles such exceptions by printing the stack trace to  and exiting
with code 1, overriding any previously set .
adding a handler for the  event overrides this default
behavior. alternatively, change the  in the
handler which will result in the process exiting with the
provided exit code. otherwise, in the presence of such handler the process will
exit with 0.
it is possible to monitor  events without overriding the
default behavior to exit the process by installing a
listener.
warning: using  correctly
is a crude mechanism for exception handling
intended to be used only as a last resort. the event should not be used as
an equivalent to . unhandled exceptions inherently mean
that an application is in an undefined state. attempting to resume application
code without properly recovering from the exception can cause additional
unforeseen and unpredictable issues.
exceptions thrown from within the event handler will not be caught. instead the
process will exit with a non-zero exit code and the stack trace will be printed.
this is to avoid infinite recursion.
attempting to resume normally after an uncaught exception can be similar to
pulling out the power cord when upgrading a computer. nine out of ten
times, nothing happens. but the tenth time, the system becomes corrupted.
the correct use of  is to perform synchronous cleanup
of allocated resources (e.g. file descriptors, handles, etc) before shutting
down the process. it is not safe to resume normal operation after
to restart a crashed application in a more reliable way, whether
is emitted or not, an external monitor should be employed
in a separate process to detect application failures and recover or restart as
needed.
rejection or from synchronous errors. can either be  or
the  event is emitted before an
event is emitted or a hook installed via
installing an  listener does not change the behavior
once an  event is emitted. the process will
still crash if no  listener is installed.
|  the object with which the promise was rejected
(typically an  object).
the rejected promise.
the  event is emitted whenever a  is rejected and
no error handler is attached to the promise within a turn of the event loop.
when programming with promises, exceptions are encapsulated as "rejected
promises". rejections can be caught and handled using  and
are propagated through a  chain. the  event is
useful for detecting and keeping track of promises that were rejected whose
rejections have not yet been handled.
the following will also trigger the  event to be
emitted:
in this example case, it is possible to track the rejection as a developer error
as would typically be the case for other  events. to
address such failures, a non-operational
handler may be attached to
, which would prevent the  event from
being emitted.
key properties of the warning are:
the name of the warning. default: .
a system-provided description of the warning.
a stack trace to the location in the code where the warning
was issued.
the  event is emitted whenever node.js emits a process warning.
a process warning is similar to an error in that it describes exceptional
conditions that are being brought to the user's attention. however, warnings
are not part of the normal node.js and javascript error handling flow.
node.js can emit warnings whenever it detects bad coding practices that could
lead to sub-optimal application performance, bugs, or security vulnerabilities.
by default, node.js will print process warnings to . the
command-line option can be used to suppress the default console output but the
event will still be emitted by the  object.
the following example illustrates the warning that is printed to  when
too many listeners have been added to an event:
in contrast, the following example turns off the default warning output and
adds a custom handler to the  event:
the  command-line option can be used to have the default
console output for warnings include the full stack trace of the warning.
launching node.js using the  command-line flag will
cause custom deprecation warnings to be thrown as exceptions.
using the  command-line flag will cause the custom
deprecation to be printed to  along with the stack trace.
using the  command-line flag will suppress all reporting
of the custom deprecation.
the  command-line flags only affect warnings that use the name
added in: v16.2.0, v14.18.0
the  that was created.
the  event is emitted after a new  thread has been created.
emitting custom warnings
see the  method for issuing
custom or application-specific warnings.
node.js warning names
there are no strict guidelines for warning types (as identified by the
property) emitted by node.js. new types of warnings can be added at any time.
a few of the warning types that are most common include:
- indicates use of a deprecated node.js api or feature.
such warnings must include a  property identifying the
- indicates use of an experimental node.js api or
feature. such features must be used with caution as they may change at any
time and are not subject to the same strict semantic-versioning and long-term
support policies as supported features.
- indicates that too many listeners for a
given event have been registered on either an  or .
this is often an indication of a memory leak.
- indicates that a numeric value that cannot fit
within a 32-bit signed integer has been provided to either the
or  functions.
- indicates use of an unsupported option or feature
that will be ignored rather than treated as an error. one example is use of
the http response status message when using the http/2 compatibility api.
signal events
signal events will be emitted when the node.js process receives a signal. please
refer to  for a listing of standard posix signal names such as
signals are not available on  threads.
the signal handler will receive the signal's name (,
, etc.) as the first argument.
the name of each event will be the uppercase common name for the signal (e.g.
for  signals).
is reserved by node.js to start the . it's possible to
install a listener but doing so might interfere with the debugger.
and  have default handlers on non-windows platforms that
reset the terminal mode before exiting with code . if one
of these signals has a listener installed, its default behavior will be
removed (node.js will no longer exit).
is ignored by default. it can have a listener installed.
is generated on windows when the console window is closed, and on
other platforms under various similar conditions. see . it can have a
listener installed, however node.js will be unconditionally terminated by
windows about 10 seconds later. on non-windows platforms, the default
behavior of  is to terminate node.js, but once a listener has been
installed its default behavior will be removed.
is not supported on windows, it can be listened on.
from the terminal is supported on all platforms, and can usually be
generated with ctrl+c (though this may be configurable).
it is not generated when  is enabled
and ctrl+c is used.
is delivered on windows when ctrl+break is
pressed. on non-windows platforms, it can be listened on, but there is no way
to send or generate it.
is delivered when the console has been resized. on windows, this
will only happen on write to the console when the cursor is being moved, or
when a readable tty is used in raw mode.
cannot have a listener installed, it will unconditionally
terminate node.js on all platforms.
cannot have a listener installed.
, , , and , when not raised
artificially using , inherently leave the process in a state from
which it is not safe to call js listeners. doing so might cause the process
to stop responding.
can be sent to test for the existence of a process, it has no effect if
the process exists, but will throw an error if the process does not exist.
windows does not support signals so has no equivalent to termination by signal,
but node.js offers some emulation with , and
sending , , and  will cause the unconditional
termination of the target process, and afterwards, subprocess will report that
the process was terminated by signal.
sending signal  can be used as a platform independent way to test for the
existence of a process.
the  method causes the node.js process to exit immediately and
generate a core file.
this feature is not available in  threads.
the  property is a special,
read-only  of flags allowable within the
extends , but overrides
to recognize several different possible flag
representations.  will
return  in the following cases:
flags may omit leading single () or double () dashes; e.g.,
for , or  for .
flags passed through to v8 (as listed in ) may replace
one or more non-leading dashes for an underscore, or vice-versa;
e.g., , , ,
etc.
flags may contain one or more equals () characters; all
characters after and including the first equals will be ignored;
e.g., .
flags must be allowable within .
when iterating over , flags will
appear only once; each will begin with one or more dashes. flags
passed through to v8 will contain underscores instead of non-leading
dashes:
the methods , , and  of
do nothing, and will fail
silently.
if node.js was compiled without  support (shown in
),  will
contain what would have been allowable.
the operating system cpu architecture for which the node.js binary was compiled.
possible values are: , , , ,, ,
the  property returns an array containing the command-line
arguments passed when the node.js process was launched. the first element will
be . see  if access to the original value
of  is needed. the second element will be the path to the javascript
file being executed. the remaining elements will be any additional command-line
for example, assuming the following script for :
launching the node.js process as:
would generate the output:
the  property stores a read-only copy of the original value of
passed when node.js starts.
if the node.js process was spawned with an ipc channel (see the
documentation), the
property is a reference to the ipc channel. if no ipc channel exists, this
this method makes the ipc channel keep the event loop of the process
typically, this is managed through the number of  and
listeners on the  object. however, this method can be used to
explicitly request a specific behavior.
this method makes the ipc channel not keep the event loop of the process
the  method changes the current working directory of the
node.js process or throws an exception if doing so fails (for instance, if
the specified  does not exist).
the  property returns an  containing the javascript
representation of the configure options used to compile the current node.js
executable. this is the same as the  file that was produced when
running the  script.
an example of the possible output looks like:
the  property is not read-only and there are existing
modules in the ecosystem that are known to extend, modify, or entirely replace
the value of .
modifying the  property, or any child-property of the
object has been deprecated. the  will be made
read-only in a future release.
and  documentation), the  property will return
so long as the ipc channel is connected and will return  after
once  is , it is no longer possible to send messages
over the ipc channel using .
a previous return value from calling
the  method returns the user and system cpu time usage of
the current process, in an object with properties  and , whose
values are microsecond values (millionth of a second). these values measure time
spent in user and system code respectively, and may end up being greater than
actual elapsed time if multiple cpu cores are performing work for this process.
the result of a previous call to  can be passed as the
argument to the function, to get a diff reading.
added in: v0.1.8
the  method returns the current working directory of the node.js
the port used by the node.js debugger when enabled.
and  documentation), the  method will close the
ipc channel to the parent process, allowing the child process to exit gracefully
once there are no other connections keeping it alive.
the effect of calling  is the same as calling
from the parent process.
if the node.js process was not spawned with an ipc channel,
the  method allows dynamically loading shared objects. it is
primarily used by  to load c++ addons, and should not be used
directly, except in special cases. in other words,  should be
preferred over  unless there are specific reasons such as
custom dlopen flags or loading from es modules.
the  argument is an integer that allows to specify dlopen
behavior. see the  documentation for details.
an important requirement when calling  is that the
instance must be passed. functions exported by the c++ addon are then
accessible via .
the example below shows how to load a c++ addon, named ,
that exports a  function. all the symbols are loaded before
the call returns, by passing the  constant. in this example
the constant is assumed to be available.
|  the warning to emit.
when  is a ,  is the name to use
for the type of warning being emitted. default: .
a unique identifier for the warning instance being emitted.
when  is a ,  is an optional
function used to limit the generated stack trace. default:
additional text to include with the error.
the  method can be used to emit custom or application
specific process warnings. these can be listened for by adding a handler to the
in this example, an  object is generated internally by
and passed through to the
if  is passed as an  object, the  argument is ignored.
in each of the previous examples, an  object is generated internally by
if  is passed as an  object, it will be passed through to the
event handler unmodified (and the optional ,
and  arguments will be ignored):
a  is thrown if  is anything other than a string or
while process warnings use  objects, the process warning
mechanism is not a replacement for normal error handling mechanisms.
the following additional handling is implemented if the warning  is
if the  command-line flag is used, the deprecation
warning is thrown as an exception rather than being emitted as an event.
warning is suppressed.
warning is printed to  along with the full stack trace.
avoiding duplicate warnings
as a best practice, warnings should be emitted only once per process. to do
so, place the  behind a boolean.
the  property returns an object containing the user environment.
an example of this object looks like:
it is possible to modify this object, but such modifications will not be
reflected outside the node.js process, or (unless explicitly requested)
to other  threads.
in other words, the following example would not work:
while the following will:
assigning a property on  will implicitly convert the value
to a string. this behavior is deprecated. future versions of node.js may
throw an error when the value is not a string, number, or boolean.
use  to delete a property from .
on windows operating systems, environment variables are case-insensitive.
unless explicitly specified when creating a  instance,
each  thread has its own copy of , based on its
parent thread's , or whatever was specified as the  option
to the  constructor. changes to  will not be visible
across  threads, and only the main thread can make changes that
are visible to the operating system or to native add-ons.
the  property returns the set of node.js-specific command-line
options passed when the node.js process was launched. these options do not
appear in the array returned by the  property, and do not
include the node.js executable, the name of the script, or any options following
the script name. these options are useful in order to spawn child processes with
the same execution environment as the parent.
results in :
and :
refer to  for the detailed behavior of worker
threads with this property.
the  property returns the absolute pathname of the executable
that started the node.js process. symbolic links, if any, are resolved.
the exit code. default: .
the  method instructs node.js to terminate the process
synchronously with an exit status of . if  is omitted, exit uses
either the 'success' code  or the value of  if it has been
set. node.js will not terminate until all the  event listeners are
to exit with a 'failure' code:
the shell that executed node.js should see the exit code as .
calling  will force the process to exit as quickly as possible
even if there are still asynchronous operations pending that have not yet
completed fully, including i/o operations to  and
in most situations, it is not actually necessary to call
explicitly. the node.js process will exit on its own if there is no additional
work pending in the event loop. the  property can be set to
tell the process which exit code to use when the process exits gracefully.
for instance, the following example illustrates a misuse of the
method that could lead to data printed to stdout being
truncated and lost:
the reason this is problematic is because writes to  in node.js
are sometimes asynchronous and may occur over multiple ticks of the node.js
event loop. calling , however, forces the process to exit
before those additional writes to  can be performed.
rather than calling  directly, the code should set the
and allow the process to exit naturally by avoiding
scheduling any additional work for the event loop:
if it is necessary to terminate the node.js process due to an error condition,
throwing an uncaught error and allowing the process to terminate accordingly
is safer than calling .
in  threads, this function stops the current thread rather
than the current process.
a number which will be the process exit code, when the process either
exits gracefully, or is exited via  without specifying
a code.
specifying a code to  will override any
previous setting of .
the  method returns an array of strings
containing the types of the active resources that are currently keeping the
event loop alive.
the  method returns the numerical effective group identity
of the node.js process. (see .)
this function is only available on posix platforms (i.e. not windows or
android).
the  method returns the numerical effective user identity of
the process. (see .)
the  method returns the numerical group identity of the
process. (see .)
the  method returns an array with the supplementary group
ids. posix leaves it unspecified if the effective group id is included but
node.js ensures it always is.
added in: v0.1.28
the  method returns the numeric user identity of the process.
(see .)
added in: v9.3.0
indicates whether a callback has been set using
added in: v0.7.6
this is the legacy version of
before  was introduced in javascript.
the  method returns the current high-resolution real time
in a  tuple , where  is the
remaining part of the real time that can't be represented in second precision.
is an optional parameter that must be the result of a previous
call to diff with the current time. if the parameter
passed in is not a tuple , a  will be thrown. passing in a
user-defined array instead of the result of a previous call to
will lead to undefined behavior.
these times are relative to an arbitrary time in the
past, and not related to the time of day and therefore not subject to clock
drift. the primary use is for measuring performance between intervals:
the  version of the  method returning the
current high-resolution real time in nanoseconds as a .
unlike , it does not support an additional
argument since the difference can just be computed directly
by subtraction of the two s.
|  the user name or numeric identifier.
|  a group name or numeric identifier.
the  method reads the  file and initializes
the group access list, using all groups of which the user is a member. this is
a privileged operation that requires that the node.js process either have
access or the  capability.
use care when dropping privileges:
added in: v0.0.6
a process id
|  the signal to send, either as a string or number.
the  method sends the  to the process identified by
signal names are strings such as  or . see
and  for more information.
this method will throw an error if the target  does not exist. as a special
case, a signal of  can be used to test for the existence of a process.
windows platforms will throw an error if the  is used to kill a process
group.
even though the name of this function is , it is really just a
signal sender, like the  system call. the signal sent may do something
other than kill the target process.
when  is received by a node.js process, node.js will start the
debugger. see .
added in: v0.1.17deprecated since: v14.0.0
the  property provides an alternative way of retrieving
. the difference is that if the main module changes at
runtime,  may still refer to the original main module in
modules that were required before the change occurred. generally, it's
safe to assume that the two refer to the same module.
as with ,  will be  if there
is no entry script.
returns an object describing the memory usage of the node.js process measured in
and  refer to v8's memory usage.
refers to the memory usage of c++ objects bound to javascript
objects managed by v8.
, resident set size, is the amount of space occupied in the main
memory device (that is a subset of the total allocated memory) for the
process, including all c++ and javascript objects and code.
refers to memory allocated for s and
s, including all node.js s.
this is also included in the  value. when node.js is used as an
embedded library, this value may be  because allocations for s
may not be tracked in that case.
when using  threads,  will be a value that is valid for the
entire process, while the other fields will only refer to the current thread.
the  method iterates over each page to gather
information about memory usage which might be slow depending on the
program memory allocations.
added in: v15.6.0, v14.18.0
the  method returns an integer representing the
resident set size (rss) in bytes.
the resident set size, is the amount of space occupied in the main
this is the same value as the  property provided by
but  is faster.
additional arguments to pass when invoking the
adds  to the "next tick queue". this queue is
fully drained after the current operation on the javascript stack runs to
completion and before the event loop is allowed to continue. it's possible to
create an infinite loop if one were to recursively call .
see the  guide for more background.
this is important when developing apis in order to give users the opportunity
to assign event handlers after an object has been constructed but before any
i/o has occurred:
it is very important for apis to be either 100% synchronous or 100%
asynchronous. consider this example:
this api is hazardous because in the following case:
it is not clear whether  or  will be called first.
the following approach is much better:
when to use  vs.
the  api is an alternative to  that
also defers execution of a function using the same microtask queue used to
execute the then, catch, and finally handlers of resolved promises. within
node.js, every time the "next tick queue" is drained, the microtask queue
is drained immediately after.
for most userland use cases, the  api provides a portable
and reliable mechanism for deferring execution that works across multiple
javascript platform environments and should be favored over .
in simple scenarios,  can be a drop-in replacement for
one note-worthy difference between the two apis is that
allows specifying additional values that will be passed as arguments to the
deferred function when it is called. achieving the same result with
requires using either a closure or a bound function:
there are minor differences in the way errors raised from within the next tick
queue and microtask queue are handled. errors thrown within a queued microtask
callback should be handled within the queued callback when possible. if they are
not, the  event handler can be used to capture
and handle the errors.
when in doubt, unless the specific capabilities of  are
needed, use .
the  property indicates whether the
flag is set on the current node.js process. see the documentation for
the  and the
for more information about this
flag's behavior.
added in: v0.1.15
the  property returns the pid of the process.
the  property returns a string identifying the operating
system platform for which the node.js binary was compiled.
currently possible values are:
the value  may also be returned if the node.js is built on the
android operating system. however, android support in node.js
added in: v9.2.0, v8.10.0, v6.13.0
the  property returns the pid of the parent of the
the  property returns an  containing metadata related
to the current release, including urls for the source tarball and headers-only
tarball.
contains the following properties:
a value that will always be .
an absolute url pointing to a  file containing
the source code of the current release.
only the source header files for the current release. this file is
significantly smaller than the full source file and can be used for compiling
node.js native add-ons.
an absolute url pointing to a  file matching the
architecture and version of the current release. this file is used for
compiling node.js native add-ons. this property is only present on windows
builds of node.js and will be missing on all other platforms.
a string label identifying the  label for this release.
this property only exists for lts releases and is  for all other
release types, including current releases.
valid values include the lts release code names (including those
that are no longer supported).
for the 10.x lts line beginning with 10.13.0.
for the 12.x lts line beginning with 12.13.0.
for other lts release code names, see
in custom builds from non-release versions of the source tree, only the
property may be present. the additional properties should not be
relied upon to exist.
is an object whose methods are used to generate diagnostic
reports for the current process. additional documentation is available in the
directory where the report is written. the default value is the empty string,
indicating that reports are written to the current working directory of the
filename where the report is written. if set to the empty string, the output
filename will be comprised of a timestamp, pid, and sequence number. the default
value is the empty string.
if the value of  is set to  or ,
the report is written to the stdout or stderr of the process respectively.
a custom error used for reporting the javascript stack.
returns a javascript object representation of a diagnostic report for the
running process. the report's javascript stack trace is taken from , if
present.
additional documentation is available in the .
if , a diagnostic report is generated on fatal errors, such as out of
memory errors or failed c++ assertions.
if , a diagnostic report is generated when the process receives the
signal specified by .
if , a diagnostic report is generated on uncaught exception.
the signal used to trigger the creation of a diagnostic report. defaults to
name of the file where the report is written. this
should be a relative path, that will be appended to the directory specified in
, or the current working directory of the node.js
process, if unspecified.
returns:  returns the filename of the generated report.
writes a diagnostic report to a file. if  is not provided, the default
filename includes the date, time, pid, and a sequence number. the report's
javascript stack trace is taken from , if present.
if the value of  is set to  or , the report is
written to the stdout or stderr of the process respectively.
added in: v12.6.0
returns:  the resource usage for the current process. all of these
values come from the  call which returns
maps to  computed in microseconds.
it is the same value as .
maps to  which is the maximum resident set
size used in kilobytes.
maps to  but is not supported by
any platform.
maps to  which is the number of
minor page faults for the process, see
major page faults for the process, see
. this field is not
supported on windows.
maps to  but is not supported by any
platform.
maps to  which is the number of times the
file system had to perform input.
file system had to perform output.
maps to  which is the
number of times a cpu context switch resulted due to a process voluntarily
giving up the processor before its time slice was completed (usually to
await availability of a resource). this field is not supported on windows.
number of times a cpu context switch resulted due to a higher priority
process becoming runnable or because the current process exceeded its
time slice. this field is not supported on windows.
used to parameterize the sending of certain types of
handles. supports the following properties:
if node.js is spawned with an ipc channel, the  method can be
used to send messages to the parent process. messages will be received as a
event on the parent's  object.
if node.js was not spawned with an ipc channel,  will be
|  a group name or id
the  method sets the effective group identity of the process.
(see .) the  can be passed as either a numeric id or a group
name string. if a group name is specified, this method blocks while resolving
the associated a numeric id.
|  a user name or id
the  method sets the effective user identity of the process.
(see .) the  can be passed as either a numeric id or a username
string. if a username is specified, the method blocks while resolving the
associated numeric id.
|  the group name or id
the  method sets the group identity of the process. (see
.) the  can be passed as either a numeric id or a group name
string. if a group name is specified, this method blocks while resolving the
the  method sets the supplementary group ids for the
node.js process. this is a privileged operation that requires the node.js
process to have  or the  capability.
the  array can contain numeric group ids, group names, or both.
the  method sets the user identity of the process. (see
.) the  can be passed as either a numeric id or a username string.
if a username is specified, the method blocks while resolving the associated
numeric id.
added in: v16.6.0, v14.18.0
this function enables or disables the  support for
stack traces.
it provides same features as launching node.js process with commandline options
only source maps in javascript files that are loaded after source maps has been
enabled will be parsed and loaded.
the  function sets a function
that will be invoked when an uncaught exception occurs, which will receive the
exception value itself as its first argument.
if such a function is set, the  event will
not be emitted. if  was passed from the
command line or set through , the process will
not abort. actions configured to take place on exceptions such as report
generations will be affected too
to unset the capture function,
may be used. calling this
method with a non- argument while another capture function is set will
throw an error.
using this function is mutually exclusive with using the deprecated
built-in module.
the  property returns a stream connected to
(fd ). it is a  (which is a
stream) unless fd  refers to a file, in which case it is
a  stream.
differs from other node.js streams in important ways. see
this property refers to the value of underlying file descriptor of
. the value is fixed at . in  threads,
this field does not exist.
for details of how to read from  see .
as a  stream,  can also be used in "old" mode that
is compatible with scripts written for node.js prior to v0.10.
for more information see .
in "old" streams mode the  stream is paused by default, so one
must call  to read from it. note also that calling
itself would switch stream to "old" mode.
for example, to copy  to :
a note on process i/o
and  differ from other node.js streams in
important ways:
they are used internally by  and ,
respectively.
writes may be synchronous depending on what the stream is connected to
and whether the system is windows or posix:
files: synchronous on windows and posix
ttys (terminals): asynchronous on windows, synchronous on posix
pipes (and sockets): synchronous on windows, asynchronous on posix
these behaviors are partly for historical reasons, as changing them would
create backward incompatibility, but they are also expected by some users.
synchronous writes avoid problems such as output written with  or
being unexpectedly interleaved, or not written at all if
is called before an asynchronous write completes. see
warning: synchronous writes block the event loop until the write has
completed. this can be near instantaneous in the case of output to a file, but
under high system load, pipes that are not being read at the receiving end, or
with slow terminals or file systems, it's possible for the event loop to be
blocked often enough and long enough to have severe negative performance
impacts. this may not be a problem when writing to an interactive terminal
session, but consider this particularly careful when doing production logging to
the process output streams.
to check if a stream is connected to a  context, check the
for instance:
see the  documentation for more information.
the initial value of  indicates whether the
flag is set on the current node.js process.
is mutable, so whether or not deprecation
warnings result in errors may be altered at runtime. see the
documentation for the  and the
the  property returns the current process title (i.e. returns
the current value of ). assigning a new value to  modifies
the current value of .
when a new value is assigned, different platforms will impose different maximum
length restrictions on the title. usually such restrictions are quite limited.
for instance, on linux and macos,  is limited to the size of the
binary name plus the length of the command-line arguments because setting the
overwrites the  memory of the process. node.js v0.8
allowed for longer process title strings by also overwriting the
memory but that was potentially insecure and confusing in some (rather obscure)
cases.
assigning a value to  might not result in an accurate label
within process manager applications such as macos activity monitor or windows
services manager.
flag is set on the current node.js process. see the
- deprecated. calling  with no argument causes
the process-wide umask to be written twice. this introduces a race condition
between threads, and is a potential security vulnerability. there is no safe,
cross-platform alternative api.
returns the node.js process's file mode creation mask. child
processes inherit the mask from the parent process.
added in: v0.1.19
sets the node.js process's file mode creation mask. child
processes inherit the mask from the parent process. returns the previous mask.
in  threads,  will throw an exception.
the  method returns the number of seconds the current node.js
process has been running.
the return value includes fractions of a second. use  to get whole
seconds.
the  property contains the node.js version string.
to get the version string without the prepended v, use
the  property returns an object listing the version strings of
node.js and its dependencies.  indicates the current
abi version, which is increased whenever a c++ api changes. node.js will refuse
to load modules that were compiled against a different module abi version.
will generate an object similar to:
exit codes
node.js will normally exit with a  status code when no more async
operations are pending. the following status codes are used in other
cases:
uncaught fatal exception: there was an uncaught exception,
and it was not handled by a domain or an  event
: unused (reserved by bash for builtin misuse)
internal javascript parse error: the javascript source code
internal in the node.js bootstrapping process caused a parse error. this
is extremely rare, and generally can only happen during development
of node.js itself.
internal javascript evaluation failure: the javascript
source code internal in the node.js bootstrapping process failed to
return a function value when evaluated. this is extremely rare, and
generally can only happen during development of node.js itself.
fatal error: there was a fatal unrecoverable error in v8.
typically a message will be printed to stderr with the prefix .
non-function internal exception handler: there was an
uncaught exception, but the internal fatal exception handler
function was somehow set to a non-function, and could not be called.
internal exception handler run-time failure: there was an
uncaught exception, and the internal fatal exception handler
function itself threw an error while attempting to handle it. this
can happen, for example, if an  or
handler throws an error.
: unused. in previous versions of node.js, exit code 8 sometimes
indicated an uncaught exception.
invalid argument: either an unknown option was specified,
or an option requiring a value was provided without a value.
internal javascript run-time failure: the javascript
source code internal in the node.js bootstrapping process threw an error
when the bootstrapping function was called. this is extremely rare,
and generally can only happen during development of node.js itself.
invalid debug argument: the  and/or
options were set, but the port number chosen was invalid or unavailable.
unfinished top-level await:  was used outside of a function
in the top-level code, but the passed  never resolved.
snapshot failure: node.js was started to build a v8 startup
snapshot and it failed because certain requirements of the state of
the application were not met.
signal exits: if node.js receives a fatal signal such as
or , then its exit code will be  plus the
value of the signal code. this is a standard posix practice, since
exit codes are defined to be 7-bit integers, and signal exits set
the high-order bit, and then contain the value of the signal code.
for example, signal  has value , so the expected exit
code will be  + , or .
punycode
deprecated since: v7.0.0
the version of the punycode module bundled in node.js is being deprecated.
in a future major version of node.js this module will be removed. users
currently depending on the  module should switch to using the
userland-provided  module instead. for punycode-based url
encoding, see  or, more generally, the
the  module is a bundled version of the  module. it
can be accessed using:
is a character encoding scheme defined by rfc 3492 that is
primarily intended for use in internationalized domain names. because host
names in urls are limited to ascii characters only, domain names that contain
non-ascii characters must be converted into ascii using the punycode scheme.
for instance, the japanese character that translates into the english word,
is . the internationalized domain name,  (equivalent
to ) is represented by punycode as the ascii string
the  module provides a simple implementation of the punycode standard.
the  module is a third-party dependency used by node.js and
made available to developers as a convenience. fixes or other modifications to
the module must be directed to the  project.
the  method converts a  string of ascii-only
characters to the equivalent string of unicode codepoints.
the  method converts a string of unicode codepoints to a
string of ascii-only characters.
added in: v0.6.1
the  method converts a unicode string representing an
internationalized domain name to . only the non-ascii parts of the
domain name will be converted. calling  on a string that
already only contains ascii characters will have no effect.
the  method converts a string representing a domain name
containing  encoded characters into unicode. only the
encoded parts of the domain name are be converted.
the  method returns an array containing the numeric
codepoint values of each unicode symbol in the string.
the  method returns a string based on an array of
numeric code point values.
returns a string identifying the current  version number.
query string
the  module provides utilities for parsing and formatting url
query strings. it can be accessed using:
is more performant than  but is not a
standardized api. use  when performance is not critical or
when compatibility with browser code is desirable.
added in: v0.1.99
added in: v0.1.25
the  method performs url percent-encoding on the given
in a manner that is optimized for the specific requirements of url
query strings.
the  method is used by  and is
generally not expected to be used directly. it is exported primarily to allow
application code to provide a replacement percent-encoding implementation if
necessary by assigning  to an alternative function.
the url query string to parse
the substring used to delimit key and value pairs in the
query string. default: .
. the substring used to delimit keys and values in the
the function to use when decoding
percent-encoded characters in the query string. default:
specifies the maximum number of keys to parse.
specify  to remove key counting limitations. default: .
the  method parses a url query string () into a
collection of key and value pairs.
for example, the query string  is parsed into:
by default, percent-encoded characters within the query string will be assumed
to use utf-8 encoding. if an alternative character encoding is used, then an
alternative  option will need to be specified:
the object to serialize into a url query string
the function to use when converting
url-unsafe characters to percent-encoding in the query string. default:
the  method produces a url query string from a
given  by iterating through the object's "own properties".
it serializes the following types of values passed in :
the numeric values must be finite. any other input values will be coerced to
empty strings.
by default, characters requiring percent-encoding within the query string will
be encoded as utf-8. if an alternative encoding is required, then an alternative
option will need to be specified:
the  method performs decoding of url percent-encoded
characters on the given .
application code to provide a replacement decoding implementation if
by default, the  method will attempt to use the
javascript built-in  method to decode. if that fails,
a safer equivalent that does not throw on malformed urls will be used.
readline
the  module provides an interface for reading data from a
stream (such as ) one line at a time.
the following simple example illustrates the basic use of the
once this code is invoked, the node.js application will not terminate until the
is closed because the interface waits for data to be
received on the  stream.
instances of the  class are constructed using the
or  method.
every instance is associated with a single   stream and a
single   stream.
the  stream is used to print prompts for user input that arrives on,
and is read from, the  stream.
added in: v0.1.98
the  event is emitted when one of the following occur:
the  method is called and the  instance has
relinquished control over the  and  streams;
the  stream receives its  event;
the  stream receives ctrl+d to signal
end-of-transmission (eot);
the  stream receives ctrl+c to signal
and there is no  event listener registered on the
the listener function is called without passing any arguments.
the  instance is finished once the  event is
the  event is emitted whenever the  stream receives an
end-of-line input (, , or ). this usually occurs when the user
presses enter or return.
the  event is also emitted if new data has been read from a stream and
that stream ends without a final end-of-line marker.
the listener function is called with a string containing the single line of
received input.
added in: v15.8.0, v14.18.0
the  event is emitted whenever the history array has changed.
the listener function is called with an array containing the history array.
it will reflect all changes, added lines and removed lines due to
the primary purpose is to allow a listener to persist the history.
it is also possible for the listener to change the history object. this
could be useful to prevent certain lines to be added to the history, like
a password.
the  stream is paused.
the  stream is not paused and receives the  event. (see
events  and .)
the  event is emitted whenever the  stream is resumed.
the  event is emitted when a node.js process previously moved into
the background using ctrl+z (i.e. ) is then
brought back to the foreground using .
if the  stream was paused before the  request, this event will
the listener function is invoked without passing any arguments.
the  event is not supported on windows.
the  event is emitted whenever the  stream receives
a ctrl+c input, known typically as . if there are no
event listeners registered when the  stream receives a
, the  event will be emitted.
the  event is emitted when the  stream receives
a ctrl+z input, typically known as . if there are
no  event listeners registered when the  stream receives a
, the node.js process will be sent to the background.
when the program is resumed using , the  and  events
will be emitted. these can be used to resume the  stream.
the  and  events will not be emitted if the  was
paused before the process was sent to the background.
the  method closes the  instance and
relinquishes control over the  and  streams. when called,
the  event will be emitted.
calling  does not immediately stop other events (including )
from being emitted by the  instance.
the  method pauses the  stream, allowing it to be resumed
later if necessary.
calling  does not immediately pause other events (including
) from being emitted by the  instance.
if , prevents the cursor placement from
being reset to .
the  method writes the  instances configured
to a new line in  in order to provide a user with a new
location at which to provide input.
when called,  will resume the  stream if it has been
paused.
if the  was created with  set to  or
the prompt is not written.
a statement or query to write to , prepended to the
prompt.
optionally allows the  to be canceled
using an .
a callback function that is invoked with the user's
input in response to the .
the  method displays the  by writing it to the ,
waits for user input to be provided on , then invokes the
function passing the provided input as the first argument.
the  is not written.
the  function passed to  does not follow the typical
pattern of accepting an  object or  as the first argument.
the  is called with the provided answer as the only argument.
an error will be thrown if calling  after .
using an  to cancel a question.
the  method resumes the  stream if it has been paused.
the  method sets the prompt that will be written to
whenever  is called.
returns:  the current prompt string
the  method returns the current prompt used by .
to indicate the ctrl key.
to indicate the meta key.
to indicate the shift key.
the name of the a key.
the  method will write either  or a key sequence identified
by  to the . the  argument is supported only if  is
a  text terminal. see  for a list of key
combinations.
if  is specified,  is ignored.
the  and  are not written.
the  method will write the data to the  's
as if it were provided by the user.
create an  object that iterates through each line in the input
stream as a string. this method allows asynchronous iteration of
objects through  loops.
errors in the input stream are not forwarded.
if the loop is terminated with , , or ,
will be called. in other words, iterating over a
will always consume the input stream fully.
performance is not on par with the traditional  event api. use
instead for performance-sensitive applications.
will start to consume the input stream once
invoked. having asynchronous operations between interface creation and
asynchronous iteration may result in missed lines.
the current input data being processed by node.
this can be used when collecting input from a tty stream to retrieve the
current value that has been processed thus far, prior to the  event
being emitted. once the  event has been emitted, this property will
be aware that modifying the value during the instance runtime may have
unintended consequences if  is not also controlled.
if not using a tty stream for input, use the  event.
one possible use case would be as follows:
the cursor position relative to .
this will track where the current cursor lands in the input string, when
reading input from a tty stream. the position of cursor determines the
portion of the input string that will be modified as input is processed,
as well as the column where the terminal caret will be rendered.
the row of the prompt the cursor currently lands on
the screen column the cursor currently lands on
returns the real position of the cursor in relation to the input
prompt + string. long input (wrapping) strings, as well as multiple
line prompts are included in the calculations.
method. every instance is associated with a
single   stream and a single   stream.
returns:  a promise that is fulfilled with the user's
if the question is called after , it returns a rejected promise.
if , no need to call .
: to the left from cursor
: to the right from cursor
: the entire line
returns: this
the  method adds to the internal list of pending action an
action that clears current line of the associated  in a specified
direction identified by .
call  to see the effect of this method, unless
was passed to the constructor.
action that clears the associated stream from the current position of the
cursor down.
the  method sends all the pending actions to the associated
and clears the internal list of pending actions.
the  method adds to the internal list of pending action an action
that moves cursor to the specified position in the associated .
action that moves the cursor relative to its current position in the
associated .
the  methods clears the internal list of pending actions without
sending it to the associated .
the  stream to listen to. this option
the  stream to write readline data
an optional function used for tab autocompletion.
if the  and  streams should be
treated like a tty, and have ansi/vt100 escape codes written to it.
default: checking  on the  stream upon instantiation.
initial list of history lines. this option makes sense
only if  is set to  by the user or by an internal
check, otherwise the history caching mechanism is not initialized at all.
maximum number of history lines retained. to disable
the history set this value to . this option makes sense only if
is set to  by the user or by an internal  check,
otherwise the history caching mechanism is not initialized at all.
if , when a new input line added
to the history list duplicates an older one, this removes the older line
from the list. default: .
the prompt string to use. default: .
if the delay between  and  exceeds
milliseconds, both  and  will be treated as separate
end-of-line input.  will be coerced to a number no less than
. it can be set to , in which case  followed by
will always be considered a single newline (which may be reasonable for
with  line delimiter). default: .
the duration  will wait for a
character (when reading an ambiguous key sequence in milliseconds one that
can both form a complete key sequence using the input read so far and can
take additional input to complete a longer key sequence).
the number of spaces a tab is equal to (minimum 1).
the  method creates a new
once the  instance is created, the most common case
is to listen for the  event:
if  is  for this instance then the  stream will get
the best compatibility if it defines an  property and emits
a  event on the  if or when the columns ever change
( does this automatically when it is a tty).
use of the  function
the  function takes the current line entered by the user
as an argument, and returns an  with 2 entries:
an  with matching entries for the completion.
the substring that was used for the matching.
for instance: .
the  function can also returns a , or be asynchronous:
invoked once the operation completes.
returns:   if  wishes for the calling code to wait for
the  event to be emitted before continuing to write additional data;
otherwise .
the  method clears current line of given  stream
in a specified direction identified by .
the  method clears the given  stream from
the current position of the cursor down.
allows closing the interface using an abortsignal.
aborting the signal will internally call  on the interface.
once the  instance is created, the most common case is to
listen for the  event:
when creating a  using  as input, the program
will not terminate until it receives an . to exit without
waiting for user input, call .
the  function can be called asynchronously if it accepts two
arguments:
the  method moves cursor to the specified position in a
given  .
the  method moves the cursor relative to its current
position in a given  .
the  method causes the given
stream to begin emitting  events corresponding to received input.
optionally,  specifies a  instance for which
autocompletion is disabled when copy-pasted input is detected.
if the  is a , then it must be in raw mode.
this is automatically called by any readline instance on its  if the
is a terminal. closing the  instance does not stop
the  from emitting  events.
example: tiny cli
the following example illustrates the use of  class to
implement a small command-line interface:
example: read file stream line-by-line
a common use case for  is to consume an input file one line at a
time. the easiest way to do so is leveraging the  api as
well as a  loop:
alternatively, one could use the  event:
currently,  loop can be a bit slower. if  /
flow and speed are both essential, a mixed approach can be applied:
tty keybindings
repl
the  module provides a read-eval-print-loop (repl) implementation
that is available both as a standalone program or includible in other
applications. it can be accessed using:
design and features
the  module exports the  class. while running,
instances of  will accept individual lines of user input,
evaluate those according to a user-defined evaluation function, then output the
result. input and output may be from  and , respectively, or may
be connected to any node.js .
instances of  support automatic completion of inputs,
completion preview, simplistic emacs-style line editing, multi-line inputs,
-like reverse-i-search, -like substring-based history search,
ansi-styled output, saving and restoring current repl session state, error
recovery, and customizable evaluation functions. terminals that do not support
ansi styles and emacs-style line editing automatically fall back to a limited
feature set.
commands and special keys
the following special commands are supported by all repl instances:
: when in the process of inputting a multi-line expression, enter
the  command (or press ctrl+c) to abort
further input or processing of that expression.
: resets the repl  to an empty object and clears any
multi-line expression being input.
: close the i/o stream, causing the repl to exit.
: show this list of special commands.
: save the current repl session to a file:
: load a file into the current repl session.
: enter editor mode (ctrl+d to
finish, ctrl+c to cancel).
the following key combinations in the repl have these special effects:
ctrl+c: when pressed once, has the same effect as the
when pressed twice on a blank line, has the same effect as the
ctrl+d: has the same effect as the  command.
tab: when pressed on a blank line, displays global and local
(scope) variables. when pressed while entering other input, displays relevant
autocompletion options.
for key bindings related to the reverse-i-search, see .
for all other key bindings, see .
default evaluation
by default, all instances of  use an evaluation function
that evaluates javascript expressions and provides access to node.js built-in
modules. this default behavior can be overridden by passing in an alternative
evaluation function when the  instance is created.
javascript expressions
the default evaluator supports direct evaluation of javascript expressions:
unless otherwise scoped within blocks or functions, variables declared
either implicitly or using the , , or  keywords
are declared at the global scope.
global and local scope
the default evaluator provides access to any variables that exist in the global
scope. it is possible to expose a variable to the repl explicitly by assigning
it to the  object associated with each :
properties in the  object appear as local within the repl:
context properties are not read-only by default. to specify read-only globals,
context properties must be defined using :
accessing core node.js modules
the default evaluator will automatically load node.js core modules into the
repl environment when used. for instance, unless otherwise declared as a
global or scoped variable, the input  will be evaluated on-demand as
global uncaught exceptions
the repl uses the  module to catch all uncaught exceptions for that
repl session.
this use of the  module in the repl has these side effects:
uncaught exceptions only emit the  event in the
standalone repl. adding a listener for this event in a repl within
another node.js program results in .
trying to use  throws
an  error.
assignment of the  (underscore) variable
the default evaluator will, by default, assign the result of the most recently
evaluated expression to the special variable  (underscore).
explicitly setting  to a value will disable this behavior.
similarly,  will refer to the last seen error, if there was any.
keyword
support for the  keyword is enabled at the top level.
one known limitation of using the  keyword in the repl is that
it will invalidate the lexical scoping of the  and
keywords.
shall disable top-level await in repl.
reverse-i-search
the repl supports bi-directional reverse-i-search similar to . it is
triggered with ctrl+r to search backward
and ctrl+s to search forwards.
duplicated history entries will be skipped.
entries are accepted as soon as any key is pressed that doesn't correspond
with the reverse search. cancelling is possible by pressing esc
or ctrl+c.
changing the direction immediately searches for the next entry in the expected
direction from the current position on.
custom evaluation functions
when a new  is created, a custom evaluation function may be
provided. this can be used, for instance, to implement fully customized repl
applications.
the following illustrates a hypothetical example of a repl that performs
translation of text from one language to another:
recoverable errors
at the repl prompt, pressing enter sends the current line of input to
the  function. in order to support multi-line input, the  function
can return an instance of  to the provided callback function:
customizing repl output
by default,  instances format output using the
method before writing the output to the provided
stream ( by default). the  inspection option is set
to true by default and the  option is set to true depending on the
repl's  option.
the  boolean option can be specified at construction to instruct the
default writer to use ansi style codes to colorize the output from the
if the repl is run as standalone program, it is also possible to change the
repl's  from inside the repl by using the
property which mirrors the  from
to fully customize the output of a  instance pass in a new
function for the  option on construction. the following example, for
instance, simply converts any input text to upper case:
added in: v0.1.91
|  see
instances of  are created using the  method
or directly using the javascript  keyword.
the  event is emitted when the repl is exited either by receiving the
command as input, the user pressing ctrl+c twice
to signal ,
or by pressing ctrl+d to signal  on the input
stream. the listener
callback is invoked without any arguments.
added in: v0.11.0
the  event is emitted when the repl's context is reset. this occurs
whenever the  command is received as input unless the repl is using
the default evaluator and the  instance was created with the
option set to . the listener callback will be called with a
reference to the  object as the only argument.
this can be used primarily to re-initialize repl context to some pre-defined
state:
when this code is executed, the global  variable can be modified but then
reset to its initial value using the  command:
the command keyword (without a leading  character).
|  the function to invoke when the command is processed.
the  method is used to add new -prefixed commands
to the repl instance. such commands are invoked by typing a  followed by the
. the  is either a  or an  with the following
help text to be displayed when  is entered (optional).
the function to execute, optionally accepting a single
string argument.
the following example shows two new commands added to the repl instance:
the new commands can then be used from within the repl instance:
the  method readies the repl instance for input
from the user, printing the configured  to a new line in the
and resuming the  to accept new input.
when multi-line input is being entered, an ellipsis is printed rather than the
'prompt'.
when  is , the cursor placement will not be reset to .
the  method is primarily intended to be called from
within the action function for commands registered using the
the  method clears any command that has been
buffered but not yet executed. this method is primarily intended to be
called from within the action function for commands registered using the
added in: v0.8.9deprecated since: v9.0.0
- deprecated.
the potential keyword to parse and execute
any parameters to the keyword command
an internal method used to parse and execute  keywords.
returns  if  is a valid keyword, otherwise .
the path to the history file
called when history writes are ready or upon error
initializes a history log file for the repl instance. when executing the
node.js binary and using the command-line repl, a history file is initialized
by default. however, this is not the case when creating a repl
programmatically. use this method to initialize a history log file when working
with repl instances programmatically.
a list of the names of all node.js modules, e.g., .
the input prompt to display. default:
(with a trailing space).
the  stream from which repl input will
be read. default: .
the  stream to which repl output will
be written. default: .
if , specifies that the  should be
treated as a tty terminal.
default: checking the value of the  property on the
stream upon instantiation.
the function to be used when evaluating each given line
of input. default: an async wrapper for the javascript
function. an  function can error with  to indicate
the input was incomplete and prompt for additional lines.
if , specifies that the default
function should include ansi color styling to repl output. if a custom
function is provided then this has no effect. default: checking
color support on the  stream if the repl instance's  value
if , specifies that the default evaluation
function will use the javascript  as the context as opposed to
creating a new separate context for the repl instance. the node cli repl
sets this value to . default: .
if , specifies that the default writer
will not output the return value of a command if it evaluates to
the function to invoke to format the output of each
command before writing to . default: .
an optional function used for custom tab auto
completion. see  for an example.
a flag that specifies whether the default evaluator
executes all javascript commands in strict mode or default (sloppy) mode.
acceptable values are:
to evaluate expressions in sloppy mode.
to evaluate expressions in strict mode. this is
equivalent to prefacing every repl statement with .
stop evaluating the current piece of code when
is received, such as when ctrl+c is pressed.
this cannot be used
together with a custom  function. default: .
defines if the repl prints autocomplete and output
previews or not. default:  with the default eval function and
in case a custom eval function is used. if  is falsy, then
there are no previews and the value of  has no effect.
the  method creates and starts a  instance.
if  is a string, then it specifies the input prompt:
the node.js repl
node.js itself uses the  module to provide its own interactive
interface for executing javascript. this can be used by executing the node.js
binary without passing any arguments (or by passing the  argument):
environment variable options
various behaviors of the node.js repl can be customized using the following
environment variables:
: when a valid path is given, persistent repl history
will be saved to the specified file rather than  in the
user's home directory. setting this value to  (an empty string) will
disable persistent repl history. whitespace will be trimmed from the value.
on windows platforms environment variables with empty values are invalid so
set this variable to one or more spaces to disable persistent repl history.
: controls how many lines of history will be
persisted if history is available. must be a positive number.
: may be either  or . default:
, which will allow non-strict mode code to be run.
persistent history
by default, the node.js repl will persist history between  repl sessions
by saving inputs to a  file located in the user's home
directory. this can be disabled by setting the environment variable
using the node.js repl with advanced line-editors
for advanced line-editors, start node.js with the environment variable
. this will start the main and debugger repl in canonical
terminal settings, which will allow use with .
for example, the following can be added to a  file:
starting multiple repl instances against a single running instance
it is possible to create and run multiple repl instances against a single
running instance of node.js that share a single  object but have
separate i/o interfaces.
the following example, for instance, provides separate repls on , a unix
socket, and a tcp socket:
running this application from the command line will start a repl on stdin.
other repl clients may connect through the unix socket or tcp socket. ,
for instance, is useful for connecting to tcp sockets, while  can be used
to connect to both unix and tcp sockets.
by starting a repl from a unix socket-based server instead of stdin, it is
possible to connect to a long-running node.js process without restarting it.
for an example of running a "full-featured" () repl over
a  and  instance, see:
for an example of running a repl instance over , see:
diagnostic report
delivers a json-formatted diagnostic summary, written to a file.
the report is intended for development, test, and production use, to capture
and preserve information for problem determination. it includes javascript
and native stack traces, heap statistics, platform information, resource
usage etc. with the report option enabled, diagnostic reports can be triggered
on unhandled exceptions, fatal errors and user signals, in addition to
triggering programmatically through api calls.
a complete example report that was generated on an uncaught exception
is provided below for reference.
enables report to be generated on
un-caught exceptions. useful when inspecting javascript stack in conjunction
with native stack and other runtime environment data.
enables report to be generated upon receiving
the specified (or predefined) signal to the running node.js process. (see
below on how to modify the signal that triggers the report.) default signal is
. useful when a report needs to be triggered from another program.
application monitors may leverage this feature to collect report at regular
intervals and plot rich set of internal runtime data to their views.
signal based report generation is not supported in windows.
under normal circumstances, there is no need to modify the report triggering
signal. however, if  is already used for other purposes, then this
flag helps to change the signal for report generation and preserve the original
meaning of  for the said purposes.
enables the report to be triggered on fatal errors
(internal errors within the node.js runtime, such as out of memory)
that leads to termination of the application. useful to inspect various
diagnostic data elements such as heap, stack, event loop state, resource
consumption etc. to reason about the fatal error.
write reports in a compact format, single-line json, more
easily consumable by log processing systems than the default multi-line format
designed for human consumption.
location at which the report will be
name of the file to which the report will be
sets or resets the signal for report generation
(not supported on windows). default signal is .
a report can also be triggered via an api call from a javascript application:
this function takes an optional additional argument , which is
the name of a file into which the report is written.
this function takes an optional additional argument  which is an
object that will be used as the context for the javascript stack printed in the
report. when using report to handle errors in a callback or an exception
handler, this allows the report to include the location of the original error as
well as where it was handled.
if both filename and error object are passed to  the
error object must be the second parameter.
the content of the diagnostic report can be returned as a javascript object
via an api call from a javascript application:
this function takes an optional additional argument , which is an
report.
the api versions are useful when inspecting the runtime state from within
the application, in expectation of self-adjusting the resource consumption,
load balancing, monitoring etc.
the content of the report consists of a header section containing the event
type, date, time, pid, and node.js version, sections containing javascript and
native stack traces, a section containing v8 heap information, a section
containing  handle information, and an os platform information section
showing cpu and memory usage and system limits. an example report can be
triggered using the node.js repl:
when a report is written, start and end messages are issued to stderr
and the filename of the report is returned to the caller. the default filename
includes the date, time, pid, and a sequence number. the sequence number helps
in associating the report dump with the runtime state if generated multiple
times for the same node.js process.
additional runtime configuration of report generation is available via
the following properties of :
triggers diagnostic reporting on fatal errors when .
triggers diagnostic reporting on signal when . this is
not supported on windows. defaults to .
triggers diagnostic reporting on uncaught exception
when . defaults to .
specifies the posix signal identifier that will be used
to intercept external triggers for report generation. defaults to
specifies the name of the output file in the file system.
special meaning is attached to  and . usage of these
will result in report being written to the associated standard streams.
in cases where standard streams are used, the value in  is ignored.
urls are not supported. defaults to a composite filename that contains
timestamp, pid, and sequence number.
specifies the filesystem directory where the report will be written.
urls are not supported. defaults to the current working directory of the
configuration on module initialization is also available via
specific api documentation can be found under
interaction with workers
threads can create reports in the same way that the main thread
reports will include information on any workers that are children of the current
thread as part of the  section, with each worker generating a report
in the standard report format.
the thread which is generating the report will wait for the reports from worker
threads to finish. however, the latency for this will usually be low, as both
running javascript and the event loop are interrupted to generate the report.
stream
a stream is an abstract interface for working with streaming data in node.js.
the  module provides an api for implementing the stream interface.
there are many stream objects provided by node.js. for instance, a
are both stream instances.
streams can be readable, writable, or both. all streams are instances of
to access the  module:
the  module is useful for creating new types of stream instances.
it is usually not necessary to use the  module to consume streams.
organization of this document
this document contains two primary sections and a third section for notes. the
first section explains how to use existing streams within an application. the
second section explains how to create new types of streams.
types of streams
there are four fundamental stream types within node.js:
: streams to which data can be written (for example,
: streams from which data can be read (for example,
: streams that are both  and  (for example,
:  streams that can modify or transform the data as it
is written and read (for example, ).
additionally, this module includes the utility functions
streams promises api
the  api provides an alternative set of asynchronous utility
functions for streams that return  objects rather than using
callbacks. the api is accessible via
object mode
all streams created by node.js apis operate exclusively on strings and
(or ) objects. it is possible, however, for stream implementations
to work with other types of javascript values (with the exception of ,
which serves a special purpose within streams). such streams are considered to
operate in "object mode".
stream instances are switched into object mode using the  option
when the stream is created. attempting to switch an existing stream into
object mode is not safe.
buffering
both  and  streams will store data in an internal
buffer.
the amount of data potentially buffered depends on the  option
passed into the stream's constructor. for normal streams, the
option specifies a . for streams operating
in object mode, the  specifies a total number of objects.
data is buffered in  streams when the implementation calls
. if the consumer of the stream does not
call , the data will sit in the internal
queue until it is consumed.
once the total size of the internal read buffer reaches the threshold specified
by , the stream will temporarily stop reading data from the
underlying resource until the data currently buffered can be consumed (that is,
the stream will stop calling the internal  method that is
used to fill the read buffer).
data is buffered in  streams when the
method is called repeatedly. while the
total size of the internal write buffer is below the threshold set by
, calls to  will return . once
the size of the internal buffer reaches or exceeds the ,
a key goal of the  api, particularly the  method,
is to limit the buffering of data to acceptable levels such that sources and
destinations of differing speeds will not overwhelm the available memory.
the  option is a threshold, not a limit: it dictates the amount
of data that a stream buffers before it stops asking for more data. it does not
enforce a strict memory limitation in general. specific stream implementations
may choose to enforce stricter limits but doing so is optional.
because  and  streams are both  and
, each maintains two separate internal buffers used for reading and
writing, allowing each side to operate independently of the other while
maintaining an appropriate and efficient flow of data. for example,
instances are  streams whose  side allows
consumption of data received from the socket and whose  side allows
writing data to the socket. because data may be written to the socket at a
faster or slower rate than data is received, each side should
operate (and buffer) independently of the other.
the mechanics of the internal buffering are an internal implementation detail
and may be changed at any time. however, for certain advanced implementations,
the internal buffers can be retrieved using  or
. use of these undocumented properties is discouraged.
api for stream consumers
almost all node.js applications, no matter how simple, use streams in some
manner. the following is an example of using streams in a node.js application
that implements an http server:
streams (such as  in the example) expose methods such as
and  that are used to write data onto the stream.
streams use the  api for notifying application
code when data is available to be read off the stream. that available data can
be read from the stream in multiple ways.
both  and  streams use the  api in
various ways to communicate the current state of the stream.
and  streams are both  and
applications that are either writing data to or consuming data from a stream
are not required to implement the stream interfaces directly and will generally
have no reason to call .
developers wishing to implement new types of streams should refer to the
section .
writable streams
writable streams are an abstraction for a destination to which data is
examples of  streams include:
some of these examples are actually  streams that implement the
interface.
all  streams implement the interface defined by the
class.
while specific instances of  streams may differ in various ways,
all  streams follow the same fundamental usage pattern as illustrated
in the example below:
the  event is emitted when the stream and any of its underlying
resources (a file descriptor, for example) have been closed. the event indicates
that no more events will be emitted, and no further computation will occur.
a  stream will always emit the  event if it is
created with the  option.
if a call to  returns , the
event will be emitted when it is appropriate to resume writing data
to the stream.
the  event is emitted if an error occurred while writing or piping
data. the listener callback is passed a single  argument when called.
the stream is closed when the  event is emitted unless the
option was set to  when creating the
stream.
after , no further events other than  should be emitted
(including  events).
the  event is emitted after the  method
has been called, and all data has been flushed to the underlying system.
source stream that is piping to this writable
the  event is emitted when the  method is called on
a readable stream, adding this writable to its set of destinations.
the source stream that
this writable
the  event is emitted when the  method is called
on a  stream, removing this  from its set of
destinations.
this is also emitted in case this  stream emits an error when a
stream pipes into it.
the  method forces all written data to be buffered in memory.
the buffered data will be flushed when either the  or
the primary intent of  is to accommodate a situation in which
several small chunks are written to the stream in rapid succession. instead of
immediately forwarding them to the underlying destination,
buffers all the chunks until  is called, which will pass them
all to , if present. this prevents a head-of-line blocking
situation where data is being buffered while waiting for the first small chunk
to be processed. however, use of  without implementing
may have an adverse effect on throughput.
see also: , .
destroy the stream. optionally emit an  event, and emit a
event (unless  is set to ). after this call, the writable
stream has ended and subsequent calls to  or  will result in
this is a destructive and immediate way to destroy a stream. previous calls to
may not have drained, and may trigger an  error.
use  instead of destroy if data should flush before close, or wait for
the  event before destroying the stream.
once  has been called any further calls will be a no-op and no
further errors except from  may be emitted as .
implementors should not override this method,
but instead implement .
is  after  has been emitted.
|  |  |  optional data to write. for streams
not operating in object mode,  must be a string,  or
. for object mode streams,  may be any javascript value
other than .
the encoding if  is a string
callback for when the stream is finished.
calling the  method signals that no more data will be written
to the . the optional  and  arguments allow one
final additional chunk of data to be written immediately before closing the
calling the  method after calling
will raise an error.
the new default encoding
the  method sets the default  for a
the  method flushes all data buffered since
when using  and  to manage the buffering
of writes to a stream, defer calls to  using
. doing so allows batching of all
calls that occur within a given node.js event loop phase.
if the  method is called multiple times on a stream, the
same number of calls to  must be called to flush the buffered
added in: v11.4.0
is  if it is safe to call , which means
the stream has not been destroyed, errored, or ended.
returns whether the stream was destroyed or errored before emitting .
number of times  needs to be
called in order to fully uncork the stream.
returns error if the stream has been destroyed with an error.
is set to  immediately before the  event is emitted.
return the value of  passed when creating this .
this property contains the number of bytes (or objects) in the queue
ready to be written. the value provides introspection data regarding
the status of the .
is  if the stream's buffer has been full and stream will emit .
getter for the property  of a given  stream.
|  the encoding, if  is a string. default:
callback for when this chunk of data is flushed.
returns:   if the stream wishes for the calling code to
wait for the  event to be emitted before continuing to write
additional data; otherwise .
the  method writes some data to the stream, and calls the
supplied  once the data has been fully handled. if an error
occurs, the  will be called with the error as its
first argument. the  is called asynchronously and before  is
the return value is  if the internal buffer is less than the
configured when the stream was created after admitting .
if  is returned, further attempts to write data to the stream should
stop until the  event is emitted.
while a stream is not draining, calls to  will buffer , and
return false. once all currently buffered chunks are drained (accepted for
delivery by the operating system), the  event will be emitted.
once  returns false, do not write more chunks
until the  event is emitted. while calling  on a stream that
is not draining is allowed, node.js will buffer all written chunks until
maximum memory usage occurs, at which point it will abort unconditionally.
even before it aborts, high memory usage will cause poor garbage collector
performance and high rss (which is not typically released back to the system,
even after the memory is no longer required). since tcp sockets may never
drain if the remote peer does not read the data, writing a socket that is
not draining may lead to a remotely exploitable vulnerability.
writing data while the stream is not draining is particularly
problematic for a , because the  streams are paused
by default until they are piped or a  or  event handler
is added.
if the data to be written can be generated or fetched on demand, it is
recommended to encapsulate the logic into a  and use
. however, if calling  is preferred, it is
possible to respect backpressure and avoid memory issues using the
a  stream in object mode will always ignore the  argument.
readable streams
readable streams are an abstraction for a source from which data is
consumed.
two reading modes
streams effectively operate in one of two modes: flowing and
paused. these modes are separate from .
a  stream can be in object mode or not, regardless of whether
it is in flowing mode or paused mode.
in flowing mode, data is read from the underlying system automatically
and provided to an application as quickly as possible using events via the
in paused mode, the  method must be called
explicitly to read chunks of data from the stream.
all  streams begin in paused mode but can be switched to flowing
mode in one of the following ways:
adding a  event handler.
calling the  method.
calling the  method to send the data to a .
the  can switch back to paused mode using one of the following:
if there are no pipe destinations, by calling the
if there are pipe destinations, by removing all pipe destinations.
multiple pipe destinations may be removed by calling the
the important concept to remember is that a  will not generate data
until a mechanism for either consuming or ignoring that data is provided. if
the consuming mechanism is disabled or taken away, the  will attempt
to stop generating the data.
for backward compatibility reasons, removing  event handlers will
not automatically pause the stream. also, if there are piped destinations,
then calling  will not guarantee that the
stream will remain paused once those destinations drain and ask for more data.
if a  is switched into flowing mode and there are no consumers
available to handle the data, that data will be lost. this can occur, for
instance, when the  method is called without a listener
attached to the  event, or when a  event handler is removed
from the stream.
adding a  event handler automatically makes the stream
stop flowing, and the data has to be consumed via
. if the  event handler is
removed, then the stream will start flowing again if there is a
event handler.
three states
the "two modes" of operation for a  stream are a simplified
abstraction for the more complicated internal state management that is happening
within the  stream implementation.
specifically, at any given point in time, every  is in one of three
possible states:
when  is , no mechanism for consuming the
stream's data is provided. therefore, the stream will not generate data.
while in this state, attaching a listener for the  event, calling the
method, or calling the  method will switch
to , causing the  to begin actively
emitting events as data is generated.
calling , , or receiving backpressure
will cause the  to be set as ,
temporarily halting the flowing of events but not halting the generation of
data. while in this state, attaching a listener for the  event
will not switch  to .
while  is , data may be accumulating
within the stream's internal buffer.
choose one api style
the  stream api evolved across multiple node.js versions and provides
multiple methods of consuming stream data. in general, developers should choose
one of the methods of consuming data and should never use multiple methods
to consume data from a single stream. specifically, using a combination
of , , , or async iterators could
lead to unintuitive behavior.
|  |  the chunk of data. for streams that are not
operating in object mode, the chunk will be either a string or .
for streams that are in object mode, the chunk can be any javascript value
the  event is emitted whenever the stream is relinquishing ownership of
a chunk of data to a consumer. this may occur whenever the stream is switched
in flowing mode by calling , , or by
attaching a listener callback to the  event. the  event will
also be emitted whenever the  method is called and a chunk of
data is available to be returned.
attaching a  event listener to a stream that has not been explicitly
paused will switch the stream into flowing mode. data will then be passed as
soon as it is available.
the listener callback will be passed the chunk of data as a string if a default
encoding has been specified for the stream using the
method; otherwise the data will be passed as a
the  event is emitted when there is no more data to be consumed from
the stream.
the  event will not be emitted unless the data is completely
consumed. this can be accomplished by switching the stream into flowing mode,
or by calling  repeatedly until all data has been
the  event may be emitted by a  implementation at any time.
typically, this may occur if the underlying stream is unable to generate data
due to an underlying internal failure, or when a stream implementation attempts
to push an invalid chunk of data.
the listener callback will be passed a single  object.
the  event is emitted when  is called
and  is not .
the  event is emitted when there is data available to be read from
the stream or when the end of the stream has been reached. effectively, the
event indicates that the stream has new information. if data is
available,  will return that data.
if the end of the stream has been reached, calling
will return  and trigger the
event. this is also true if there never was any data to be read. for instance,
in the following example,  is an empty file:
the output of running this script is:
in some cases, attaching a listener for the  event will cause some
amount of data to be read into an internal buffer.
in general, the  and  event mechanisms are easier to
understand than the  event. however, handling  might
result in increased throughput.
if both  and  are used at the same time,
takes precedence in controlling the flow, i.e.  will be emitted
only when  is called. the
property would become .
if there are  listeners when  is removed, the stream
will start flowing, i.e.  events will be emitted without calling
the  event is emitted when  is
called and  is not .
error which will be passed as payload in  event
event (unless  is set to ). after this call, the readable
stream will release any internal resources and subsequent calls to
will be ignored.
implementors should not override this method, but instead implement
the  method returns the current operating state of the
. this is used primarily by the mechanism that underlies the
method. in most typical cases, there will be no reason to
use this method directly.
the  method will cause a stream in flowing mode to stop
emitting  events, switching out of flowing mode. any data that
becomes available will remain in the internal buffer.
the  method has no effect if there is a
the destination for writing data
pipe options
end the writer when the reader ends. default: .
returns:  the destination, allowing for a chain of pipes if
it is a  or a  stream
the  method attaches a  stream to the ,
causing it to switch automatically into flowing mode and push all of its data
to the attached . the flow of data will be automatically managed
so that the destination  stream is not overwhelmed by a faster
the following example pipes all of the data from the  into a file
named :
it is possible to attach multiple  streams to a single
the  method returns a reference to the destination stream
making it possible to set up chains of piped streams:
by default,  is called on the destination
stream when the source  stream emits , so that the
destination is no longer writable. to disable this default behavior, the
option can be passed as , causing the destination stream to remain open:
one important caveat is that if the  stream emits an error during
processing, the  destination is not closed automatically. if an
error occurs, it will be necessary to manually close each stream in order
to prevent memory leaks.
the  and   streams are never
closed until the node.js process exits, regardless of the specified options.
optional argument to specify how much data to read.
returns:  |  |  |
the  method reads data out of the internal buffer and
returns it. if no data is available to be read,  is returned. by default,
the data is returned as a  object unless an encoding has been
specified using the  method or the stream is operating
in object mode.
the optional  argument specifies a specific number of bytes to read. if
bytes are not available to be read,  will be returned unless
the stream has ended, in which case all of the data remaining in the internal
buffer will be returned.
if the  argument is not specified, all of the data contained in the
internal buffer will be returned.
the  argument must be less than or equal to 1 gib.
the  method should only be called on  streams
operating in paused mode. in flowing mode,  is called
automatically until the internal buffer is fully drained.
each call to  returns a chunk of data, or . the chunks
are not concatenated. a  loop is necessary to consume all data
currently in the buffer. when reading a large file  may return ,
having consumed all buffered content so far, but there is still more data to
come not yet buffered. in this case a new  event will be emitted
when there is more data in the buffer. finally the  event will be
emitted when there is no more data to come.
therefore to read a file's whole contents from a , it is necessary
to collect chunks across multiple  events:
a  stream in object mode will always return a single item from
a call to , regardless of the value of the
if the  method returns a chunk of data, a  event will
also be emitted.
calling  after the  event has
been emitted will return . no runtime error will be raised.
the stream has not been destroyed or emitted  or .
added in: v16.8.0
added in: v16.7.0, v14.18.0
returns whether  has been emitted.
getter for the property  of a given  stream. the
property can be set using the  method.
becomes  when  event is emitted.
this property reflects the current state of a  stream as described
in the  section.
returns the value of  passed when creating this .
ready to be read. the value provides introspection data regarding
the  method causes an explicitly paused  stream to
resume emitting  events, switching the stream into flowing mode.
the  method can be used to fully consume the data from a
stream without actually processing any of that data:
the encoding to use.
the  method sets the character encoding for
data read from the  stream.
by default, no encoding is assigned and stream data will be returned as
objects. setting an encoding causes the stream data
to be returned as strings of the specified encoding rather than as
objects. for instance, calling  will cause the
output data to be interpreted as utf-8 data, and passed as strings. calling
will cause the data to be encoded in hexadecimal
string format.
the  stream will properly handle multi-byte characters delivered
through the stream that would otherwise become improperly decoded if simply
pulled from the stream as  objects.
optional specific stream to unpipe
the  method detaches a  stream previously attached
using the  method.
if the  is not specified, then all pipes are detached.
if the  is specified, but no pipe is set up for it, then
the method does nothing.
|  |  |  |  chunk of data to unshift onto the
read queue. for streams not operating in object mode,  must be a
string, , , or . for object mode streams,
may be any javascript value.
encoding of string chunks. must be a valid
encoding, such as  or .
passing  as  signals the end of the stream (eof) and behaves the
same as , after which no more data can be written. the eof
signal is put at the end of the buffer and any buffered data will still be
flushed.
the  method pushes a chunk of data back into the internal
buffer. this is useful in certain situations where a stream is being consumed by
code that needs to "un-consume" some amount of data that it has optimistically
pulled out of the source, so that the data can be passed on to some other party.
the  method cannot be called after the  event
has been emitted or a runtime error will be thrown.
developers using  often should consider switching to
use of a  stream instead. see the
section for more information.
unlike ,  will not
end the reading process by resetting the internal reading state of the stream.
this can cause unexpected results if  is called during a
read (i.e. from within a  implementation on a
custom stream). following the call to  with an immediate
will reset the reading state appropriately,
however it is best to simply avoid calling  while in the
process of performing a read.
an "old style" readable stream
prior to node.js 0.10, streams did not implement the entire
module api as it is currently defined. (see  for more
information.)
when using an older node.js library that emits  events and has a
method that is advisory only, the
method can be used to create a  stream that uses
the old stream as its data source.
it will rarely be necessary to use  but the method has been
provided as a convenience for interacting with older node.js applications and
returns:  to fully consume the stream.
if the loop terminates with a , , or a , the stream will
be destroyed. in other terms, iterating over a stream will consume the stream
fully. the stream will be read in chunks of size equal to the
option. in the code example above, data will be in a single chunk if the file
has less then 64 kib of data because no  option is provided to
added in: v16.3.0
when set to , calling  on the
async iterator, or exiting a  iteration using a ,
, or  will not destroy the stream. default: .
returns:  to consume the stream.
the iterator created by this method gives users the option to cancel the
destruction of the stream if the  loop is exited by ,
, or , or if the iterator should destroy the stream if the stream
emitted an error during iteration.
|  a function to map over every chunk in the
a chunk of data from the stream.
aborted if the stream is destroyed allowing to
abort the  call early.
the maximum concurrent invocation of  to call
on the stream at once. default: .
allows destroying the stream if the signal is
aborted.
returns:  a stream mapped with the function .
this method allows mapping over the stream. the  function will be called
for every chunk in the stream. if the  function returns a promise - that
promise will be ed before being passed to the result stream.
|  a function to filter chunks from the stream.
returns:  a stream filtered with the predicate .
this method allows filtering the stream. for each chunk in the stream the
function will be called and if it returns a truthy value, the chunk will be
passed to the result stream. if the  function returns a promise - that
promise will be ed.
|  a function to call on each chunk of the stream.
returns:  a promise for when the stream has finished.
this method allows iterating a stream. for each chunk in the stream the
function will be called. if the  function returns a promise - that
this method is different from  loops in that it can optionally
process chunks concurrently. in addition, a  iteration can only be
stopped by having passed a  option and aborting the related
while  can be stopped with  or
. in either case the stream will be destroyed.
this method is different from listening to the  event in that it
uses the  event in the underlying machinary and can limit the
number of concurrent  calls.
allows cancelling the toarray operation if the
signal is aborted.
returns:  a promise containing an array with the contents of the
this method allows easily obtaining the contents of a stream.
as this method reads the entire stream into memory, it negates the benefits of
streams. it's intended for interoperability and convenience, not as the primary
way to consume streams.
returns:  a promise evaluating to  if  returned a truthy
value for at least one of the chunks.
this method is similar to  and calls  on each chunk
in the stream until the awaited return value is  (or any truthy value).
once an  call on a chunk awaited return value is truthy, the stream is
destroyed and the promise is fulfilled with . if none of the
calls on the chunks return a truthy value, the promise is fulfilled with
returns:  a promise evaluating to the first chunk for which
evaluated with a truthy value, or  if no element was found.
in the stream to find a chunk with a truthy value for . once an  call's
awaited return value is truthy, the stream is destroyed and the promise is
fulfilled with value for which  returned a truthy value. if all of the
calls on the chunks return a falsy value, the promise is fulfilled with
value for all of the chunks.
in the stream to check if all awaited return values are truthy value for .
once an  call on a chunk awaited return value is falsy, the stream is
destroyed and the promise is fulfilled with . if all of the  calls
on the chunks return a truthy value, the promise is fulfilled with .
|  |  a function to map over
every chunk in the stream.
returns:  a stream flat-mapped with the function .
this method returns a new stream by applying the given callback to each
chunk of the stream and then flattening the result.
it is possible to return a stream or another iterable or async iterable from
and the result streams will be merged (flattened) into the returned
the number of chunks to drop from the readable.
returns:  a stream with  chunks dropped.
this method returns a new stream with the first  chunks dropped.
the number of chunks to take from the readable.
returns:  a stream with  chunks taken.
this method returns a new stream with the first  chunks.
returns:  a stream of indexed pairs.
this method returns a new stream with chunks of the underlying stream paired
with a counter in the form . the first index value is 0 and it
increases by 1 for each chunk produced.
|  a reducer function to call over every chunk
in the stream.
the value obtained from the last call to  or the
value if specified or the first chunk of the stream otherwise.
the initial value to use in the reduction.
returns:  a promise for the final value of the reduction.
this method calls  on each chunk of the stream in order, passing it the
result from the calculation on the previous element. it returns a promise for
the final value of the reduction.
the reducer function iterates the stream element-by-element which means that
there is no  parameter or parallelism. to perform a
concurrently, it can be chained to the  method.
if no  value is supplied the first chunk of the stream is used as the
initial value. if the stream is empty, the promise is rejected with a
with the  code property.
duplex and transform streams
duplex streams are streams that implement both the  and
interfaces.
if  then the stream will automatically end the writable side when the
readable side ends. set initially by the  constructor option,
which defaults to .
this can be changed manually to change the half-open behavior of an existing
stream instance, but must be changed before the  event is
transform streams are  streams where the output is in some way
related to the input. like all  streams,  streams
implement both the  and  interfaces.
destroy the stream, and optionally emit an  event. after this call, the
transform stream would release any internal resources.
the default implementation of  for  also emit
unless  is set in false.
once  has been called, any further calls will be a no-op and no
a readable and/or writable stream.
if set to , then a call to  is
not treated as finished. default: .
when set to , the callback will be called when
the stream ends even though the stream might still be readable.
the stream ends even though the stream might still be writable.
allows aborting the wait for the stream finish. the
underlying stream will not be aborted if the signal is aborted. the
callback will get called with an . all registered
listeners added by this function will also be removed.
a callback function that takes an optional error
returns:  a cleanup function which removes all registered
a function to get notified when a stream is no longer readable, writable
or has experienced an error or a premature close event.
especially useful in error handling scenarios where a stream is destroyed
prematurely (like an aborted http request), and will not emit
the  api provides promise version:
leaves dangling event listeners (in particular
, ,  and ) after  has been
invoked. the reason for this is so that unexpected  events (due to
incorrect stream implementations) do not cause unexpected crashes.
if this is unwanted behavior then the returned cleanup function needs to be
invoked in the callback:
called when the pipeline is fully done.
resolved value of  returned by .
a module method to pipe between streams and generators forwarding errors and
properly cleaning up and provide a callback when the pipeline is complete.
the  api provides a promise version, which can also
receive an options argument as the last parameter with a
property. when the signal is aborted,
will be called on the underlying pipeline, with an
to use an , pass it inside an options object,
as the last argument:
the  api also supports async generators:
remember to handle the  argument passed into the async generator.
especially in the case where the async generator is the source for the
pipeline (i.e. first argument) or the pipeline will never complete.
will call  on all streams except:
streams which have emitted  or .
leaves dangling event listeners on the streams
after the  has been invoked. in the case of reuse of streams after
failure, this can cause event listener leaks and swallowed errors. if the last
stream is readable, dangling event listeners will be removed so that the last
stream can be consumed later.
closes all the streams when an error is raised.
the  usage with  could lead to an unexpected behavior
once it would destroy the socket without sending the expected response.
see the example below:
-  is experimental.
combines two or more streams into a  stream that writes to the
first stream and reads from the last. each provided stream is piped into
the next, using . if any of the streams error then all
are destroyed, including the outer  stream.
because  returns a new stream that in turn can (and
should) be piped into other streams, it enables composition. in contrast,
when passing streams to , typically the first stream is
a readable stream and the last a writable stream, forming a closed
circuit.
if passed a  it must be a factory method taking a
can be used to convert async iterables, generators and
functions into streams.
converts into a readable . cannot yield
converts into a readable/writable transform .
must take a source  as first parameter. cannot yield
converts into a writable . must return
either  or .
added in: v12.3.0, v10.17.0
object implementing the  or
iterable protocol. emits an 'error' event if a null
value is passed.
options provided to .
by default,  will set  to , unless
this is explicitly opted out by setting  to .
a utility method for creating readable streams out of iterators.
calling  or  will not have
the strings or buffers be iterated to match the other streams semantics
for performance reasons.
returns whether the stream has been read from or cancelled.
returns whether the stream has encountered an error.
returns whether the stream is readable.
|  |  |  |  |  |  |  |  |
a utility method for creating duplex streams.
converts writable stream into writable  and readable stream
converts into readable .
converts into a readable/writable transform
. must take a source  as first parameter. cannot yield
converts  and
into  and then combines them into  where the
will write to the  and read from the .
converts into readable . value  is ignored.
a signal representing possible cancellation
a stream to attach a signal to
attaches an abortsignal to a readable or writeable stream. this lets code
control stream destruction using an .
calling  on the  corresponding to the passed
will behave the same way as calling
on the stream.
or using an  with a readable stream as an async iterable:
api for stream implementers
the  module api has been designed to make it possible to easily
implement streams using javascript's prototypal inheritance model.
first, a stream developer would declare a new javascript class that extends one
of the four basic stream classes (, ,
, or ), making sure they call the appropriate
parent class constructor:
when extending streams, keep in mind what options the user
can and should provide before forwarding these to the base constructor. for
example, if the implementation makes assumptions in regard to the
and  options, do not allow the
user to override these. be explicit about what
options are forwarded instead of implicitly forwarding all options.
the new stream class must then implement one or more specific methods, depending
on the type of stream being created, as detailed in the chart below:
the implementation code for a stream should never call the "public" methods
of a stream that are intended for use by consumers (as described in the
section). doing so may lead to adverse side effects
in application code consuming the stream.
avoid overriding public methods such as , , ,
,  and , or emitting internal events such
as , , ,  and  through .
doing so can break current and future stream invariants leading to behavior
and/or compatibility issues with other streams, stream utilities, and user
expectations.
simplified construction
added in: v1.2.0
for many simple cases, it is possible to create a stream without relying on
inheritance. this can be accomplished by directly creating instances of the
, , , or
objects and passing appropriate methods as constructor options.
implementing a writable stream
the  class is extended to implement a  stream.
custom  streams must call the
constructor and implement the  and/or
buffer level when
starts returning . default:
(16 kib), or  for  streams.
whether to encode s passed to
to s (with the encoding
specified in the  call) before passing
them to . other types of data are not
converted (i.e. s are not decoded into s). setting to
false will prevent s from being converted. default: .
the default encoding that is used when no
encoding is specified as an argument to .
whether or not the
is a valid operation. when set,
it becomes possible to write javascript values other than string,
or  if supported by the stream implementation.
whether or not the stream should emit
after it has been destroyed. default: .
implementation for the
whether this stream should automatically call
on itself after ending. default: .
a signal representing possible cancellation.
or, when using pre-es6 style constructors:
or, using the simplified constructor approach:
on the writeable stream.
call this function (optionally with an error
argument) when the stream has finished initializing.
the  method must not be called directly. it may be implemented
by child classes, and if so, will be called by the internal
class methods only.
this optional function will be called in a tick after the stream constructor
has returned, delaying any ,  and  calls until
is called. this is useful to initialize state or asynchronously
initialize resources before the stream can be used.
|  |  the  to be written, converted from the
passed to . if the stream's
option is  or the stream is operating in object mode,
the chunk will not be converted & will be whatever was passed to
if the chunk is a string, then  is the
character encoding of that string. if chunk is a , or if the
stream is operating in object mode,  may be ignored.
argument) when processing is complete for the supplied chunk.
all  stream implementations must provide a
and/or
method to send data to the underlying
streams provide their own implementation of the
this function must not be called by application code directly. it should be
implemented by child classes, and called by the internal  class
methods only.
the  function must be called synchronously inside of
or asynchronously (i.e. different tick) to signal either
that the write completed successfully or failed with an error.
the first argument passed to the  must be the  object if the
call failed or  if the write succeeded.
all calls to  that occur between the time
is called and the  is called will cause the written data to be
buffered. when the  is invoked, the stream might emit a
event. if a stream implementation is capable of processing multiple chunks of
data at once, the  method should be implemented.
if the  property is explicitly set to  in the constructor
options, then  will remain the same object that is passed to ,
and may be a string rather than a . this is to support implementations
that have an optimized handling for certain string data encodings. in that case,
the  argument will indicate the character encoding of the string.
otherwise, the  argument can be safely ignored.
the  method is prefixed with an underscore because it is
internal to the class that defines it, and should never be called directly by
user programs.
the data to be written. the value is an array of
that each represent a discrete chunk of data to write. the properties of
these objects are:
|  a buffer instance or string containing the data to
be written. the  will be a string if the  was created with
the  option set to  and a string was passed to .
the character encoding of the . if  is
a , the  will be .
a callback function (optionally with an error
argument) to be invoked when processing is complete for the supplied chunks.
the  method may be implemented in addition or alternatively
to  in stream implementations that are capable of processing
multiple chunks of data at once. if implemented and if there is buffered data
from previous writes,  will be called instead of .
a possible error.
the  method is called by .
it can be overridden by child classes but it must not be called directly.
furthermore, the  should not be mixed with async/await
once it is executed when a promise is resolved.
argument) when finished writing any remaining data.
this optional function will be called before the stream closes, delaying the
event until  is called. this is useful to close resources
or write buffered data before a stream ends.
errors while writing
errors occurring during the processing of the ,
and  methods must be propagated
by invoking the callback and passing the error as the first argument.
throwing an  from within these methods or manually emitting an
event results in undefined behavior.
if a  stream pipes into a  stream when  emits an
error, the  stream will be unpiped.
an example writable stream
the following illustrates a rather simplistic (and somewhat pointless) custom
stream implementation. while this specific  stream instance
is not of any real particular usefulness, the example illustrates each of the
required elements of a custom  stream instance:
decoding buffers in a writable stream
decoding buffers is a common task, for instance, when using transformers whose
input is a string. this is not a trivial process when using multi-byte
characters encoding, such as utf-8. the following example shows how to decode
multi-byte strings using  and .
implementing a readable stream
constructor and implement the  method.
the maximum  to store
in the internal buffer before ceasing to read from the underlying resource.
default:  (16 kib), or  for  streams.
if specified, then buffers will be decoded to
strings using the specified encoding. default: .
whether this stream should behave
as a stream of objects. meaning that  returns
a single value instead of a  of size . default: .
on the readable created.
this optional function will be scheduled in the next tick by the stream
constructor, delaying any  and  calls until  is
called. this is useful to initialize state or asynchronously initialize
resources before the stream can be used.
number of bytes to read asynchronously
all  stream implementations must provide an implementation of the
method to fetch data from the underlying resource.
when  is called, if data is available from the resource,
the implementation should begin pushing that data into the read queue using the
method.  will be called again
after each call to  once the stream is
ready to accept more data.  may continue reading from the resource and
pushing data until  returns . only when  is
called again after it has stopped should it resume pushing additional data into
the queue.
once the  method has been called, it will not be called
again until more data is pushed through the
method. empty data such as empty buffers and strings will not cause
to be called.
the  argument is advisory. for implementations where a "read" is a
single operation that returns data can use the  argument to determine how
much data to fetch. other implementations may ignore this argument and simply
provide data whenever it becomes available. there is no need to "wait" until
bytes are available before calling .
|  |  |  |  chunk of data to push into the
string,  or . for object mode streams,  may be
any javascript value.
returns:   if additional chunks of data may continue to be
pushed;  otherwise.
when  is a , , or , the  of data will
be added to the internal queue for users of the stream to consume.
passing  as  signals the end of the stream (eof), after which no
more data can be written.
when the  is operating in paused mode, the data added with
can be read out by calling the
method when the  event is
when the  is operating in flowing mode, the data added with
will be delivered by emitting a  event.
the  method is designed to be as flexible as possible. for
example, when wrapping a lower-level source that provides some form of
pause/resume mechanism, and a data callback, the low-level source can be wrapped
by the custom  instance:
the  method is used to push the content
into the internal buffer. it can be driven by the  method.
for streams not operating in object mode, if the  parameter of
is , it will be treated as empty string or
buffer. see  for more information.
errors while reading
errors occurring during processing of the  must be
propagated through the  method.
throwing an  from within  or manually emitting an
an example counting stream
the following is a basic example of a  stream that emits the numerals
from 1 to 1,000,000 in ascending order, and then ends.
implementing a duplex stream
a  stream is one that implements both  and
, such as a tcp socket connection.
because javascript does not have support for multiple inheritance, the
class is extended to implement a  stream (as opposed
to extending the  and  classes).
the  class prototypically inherits from  and
parasitically from , but  will work properly for
both base classes due to overriding  on
constructor and implement both the  and
passed to both  and
constructors. also has the following fields:
if set to , then the stream will
sets whether the  should be readable.
sets whether the  should be writable.
sets  for readable side of the
stream. has no effect if  is . default: .
sets  for writable side of the
sets  for the readable side
of the stream. has no effect if  is provided.
sets  for the writable side
when using pipeline:
an example duplex stream
the following illustrates a simple example of a  stream that wraps a
hypothetical lower-level source object to which data can be written, and
from which data can be read, albeit using an api that is not compatible with
node.js streams.
the following illustrates a simple example of a  stream that buffers
incoming written data via the  interface that is read back out
via the  interface.
the most important aspect of a  stream is that the  and
sides operate independently of one another despite co-existing within
a single object instance.
object mode duplex streams
for  streams,  can be set exclusively for either the
or  side using the  and
options respectively.
in the following example, for instance, a new  stream (which is a
type of  stream) is created that has an object mode  side
that accepts javascript numbers that are converted to hexadecimal strings on
the  side.
implementing a transform stream
a  stream is a  stream where the output is computed
in some way from the input. examples include  streams or
streams that compress, encrypt, or decrypt data.
there is no requirement that the output be the same size as the input, the same
number of chunks, or arrive at the same time. for example, a  stream will
only ever have a single chunk of output which is provided when the input is
ended. a  stream will produce output that is either much smaller or much
larger than its input.
implements its own versions of the  and
methods. custom  implementations must
implement the  method and may
also implement the  method.
care must be taken when using  streams in that data written to the
stream can cause the  side of the stream to become paused if the
output on the  side is not consumed.
the  event is from the  class. the  event is
emitted after all data has been output, which occurs after the callback in
has been called. in the case of an error,
should not be emitted.
the  event is from the  class. the
event is emitted after  is called and all chunks
have been processed by . in the case
of an error,  should not be emitted.
argument and data) to be called when remaining data has been flushed.
in some cases, a transform operation may need to emit an additional bit of
data at the end of the stream. for example, a  compression stream will
store an amount of internal state used to optimally compress the output. when
the stream ends, however, that additional data needs to be flushed so that the
compressed data will be complete.
custom  implementations may implement the
method. this will be called when there is no more written data to be consumed,
but before the  event is emitted signaling the end of the
within the  implementation, the  method
may be called zero or more times, as appropriate. the  function must
be called when the flush operation is complete.
|  |  the  to be transformed, converted from
the  passed to . if the stream's
if the chunk is a string, then this is the
encoding type. if chunk is a buffer, then this is the special
value . ignore it in that case.
argument and data) to be called after the supplied  has been
processed.
method to accept input and produce output. the
implementation handles the bytes being written, computes an output, then passes
that output off to the readable portion using the  method.
the  method may be called zero or more times to generate
output from a single input chunk, depending on how much is to be output
as a result of the chunk.
it is possible that no output is generated from any given chunk of input data.
the  function must be called only when the current chunk is completely
consumed. the first argument passed to the  must be an  object
if an error occurred while processing the input or  otherwise. if a second
argument is passed to the , it will be forwarded on to the
method. in other words, the following are equivalent:
the  method is prefixed with an underscore because it
is internal to the class that defines it, and should never be called directly by
is never called in parallel; streams implement a
queue mechanism, and to receive the next chunk,  must be
called, either synchronously or asynchronously.
the  class is a trivial implementation of a
stream that simply passes the input bytes across to the output. its purpose is
primarily for examples and testing, but there are some use cases where
is useful as a building block for novel sorts of streams.
additional notes
streams compatibility with async generators and async iterators
with the support of async generators and iterators in javascript, async
generators are effectively a first-class language-level stream construct at
this point.
some common interop cases of using node.js streams with async generators
and async iterators are provided below.
consuming readable streams with async iterators
async iterators register a permanent error handler on the stream to prevent any
unhandled post-destroy errors.
creating readable streams with async generators
a node.js readable stream can be created from an asynchronous generator using
the  utility method:
piping to writable streams from async iterators
when writing to a writable stream from an async iterator, ensure correct
handling of backpressure and errors.  abstracts away
the handling of backpressure and backpressure-related errors:
compatibility with older node.js versions
prior to node.js 0.10, the  stream interface was simpler, but also
less powerful and less useful.
rather than waiting for calls to the  method,
events would begin emitting immediately. applications that
would need to perform some amount of work to decide how to handle data
were required to store read data into buffers so the data would not be lost.
the  method was advisory, rather than
guaranteed. this meant that it was still necessary to be prepared to receive
events even when the stream was in a paused state.
in node.js 0.10, the  class was added. for backward
compatibility with older node.js programs,  streams switch into
"flowing mode" when a  event handler is added, or when the
method is called. the effect is that, even
when not using the new  method and
event, it is no longer necessary to worry about losing
chunks.
while most applications will continue to function normally, this introduces an
edge case in the following conditions:
no  event listener is added.
the  method is never called.
the stream is not piped to any writable destination.
prior to node.js 0.10, the incoming message data would be simply discarded.
however, in node.js 0.10 and beyond, the socket remains paused forever.
the workaround in this situation is to call the
method to begin the flow of data:
in addition to new  streams switching into flowing mode,
pre-0.10 style streams can be wrapped in a  class using the
there are some cases where it is necessary to trigger a refresh of the
underlying readable stream mechanisms, without actually consuming any
data. in such cases, it is possible to call , which will
always return .
if the internal read buffer is below the , and the
stream is not currently reading, then calling  will trigger
a low-level  call.
while most applications will almost never need to do this, there are
situations within node.js where this is done, particularly in the
stream class internals.
use of  is not recommended.
pushing a zero-byte string, , or  to a stream that is not in
object mode has an interesting side effect. because it is a call to
, the call will end the reading process.
however, because the argument is an empty string, no data is added to the
readable buffer so there is nothing for a user to consume.
discrepancy after calling
the use of  will change the behavior of how the
operates in non-object mode.
typically, the size of the current buffer is measured against the
in bytes. however, after  is called, the
comparison function will begin to measure the buffer's size in characters.
this is not a problem in common cases with  or . but it is
advised to be mindful about this behavior when working with strings that could
contain multi-byte characters.
string decoder
the  module provides an api for decoding  objects
into strings in a manner that preserves encoded multi-byte utf-8 and utf-16
characters. it can be accessed using:
the following example shows the basic use of the  class.
when a  instance is written to the  instance, an
internal buffer is used to ensure that the decoded string does not contain
any incomplete multibyte characters. these are held in the buffer until the
next call to  or until  is called.
in the following example, the three utf-8 encoded bytes of the european euro
symbol () are written over three separate operations:
the character  the  will use.
|  |  a , or , or
containing the bytes to decode.
returns any remaining input stored in the internal buffer as a string. bytes
representing incomplete utf-8 and utf-16 characters will be replaced with
substitution characters appropriate for the character encoding.
if the  argument is provided, one final call to
is performed before returning the remaining input.
after  is called, the  object can be reused for new input.
returns a decoded string, ensuring that any incomplete multibyte characters at
the end of the , or , or  are omitted from the
returned string and stored in an internal buffer for the next call to
test runner
the  module facilitates the creation of javascript tests that
report results in  format. to access it:
this module is only available under the  scheme. the following will not
work:
tests created via the  module consist of a single function that is
processed in one of three ways:
a synchronous function that is considered failing if it throws an exception,
and is considered passing otherwise.
a function that returns a  that is considered failing if the
rejects, and is considered passing if the  resolves.
a function that receives a callback function. if the callback receives any
truthy value as its first argument, the test is considered failing. if a
falsy value is passed as the first argument to the callback, the test is
considered passing. if the test function receives a callback function and
also returns a , the test will fail.
the following example illustrates how tests are written using the
as a test file executes, tap is written to the standard output of the node.js
process. this output can be interpreted by any test harness that understands
the tap format. if any tests fail, the process exit code is set to .
subtests
the test context's  method allows subtests to be created. this method
behaves identically to the top level  function. the following example
demonstrates the creation of a top level test with two subtests.
in this example,  is used to ensure that both subtests have completed.
this is necessary because parent tests do not wait for their subtests to
complete. any subtests that are still outstanding when their parent finishes
are cancelled and treated as failures. any subtest failures cause the parent
test to fail.
skipping tests
individual tests can be skipped by passing the  option to the test, or by
calling the test context's  method. both of these options support
including a message that is displayed in the tap output as shown in the
following example.
/ syntax
running tests can also be done using  to declare a suite
and  to declare a test.
a suite is used to organize and group related tests together.
is an alias for , except there is no test context passed,
since nesting is done using suites.
and  are imported from the  module.
tests
if node.js is started with the  command-line option, it is
possible to skip all top level tests except for a selected subset by passing
the  option to the tests that should be run. when a test with the
option set is run, all subtests are also run. the test context's
method can be used to implement the same behavior at the subtest level.
filtering tests by name
the  command-line option can be used to only run tests
whose name matches the provided pattern. test name patterns are interpreted as
javascript regular expressions. the  option can be
specified multiple times in order to run nested tests. for each test that is
executed, any corresponding test hooks, such as , are also
run.
given the following test file, starting node.js with the
option would cause the test runner to execute
, , and . if  did not match the test name
pattern, then its subtests would not execute, despite matching the pattern. the
same set of tests could also be executed by passing
multiple times (e.g. ,
, etc.).
test name patterns can also be specified using regular expression literals. this
allows regular expression flags to be used. in the previous example, starting
node.js with  would match  and
because the pattern is case-insensitive.
test name patterns do not change the set of files that the test runner executes.
extraneous asynchronous activity
once a test function finishes executing, the tap results are output as quickly
as possible while maintaining the order of the tests. however, it is possible
for the test function to generate asynchronous activity that outlives the test
itself. the test runner handles this type of activity, but does not delay the
reporting of test results in order to accommodate it.
in the following example, a test completes with two
operations still outstanding. the first  attempts to create a
new subtest. because the parent test has already finished and output its
results, the new subtest is immediately marked as failed, and reported in the
top level of the file's tap output.
the second  creates an  event.
and  events originating from a completed
test are handled by the  module and reported as diagnostic warnings in
the top level of the file's tap output.
running tests from the command line
the node.js test runner can be invoked from the command line by passing the
by default, node.js will recursively search the current directory for
javascript source files matching a specific naming convention. matching files
are executed as test files. more information on the expected test file naming
convention and behavior can be found in the
alternatively, one or more paths can be provided as the final argument(s) to
the node.js command, as shown below.
in this example, the test runner will execute the files  and
. the test runner will also recursively search the
directory for test files to execute.
test runner execution model
when searching for test files to execute, the test runner behaves as follows:
any files explicitly provided by the user are executed.
if the user did not explicitly specify any paths, the current working
directory is recursively searched for files as specified in the following
steps.
directories are skipped unless explicitly provided by the
user.
if a directory named  is encountered, the test runner will search it
recursively for all all , , and  files. all of these files
are treated as test files, and do not need to match the specific naming
convention detailed below. this is to accommodate projects that place all of
their tests in a single  directory.
in all other directories, , , and  files matching the
following patterns are treated as test files:
- files whose basename is the string . examples:
, , .
- files whose basename starts with the string
followed by one or more characters. examples: ,
- files whose basename ends with , , or
, preceded by one or more characters. examples: ,
other file types understood by node.js such as  and  are not
automatically executed by the test runner, but are supported if explicitly
provided on the command line.
each matching test file is executed in a separate child process. if the child
process finishes with an exit code of 0, the test is considered passing.
otherwise, the test is considered to be a failure. test files must be
executable by node.js, but are not required to use the  module
internally.
configuration options for running tests. the following
properties are supported:
|  if a number is provided,
then that many files would run in parallel.
if truthy, it would run (number of cpu cores - 1)
files in parallel.
if falsy, it would only run one file at a time.
if unspecified, subtests inherit this value from their parent.
:  an array containing the list of files to run.
default matching files from .
allows aborting an in-progress test execution.
a number of milliseconds the test execution will
fail after.
|  sets inspector port of test child process.
number. if a nullish value is provided, each process gets its own port,
incremented from the primary's .
the name of the test, which is displayed when reporting test
results. default: the  property of , or  if
does not have a name.
configuration options for the test. the following
then that many tests would run in parallel.
tests in parallel.
for subtests, it will be  tests in parallel.
if falsy, it would only run one test at a time.
if truthy, and the test context is configured to run
tests, then this test will be run. otherwise, the test is skipped.
allows aborting an in-progress test.
|  if truthy, the test is skipped. if a string is
provided, that string is displayed in the test results as the reason for
skipping the test. default: .
|  if truthy, the test marked as . if a string
is provided, that string is displayed in the test results as the reason why
the test is . default: .
a number of milliseconds the test will fail after.
|  the function under test. the first argument
to this function is a  object. if the test uses callbacks,
the callback function is passed as the second argument. default: a no-op
returns:  resolved with  once the test completes.
the  function is the value imported from the  module. each
invocation of this function results in the creation of a test point in the tap
output.
the  object passed to the  argument can be used to perform
actions related to the current test. examples include skipping the test, adding
additional tap diagnostic information, or creating subtests.
returns a  that resolves once the test completes. the return
value can usually be discarded for top level tests. however, the return value
from subtests should be used to prevent the parent test from finishing first
and cancelling the subtest as shown in the following example.
the  option can be used to fail the test if it takes longer than
milliseconds to complete. however, it is not a reliable mechanism for
canceling tests because a running test might block the application thread and
thus prevent the scheduled cancellation.
the name of the suite, which is displayed when reporting test
configuration options for the suite.
supports the same options as .
|  the function under suite
declaring all subtests and subsuites.
the first argument to this function is a  object.
returns: .
the  function imported from the  module. each
invocation of this function results in the creation of a subtest
and a test point in the tap output.
after invocation of top level  functions,
all top level tests and suites will execute.
shorthand for skipping a suite, same as .
shorthand for marking a suite as , same as
|  the function under test.
if the test uses callbacks, the callback function is passed as an argument.
the  function is the value imported from the  module.
each invocation of this function results in the creation of a test point in the
tap output.
shorthand for skipping a test,
same as .
shorthand for marking a test as ,
|  the hook function.
if the hook uses callbacks,
configuration options for the hook. the following
allows aborting an in-progress hook.
a number of milliseconds the hook will fail after.
this function is used to create a hook running before running a suite.
this function is used to create a hook running after  running a suite.
this function is used to create a hook running
before each subtest of the current suite.
after each subtest of the current test.
object, streaming a  output
will emit events, in the order of the tests definition
the diagnostic message.
emitted when  is called.
the test duration.
the failure casing test to fail.
the test name.
the ordinal number of the test.
|  present if  is called
emitted when a test fails.
emitted when a test passes.
an instance of  is passed to each test function in order to
interact with the test runner. however, the  constructor is not
exposed as part of the api.
|  the hook function. the first argument
to this function is a  object. if the hook uses callbacks,
before each subtest of the current test.
message to be displayed as a tap diagnostic.
this function is used to write tap diagnostics to the output. any diagnostic
information is included at the end of the test's results. this function does
not return a value.
the name of the test.
whether or not to run  tests.
if  is truthy, the test context will only run tests that
have the  option set. otherwise, all tests are run. if node.js was not
started with the  command-line option, this function is a
no-op.
can be used to abort test subtasks when the test has been
optional skip message to be displayed in tap output.
this function causes the test's output to indicate the test as skipped. if
is provided, it is included in the tap output. calling  does
not terminate execution of the test function. this function does not return a
optional  message to be displayed in tap output.
this function adds a  directive to the test's output. if  is
provided, it is included in the tap output. calling  does not terminate
execution of the test function. this function does not return a value.
the name of the subtest, which is displayed when reporting
test results. default: the  property of , or  if
configuration options for the subtest. the following
the number of tests that can be run at the same time.
this function is used to create subtests under the current test. this function
behaves in the same fashion as the top level  function.
an instance of  is passed to each suite function in order to
the name of the suite.
the  module exposes a global api for scheduling functions to
be called at some future period of time. because the timer functions are
globals, there is no need to call  to use the api.
the timer functions within node.js implement a similar api as the timers api
provided by web browsers but use a different internal implementation that is
built around the node.js .
this object is created internally and is returned from . it
can be passed to  in order to cancel the scheduled
actions.
by default, when an immediate is scheduled, the node.js event loop will continue
running as long as the immediate is active. the  object returned by
exports both  and
functions that can be used to control this default behavior.
if true, the  object will keep the node.js event loop active.
added in: v9.7.0
returns:  a reference to
is active. calling  multiple times will have no
effect.
by default, all  objects are "ref'ed", making it normally unnecessary
to call  unless  had been called previously.
when called, the active  object will not require the node.js event
loop to remain active. if there is no other activity keeping the event loop
running, the process may exit before the  object's callback is
invoked. calling  multiple times will have no effect.
this object is created internally and is returned from  and
. it can be passed to either  or
in order to cancel the scheduled actions.
by default, when a timer is scheduled using either  or
, the node.js event loop will continue running as long as the
timer is active. each of the  objects returned by these functions
export both  and  functions that can be used to
control this default behavior.
cancels the timeout.
is active. calling  multiple times will have no effect.
sets the timer's start time to the current time, and reschedules the timer to
call its callback at the previously specified duration adjusted to the current
time. this is useful for refreshing a timer without allocating a new
using this on a timer that has already called its callback will reactivate the
timer.
when called, the active  object will not require the node.js event loop
to remain active. if there is no other activity keeping the event loop running,
the process may exit before the  object's callback is invoked. calling
multiple times will have no effect.
returns:  a number that can be used to reference this
coerce a  to a primitive. the primitive can be used to
clear the . the primitive can only be used in the
same thread where the timeout was created. therefore, to use it
across  it must first be passed to the correct
thread. this allows enhanced compatibility with browser
and  implementations.
scheduling timers
a timer in node.js is an internal construct that calls a given function after
a certain period of time. when a timer's function is called varies depending on
which method was used to create the timer and what other work the node.js
event loop is doing.
the function to call at the end of this turn of
the node.js
optional arguments to pass when the  is called.
returns:  for use with
schedules the "immediate" execution of the  after i/o events'
callbacks.
when multiple calls to  are made, the  functions are
queued for execution in the order in which they are created. the entire callback
queue is processed every event loop iteration. if an immediate timer is queued
from inside an executing callback, that timer will not be triggered until the
next event loop iteration.
if  is not a function, a  will be thrown.
this method has a custom variant for promises that is available using
the function to call when the timer elapses.
the number of milliseconds to wait before calling the
schedules repeated execution of  every  milliseconds.
when  is larger than  or less than , the  will be
set to . non-integer delays are truncated to an integer.
schedules execution of a one-time  after  milliseconds.
the  will likely not be invoked in precisely  milliseconds.
node.js makes no guarantees about the exact timing of when callbacks will fire,
nor of their ordering. the callback will be called as close as possible to the
time specified.
when  is larger than  or less than , the
will be set to . non-integer delays are truncated to an integer.
cancelling timers
the , , and  methods
each return objects that represent the scheduled timers. these can be used to
cancel the timer and prevent it from triggering.
for the promisified variants of  and ,
an  may be used to cancel the timer. when canceled, the
returned promises will be rejected with an .
for :
an  object as returned by
cancels an  object created by .
|  |  a  object as returned by
or the  of the  object as a string or a number.
cancels a  object created by .
timers promises api
the  api provides an alternative set of timer functions
that return  objects. the api is accessible via
the number of milliseconds to wait before fulfilling the
promise. default: .
a value with which the promise is fulfilled.
set to  to indicate that the scheduled
should not require the node.js event loop to remain active.
an optional  that can be used to
cancel the scheduled .
returns an async iterator that generates values in an interval of  ms.
the number of milliseconds to wait between iterations.
a value with which the iterator returns.
between iterations should not require the node.js event loop to
remain active.
cancel the scheduled  between operations.
the number of milliseconds to wait before resolving the
promise.
cancel waiting.
an experimental api defined by the  draft specification
being developed as a standard web platform api.
calling  is roughly equivalent
to calling  except that
the  option is not supported.
calling  is equivalent to calling
with no arguments.
tls (ssl)
the  module provides an implementation of the transport layer security
(tls) and secure socket layer (ssl) protocols that is built on top of openssl.
the module can be accessed using:
tls/ssl concepts
tls/ssl is a set of protocols that rely on a public key infrastructure (pki) to
enable secure communication between a client and a server. for most common
cases, each server must have a private key.
private keys can be generated in multiple ways. the example below illustrates
use of the openssl command-line interface to generate a 2048-bit rsa private
key:
with tls/ssl, all servers (and some clients) must have a certificate.
certificates are public keys that correspond to a private key, and that are
digitally signed either by a certificate authority or by the owner of the
private key (such certificates are referred to as "self-signed"). the first
step to obtaining a certificate is to create a certificate signing request
(csr) file.
the openssl command-line interface can be used to generate a csr for a private
once the csr file is generated, it can either be sent to a certificate
authority for signing or used to generate a self-signed certificate.
creating a self-signed certificate using the openssl command-line interface
is illustrated in the example below:
once the certificate is generated, it can be used to generate a  or
where:
: is the signed certificate
: is the associated private key
: is a concatenation of all certificate authority (ca) certs into
a single file, e.g.
perfect forward secrecy
the term  or perfect forward secrecy describes a feature
of key-agreement (i.e., key-exchange) methods. that is, the server and client
keys are used to negotiate new temporary keys that are used specifically and
only for the current communication session. practically, this means that even
if the server's private key is compromised, communication can only be decrypted
by eavesdroppers if the attacker manages to obtain the key-pair specifically
generated for the session.
perfect forward secrecy is achieved by randomly generating a key pair for
key-agreement on every tls/ssl handshake (in contrast to using the same key for
all sessions). methods implementing this technique are called "ephemeral".
currently two methods are commonly used to achieve perfect forward secrecy (note
the character "e" appended to the traditional abbreviations):
: an ephemeral version of the diffie-hellman key-agreement protocol.
: an ephemeral version of the elliptic curve diffie-hellman
key-agreement protocol.
to use perfect forward secrecy using  with the  module, it is
required to generate diffie-hellman parameters and specify them with the
option to . the following illustrates
the use of the openssl command-line interface to generate such parameters:
if using perfect forward secrecy using , diffie-hellman parameters are
not required and a default ecdhe curve will be used. the  property
can be used when creating a tls server to specify the list of names of supported
curves to use, see  for more info.
perfect forward secrecy was optional up to tlsv1.2, but it is not optional for
tlsv1.3, because all tlsv1.3 cipher suites use ecdhe.
alpn and sni
alpn (application-layer protocol negotiation extension) and
sni (server name indication) are tls handshake extensions:
alpn: allows the use of one tls server for multiple protocols (http, http/2)
sni: allows the use of one tls server for multiple hostnames with different
certificates.
pre-shared keys
tls-psk support is available as an alternative to normal certificate-based
authentication. it uses a pre-shared key instead of certificates to
authenticate a tls connection, providing mutual authentication.
tls-psk and public key infrastructure are not mutually exclusive. clients and
servers can accommodate both, choosing either of them during the normal cipher
negotiation step.
tls-psk is only a good choice where means exist to securely share a
key with every connecting machine, so it does not replace the public key
infrastructure (pki) for the majority of tls uses.
the tls-psk implementation in openssl has seen many security flaws in
recent years, mostly because it is used only by a minority of applications.
please consider all alternative solutions before switching to psk ciphers.
upon generating psk it is of critical importance to use sufficient entropy as
discussed in . deriving a shared secret from a password or other
low-entropy sources is not secure.
psk ciphers are disabled by default, and using tls-psk thus requires explicitly
specifying a cipher suite with the  option. the list of available
ciphers can be retrieved via . all tls 1.3
ciphers are eligible for psk but currently only those that use sha256 digest are
supported they can be retrieved via .
according to the , psk identities up to 128 bytes in length and
psks up to 64 bytes in length must be supported. as of openssl 1.1.0
maximum identity size is 128 bytes, and maximum psk length is 256 bytes.
the current implementation doesn't support asynchronous psk callbacks due to the
limitations of the underlying openssl api.
client-initiated renegotiation attack mitigation
the tls protocol allows clients to renegotiate certain aspects of the tls
session. unfortunately, session renegotiation requires a disproportionate amount
of server-side resources, making it a potential vector for denial-of-service
attacks.
to mitigate the risk, renegotiation is limited to three times every ten minutes.
an  event is emitted on the  instance when this
threshold is exceeded. the limits are configurable:
specifies the number of renegotiation
requests. default: .
specifies the time renegotiation window
in seconds. default:  (10 minutes).
the default renegotiation limits should not be modified without a full
understanding of the implications and risks.
tlsv1.3 does not support renegotiation.
session resumption
establishing a tls session can be relatively slow. the process can be sped
up by saving and later reusing the session state. there are several mechanisms
to do so, discussed here from oldest to newest (and preferred).
session identifiers
servers generate a unique id for new connections and
send it to the client. clients and servers save the session state. when
reconnecting, clients send the id of their saved session state and if the server
also has the state for that id, it can agree to use it. otherwise, the server
will create a new session. see  for more information, page 23 and
resumption using session identifiers is supported by most web browsers when
making https requests.
for node.js, clients wait for the  event to get the session data,
and provide the data to the  option of a subsequent
to reuse the session. servers must
implement handlers for the  and  events
to save and restore the session data using the session id as the lookup key to
reuse sessions. to reuse sessions across load balancers or cluster workers,
servers must use a shared session cache (such as redis) in their session
handlers.
session tickets
the servers encrypt the entire session state and send it
to the client as a "ticket". when reconnecting, the state is sent to the server
in the initial connection. this mechanism avoids the need for a server-side
session cache. if the server doesn't use the ticket, for any reason (failure
to decrypt it, it's too old, etc.), it will create a new session and send a new
ticket. see  for more information.
resumption using session tickets is becoming commonly supported by many web
browsers when making https requests.
for node.js, clients use the same apis for resumption with session identifiers
as for resumption with session tickets. for debugging, if
returns a value, the session data contains a
ticket, otherwise it contains client-side session state.
with tlsv1.3, be aware that multiple tickets may be sent by the server,
resulting in multiple  events, see  for more
single process servers need no specific implementation to use session tickets.
to use session tickets across server restarts or load balancers, servers must
all have the same ticket keys. there are three 16-byte keys internally, but the
tls api exposes them as a single 48-byte buffer for convenience.
it's possible to get the ticket keys by calling  on
one server instance and then distribute them, but it is more reasonable to
securely generate 48 bytes of secure random data and set them with the
option of . the keys should be regularly
regenerated and server's keys can be reset with
session ticket keys are cryptographic keys, and they must be stored
securely. with tls 1.2 and below, if they are compromised all sessions that
used tickets encrypted with them can be decrypted. they should not be stored
on disk, and they should be regenerated regularly.
if clients advertise support for tickets, the server will send them. the
server can disable tickets by supplying
both session identifiers and session tickets timeout, causing the server to
create new sessions. the timeout can be configured with the
option of .
for all the mechanisms, when resumption fails, servers will create new sessions.
since failing to resume the session does not cause tls/https connection
failures, it is easy to not notice unnecessarily poor tls performance. the
openssl cli can be used to verify that servers are resuming sessions. use the
option to , for example:
read through the debug output. the first connection should say "new", for
subsequent connections should say "reused", for example:
modifying the default tls cipher suite
node.js is built with a default suite of enabled and disabled tls ciphers. this
default cipher list can be configured when building node.js to allow
distributions to provide their own default list.
the following command can be used to show the default cipher suite:
this default can be replaced entirely using the
command-line switch (directly, or via the  environment
variable). for instance, the following makes
the default tls cipher suite:
the default can also be replaced on a per client or server basis using the
option from , which is also available
in , , and when creating new
the ciphers list can contain a mixture of tlsv1.3 cipher suite names, the ones
that start with , and specifications for tlsv1.2 and below cipher
suites. the tlsv1.2 ciphers support a legacy specification format, consult
the openssl  documentation for details, but those
specifications do not apply to tlsv1.3 ciphers. the tlsv1.3 suites can only
be enabled by including their full name in the cipher list. they cannot, for
example, be enabled or disabled by using the legacy tlsv1.2  or
despite the relative order of tlsv1.3 and tlsv1.2 cipher suites, the tlsv1.3
protocol is significantly more secure than tlsv1.2, and will always be chosen
over tlsv1.2 if the handshake indicates it is supported, and if any tlsv1.3
cipher suites are enabled.
the default cipher suite included within node.js has been carefully
selected to reflect current security best practices and risk mitigation.
changing the default cipher suite can have a significant impact on the security
of an application. the  switch and  option should by
used only if absolutely necessary.
the default cipher suite prefers gcm ciphers for  and also prefers ecdhe and dhe ciphers for perfect
forward secrecy, while offering some backward compatibility.
old clients that rely on insecure and deprecated rc4 or des-based ciphers
(like internet explorer 6) cannot complete the handshaking process with
the default configuration. if these clients must be supported, the
may offer a compatible cipher suite. for more details
on the format, see the openssl  documentation.
there are only five tlsv1.3 cipher suites:
the first three are enabled by default. the two -based suites are supported
by tlsv1.3 because they may be more performant on constrained systems, but they
are not enabled by default since they offer less security.
x509 certificate error codes
multiple functions can fail due to certificate errors that are reported by
openssl. in such a case, the function provides an  via its callback that
has the property  which can take one of the following values:
: unable to get issuer certificate.
: unable to get certificate crl.
: unable to decrypt certificate's
: unable to decrypt crl's signature.
: unable to decode issuer public key.
: certificate signature failure.
: crl signature failure.
: certificate is not yet valid.
: certificate has expired.
: crl is not yet valid.
: crl has expired.
: format error in certificate's notbefore
field.
: format error in certificate's notafter
: format error in crl's lastupdate field.
: format error in crl's nextupdate field.
: self signed certificate.
: self signed certificate in certificate chain.
: unable to get local issuer certificate.
: unable to verify the first certificate.
: certificate chain too long.
: certificate revoked.
: invalid ca certificate.
: path length constraint exceeded.
: unsupported certificate purpose.
: certificate not trusted.
: certificate rejected.
: hostname mismatch.
added in: v0.3.4deprecated since: v0.11.3
the  class represents a stream of encrypted data. this class
is deprecated and should no longer be used.
the  property returns the total number of bytes
written to the underlying socket including the bytes required for the
implementation of the tls protocol.
added in: v0.3.2deprecated since: v0.11.3
the  event is emitted by the  object once a secure
connection has been established.
as with checking for the server
event,  should be inspected to confirm whether the
certificate used is properly authorized.
accepts encrypted connections using tls or ssl.
handshake begins.  is typically an object of type  but
will not receive events unlike the socket created from the
event. usually users will not want to access this event.
into the tls server. in that case, any  stream can be passed.
added in: v12.3.0, v10.20.0
the  event is emitted when key material is generated or received by
a connection to this server (typically before handshake has completed, but not
necessarily). this keying material can be stored for debugging, as it allows
captured tls traffic to be decrypted. it may be emitted multiple times for
each socket.
a typical use case is to append received lines to a common text file, which
is later used by software (such as wireshark) to decrypt the traffic:
the  event is emitted upon creation of a new tls session. this may
be used to store sessions in external storage. the data should be provided to
the  callback.
the listener callback is passed three arguments when called:
the tls session identifier
the tls session data
a callback function taking no arguments that must be
invoked in order for data to be sent or received over the secure connection.
listening for this event will have an effect only on connections established
after the addition of the event listener.
the  event is emitted when the client sends a certificate status
request. the listener callback is passed three arguments when called:
the server certificate
the issuer's certificate
a callback function that must be invoked to provide
the results of the ocsp request.
the server's current certificate can be parsed to obtain the ocsp url
and certificate id; after obtaining an ocsp response,  is
then invoked, where  is a  instance containing the ocsp response.
both  and  are  der-representations of the
primary and issuer's certificates. these can be used to obtain the ocsp
certificate id and ocsp endpoint url.
alternatively,  may be called, indicating that there was
no ocsp response.
calling  will result in a  call.
the typical flow of an ocsp request is as follows:
client connects to the server and sends an  (via the status
info extension in clienthello).
server receives the request and emits the  event, calling the
listener if registered.
server extracts the ocsp url from either the  or  and
performs an  to the ca.
server receives  from the ca and sends it back to the client
via the  argument
client validates the response and either destroys the socket or performs a
handshake.
the  can be  if the certificate is either self-signed or the
issuer is not in the root certificates list. (an issuer may be provided
via the  option when establishing the tls connection.)
an npm module like  may be used to parse the certificates.
the  event is emitted when the client requests to resume a
previous tls session. the listener callback is passed two arguments when
called:
a callback function to be called when the prior session
has been recovered:
the event listener should perform a lookup in external storage for the
saved by the  event handler using the given
. if found, call  to resume the session.
if not found, the session cannot be resumed.  must be called
without  so that the handshake can continue and a new session can
be created. it is possible to call  to terminate the incoming
connection and destroy the socket.
the following illustrates resuming a tls session:
the  event is emitted after the handshaking process for a
new connection has successfully completed. the listener callback is passed a
single argument when called:
the established tls socket.
the  property is a  indicating whether the
client has been verified by one of the supplied certificate authorities for the
server. if  is , then
is set to describe how authorization failed. depending on the settings
of the tls server, unauthorized connections may still be accepted.
the  property is a string that contains the selected
alpn protocol. when alpn has no selected protocol,
equals .
the  property is a string containing the server name
requested via sni.
the  event is emitted when an error occurs before a secure
connection is established. the listener callback is passed two arguments when
the  object describing the error
the  instance from which the
error originated.
a sni host name or wildcard (e.g. )
an object containing any of the possible properties
from the   arguments (e.g. ,
, , etc).
the  method adds a secure context that will be used if
the client request's sni name matches the supplied  (or wildcard).
when there are multiple matching contexts, the most recently added one is
returns the bound address, the address family name, and port of the
server as reported by the operating system. see  for
a listener callback that will be registered to listen
for the server instance's  event.
the  method stops the server from accepting new connections.
this function operates asynchronously. the  event will be emitted
when the server has no more open connections.
returns:  a 48-byte buffer containing the session ticket keys.
returns the session ticket keys.
starts the server listening for encrypted connections.
an object containing any of the possible properties from
the   arguments (e.g. , ,
, etc).
the  method replaces the secure context of an
existing server. existing connections to the server are not interrupted.
|  |  a 48-byte buffer containing the session
ticket keys.
sets the session ticket keys.
changes to the ticket keys are effective only for future server connections.
existing or currently pending server connections will use the previous keys.
performs transparent encryption of written data and all required tls
negotiation.
instances of  implement the duplex  interface.
methods that return tls connection metadata (e.g.
) will only return data while the
connection is open.
on the server side, any  stream. on the client side, any
instance of  (for generic  stream support
on the client side,  must be used).
: see
: the ssl/tls protocol is asymmetrical, tlssockets must know if
they are to behave as a server or a client. if  the tls socket will be
instantiated as a server. default: .
a  instance.
: whether to authenticate the remote peer by requesting a
certificate. clients always request a server certificate. servers
( is true) may set  to true to request a client
a  instance containing a tls session.
if , specifies that the ocsp status request
extension will be added to the client hello and an  event
will be emitted on the socket before establishing a secure communication
: tls context object created with
. if a  is not provided, one
will be created by passing the entire  object to
...:  options that are used if the
option is missing. otherwise, they are ignored.
construct a new  object from an existing tcp socket.
the  event is emitted on a  when key material
is generated or received by the socket. this keying material can be stored
for debugging, as it allows captured tls traffic to be decrypted. it may
be emitted multiple times, before or after the handshake completes.
the  event is emitted if the  option was set
when the  was created and an ocsp response has been received.
the listener callback is passed a single argument when called:
the server's ocsp response
typically, the  is a digitally signed object from the server's ca that
contains information about server's certificate revocation status.
the  event is emitted after the handshaking process for a new
connection has successfully completed. the listener callback will be called
regardless of whether or not the server's certificate has been authorized. it
is the client's responsibility to check the  property to
determine if the server certificate was signed by one of the specified cas. if
, then the error can be found by examining the
property. if alpn was used, the
property can be checked to determine the negotiated
the  event is not emitted when a  is created
using the  constructor.
the  event is emitted on a client  when a new session
or tls ticket is available. this may or may not be before the handshake is
complete, depending on the tls protocol version that was negotiated. the event
is not emitted on the server, or if a new session was not created, for example,
when the connection was resumed. for some tls protocol versions the event may be
emitted multiple times, in which case all the sessions can be used for
resumption.
on the client, the  can be provided to the  option of
to resume the connection.
for tlsv1.2 and below,  can be called once
the handshake is complete. for tlsv1.3, only ticket-based resumption is allowed
by the protocol, multiple tickets are sent, and the tickets aren't sent until
after the handshake completes. so it is necessary to wait for the
event to get a resumable session. applications
should use the  event instead of  to ensure
they will work for all tls versions. applications that only expect to
get or use one session should listen for this event only once:
returns the bound , the address  name, and  of the
underlying socket as reported by the operating system:
returns the reason why the peer's certificate was not been verified. this
property is set only when .
this property is  if the peer certificate was signed by one of the cas
specified when creating the  instance, otherwise .
disables tls renegotiation for this  instance. once called, attempts
to renegotiate will trigger an  event on the .
when enabled, tls packet trace information is written to . this can be
used to debug tls connection problems.
the format of the output is identical to the output of
or . while it is produced by
openssl's  function, the format is undocumented, can change
without notice, and should not be relied on.
always returns . this may be used to distinguish tls sockets from regular
number of bytes to retrieve from keying material
an application specific label, typically this will be a
value from the
optionally provide a context.
returns:  requested bytes of the keying material
keying material is used for validations to prevent different kind of attacks in
network protocols, for example in the specifications of ieee 802.1x.
see the openssl  documentation for more
returns an object representing the local certificate. the returned object has
some properties corresponding to the fields of the certificate.
see  for an example of the certificate
structure.
if there is no local certificate, an empty object will be returned. if the
socket has been destroyed,  will be returned.
openssl name for the cipher suite.
ietf name for the cipher suite.
the minimum tls protocol version supported by this cipher
suite. for the actual negotiated protocol, see .
returns an object containing information on the negotiated cipher suite.
for example, a tlsv1.2 protocol with aes256-sha cipher:
added in: v5.0.0
returns an object representing the type, name, and size of parameter of
an ephemeral key exchange in  on a client
connection. it returns an empty object when the key exchange is not
ephemeral. as this is only supported on a client socket;  is returned
if called on a server socket. the supported types are  and . the
property is available only when type is .
for example: .
added in: v9.9.0
returns:  |  the latest  message that has been
sent to the socket as part of a ssl/tls handshake, or  if
no  message has been sent yet.
as the  messages are message digests of the complete handshake
(with a total of 192 bits for tls 1.0 and more for ssl 3.0), they can
be used for external authentication procedures when the authentication
provided by ssl/tls is not desired or is not enough.
corresponds to the  routine in openssl and may be used
to implement the  channel binding from .
include the full certificate chain if , otherwise
include just the peer's certificate.
returns:  a certificate object.
returns an object representing the peer's certificate. if the peer does not
provide a certificate, an empty object will be returned. if the socket has been
destroyed,  will be returned.
if the full certificate chain was requested, each certificate will include an
property containing an object representing its issuer's
certificate object
a certificate object has properties corresponding to the fields of the
the der encoded x.509 certificate data.
the certificate subject, described in terms of
country (), stateorprovince (), locality (), organization (),
organizationalunit (), and commonname (). the commonname is typically
a dns name with tls certificates. example:
the certificate issuer, described in the same terms as the
the date-time the certificate is valid from.
the date-time the certificate is valid to.
the certificate serial number, as a hex string.
example: .
the sha-1 digest of the der encoded certificate. it is
returned as a  separated hexadecimal string. example: .
the sha-256 digest of the der encoded certificate.
it is returned as a  separated hexadecimal string. example:
the sha-512 digest of the der encoded certificate.
(optional) the extended key usage, a set of oids.
(optional) a string containing concatenated names
for the subject, an alternative to the  names.
(optional) an array describing the authorityinfoaccess,
used with ocsp.
(optional) the issuer certificate object. for
self-signed certificates, this may be a circular reference.
the certificate may contain information about the public key, depending on
the key type.
for rsa keys, the following properties may be defined:
the rsa bit size. example: .
the rsa exponent, as a string in hexadecimal number
notation. example: .
the rsa modulus, as a hexadecimal string. example:
the public key.
for ec keys, the following properties may be defined:
the key size in bits. example: .
(optional) the asn.1 name of the oid of the elliptic
curve. well-known curves are identified by an oid. while it is unusual, it is
possible that the curve is identified by its mathematical properties, in which
case it will not have an oid. example: .
(optional) the nist name for the elliptic curve, if it
has one (not all well-known curves have been assigned names by nist). example:
example certificate:
returns:  |  the latest  message that is expected
or has actually been received from the socket as part of a ssl/tls handshake,
or  if there is no  message so far.
returns the peer certificate as an  object.
if there is no peer certificate, or the socket has been destroyed,
returns a string containing the negotiated ssl/tls protocol version of the
current connection. the value  will be returned for connected
sockets that have not completed the handshaking process. the value  will
be returned for server sockets or disconnected client sockets.
protocol versions are:
see the openssl  documentation for more information.
returns the tls session data or  if no session was
negotiated. on the client, the data can be provided to the  option of
to resume the connection. on the server, it may be useful
for debugging.
note:  works only for tlsv1.2 and below. for tlsv1.3, applications
must use the  event (it also works for tlsv1.2 and below).
returns:  list of signature algorithms shared between the server and
the client in the order of decreasing preference.
for a client, returns the tls session ticket if one is available, or
. for a server, always returns .
it may be useful for debugging.
returns the local certificate as an  object.
if there is no local certificate, or the socket has been destroyed,
added in: v0.5.6
returns:   if the session was reused,  otherwise.
returns the string representation of the local ip address.
returns the numeric representation of the local port.
returns the string representation of the remote ip address. for example,
returns the string representation of the remote ip family.  or .
returns the numeric representation of the remote port. for example, .
if not , the server certificate is
verified against the list of supplied cas. an  event is emitted if
verification fails;  contains the openssl error code. default:
if  returned , callback is
attached once to the  event. if  returned ,
will be called in the next tick with an error, unless the
has been destroyed, in which case  will not be called
at all.
returns:   if renegotiation was initiated,  otherwise.
the  method initiates a tls renegotiation process.
upon completion, the  function will be passed a single argument
that is either an  (if the request failed) or .
this method can be used to request a peer's certificate after the secure
when running as the server, the socket will be destroyed with an error after
timeout.
for tlsv1.3, renegotiation cannot be initiated, it is not supported by the
the maximum tls fragment size. the maximum value is .
the  method sets the maximum tls fragment size.
returns  if setting the limit succeeded;  otherwise.
smaller fragment sizes decrease the buffering latency on the client: larger
fragments are buffered by the tls layer until the entire fragment is received
and its integrity is verified; large fragments can span multiple roundtrips
and their processing can be delayed due to packet loss or reordering. however,
smaller fragments add extra tls framing bytes and cpu overhead, which may
decrease overall server throughput.
the host name or ip address to verify the certificate
against.
a  representing the peer's certificate.
verifies the certificate  is issued to .
returns  object, populating it with , , and  on
failure. on success, returns .
this function is intended to be used in combination with the
option that can be passed to  and as
such operates on a . for other purposes, consider using
this function can be overwritten by providing an alternative function as the
option that is passed to . the
overwriting function can call  of course, to augment
the checks done with additional verification.
this function is only called if the certificate passed all other checks, such as
being issued by trusted ca ().
earlier versions of node.js incorrectly accepted certificates for a given
if a matching  subject alternative name
was present (see ). applications that wish to accept
subject alternative names can use a custom
function that implements the desired behavior.
host the client should connect to. default:
creates unix socket connection to path. if this option is
specified,  and  are ignored.
establish secure connection on a given socket
rather than creating a new socket. typically, this is an instance of
, but any  stream is allowed.
if this option is specified, , , and  are ignored,
except for certificate validation. usually, a socket is already connected
when passed to , but it can be connected later.
connection/disconnection/destruction of  is the user's
responsibility; calling  will not cause  to be
automatically end the writable side when the readable side ends. if the
option is set, this option has no effect. see the
option of  for details. default: .
hint:  optional message sent from the server to help client
decide which identity to use during negotiation.
always  if tls 1.3 is used.
returns:  in the form
or  to stop the negotiation process.  must be
compatible with the selected cipher's digest.
must use utf-8 encoding.
when negotiating tls-psk (pre-shared keys), this function is called
with optional identity  provided by the server or
in case of tls 1.3 where  was removed.
it will be necessary to provide a custom
for the connection as the default one will try to check host name/ip
of the server against the certificate but that's not applicable for psk
because there won't be a certificate present.
more information can be found in the .
:  |  |  |  |  |  |
an array of strings, s, s, or s, or a
single , , or  containing the supported alpn
protocols. s should have the format
e.g. , where the  byte is the length of the
next protocol name. passing an array is usually much simpler, e.g.
. protocols earlier in the list have higher
preference than those later.
:  server name for the sni (server name indication) tls
extension. it is the name of the host being connected to, and must be a host
name, and not an ip address. it can be used by a multi-homed server to
choose the correct certificate to present to the client, see the
option to .
a callback function
to be used (instead of the builtin  function)
when checking the server's host name (or the provided  when
explicitly set) against the certificate. this should return an  if
verification fails. the method should return  if the
and  are verified.
a  instance, containing tls session.
minimum size of the dh parameter in bits to accept a
tls connection. when a server offers a dh parameter with a size less
than , the tls connection is destroyed and an error is thrown.
:  consistent with the readable stream  parameter.
if the  option is missing, incoming data is
stored in a single  and passed to the supplied  when
data arrives on the socket, otherwise the option is ignored. see the
option of  for details.
option is missing, otherwise they are ignored.
...: any  option not already listed.
the  function, if specified, will be added as a listener for the
returns a  object.
unlike the  api,  does not enable the
sni (server name indication) extension by default, which may cause some
servers to return an incorrect certificate or reject the connection
altogether. to enable sni, set the  option in addition
the following illustrates a client for the echo server example from
default value for .
same as  except that  can be provided
as an argument instead of an option.
a path option, if specified, will take precedence over the path argument.
same as  except that  and  can be provided
as arguments instead of options.
a port or host option, if specified, will take precedence over any port or host
|  |  |  optionally override the trusted ca
certificates. default is to trust the well-known cas curated by mozilla.
mozilla's cas are completely replaced when cas are explicitly specified
using this option. the value can be a string or , or an  of
strings and/or s. any string or  can contain multiple pem
cas concatenated together. the peer's certificate must be chainable to a ca
trusted by the server for the connection to be authenticated. when using
certificates that are not chainable to a well-known ca, the certificate's ca
must be explicitly specified as a trusted or the connection will fail to
authenticate.
if the peer uses a certificate that doesn't match or chain to one of the
default cas, use the  option to provide a ca certificate that the peer's
certificate can match or chain to.
for self-signed certificates, the certificate is its own ca, and must be
for pem encoded certificates, supported types are "trusted certificate",
"x509 certificate", and "certificate".
|  |  |  cert chains in pem format. one
cert chain should be provided per private key. each cert chain should
consist of the pem formatted certificate for a provided private ,
followed by the pem formatted intermediate certificates (if any), in order,
and not including the root ca (the root ca must be pre-known to the peer,
see ). when providing multiple cert chains, they do not have to be in
the same order as their private keys in . if the intermediate
certificates are not provided, the peer will not be able to validate the
certificate, and the handshake will fail.
colon-separated list of supported signature algorithms.
the list can contain digest algorithms (,  etc.), public key
algorithms (,  etc.), combination of both (e.g
'rsa+sha384') or tls v1.3 scheme names (e.g. ).
cipher suite specification, replacing the default. for
more information, see . permitted
ciphers can be obtained via . cipher names must be
uppercased in order for openssl to accept them.
name of an openssl engine which can provide the
client certificate.
|  |  |  pem formatted crls (certificate
revocation lists).
|  diffie-hellman parameters, required for
. use  to create the parameters.
the key length must be greater than or equal to 1024 bits or else an error
will be thrown. although 1024 bits is permissible, use 2048 bits or larger
for stronger security. if omitted or invalid, the parameters are silently
discarded and dhe ciphers will not be available.
a string describing a named curve or a colon separated
list of curve nids or names, for example , to use for
ecdh key agreement. set to  to select the
curve automatically. use  to obtain a list of
available curve names. on recent releases,
will also display the name and description of each available elliptic curve.
attempt to use the server's cipher suite
preferences instead of the client's. when , causes
to be set in , see
|  |  |  |  private keys in pem
format. pem allows the option of private keys being encrypted. encrypted
keys will be decrypted with . multiple keys using
different algorithms can be provided either as an array of unencrypted key
strings or buffers, or an array of objects in the form
. the object form can only
occur in an array.  is optional. encrypted keys will be
decrypted with  if provided, or  if
it is not.
name of an openssl engine to get private key
from. should be used together with .
identifier of a private key managed by
an openssl engine. should be used together with .
should not be set together with , because both options define a
private key in different ways.
optionally set the maximum tls version to allow. one
of , , , or . cannot be specified
along with the  option; use one or the other.
optionally set the minimum tls version to allow. one
along with the  option; use one or the other. avoid
setting to less than tlsv1.2, but it may be required for
interoperability.
shared passphrase used for a single private key and/or
a pfx.
|  |  |  |  pfx or pkcs12 encoded
private key and certificate chain.  is an alternative to providing
and  individually. pfx is usually encrypted, if it is,
will be used to decrypt it. multiple pfx can be provided either
as an array of unencrypted pfx buffers, or an array of objects in the form
occur in an array.  is optional. encrypted pfx will be
optionally affect the openssl protocol behavior,
which is not usually necessary. this should be used carefully if at all!
value is a numeric bitmask of the  options from
legacy mechanism to select the tls protocol
version to use, it does not support independent control of the minimum and
maximum version, and does not support limiting the protocol to tlsv1.3. use
and  instead. the possible values are listed as
, use the function names as strings. for example,
use  to force tls version 1.1, or  to allow
any tls protocol version up to tlsv1.3. it is not recommended to use tls
versions less than 1.2, but it may be required for interoperability.
default: none, see .
opaque identifier used by servers to ensure
session state is not shared between applications. unused by clients.
:  48-bytes of cryptographically strong pseudorandom
data. see  for more information.
the number of seconds after which a tls session
created by the server will no longer be resumable. see
for more information. default: .
sets the default value of the  option
to , other apis that create secure contexts leave it unset.
uses a 128 bit truncated sha1 hash value generated
from  as the default value of the  option, other
apis that create secure contexts have no default value.
the  method creates a  object. it is
usable as an argument to several  apis, such as
and , but has no public methods.
a key is required for ciphers that use certificates. either  or
can be used to provide it.
if the  option is not given, then node.js will default to using
a secure context object as returned by
to specify that this tls connection should be
opened as a server.
to specify whether a server should request a
certificate from a connecting client. only applies when  is .
if not  a server automatically reject
clients with invalid certificates. only applies when  is .
: a tls context object from
: if  the tls socket will be instantiated in server-mode.
a  instance
will be emitted on the socket before establishing a secure communication.
creates a new secure pair object with two streams, one of which reads and writes
the encrypted data and the other of which reads and writes the cleartext data.
generally, the encrypted stream is piped to/from an incoming encrypted data
stream and the cleartext one is used as a replacement for the initial encrypted
returns a  object with  and
stream properties.
using  has the same api as .
the  method is now deprecated in favor of
. for example, the code:
can be replaced by:
where  has the same api as .
an array of strings, s, s, or s, or a single
, , or  containing the supported alpn
e.g. , where the first byte is the length of the next
protocol name. passing an array is usually much simpler, e.g.
. (protocols should be ordered by their priority.)
if ,  will be
called on new connections. tracing can be enabled after the secure
connection is established, but this option must be used to trace the secure
connection setup. default: .
abort the connection if the ssl/tls handshake
does not finish in the specified number of milliseconds.
a  is emitted on the  object whenever
a handshake times out. default:  (120 seconds).
if not  the server will reject any
connection which is not authorized with the list of supplied cas. this
option only has an effect if  is . default: .
if  the server will request a certificate from
clients that connect and attempt to verify that certificate. default:
a function that will be
called if the client supports sni tls extension. two arguments will be
passed when called:  and .  is an
error-first callback that takes two optional arguments:  and .
, if provided, is a  instance.
can be used to get a proper .
if  is called with a falsy  argument, the default secure
context of the server will be used. if  wasn't provided the
default callback with high-level api will be used (see below).
socket:  the server  instance for
this connection.
identity:  identity parameter sent from the client.
returns:  |  |  pre-shared key that must either be
a buffer or  to stop the negotiation process. returned psk must be
with the identity provided by the client.
if the return value is  the negotiation process will stop and an
"unknown_psk_identity" alert message will be sent to the other party.
if the server wishes to hide the fact that the psk identity was not known,
the callback must provide some random data as  to make the connection
fail with "decrypt_error" before negotiation is finished.
psk ciphers are disabled by default, and using tls-psk thus
requires explicitly specifying a cipher suite with the  option.
optional hint to send to a client to help
with selecting the identity during tls-psk negotiation. will be ignored
in tls 1.3. upon failing to set pskidentityhint  will be
emitted with  code.
...: any  option can be provided. for
servers, the identity options (, /, or )
are usually required.
creates a new . the , if provided, is
automatically set as a listener for the  event.
the  options is automatically shared between  module
workers.
the following illustrates a simple echo server:
the server can be tested by connecting to it using the example client from
added in: v0.10.2
returns an array with the names of the supported tls ciphers. the names are
lower-case for historical reasons, but must be uppercased to be used in
the  option of .
not all supported ciphers are enabled by default. see
cipher names that start with  are for tlsv1.3, all the others are for
tlsv1.2 and below.
an immutable array of strings representing the root certificates (in pem format)
from the bundled mozilla ca store as supplied by the current node.js version.
the default curve name to use for ecdh key agreement in a tls server. the
default value is . see  for further
the default value of the  option of
. it can be assigned any of the supported tls
protocol versions, , , , or .
default: , unless changed using cli options. using
sets the default to . using  sets
the default to . if multiple of the options are provided, the
highest maximum is used.
the default to . using  sets the default to
. if multiple of the options are provided, the lowest minimum is
trace events
the  module provides a mechanism to centralize tracing
information generated by v8, node.js core, and userspace code.
tracing can be enabled with the  command-line flag
or by using the  module. the  flag
accepts a list of comma-separated category names.
the available categories are:
: an empty placeholder.
: enables capture of detailed  trace data.
the  events have a unique  and a special
: enables capture of node.js bootstrap milestones.
: enables capture of  and
: enables capture of trace data for dns queries.
: enables capture of trace data for network.
: enables capture of node.js environment milestones.
: enables capture of trace data for file system sync methods.
: enables capture of trace data for file system sync
directory methods.
: enables capture of trace data for file system async methods.
: enables capture of trace data for file system async
: enables capture of  measurements.
: enables capture of only performance api user timing
measures and marks.
: enables capture of only performance api timerify
measurements.
: enables capture of trace data tracking the number
of unhandled promise rejections and handled-after-rejections.
: enables capture of trace data for the  module's
, , and  methods.
: the  events are gc, compiling, and execution related.
: enables capture of trace data for http request / response.
by default the , , and  categories are enabled.
prior versions of node.js required the use of the
flag to enable trace events. this requirement has been removed. however, the
flag may still be used and will enable the
, , and  trace event categories by default.
alternatively, trace events may be enabled using the  module:
running node.js with tracing enabled will produce log files that can be opened
in the
tab of chrome.
the logging file is by default called , where
is an incrementing log-rotation id. the filepath pattern can
be specified with  that accepts a template
string that supports  and :
to guarantee that the log file is properly generated after signal events like
, , or , make sure to have the appropriate handlers
in your code, such as:
the tracing system uses the same time source
as the one used by .
however the trace-event timestamps are expressed in microseconds,
unlike  which returns nanoseconds.
the features from this module are not available in  threads.
the  module
the  object is used to enable or disable tracing for sets of
categories. instances are created using the
when created, the  object is disabled. calling the
method adds the categories to the set of enabled trace event
categories. calling  will remove the categories from the
set of enabled trace event categories.
a comma-separated list of the trace event categories covered by this
disables this  object.
only trace event categories not covered by other enabled  objects
and not specified by the  flag will be disabled.
enables this  object for the set of categories covered by the
only if the  object has been enabled.
an array of trace category names. values included
in the array are coerced to a string when possible. an error will be
thrown if the value cannot be coerced.
creates and returns a  object for the given set of .
returns a comma-separated list of all currently-enabled trace event
categories. the current set of enabled trace event categories is determined
by the union of all currently-enabled  objects and any categories
enabled using the  flag.
given the file  below, the command
will print
to the console.
collect trace events data by inspector
the  module provides the  and
classes. in most cases, it will not be necessary or possible to use this module
directly. however, it can be accessed using:
when node.js detects that it is being run with a text terminal ("tty")
attached,  will, by default, be initialized as an instance of
and both  and  will, by
default, be instances of . the preferred method of determining
whether node.js is being run within a tty context is to check that the value of
the  property is :
in most cases, there should be little to no reason for an application to
manually create instances of the  and
classes.
represents the readable side of a tty. in normal circumstances
will be the only  instance in a node.js
process and there should be no reason to create additional instances.
a  that is  if the tty is currently configured to operate as a
raw device. defaults to .
a  that is always  for  instances.
if , configures the  to operate as a
raw device. if , configures the  to operate in its
default mode. the  property will be set to the resulting
mode.
returns:  the read stream instance.
allows configuration of  so that it operates as a raw device.
when in raw mode, input is always available character-by-character, not
including modifiers. additionally, all special processing of characters by the
terminal is disabled, including echoing input
characters. ctrl+c will no longer cause a  when
in this mode.
represents the writable side of a tty. in normal circumstances,
and  will be the only
instances created for a node.js process and there
should be no reason to create additional instances.
the  event is emitted whenever either of the
or  properties have changed. no arguments are passed to the
listener callback when called.
returns:   if the stream wishes for the calling code to wait
for the  event to be emitted before continuing to write additional
data; otherwise .
clears the current line of this  in a
clears this  from the current
a  specifying the number of columns the tty currently has. this property
is updated whenever the  event is emitted.
moves this 's cursor to the specified
position.
an object containing the environment variables to check. this
enables simulating the usage of a specific terminal. default:
for 2,
for 16,
for 256,
for 16,777,216 colors supported.
use this to determine what colors the terminal supports. due to the nature of
colors in terminals it is possible to either have false positives or false
negatives. it depends on process information and the environment variables that
may lie about what terminal is used.
it is possible to pass in an  object to simulate the usage of a specific
terminal. this can be useful to check how specific environment settings behave.
to enforce a specific color support, use one of the below environment settings.
2 colors:  (disables colors)
16 colors:
256 colors:
16,777,216 colors:
disabling color support is also possible by using the  and
returns the size of the tty
corresponding to this . the array is of the type
where  and  represent the number
of columns and rows in the corresponding tty.
added in: v11.13.0, v10.16.0
the number of colors that are requested (minimum 2).
default: 16.
returns  if the  supports at least as many colors as provided
in . minimum support is 2 (black and white).
this has the same false positives and negatives as described in
a  that is always .
moves this 's cursor relative to its
current position.
a  specifying the number of rows the tty currently has. this property
a numeric file descriptor
the  method returns  if the given  is associated with
a tty and  if it is not, including whenever  is not a non-negative
integer.
udp/datagram sockets
the  module provides an implementation of udp datagram sockets.
encapsulates the datagram functionality.
new instances of  are created using .
the  keyword is not to be used to create  instances.
the  event is emitted after a socket is closed with .
once triggered, no new  events will be emitted on this socket.
the  event is emitted after a socket is associated to a remote
address as a result of a successful  call.
the  event is emitted whenever any error occurs. the event handler
function is passed a single  object.
the  event is emitted once the  is addressable and
can receive data. this happens either explicitly with  or
implicitly the first time data is sent using .
until the  is listening, the underlying system resources do not
exist and calls such as  and  will fail.
the  event is emitted when a new datagram is available on a socket.
the event handler function is passed two arguments:  and .
the message.
remote address information.
the sender address.
the address family ( or ).
the sender port.
the message size.
if the source address of the incoming packet is an ipv6 link-local
address, the interface name is added to the . for
example, a packet received on the  interface might have the
address field set to , where
is the interface name as a zone id suffix.
added in: v0.6.9
tells the kernel to join a multicast group at the given  and
using the  socket option. if the
argument is not specified, the operating system will choose
one interface and will add membership to it. to add membership to every
available interface, call  multiple times, once per interface.
when called on an unbound socket, this method will implicitly bind to a random
port, listening on all interfaces.
when sharing a udp socket across multiple  workers, the
function must be called only once or an
error will occur:
added in: v13.1.0, v12.16.0
tells the kernel to join a source-specific multicast channel at the given
and , using the  with the
socket option. if the  argument
is not specified, the operating system will choose one interface and will add
membership to it. to add membership to every available interface, call
multiple times, once per interface.
returns an object containing the address information for a socket.
for udp sockets, this object will contain , , and
this method throws  if called on an unbound socket.
with no parameters. called when binding is complete.
for udp sockets, causes the  to listen for datagram
messages on a named  and optional . if  is not
specified or is , the operating system will attempt to bind to a
random port. if  is not specified, the operating system will
attempt to listen on all addresses. once binding is complete, a
event is emitted and the optional  function is
specifying both a  event listener and passing a
to the  method is not harmful but not very
useful.
a bound datagram socket keeps the node.js process running to receive
datagram messages.
if binding fails, an  event is generated. in rare case (e.g.
attempting to bind with a closed socket), an  may be thrown.
example of a udp server listening on port 41234:
messages on a named  and optional  that are passed as
properties of an  object passed as the first argument. if
is not specified or is , the operating system will attempt
to bind to a random port. if  is not specified, the operating
system will attempt to listen on all addresses. once binding is
complete, a  event is emitted and the optional
function is called.
the  object may contain a  property. when a  greater
than  is set, it will wrap around an existing socket with the given
file descriptor. in this case, the properties of  and
the  object may contain an additional  property that is
used when using  objects with the  module. when
is set to  (the default), cluster workers will use the same
underlying socket handle allowing connection handling duties to be shared.
when  is , however, the handle is not shared and attempted
port sharing results in an error.
an example socket listening on an exclusive port is shown below.
called when the socket has been closed.
close the underlying socket and stop listening for data on it. if a callback is
provided, it is added as a listener for the  event.
called when the connection is completed or on error.
associates the  to a remote address and port. every
message sent by this handle is automatically sent to that destination. also,
the socket will only receive messages from that remote peer.
trying to call  on an already connected socket will result
in an  exception. if  is not
provided,  (for  sockets) or  (for  sockets)
will be used by default. once the connection is complete, a  event
is emitted and the optional  function is called. in case of failure,
the  is called or, failing this, an  event is emitted.
a synchronous function that disassociates a connected  from
its remote address. trying to call  on an unbound or already
disconnected socket will result in an
exception.
instructs the kernel to leave a multicast group at  using the
socket option. this method is automatically called by the
kernel when the socket is closed or the process terminates, so most apps will
never have reason to call this.
if  is not specified, the operating system will attempt to
drop membership on all valid interfaces.
instructs the kernel to leave a source-specific multicast channel at the given
and  using the
socket option. this method is automatically called by the kernel when the
socket is closed or the process terminates, so most apps will never have
reason to call this.
added in: v8.7.0
returns:  the  socket receive buffer size in bytes.
returns:  the  socket send buffer size in bytes.
returns:  number of bytes queued for sending.
returns:  number of send requests currently in the queue awaiting
to be processed.
by default, binding a socket will cause it to block the node.js process from
exiting as long as the socket is open. the  method can be used
to exclude the socket from the reference counting that keeps the node.js
process active. the  method adds the socket back to the reference
counting and restores the default behavior.
calling  multiples times will have no additional effect.
the  method returns a reference to the socket so calls can be
chained.
returns an object containing the , , and  of the remote
endpoint. this method throws an  exception
if the socket is not connected.
|  |  |  |  message to be sent.
offset in the buffer where the message starts.
number of bytes in the message.
destination port.
destination host name or ip address.
called when the message has been sent.
broadcasts a datagram on the socket.
for connectionless sockets, the destination  and  must be
specified. connected sockets, on the other hand, will use their associated
remote endpoint, so the  and  arguments must not be set.
the  argument contains the message to be sent.
depending on its type, different behavior can apply. if  is a ,
any  or a ,
the  and  specify the offset within the  where the
message begins and the number of bytes in the message, respectively.
if  is a , then it is automatically converted to a
with  encoding. with messages that
contain multi-byte characters,  and  will be calculated with
respect to  and not the character position.
if  is an array,  and  must not be specified.
the  argument is a string. if the value of  is a host name,
dns will be used to resolve the address of the host. if  is not
provided or otherwise nullish,  (for  sockets) or
(for  sockets) will be used by default.
if the socket has not been previously bound with a call to , the socket
is assigned a random port number and is bound to the "all interfaces" address
( for  sockets,  for  sockets.)
an optional  function may be specified to as a way of reporting
dns errors or for determining when it is safe to reuse the  object.
dns lookups delay the time to send for at least one tick of the
node.js event loop.
the only way to know for sure that the datagram has been sent is by using a
. if an error occurs and a  is given, the error will be
passed as the first argument to the . if a  is not given,
the error is emitted as an  event on the  object.
offset and length are optional but both must be set if either are used.
they are supported only when the first argument is a , a ,
or a .
example of sending a udp packet to a port on ;
example of sending a udp packet composed of multiple buffers to a port on
sending multiple buffers might be faster or slower depending on the
application and operating system. run benchmarks to
determine the optimal strategy on a case-by-case basis. generally speaking,
however, sending multiple buffers is faster.
example of sending a udp packet using a socket connected to a port on
note about udp datagram size
the maximum size of an ipv4/v6 datagram depends on the
(maximum transmission unit) and on the  field size.
the  field is 16 bits wide, which means that a normal
payload cannot exceed 64k octets including the internet header and data
(65,507 bytes = 65,535 − 8 bytes udp header − 20 bytes ip header);
this is generally true for loopback interfaces, but such long datagram
messages are impractical for most hosts and networks.
the  is the largest size a given link layer technology can support for
datagram messages. for any link, ipv4 mandates a minimum  of 68
octets, while the recommended  for ipv4 is 576 (typically recommended
as the  for dial-up type applications), whether they arrive whole or in
fragments.
for ipv6, the minimum  is 1280 octets. however, the mandatory minimum
fragment reassembly buffer size is 1500 octets. the value of 68 octets is
very small, since most current link layer technologies, like ethernet, have a
minimum  of 1500.
it is impossible to know in advance the mtu of each link through which
a packet might travel. sending a datagram greater than the receiver  will
not work because the packet will get silently dropped without informing the
source that the data did not reach its intended recipient.
sets or clears the  socket option. when set to , udp
packets may be sent to a local interface's broadcast address.
all references to scope in this section are referring to
, which are defined by . in string form, an ip
with a scope index is written as  where scope is an interface name
or interface number.
sets the default outgoing multicast interface of the socket to a chosen
interface or back to system interface selection. the  must
be a valid string representation of an ip from the socket's family.
for ipv4 sockets, this should be the ip configured for the desired physical
interface. all packets sent to multicast on the socket will be sent on the
interface determined by the most recent successful use of this call.
for ipv6 sockets,  should include a scope to indicate the
interface as in the examples that follow. in ipv6, individual  calls can
also use explicit scope in addresses, so only packets sent to a multicast
address without specifying an explicit scope are affected by the most recent
successful use of this call.
example: ipv6 outgoing multicast interface
on most systems, where scope format uses the interface name:
on windows, where scope format uses an interface number:
example: ipv4 outgoing multicast interface
all systems use an ip of the host on the desired physical interface:
call results
a call on a socket that is not ready to send or no longer open may throw a not
running .
if  can not be parsed into an ip then an einval
on ipv4, if  is a valid address but does not match any
interface, or if the address does not match the family then
a  such as  or  is thrown.
on ipv6, most errors with specifying or omitting scope will result in the socket
continuing to use (or returning to) the system's default interface selection.
a socket's address family's any address (ipv4  or ipv6 ) can be
used to return control of the sockets default outgoing interface to the system
for future multicast packets.
added in: v0.3.8
sets or clears the  socket option. when set to ,
multicast packets will also be received on the local interface.
sets the  socket option. while ttl generally stands for
"time to live", in this context it specifies the number of ip hops that a
packet is allowed to travel through, specifically for multicast traffic. each
router or gateway that forwards a packet decrements the ttl. if the ttl is
decremented to 0 by a router, it will not be forwarded.
the  argument may be between 0 and 255. the default on most systems is .
sets the  socket option. sets the maximum socket receive buffer
in bytes.
sets the  socket option. sets the maximum socket send buffer
sets the  socket option. while ttl generally stands for "time to live",
in this context it specifies the number of ip hops that a packet is allowed to
travel through. each router or gateway that forwards a packet decrements the
ttl. if the ttl is decremented to 0 by a router, it will not be forwarded.
changing ttl values is typically done for network probes or when multicasting.
the  argument may be between 1 and 255. the default on most systems
is 64.
process active, allowing the process to exit even if the socket is still
listening.
calling  multiple times will have no addition effect.
module functions
the family of socket. must be either  or .
when   will reuse the
address, even if another process has already bound a socket on it.
setting  to  will
disable dual-stack support, i.e., binding to address  won't make
sets the  socket value.
an abortsignal that may be used to close a socket.
attached as a listener for  events. optional.
creates a  object. once the socket is created, calling
will instruct the socket to begin listening for datagram
messages. when  and  are not passed to  the
method will bind the socket to the "all interfaces" address on a random port
(it does the right thing for both  and  sockets). the bound address
and port can be retrieved using  and
is similar to calling  on the socket:
attached as a listener to  events.
creates a  object of the specified .
once the socket is created, calling  will instruct the
socket to begin listening for datagram messages. when  and  are
not passed to  the method will bind the socket to the "all
interfaces" address on a random port (it does the right thing for both
and  sockets). the bound address and port can be retrieved using
the  module provides utilities for url resolution and parsing. it can
be accessed using:
url strings and url objects
a url string is a structured string containing multiple meaningful components.
when parsed, a url object is returned containing properties for each of these
components.
the  module provides two apis for working with urls: a legacy api that
is node.js specific, and a newer api that implements the same
used by web browsers.
a comparison between the whatwg and legacy apis is provided below. above the url
, properties
of an object returned by the legacy  are shown. below it are
properties of a whatwg  object.
whatwg url's  property includes  and , but not
parsing the url string using the whatwg api:
parsing the url string using the legacy api:
constructing a url from component parts and getting the constructed string
it is possible to construct a whatwg url from component parts using either the
property setters or a template literal string:
to get the constructed url string, use the  property accessor:
the whatwg url api
browser-compatible  class, implemented by following the whatwg url
standard.  may be found in the standard itself.
the  class is also available on the global object.
in accordance with browser conventions, all properties of  objects
are implemented as getters and setters on the class prototype, rather than as
data properties on the object itself. thus, unlike s,
using the  keyword on any properties of  objects (e.g. , , etc) has no effect but will still
return .
the absolute or relative input url to parse. if
is relative, then  is required. if  is absolute, the
is ignored. if  is not a string, it is  first.
the base url to resolve against if the  is not
absolute. if  is not a string, it is  first.
creates a new  object by parsing the  relative to the . if
is passed as a string, it will be parsed equivalent to .
the url constructor is accessible as a property on the global object.
it can also be imported from the built-in url module:
a  will be thrown if the  or  are not valid urls. note
that an effort will be made to coerce the given values into strings. for
instance:
unicode characters appearing within the host name of  will be
automatically converted to ascii using the  algorithm.
this feature is only available if the  executable was compiled with
enabled. if not, the domain names are passed through unchanged.
in cases where it is not known in advance if  is an absolute url
and a  is provided, it is advised to validate that the  of
the  object is what is expected.
gets and sets the fragment portion of the url.
invalid url characters included in the value assigned to the  property
are . the selection of which characters to
percent-encode may vary somewhat from what the  and
methods would produce.
gets and sets the host portion of the url.
invalid host values assigned to the  property are ignored.
gets and sets the host name portion of the url. the key difference between
and  is that  does not include the
port.
invalid host name values assigned to the  property are ignored.
gets and sets the serialized url.
getting the value of the  property is equivalent to calling
setting the value of this property to a new value is equivalent to creating a
new  object using . each of the
object's properties will be modified.
if the value assigned to the  property is not a valid url, a
gets the read-only serialization of the url's origin.
gets and sets the password portion of the url.
gets and sets the path portion of the url.
invalid url characters included in the value assigned to the
property are . the selection of which characters
to percent-encode may vary somewhat from what the  and
gets and sets the port portion of the url.
the port value may be a number or a string containing a number in the range
to  (inclusive). setting the value to the default port of the
objects given  will result in the  value becoming
the empty string ().
the port value can be an empty string in which case the port depends on
the protocol/scheme:
upon assigning a value to the port, the value will first be converted to a
string using .
if that string is invalid but it begins with a number, the leading number is
assigned to .
if the number lies outside the range denoted above, it is ignored.
numbers which contain a decimal point,
such as floating-point numbers or numbers in scientific notation,
are not an exception to this rule.
leading numbers up to the decimal point will be set as the url's port,
assuming they are valid:
gets and sets the protocol portion of the url.
invalid url protocol values assigned to the  property are ignored.
special schemes
the  considers a handful of url protocol schemes to be
special in terms of how they are parsed and serialized. when a url is
parsed using one of these special protocols, the  property
may be changed to another special protocol but cannot be changed to a
non-special protocol, and vice versa.
for instance, changing from  to  works:
however, changing from  to a hypothetical  protocol does not
because the new protocol is not special.
likewise, changing from a non-special protocol to a special protocol is also
not permitted:
according to the whatwg url standard, special protocol schemes are ,
, , , , and .
gets and sets the serialized query portion of the url.
any invalid url characters appearing in the value assigned the
property will be . the selection of which
characters to percent-encode may vary somewhat from what the
and  methods would produce.
gets the  object representing the query parameters of the
url. this property is read-only but the  object it provides
can be used to mutate the url instance; to replace the entirety of query
parameters of the url, use the  setter. see
use care when using  to modify the  because,
per the whatwg specification, the  object uses
different rules to determine which characters to percent-encode. for
instance, the  object will not percent encode the ascii tilde ()
character, while  will always encode it:
gets and sets the username portion of the url.
the  method on the  object returns the serialized url. the
value returned is equivalent to that of  and .
value returned is equivalent to that of  and
this method is automatically called when an  object is serialized
creates a  url string that represents the given
object and can be used to retrieve the  later.
the data stored by the registered  will be retained in memory until
is called to remove it.
objects are registered within the current thread. if using worker
threads,  objects registered within one worker will not be available
to other workers or the main thread.
removes the stored  identified by the given id. attempting to revoke a
id that isn't registered will silently fail.
the  api provides read and write access to the query of a
. the  class can also be used standalone with one of the
four following constructors.
the whatwg  interface and the  module have
similar purpose, but the purpose of the  module is more
general, as it allows the customization of delimiter characters ( and ).
on the other hand, this api is designed purely for url query strings.
instantiate a new empty  object.
a query string
parse the  as a query string, and use it to instantiate a new
object. a leading , if present, is ignored.
added in: v7.10.0, v6.13.0
an object representing a collection of key-value pairs
instantiate a new  object with a query hash map. the key and
value of each property of  are always coerced to strings.
unlike  module, duplicate keys in the form of array values are
not allowed. arrays are stringified using , which simply
joins all array elements with commas.
an iterable object whose elements are key-value pairs
instantiate a new  object with an iterable map in a way that
is similar to 's constructor.  can be an  or any
iterable object. that means  can be another , in
which case the constructor will simply create a clone of the provided
. elements of  are key-value pairs, and can
themselves be any iterable object.
duplicate keys are allowed.
append a new name-value pair to the query string.
remove all name-value pairs whose name is .
returns an es6  over each of the name-value pairs in the query.
each item of the iterator is a javascript . the first item of the
is the , the second item of the  is the .
invoked for each name-value pair in the query
to be used as  value for when  is called
iterates over each name-value pair in the query and invokes the given function.
returns:  or  if there is no name-value pair with the given
returns the value of the first name-value pair whose name is . if there
are no such pairs,  is returned.
returns the values of all name-value pairs whose name is . if there are
no such pairs, an empty array is returned.
returns  if there is at least one name-value pair whose name is .
returns an es6  over the names of each name-value pair.
sets the value in the  object associated with  to
. if there are any pre-existing name-value pairs whose names are ,
set the first such pair's value to  and remove all others. if not,
append the name-value pair to the query string.
added in: v7.7.0, v6.13.0
sort all existing name-value pairs in-place by their names. sorting is done
with a , so relative order between name-value pairs
with the same name is preserved.
this method can be used, in particular, to increase cache hits.
returns the search parameters serialized as a string, with characters
percent-encoded where necessary.
returns an es6  over the values of each name-value pair.
returns an es6  over each of the name-value pairs in the query string.
added in: v7.4.0, v6.13.0
returns the  ascii serialization of the . if  is an
invalid domain, the empty string is returned.
it performs the inverse operation to .
returns the unicode serialization of the . if  is an invalid
domain, the empty string is returned.
|  the file url string or url object to convert to a path.
returns:  the fully-resolved platform-specific node.js file path.
this function ensures the correct decodings of percent-encoded characters as
well as ensuring a cross-platform valid absolute path string.
a  object
if the serialized url string should include the
username and password,  otherwise. default: .
fragment,  otherwise. default: .
search query,  otherwise. default: .
if unicode characters appearing in the host
component of the url string should be encoded directly as opposed to being
punycode encoded. default: .
returns a customizable serialization of a url  representation of a
the url object has both a  method and  property that return
string serializations of the url. these are not, however, customizable in
any way. the  method allows for basic customization
of the output.
the path to convert to a file url.
returns:  the file url object.
this function ensures that  is resolved absolutely, and that the url
control characters are correctly encoded when converting into a file url.
the  object to convert to an options object.
returns:  options object
protocol to use.
request to.
the fragment portion of the url.
the serialized query portion of the url.
the path portion of the url.
may change in the future.
the serialized url.
port of remote server.
basic authentication i.e.  to compute an
this utility function converts a url object into an ordinary options object as
expected by the  and  apis.
legacy url api
- legacy: use the whatwg url api instead.
legacy
the legacy  ( or
) is
created and returned by the  function.
the  property is the username and password portion of the url, also
referred to as userinfo. this string subset follows the  and
double slashes (if present) and precedes the  component, delimited by .
the string is either the username, or it is the username and password separated
by .
the  property is the fragment identifier portion of the url including the
leading  character.
the  property is the full lower-cased host portion of the url, including
the  if specified.
the  property is the lower-cased host name portion of the
component without the  included.
the  property is the full url string that was parsed with both the
and  components converted to lower-case.
the  property is a concatenation of the  and
no decoding of the  is performed.
the  property consists of the entire path section of the url. this
is everything following the  (including the ) and before the start
of the  or  components, delimited by either the ascii question
mark () or hash () characters.
no decoding of the path string is performed.
the  property is the numeric port portion of the  component.
the  property identifies the url's lower-cased protocol scheme.
the  property is either the query string without the leading ascii
question mark (), or an object returned by the  module's
method. whether the  property is a string or object is
determined by the  argument passed to .
for example:  or .
if returned as a string, no decoding of the query string is performed. if
returned as an object, both keys and values are decoded.
the  property consists of the entire "query string" portion of the
url, including the leading ascii question mark () character.
no decoding of the query string is performed.
the  property is a  with a value of  if two ascii
forward-slash characters () are required following the colon in the
|  a url object (as returned by  or
constructed otherwise). if a string, it is converted to an object by passing
it to .
the  method returns a formatted url string derived from
if  is not an object or a string,  will throw a
the formatting process operates as follows:
a new empty string  is created.
if  is a string, it is appended as-is to .
otherwise, if  is not  and is not a string, an
for all string values of  that do not end with an ascii
colon () character, the literal string  will be appended to .
if either of the following conditions is true, then the literal string
will be appended to :
property is true;
begins with , , , , or
if the value of the  property is truthy, and either
or  are not , the value of
will be coerced into a string and appended to
followed by the literal string .
if the  property is  then:
if the  is a string, it is appended to .
otherwise, if  is not  and is not a string,
an  is thrown.
if the  property value is truthy, and
is not :
the literal string  is appended to , and
the value of  is coerced to a string and appended to
otherwise, if the  property value is truthy, the value of
is coerced to a string and appended to .
if the  property is a string that is not an empty string:
if the  does not start with an ascii forward slash
(), then the literal string  is appended to .
the value of  is appended to .
if the  property is  and if the
property is an , the literal string  is appended to
followed by the output of calling the  module's
method passing the value of .
otherwise, if  is a string:
if the value of  does not start with the ascii question
mark () character, the literal string  is appended to .
if the  property is a string:
if the value of  does not start with the ascii hash ()
character, the literal string  is appended to .
otherwise, if the  property is not  and is not a
string, an  is thrown.
the url string to parse.
if , the  property will always
be set to an object returned by the  module's
method. if , the  property on the returned url object will be an
unparsed, undecoded string. default: .
if , the first token after the literal
string  and preceding the next  will be interpreted as the .
for instance, given , the result would be
rather than .
the  method takes a url string, parses it, and returns a url
a  is thrown if the  property is present but cannot be decoded.
uses a lenient, non-standard algorithm for parsing url
strings. it is prone to security issues such as
and incorrect handling of usernames and passwords.
is an exception to most of the legacy apis. despite its security
concerns, it is legacy and not deprecated because it is:
faster than the alternative whatwg  parser.
easier to use with regards to relative urls than the alternative whatwg  api.
widely relied upon within the npm ecosystem.
use with caution.
the base url to use if  is a relative url.
the target url to resolve.
the  method resolves a target url relative to a base url in a
manner similar to that of a web browser resolving an anchor tag.
to achieve the same result using the whatwg url api:
percent-encoding in urls
urls are permitted to only contain a certain range of characters. any character
falling outside of that range must be encoded. how such characters are encoded,
and which characters to encode depends entirely on where the character is
located within the structure of the url.
within the legacy api, spaces () and the following characters will be
automatically escaped in the properties of url objects:
for example, the ascii space character () is encoded as . the ascii
forward slash () character is encoded as .
whatwg api
the  uses a more selective and fine grained approach to
selecting encoded characters than that used by the legacy api.
the whatwg algorithm defines four "percent-encode sets" that describe ranges
of characters that must be percent-encoded:
the c0 control percent-encode set includes code points in range u+0000 to
u+001f (inclusive) and all code points greater than u+007e.
the fragment percent-encode set includes the c0 control percent-encode set
and code points u+0020, u+0022, u+003c, u+003e, and u+0060.
the path percent-encode set includes the c0 control percent-encode set
and code points u+0020, u+0022, u+0023, u+003c, u+003e, u+003f, u+0060,
u+007b, and u+007d.
the userinfo encode set includes the path percent-encode set and code
points u+002f, u+003a, u+003b, u+003d, u+0040, u+005b, u+005c, u+005d,
u+005e, and u+007c.
the userinfo percent-encode set is used exclusively for username and
passwords encoded within the url. the path percent-encode set is used for the
path of most urls. the fragment percent-encode set is used for url fragments.
the c0 control percent-encode set is used for host and path under certain
specific conditions, in addition to all other cases.
when non-ascii characters appear within a host name, the host name is encoded
using the  algorithm. note, however, that a host name may contain
both punycode encoded and percent-encoded characters:
util
the  module supports the needs of node.js internal apis. many of the
utilities are useful for application and module developers as well. to access
an  function
returns:  a callback style function
takes an  function (or a function that returns a ) and returns a
function following the error-first callback style, i.e. taking
an  callback as the last argument. in the callback, the
first argument will be the rejection reason (or  if the
resolved), and the second argument will be the resolved value.
will print:
the callback is executed asynchronously, and will have a limited stack trace.
if the callback throws, the process will emit an
event, and if not handled will exit.
since  has a special meaning as the first argument to a callback, if a
wrapped function rejects a  with a falsy value as a reason, the value
is wrapped in an  with the original value stored in a field named
a string identifying the portion of the application for
which the  function is being created.
a callback invoked the first time the logging function
is called with a function argument that is a more optimized logging function.
returns:  the logging function
the  method is used to create a function that conditionally
writes debug messages to  based on the existence of the
environment variable. if the  name appears within the value of that
environment variable, then the returned function operates similar to
. if not, then the returned function is a no-op.
if this program is run with  in the environment, then
it will output something like:
where  is the process id. if it is not run with that
environment variable set, then it will not print anything.
the  supports wildcard also:
if it is run with  in the environment, then it will output
something like:
multiple comma-separated  names may be specified in the
environment variable: .
the optional  argument can be used to replace the logging function
with a different function that doesn't have any initialization or
unnecessary wrapping.
added in: v14.9.0
the  getter is used to create a test that can be used
in conditionals based on the existence of the  environment variable.
if the  name appears within the value of that environment variable,
then the returned value will be . if not, then the returned value will be
if this program is run with  in the environment, then it will
output something like:
alias for . usage allows for readability of that doesn't imply
logging when only using .
the function that is being deprecated.
a warning message to display when the deprecated function is
a deprecation code. see the  for a
list of codes.
returns:  the deprecated function wrapped to emit a warning.
the  method wraps  (which may be a function or class) in
such a way that it is marked as deprecated.
when called,  will return a function that will emit a
using the  event. the warning will
be emitted and printed to  the first time the returned function is
called. after the warning is emitted, the wrapped function is called without
emitting a warning.
if the same optional  is supplied in multiple calls to ,
the warning will be emitted only once for that .
if either the  or  command-line flags are
used, or if the  property is set to  prior to
the first deprecation warning, the  method does nothing.
if the  or  command-line flags are set,
or the  property is set to , a warning and a
stack trace are printed to  the first time the deprecated function is
if the  command-line flag is set, or the
property is set to , then an exception will be
thrown when the deprecated function is called.
the  command-line flag and
property take precedence over  and
a -like format string.
the  method returns a formatted string using the first argument
as a -like format string which can contain zero or more format
specifiers. each specifier is replaced with the converted value from the
corresponding argument. supported specifiers are:
:  will be used to convert all values except ,
and .  values will be represented with an  and objects that
have no user defined  function are inspected using
:  will be used to convert all values except  and
:  is used for all values except  and
:  is used for all values expect .
: json. replaced with the string  if the argument contains
circular references.
: . a string representation of an object with generic javascript
object formatting. similar to  with options
. this will show the full object
including non-enumerable properties and proxies.
object formatting. similar to  without options. this will show
the full object not including non-enumerable properties and proxies.
: . this specifier is ignored and will skip any css passed in.
: single percent sign (). this does not consume an argument.
returns:  the formatted string
if a specifier does not have a corresponding argument, it is not replaced:
values that are not part of the format string are formatted using
if their type is not .
if there are more arguments passed to the  method than the
number of specifiers, the extra arguments are concatenated to the returned
string, separated by spaces:
if the first argument does not contain a valid format specifier,
returns a string that is the concatenation of all arguments separated by spaces:
if only one argument is passed to , it is returned as it is
without any formatting:
is a synchronous method that is intended as a debugging tool.
some input values can have a significant performance overhead that can block the
event loop. use this function with care and never in a hot code path.
this function is identical to , except in that it takes
an  argument which specifies options that are passed along to
returns the string name for a numeric error code that comes from a node.js api.
the mapping between error codes and error names is platform-dependent.
see  for the names of common errors.
added in: v16.0.0, v14.17.0
returns a map of all system error codes available from the node.js api.
- legacy: use es2015 class syntax and  keyword instead.
usage of  is discouraged. please use the es6  and
keywords to get language level inheritance support. also note
that the two styles are .
inherit the prototype methods from one  into another. the
prototype of  will be set to a new object created from
this mainly adds some input validation on top of
as an additional convenience,  will be accessible
through the  property.
es6 example using  and :
any javascript primitive or .
if , 's non-enumerable symbols and
properties are included in the formatted result.  and
entries are also included as well as user defined prototype
properties (excluding method properties). default: .
specifies the number of times to recurse while formatting
. this is useful for inspecting large objects. to recurse up to
the maximum call stack size pass  or .
if , the output is styled with ansi color
codes. colors are customizable. see .
functions are not invoked.
if ,  inspection includes
the  objects. default: .
specifies the maximum number of ,
, , and  elements to include when
formatting. set to  or  to show all elements. set to  or
negative to show no elements. default: .
specifies the maximum number of characters to
include when formatting. set to  or  to show all elements.
set to  or negative to show no characters. default: .
the length at which input values are split across
multiple lines. set to  to format the input as a single line
(in combination with  set to  or any number >= ).
|  setting this to  causes each object key
to be displayed on a new line. it will break on new lines in text that is
longer than . if set to a number, the most  inner elements
are united on a single line as long as all properties fit into
. short array elements are also grouped together. for more
information, see the example below. default: .
|  if set to  or a function, all properties
of an object, and  and  entries are sorted in the resulting
string. if set to  the  is used. if set to a function,
it is used as a .
|  if set to , getters are inspected. if set
to , only getters without a corresponding setter are inspected. if
set to , only getters with a corresponding setter are inspected.
this might cause side effects depending on the getter function.
if set to , an underscore is used to
separate every three digits in all bigints and numbers.
returns:  the representation of .
the  method returns a string representation of  that is
intended for debugging. the output of  may change at any time
and should not be depended upon programmatically. additional  may be
passed that alter the result.
will use the constructor's name and/or  to make
an identifiable tag for an inspected value.
circular references point to their anchor by using a reference index:
the following example inspects all properties of the  object:
the following example highlights the effect of the  option:
the  option allows  and  entries to be
inspected. if there are more entries than , there is no
guarantee which entries are displayed. that means retrieving the same
entries twice may result in different output. furthermore, entries
with no remaining strong references may be garbage collected at any time.
the  option ensures that an object's property insertion order does not
impact the result of .
the  option adds an underscore every three digits to all
numbers.
is a synchronous method intended for debugging. its maximum
output length is approximately 128 mib. inputs that result in longer output will
be truncated.
customizing  colors
color output (if enabled) of  is customizable globally
via the  and  properties.
is a map associating a style name to a color from
the default styles and associated colors are:
: (no styling)
:  (e.g., )
color styling uses ansi control codes that may not be supported on all
terminals. to verify color support use .
predefined control codes are listed below (grouped as "modifiers", "foreground
colors", and "background colors").
modifiers
modifier support varies throughout different terminals. they will mostly be
ignored, if not supported.
- resets all (color) modifiers to their defaults
bold - make text bold
italic - make text italic
underline - make text underlined
strikethrough - puts a horizontal line through the center of the text
(alias: , , )
- prints the text, but makes it invisible (alias: conceal)
dim - decreased color intensity (alias:
overlined - make text overlined
blink - hides and shows the text in an interval
inverse - swap foreground and
background colors (alias: , )
doubleunderline - make text
double underlined (alias: )
framed - draw a frame around the text
foreground colors
(alias: , )
background colors
custom inspection functions on objects
objects may also define their own
function,
which  will invoke and use the result of when inspecting
custom  functions typically return
a string but may return a value of any type that will be formatted accordingly
that can be used to declare custom inspect functions.
in addition to being accessible through , this
symbol is  and can be
accessed in any environment as .
using this allows code to be written in a portable fashion, so that the custom
inspect function is used in an node.js environment and ignored in the browser.
the  function itself is passed as third argument to the custom
inspect function to allow further portability.
the  value allows customization of the default options used by
. this is useful for functions like  or
which implicitly call into . it shall be set to an
object containing one or more valid  options. setting
option properties directly is also supported.
returns  if there is deep strict equality between  and .
otherwise, returns .
see  for more information about deep strict
equality.
used to provide arguments for parsing and to configure
the parser.  supports the following properties:
array of argument strings. default:
with  and  removed.
used to describe arguments known to the parser.
keys of  are the long names of options and values are an
accepting the following properties:
type of argument, which must be either  or .
whether this option can be provided multiple
times. if , all values will be collected in an array. if
, values for the option are last-wins. default: .
a single character alias for the option.
|  |  |  the default option
value when it is not set by args. it must be of the same type as the
the  property. when  is , it must be an array.
should an error be thrown when unknown arguments
are encountered, or when arguments are passed that do not match the
configured in .
whether this command accepts positional
default:  if  is , otherwise .
return the parsed tokens. this is useful for extending
the built-in behavior, from adding additional checks through to reprocessing
the tokens in different ways.
returns:  the parsed command line arguments:
a mapping of parsed option names with their
or  values.
positional arguments.
section. only returned if  includes .
provides a higher level api for command-line argument parsing than interacting
with  directly. takes a specification for the expected arguments
and returns a structured object with the parsed options and positionals.
is experimental and behavior may change. join the
conversation in  to contribute to the design.
detailed parse information is available for adding custom behaviours by
specifying  in the configuration.
the returned tokens have properties describing:
all tokens
one of 'option', 'positional', or 'option-terminator'.
index of element in  containing token. so the
source argument for a token is .
option tokens
long name of option.
how option used in args, like  of .
|  option value specified in args.
undefined for boolean options.
|  whether option value specified inline,
like .
positional tokens
the value of the positional argument in args (i.e. ).
option-terminator token
the returned tokens are in the order encountered in the input args. options
that appear more than once in args produce a token for each use. short option
groups like  expand to a token for each option. so  produces
three tokens.
for example to use the returned tokens to add support for a negated option
like , the tokens can be reprocessed to change the value stored
for the negated option.
example usage showing negated options, and when an option is used
multiple ways then last one wins.
takes a function following the common error-first callback style, i.e. taking
an  callback as the last argument, and returns a version
that returns promises.
or, equivalently using s:
if there is an  property present,
will return its value, see .
assumes that  is a function taking a callback as its
final argument in all cases. if  is not a function,
will throw an error. if  is a function but its last argument is not
an error-first callback, it will still be passed an error-first
callback as its last argument.
using  on class methods or other methods that use  may not
work as expected unless handled specially:
custom promisified functions
using the  symbol one can override the return value of
this can be useful for cases where the original function does not follow the
standard format of taking an error-first callback as the last argument.
for example, with a function that takes in
if  is defined but is not a function,  will
that can be used to declare custom promisified variants of functions,
returns  with any ansi escape codes removed.
an implementation of the   api.
whatwg supported encodings
per the , the encodings supported by the
api are outlined in the tables below. for each encoding,
one or more aliases may be used.
different node.js build configurations support different sets of encodings.
(see )
encodings supported by default (with full icu data)
encodings supported when node.js is built with the  option
encodings supported when icu is disabled
the  encoding listed in the
identifies the  that this  instance
supports. default: .
if decoding failures are fatal.
this option is not supported when icu is disabled
(see ). default: .
when , the  will include the byte
order mark in the decoded result. when , the byte order mark will
be removed from the output. this option is only used when  is
, , or . default: .
creates a new  instance. the  may specify one of the
supported encodings or an alias.
|  |  an , , or
instance containing the encoded data.
if additional chunks of data are expected.
decodes the  and returns a string. if  is , any
incomplete byte sequences occurring at the end of the  are buffered
internally and emitted after the next call to .
if  is , decoding errors that occur will result in a
being thrown.
the encoding supported by the  instance.
the value will be  if decoding errors result in a  being
the value will be  if the decoding result will include the byte order
mark.
an implementation of the   api. all
instances of  only support utf-8 encoding.
the text to encode. default: an empty string.
utf-8 encodes the  string and returns a  containing the
encoded bytes.
the text to encode.
the array to hold the encode result.
the read unicode code units of src.
the written utf-8 bytes of dest.
utf-8 encodes the  string to the  uint8array and returns an object
containing the read unicode code units and written utf-8 bytes.
the encoding supported by the  instance. always set to .
added in: v16.8.0, v14.18.0
returns the  after replacing any surrogate code points
(or equivalently, any unpaired surrogate code units) with the
unicode "replacement character" u+fffd.
creates and returns an  instance whose  is marked
as transferable and can be used with  or .
marks the given  as transferable so that it can be used with
provides type checks for different kinds of built-in objects.
unlike  or , these checks do
not inspect properties of the object that are accessible from javascript (like
their prototype), and usually have the overhead of calling into c++.
the result generally does not make any guarantees about what kinds of
properties or behavior a value exposes in javascript. they are primarily
useful for addon developers who prefer to do type checking in javascript.
returns  if the value is a built-in  or
see also  and
returns  if the value is an instance of one of the
views, such as typed array objects or . equivalent to
returns  if the value is an  object.
returns  if the value is a built-in  instance.
this does not include  instances. usually, it is
desirable to test for both; see  for that.
returns  if the value is an .
this only reports back what the javascript engine is seeing;
in particular, the return value may not match the original source code if
a transpilation tool was used.
returns  if the value is a  instance.
returns  if the value is a boolean object, e.g. created
returns  if the value is any boxed primitive object, e.g. created
by ,  or .
added in: v16.2.0
returns  if the value is a native  value.
a native  value is a special type of object that contains a
raw c++ pointer () for access from native code, and has no other
properties. such objects are created either by node.js internals or native
addons. in javascript, they are  objects with a
prototype.
for further information on , refer to
returns  if the value is a generator function.
returns  if the value is a generator object as returned from a
built-in generator function.
returns  if the value is an iterator returned for a built-in
returns  if the value is an instance of a .
returns  if the value is an instance of a built-in  type.
returns  if the value is a number object, e.g. created
returns  if the value is a built-in .
returns  if the value is a regular expression object.
returns  if the value is a string object, e.g. created
returns  if the value is a symbol object, created
by calling  on a  primitive.
added in: v10.0.0deprecated since: v14.0.0
the following apis are deprecated and should no longer be used. existing
applications and modules should be updated to find alternative approaches.
added in: v0.7.5deprecated since: v6.0.0
the  method was never intended to be used outside of internal
node.js modules. the community found and used it anyway.
it is deprecated and should not be used in new code. javascript comes with very
similar built-in functionality through .
added in: v0.6.0deprecated since: v4.0.0
returns  if the given  is an . otherwise, returns .
added in: v0.11.5deprecated since: v4.0.0
returns  if the given  is a . otherwise, returns .
returns  if the given  is an . otherwise, returns
this method relies on  behavior. it is
possible to obtain an incorrect result when the  argument manipulates
returns  if the given  is a . otherwise, returns
returns  if the given  is strictly . otherwise, returns
returns  if the given  is  or . otherwise,
returns  if the given  is strictly an  and not a
(even though functions are objects in javascript).
returns  if the given  is a primitive type. otherwise, returns
returns  if the given  is . otherwise, returns .
added in: v0.3.0deprecated since: v6.0.0
- deprecated: use a third party module instead.
the  method prints the given  to  with an included
timestamp.
the  module exposes apis that are specific to the version of
built into the node.js binary. it can be accessed using:
returns an integer representing a version tag derived from the v8 version,
command-line flags, and detected cpu features. this is useful for determining
whether a   buffer is compatible with this instance
of v8.
added in: v12.8.0
get statistics about code and its metadata in the heap, see v8
api. returns an object with the
added in: v11.13.0
returns:  a readable stream containing the v8 heap snapshot
generates a snapshot of the current v8 heap and returns a readable
stream that may be used to read the json serialized representation.
this json stream format is intended to be used with tools such as
chrome devtools. the json schema is undocumented and specific to the
v8 engine. therefore, the schema may change from one version of v8 to the next.
creating a heap snapshot requires memory about twice the size of the heap at
the time the snapshot is created. this results in the risk of oom killers
terminating the process.
generating a snapshot is a synchronous operation which blocks the event loop
for a duration depending on the heap size.
returns statistics about the v8 heap spaces, i.e. the segments which make up
the v8 heap. neither the ordering of heap spaces, nor the availability of a
heap space can be guaranteed as the statistics are provided via the v8
function and may change from one v8 version to the
next.
the value returned is an array of objects containing the following properties:
returns an object with the following properties:
is a 0/1 boolean, which signifies whether the
option is enabled or not. this makes v8 overwrite heap
garbage with a bit pattern. the rss footprint (resident set size) gets bigger
because it continuously touches all heap pages and that makes them less likely
to get swapped out by the operating system.
the value of native_context is the number of the
top-level contexts currently active. increase of this number over time indicates
a memory leak.
the value of detached_context is the number
of contexts that were detached and not yet garbage collected. this number
being non-zero indicates a potential memory leak.
the value of total_global_handles_size is the
total memory size of v8 global handles.
the value of used_global_handles_size is the
used memory size of v8 global handles.
the value of external_memory is the memory size of array
buffers and external strings.
the  method can be used to programmatically set
v8 command-line flags. this method should be used with care. changing settings
after the vm has started may result in unpredictable behavior, including
crashes and data loss; or it may simply do nothing.
the v8 options available for a version of node.js may be determined by running
usage:
added in: v15.1.0, v14.18.0, v12.22.0
the  method allows the user to stop the coverage collection
started by , so that v8 can release the execution count
records and optimize code. this can be used in conjunction with
if the user wants to collect the coverage on demand.
the  method allows the user to write the coverage started by
to disk on demand. this method can be invoked multiple
times during the lifetime of the process. each time the execution counter will
be reset and a new coverage report will be written to the directory specified
when the process is about to exit, one last coverage will still be written to
disk unless  is invoked before the process exits.
the file path where the v8 heap snapshot is to be
saved. if not specified, a file name with the pattern
will be
generated, where  will be the pid of the node.js process,
will be  when  is called from
the main node.js thread or the id of a worker thread.
returns:  the filename where the snapshot was saved.
generates a snapshot of the current v8 heap and writes it to a json
file. this file is intended to be used with tools such as chrome
devtools. the json schema is undocumented and specific to the v8
engine, and may change from one version of v8 to the next.
a heap snapshot is specific to a single v8 isolate. when using
, a heap snapshot generated from the main thread will
not contain any information about the workers, and vice versa.
added in: v18.10.0
the api is a no-op if  is already set from the
command line or the api is called more than once.  must be a positive
integer. see  for more information.
serialization api
the serialization api provides means of serializing javascript values in a way
that is compatible with the .
the format is backward-compatible (i.e. safe to store to disk).
equal javascript values may result in different serialized output.
uses a  to serialize  into a buffer.
will be thrown when trying to
serialize a huge object which requires buffer
larger than .
|  |  a buffer returned by .
uses a  with default options to read a js value
from a buffer.
creates a new  object.
writes out a header, which includes the serialization format version.
serializes a javascript value and adds the serialized representation to the
internal buffer.
this throws an error if  cannot be serialized.
returns the stored internal buffer. this serializer should not be used once
the buffer is released. calling this method results in undefined behavior
if a previous write has failed.
a 32-bit unsigned integer.
an  instance.
marks an  as having its contents transferred out of band.
pass the corresponding  in the deserializing context to
write a raw 32-bit unsigned integer.
for use inside of a custom .
write a raw 64-bit unsigned integer, split into high and low 32-bit parts.
write a js  value.
write raw bytes into the serializer's internal buffer. the deserializer
will require a way to compute the length of the buffer.
this method is called to write some kind of host object, i.e. an object created
by native c++ bindings. if it is not possible to serialize , a suitable
exception should be thrown.
this method is not present on the  class itself but can be provided
by subclasses.
this method is called to generate error objects that will be thrown when an
object can not be cloned.
this method defaults to the  constructor and can be overridden on
subclasses.
this method is called when the serializer is going to serialize a
object. it must return an unsigned 32-bit integer id for
the object, using the same id if this  has already been
serialized. when deserializing, this id will be passed to
if the object cannot be serialized, an exception should be thrown.
indicate whether to treat  and  objects as
host objects, i.e. pass them to .
|  |  a buffer returned by
reads and validates a header (including the format version).
may, for example, reject an invalid or unsupported wire format. in that case,
deserializes a javascript value from the buffer and returns it.
|  an  instance.
pass the corresponding  in the serializing context to
(or return the  from
in the case of s).
reads the underlying wire format version. likely mostly to be useful to
legacy code reading old wire format versions. may not be called before
read a raw 32-bit unsigned integer and return it.
read a raw 64-bit unsigned integer and return it as an array
with two 32-bit unsigned integer entries.
read a js  value.
read raw bytes from the deserializer's internal buffer. the  parameter
must correspond to the length of the buffer that was passed to
this method is called to read some kind of host object, i.e. an object that is
created by native c++ bindings. if it is not possible to deserialize the data,
a suitable exception should be thrown.
this method is not present on the  class itself but can be
provided by subclasses.
a subclass of  that serializes
(in particular ) and  objects as host objects, and only
stores the part of their underlying s that they are referring to.
a subclass of  corresponding to the format written by
promise hooks
the  interface can be used to track promise lifecycle events.
to track all async activity, see  which internally uses this
module to produce promise lifecycle events in addition to events for other
async resources. for request context management, see .
the  to call when a promise is created.
returns:  call to stop the hook.
the  hook must be a plain function. providing an async function will
throw as it would produce an infinite microtask loop.
the  to call when a promise
is resolved or rejected.
the  to call before a promise
continuation executes.
the  to call after a promise
returns:  used for disabling hooks
the hook callbacks must be plain functions. providing async functions will
registers functions to be called for different lifetime events of each promise.
respective events during a promise's lifetime.
all callbacks are optional. for example, if only promise creation needs to
key events in the lifetime of a promise have been categorized into four areas:
creation of a promise, before/after a continuation handler is called or around
an await, and when the promise resolves or rejects.
while these hooks are similar to those of  they lack a
hook. other types of async resources typically represent sockets or
file descriptors which have a distinct "closed" state to express the
lifecycle event while promises remain usable for as long as code can still
reach them. garbage collection tracking is used to make promises fit into the
event model, however this tracking is very expensive and they may
not necessarily ever even be garbage collected.
via the promise hooks mechanism, the , , , and
callbacks must not be async functions as they create more
promises which would produce an infinite loop.
while this api is used to feed promise events into , the
ordering between the two is undefined. both apis are multi-tenant
and therefore could produce events in any order relative to each other.
the promise being created.
the promise continued from, if applicable.
called when a promise is constructed. this does not mean that corresponding
/ events will occur, only that the possibility exists. this will
happen if a promise is created without ever getting a continuation.
called before a promise continuation executes. this can be in the form of
, , or  handlers or an  resuming.
will typically be called 0 times if no continuation was ever made for the
promise. the  callback may be called many times in the case where
many continuations have been made from the same promise.
called immediately after a promise continuation executes. this may be after a
, , or  handler or before an  after another
called when the promise receives a resolution or rejection value. this may
occur synchronously in the case of  or .
startup snapshot api
the  interface can be used to add serialization and
deserialization hooks for custom startup snapshots. currently the startup
snapshots can only be built into the node.js binary from source.
in the example above,  can use methods from the
interface to specify how to save information for custom objects in the snapshot
during serialization and how the information can be used to synchronize these
objects during deserialization of the snapshot. for example, if the
contains the following script:
the resulted binary will simply print the data deserialized from the snapshot
during start up:
currently the api is only available to a node.js instance launched from the
default snapshot, that is, the application deserialized from a user-land
snapshot cannot use these apis again.
callback to be invoked before serialization.
optional data that will be passed to the  when it
gets called.
add a callback that will be called when the node.js instance is about to
get serialized into a snapshot and exit. this can be used to release
resources that should not or cannot be serialized or to convert user data
into a form more suitable for serialization.
callback to be invoked after the snapshot is
deserialized.
add a callback that will be called when the node.js instance is deserialized
from a snapshot. the  and the  (if provided) will be
serialized into the snapshot, they can be used to re-initialize the state
of the application or to re-acquire resources that the application needs
when the application is restarted from the snapshot.
callback to be invoked as the entry point after the
snapshot is deserialized.
this sets the entry point of the node.js application when it is deserialized
from a snapshot. this can be called only once in the snapshot building
script. if called, the deserialized application no longer needs an additional
entry point script to start up and will simply invoke the callback along with
the deserialized data (if provided), otherwise an entry point script still
needs to be provided to the deserialized application.
returns true if the node.js instance is run to build a snapshot.
vm (executing javascript)
the  module enables compiling and running code within v8 virtual
machine contexts.
the  module is not a security
mechanism. do not use it to run untrusted code.
javascript code can be compiled and run immediately or
compiled, saved, and run later.
a common use case is to run the code in a different v8 context. this means
invoked code has a different global object than the invoking code.
one can provide the context by  an
object. the invoked code treats any property in the context like a
global variable. any changes to global variables caused by the invoked
code are reflected in the context object.
added in: v0.3.1
instances of the  class contain precompiled scripts that can be
executed in specific contexts.
the javascript code to compile.
specifies the filename used in stack traces produced
by this script. default: .
specifies the line number offset that is displayed
in stack traces produced by this script. default: .
specifies the first-line column number offset that
is displayed in stack traces produced by this script. default: .
|  |  provides an optional  or
, or  with v8's code cache data for the supplied
source. when supplied, the  value will be set to
either  or  depending on acceptance of the data by v8.
when  and no  is present, v8
will attempt to produce code cache data for . upon success, a
with v8's code cache data will be produced and stored in the
property of the returned  instance.
the  value will be set to either  or
depending on whether code cache data is produced successfully.
this option is deprecated in favor of .
called during evaluation of this module
when  is called. if this option is not specified, calls to
will reject with .
this option is part of the experimental modules api. we do not recommend
using it in a production environment.
specifier passed to
the  value passed to the
optional parameter, or an empty object if no value
was provided.
returns:  |  returning a  is
recommended in order to take advantage of error tracking, and to avoid
issues with namespaces that contain  function exports.
if  is a string, then it specifies the filename.
creating a new  object compiles  but does not run it. the
compiled  can be run later multiple times. the  is not bound to
any global object; rather, it is bound before each run, just for that run.
when  is supplied to create the , this value will be set
to either  or  depending on acceptance of the data by v8.
otherwise the value is .
creates a code cache that can be used with the  constructor's
option. returns a . this method may be called at any
time and any number of times.
the code cache of the  doesn't contain any javascript observable
states. the code cache is safe to be saved along side the script source and
used to construct new  instances multiple times.
functions in the  source can be marked as lazily compiled and they are
not compiled at construction of the . these functions are going to be
compiled when they are invoked the first time. the code cache serializes the
metadata that v8 currently knows about the  that it can use to speed up
future compilations.
a  object as returned by the
when , if an  occurs
while compiling the , the line of code causing the error is attached
to the stack trace. default: .
specifies the number of milliseconds to execute
before terminating execution. if execution is terminated, an
will be thrown. this value must be a strictly positive integer.
if , receiving
(ctrl+c) will terminate execution and throw an
. existing handlers for the event that have been attached via
are disabled during script execution, but continue to
work after that. default: .
returns:  the result of the very last statement executed in the script.
runs the compiled code contained by the  object within the given
and returns the result. running code does not have access
to local scope.
the following example compiles code that increments a global variable, sets
the value of another global variable, then execute the code multiple times.
the globals are contained in the  object.
using the  or  options will result in new event loops
and corresponding threads being started, which have a non-zero performance
overhead.
an object that will be . if
, a new object will be created.
human-readable name of the newly created context.
default: , where  is an ascending numerical index of
the created context.
corresponding to the newly
created context for display purposes. the origin should be formatted like a
url, but with only the scheme, host, and port (if necessary), like the
value of the  property of a  object. most notably,
this string should omit the trailing slash, as that denotes a path.
if set to false any calls to  or function
constructors (, , etc) will throw an
if set to false any attempt to compile a webassembly
module will throw a . default: .
if set to , microtasks (tasks
scheduled through s and s) will be run immediately
after the script has run. they are included in the  and
scopes in that case.
first contextifies the given , runs the compiled code contained
by the  object within the created context, and returns the result.
running code does not have access to local scope.
the following example compiles code that sets a global variable, then executes
the code multiple times in different contexts. the globals are set on and
contained within each individual .
runs the compiled code contained by the  within the context of the
current  object. running code does not have access to local scope, but
does have access to the current  object.
the following example compiles code that increments a  variable then
executes that code multiple times:
this feature is only available with the  command
flag enabled.
the  class provides a low-level interface for using
ecmascript modules in vm contexts. it is the counterpart of the
class that closely mirrors s as defined in the ecmascript
unlike  however, every  object is bound to a context from
its creation. operations on  objects are intrinsically asynchronous,
in contrast with the synchronous nature of  objects. the use of
'async' functions can help with manipulating  objects.
using a  object requires three distinct steps: creation/parsing,
linking, and evaluation. these three steps are illustrated in the following
example.
this implementation lies at a lower level than the . there is also no way to interact with the loader yet, though
support is planned.
the specifiers of all dependencies of this module. the returned array is frozen
to disallow any changes to it.
corresponds to the  field of s in
the ecmascript specification.
if the  is , this property contains the exception
thrown by the module during evaluation. if the status is anything else,
accessing this property will result in a thrown exception.
the value  cannot be used for cases where there is not a thrown
exception due to possible ambiguity with .
corresponds to the  field of s
in the ecmascript specification.
specifies the number of milliseconds to evaluate
before terminating execution. if execution is interrupted, an
evaluate the module.
this must be called after the module has been linked; otherwise it will reject.
it could be called also when the module has already been evaluated, in which
case it will either do nothing if the initial evaluation ended in success
( is ) or it will re-throw the exception that the
initial evaluation resulted in ( is ).
this method cannot be called while the module is being evaluated
( is ).
corresponds to the  field of s in the ecmascript specification.
the identifier of the current module, as set in the constructor.
the specifier of the requested module:
the  object  is called on.
the data from the assertion:
per ecma-262, hosts are expected to ignore assertions that they do not
support, as opposed to, for example, triggering an error if an
unsupported assertion is present.
link module dependencies. this method must be called before evaluation, and
can only be called once per module.
the function is expected to return a  object or a  that
eventually resolves to a  object. the returned  must satisfy the
following two invariants:
it must belong to the same context as the parent .
its  must not be .
if the returned 's  is , this method will be
recursively called on the returned  with the same provided
returns a  that will either get resolved when all linking
instances resolve to a valid , or rejected if the linker function either
throws an exception or returns an invalid .
the linker function roughly corresponds to the implementation-defined
abstract operation in the ecmascript
specification, with a few key differences:
the linker function is allowed to be asynchronous while
is synchronous.
the actual  implementation used during module
linking is one that returns the modules linked during linking. since at
that point all modules would have been fully linked already, the
implementation is fully synchronous per
the namespace object of the module. this is only available after linking
() has completed.
corresponds to the  abstract operation in the ecmascript
the current status of the module. will be one of:
:  has not yet been called.
:  has been called, but not all promises returned
by the linker function have been resolved yet.
: the module has been linked successfully, and all of its
dependencies are linked, but  has not yet been called.
: the module is being evaluated through a  on
itself or a parent module.
: the module has been successfully evaluated.
: the module has been evaluated, but an exception was thrown.
other than , this status string corresponds to the specification's
's  field.  corresponds to
in the specification, but with  set to a
value that is not .
the  class provides the  as
defined in the ecmascript specification.
javascript module code to parse
string used in stack traces.
default:  where  is a context-specific ascending
index.
source. the  must be the same as the module from which this
was created.
the  object as returned by the
method, to compile and evaluate this  in.
in stack traces produced by this . default: .
is displayed in stack traces produced by this . default: .
called during evaluation of this
to initialize the .
properties assigned to the  object that are objects may
allow the module to access information outside the specified . use
to create objects in a specific context.
option. returns a . this method may be called any number
of times before the module has been evaluated.
the code cache of the  doesn't contain any javascript
observable states. the code cache is safe to be saved along side the script
source and used to construct new  instances multiple times.
functions in the  source can be marked as lazily compiled
and they are not compiled at construction of the . these
functions are going to be compiled when they are invoked the first time. the
code cache serializes the metadata that v8 currently knows about the
that it can use to speed up future compilations.
defined in the webidl specification. the purpose of synthetic modules is to
provide a generic interface for exposing non-javascript sources to ecmascript
module graphs.
array of names that will be exported from the
called when the module is evaluated.
objects assigned to the exports of this instance may allow importers of
the module to access information outside the specified . use
name of the export to set.
the value to set the export to.
this method is used after the module is linked to set the values of exports. if
it is called before the module is linked, an  error
the body of the function to compile.
an array of strings containing all parameters for the
source.
specifies whether to produce new cache data.
the  object in which the said
function should be compiled in.
an array containing a collection of context
extensions (objects wrapping the current scope) to be applied while
compiling. default: .
this option is part of the experimental modules api, and should not be
considered stable.
compiles the given code into the provided context (if no context is
supplied, the current context is used), and returns it wrapped inside a
function with the given .
corresponding to the newly created
context for display purposes. the origin should be formatted like a url,
but with only the scheme, host, and port (if necessary), like the value of
the  property of a  object. most notably, this
string should omit the trailing slash, as that denotes a path.
after a script has run through .
they are included in the  and  scopes in that case.
returns:  contextified object.
if given a , the  method will  so that it can be used in calls to
or . inside such scripts,
the  will be the global object, retaining all of its existing
properties but also having the built-in objects and functions any standard
has. outside of scripts run by the vm module, global variables
will remain unchanged.
if  is omitted (or passed explicitly as ), a new,
empty  object will be returned.
the  method is primarily useful for creating a single
context that can be used to run multiple scripts. for instance, if emulating a
web browser, the method can be used to create a single context representing a
window's global object, then run all  tags together within that
context.
the provided  and  of the context are made visible through the
inspector api.
returns  if the given  object has been  using
added in: v13.10.0
measure the memory known to v8 and used by all contexts known to the
current v8 isolate, or the main context.
optional.
either  or . in summary mode,
only the memory measured for the main context will be returned. in
detailed mode, the memory measured for all contexts known to the
current v8 isolate will be returned.
either  or . with default
execution, the promise will not resolve until after the next scheduled
garbage collection starts, which may take a while (or never if the program
exits before the next gc). with eager execution, the gc will be started
right away to measure the memory.
returns:  if the memory is successfully measured the promise will
resolve with an object containing information about the memory usage.
the format of the object that the returned promise may resolve with is
specific to the v8 engine and may change from one version of v8 to the next.
the returned result is different from the statistics returned by
in that  measure the
memory reachable by each v8 specific contexts in the current instance of
the v8 engine, while the result of  measure
the memory occupied by each heap space in the current v8 instance.
the javascript code to compile and run.
the  object that will be used
as the  when the  is compiled and run.
the  method compiles , runs it within the context of
the , then returns the result. running code does not have
access to the local scope. the  object must have been
previously  using the  method.
the following example compiles and executes different scripts using a single
object:
the  first contextifies the given  (or
creates a new  if passed as ), compiles the ,
runs it within the created context, then returns the result. running code
does not have access to the local scope.
the following example compiles and executes code that increments a global
variable and sets a new one. these globals are contained in the .
compiles , runs it within the context of the
current  and returns the result. running code does not have access to
local scope, but does have access to the current  object.
the following example illustrates using both  and
the javascript  function to run the same code:
because  does not have access to the local scope,
is unchanged. in contrast,  does have access to the
local scope, so the value  is changed. in this way
is much like an , e.g.
example: running an http server within a vm
when using either  or
, the code is executed within the current v8 global
context. the code passed to this vm context will have its own isolated scope.
in order to run a simple web server using the  module the code passed
to the context must either call  on its own, or have a
reference to the  module passed to it. for instance:
the  in the above case shares the state with the context it is
passed from. this may introduce risks when untrusted code is executed, e.g.
altering objects in the context in unwanted ways.
what does it mean to "contextify" an object?
all javascript executed within node.js runs within the scope of a "context".
according to the :
in v8, a context is an execution environment that allows separate, unrelated,
javascript applications to run in a single instance of v8. you must explicitly
specify the context in which you want any javascript code to be run.
when the method  is called, the  argument
(or a newly-created object if  is ) is associated
internally with a new instance of a v8 context. this v8 context provides the
run using the  module's methods with an isolated global
environment within which it can operate. the process of creating the v8 context
and associating it with the  is what this document refers to as
"contextifying" the object.
timeout interactions with asynchronous tasks and promises
s and s can schedule tasks run by the javascript
engine asynchronously. by default, these tasks are run after all javascript
functions on the current stack are done executing.
this allows escaping the functionality of the  and
options.
for example, the following code executed by  with a
timeout of 5 milliseconds schedules an infinite loop to run after a promise
resolves. the scheduled loop is never interrupted by the timeout:
this can be addressed by passing  to the code
that creates the :
in this case, the microtask scheduled through  will be run
before returning from , and will be interrupted
by the  functionality. this applies only to code running in a
, so e.g.  does not take this option.
promise callbacks are entered into the microtask queue of the context in which
they were created. for example, if  is replaced with just
in the above example, then  will be pushed into the global microtask
queue, because it is a function from the outer (main) context, and thus will
also be able to escape the timeout.
if asynchronous scheduling functions such as ,
, , , etc. are made available
inside a , functions passed to them will be added to global queues,
which are shared by all contexts. therefore, callbacks passed to those functions
are not controllable through the timeout either.
webassembly system interface (wasi)
the wasi api provides an implementation of the
specification. wasi gives sandboxed webassembly applications access to the
underlying operating system via a collection of posix-like functions.
to run the above example, create a new webassembly text format file named
use  to compile  to
the  cli argument is needed for this
example to run.
added in: v13.3.0, v12.16.0
the  class provides the wasi system call api and additional convenience
methods for working with wasi-based applications. each  instance
represents a distinct sandbox environment. for security purposes, each
instance must have its command-line arguments, environment variables, and
sandbox directory structure configured explicitly.
an array of strings that the webassembly application will
see as command-line arguments. the first argument is the virtual path to the
wasi command itself. default: .
an object similar to  that the webassembly
application will see as its environment. default: .
this object represents the webassembly application's
sandbox directory structure. the string keys of  are treated as
directories within the sandbox. the corresponding values in  are
the real paths to those directories on the host machine.
by default, wasi applications terminate the node.js
process via the  function. setting this option to
causes  to return the exit code rather than terminate the
process. default: .
the file descriptor used as standard input in the
webassembly application. default: .
the file descriptor used as standard output in the
the file descriptor used as standard error in the
attempt to begin execution of  as a wasi command by invoking its
export. if  does not contain a  export, or if
contains an  export, then an exception is thrown.
requires that  exports a  named
. if  does not have a  export an exception is thrown.
if  is called more than once, an exception is thrown.
attempt to initialize  as a wasi reactor by invoking its
export, if it is present. if  contains a
export, then an exception is thrown.
is an object that implements the wasi system call api. this object
should be passed as the  import during the instantiation
of a .
web crypto api
node.js provides an implementation of the standard .
use  to access this module.
generating keys
the  class can be used to generate symmetric (secret) keys
or asymmetric key pairs (public key and private key).
aes keys
ecdsa key pairs
ed25519/ed448/x25519/x448 key pairs
hmac keys
rsa key pairs
encryption and decryption
exporting and importing keys
wrapping and unwrapping keys
sign and verify
deriving bits and keys
digest
algorithm matrix
the table details the algorithms supported by the node.js web crypto api
implementation and the apis supported for each:
calling  returns an instance of the
class.  is a singleton that provides access to the remainder of the
crypto api.
provides access to the  api.
generates cryptographically strong random values. the given  is
filled with random values, and a reference to  is returned.
the given  must be an integer-based instance of ,
i.e.  and  are not accepted.
an error will be thrown if the given  is larger than 65,536 bytes.
type:  |  |  |
an object detailing the algorithm for which the key can be used along with
additional algorithm-specific parameters.
when , the  can be extracted using either
type:  one of , , or .
a string identifying whether the key is a symmetric () or
asymmetric ( or ) key.
an array of strings identifying the operations for which the
key may be used.
the possible usages are:
- the key may be used to encrypt data.
- the key may be used to decrypt data.
- the key may be used to generate digital signatures.
- the key may be used to verify digital signatures.
- the key may be used to derive a new key.
- the key may be used to derive bits.
- the key may be used to wrap another key.
- the key may be used to unwrap another key.
valid key usages depend on the key algorithm (identified by
the  is a simple dictionary object with  and
properties, representing an asymmetric key pair.
type:  a  whose  will be .
:  |  |  |
returns:  containing
using the method and parameters specified in  and the keying
material provided by ,  attempts to decipher the
provided . if successful, the returned promise will be resolved with
an  containing the plaintext result.
the algorithms currently supported include:
:  |
material provided by ,  attempts to generate
bits.
the node.js implementation requires that when  is a
number it must be multiple of .
when  is  the maximum number of bits for a given algorithm is
generated. this is allowed for the , , and
if successful, the returned promise will be resolved with an
containing the generated data.
using the method and parameters specified in , and the keying
a new  based on the method and parameters in .
calling  is equivalent to calling  to
generate raw keying material, then passing the result into the
method using the , , and
parameters as input.
using the method identified by ,  attempts to
generate a digest of . if successful, the returned promise is resolved
with an  containing the computed digest.
if  is provided as a , it must be one of:
if  is provided as an , it must have a  property
whose value is one of the above.
using the method and parameters specified by  and the keying
material provided by ,  attempts to encipher .
if successful, the returned promise is resolved with an
containing the encrypted result.
:  must be one of , , , or .
returns:  containing .
exports the given key into the specified format, if supported.
if the  is not extractable, the returned promise will reject.
when  is either  or  and the export is successful,
the returned promise will be resolved with an  containing the
exported key data.
when  is  and the export is successful, the returned promise
will be resolved with a javascript object conforming to the
:  |  |  |  |
using the method and parameters provided in ,
attempts to generate new keying material. depending the method used, the method
may generate either a single  or a .
the  (public and private key) generating algorithms supported
include:
the  (secret key) generating algorithms supported include:
the  method attempts to interpret the provided
as the given  to create a  instance using the provided
, , and  arguments. if the import is
successful, the returned promise will be resolved with the created .
if importing a  key,  must be .
using the method and parameters given by  and the keying material
provided by ,  attempts to generate a cryptographic
signature of . if successful, the returned promise is resolved with
an  containing the generated signature.
in cryptography, "wrapping a key" refers to exporting and then encrypting the
keying material. the  method attempts to decrypt a wrapped
key and create a  instance. it is equivalent to calling
first on the encrypted key data (using the ,
, and  arguments as input) then passing the results
in to the  method using the ,
, and  arguments as inputs. if successful, the returned
promise is resolved with a  object.
the wrapping algorithms currently supported include:
the unwrapped key algorithms supported include:
using the method and parameters given in  and the keying material
provided by ,  attempts to verify that  is
a valid cryptographic signature of . the returned promise is resolved
with either  or .
keying material. the  method exports the keying material into
the format identified by , then encrypts it using the method and
parameters specified by  and the keying material provided by
. it is the equivalent to calling  using
and  as the arguments, then passing the result to the
method using  and  as inputs. if
successful, the returned promise will be resolved with an
containing the encrypted key data.
algorithm parameters
the algorithm parameter objects define the methods and parameters used by
the various  methods. while described here as "classes", they
are simple javascript dictionary objects.
added in: v18.4.0
provides the initialization vector. it must be exactly 16-bytes in length
and should be unpredictable and cryptographically random.
type:  must be .
the initial value of the counter block. this must be exactly 16 bytes long.
the  method uses the rightmost  bits of the block as the
counter and the remaining bits as the nonce.
type:  the number of bits in the  that are
to be used as the counter.
type:  |  |  |  |
with the aes-gcm method, the  is extra input that is not
encrypted but is included in the authentication of the data. the use of
is optional.
the initialization vector must be unique for every encryption operation using a
given key.
ideally, this is a deterministic 12-byte value that is computed in such a way
that it is guaranteed to be unique across all invocations that use the same key.
alternatively, the initialization vector may consist of at least 12
cryptographically random bytes. for more information on constructing
initialization vectors for aes-gcm, refer to section 8 of .
type:  the size in bits of the generated authentication tag.
this values must be one of , , , , , , or
the length of the aes key to be generated. this must be either , ,
type:  must be one of , , , or
type:  must be , , or .
ecdh key derivation operates by taking as input one parties private key and
another parties public key -- using both to generate a common shared secret.
the  property is set to the other parties public
key.
type:  |
if represented as a , the value must be one of:
if represented as an , the object must have a  property
whose value is one of the above listed values.
type:  must be one of  or .
type:  must be one of , , .
the  member represents the optional context data to associate with
the node.js web crypto api implementation only supports zero-length context
which is equivalent to not providing context at all.
provides application-specific contextual input to the hkdf algorithm.
this can be zero-length but must be provided.
the salt value significantly improves the strength of the hkdf algorithm.
it should be random or pseudorandom and should be the same length as the
output of the digest function (for instance, if using  as the
digest, the salt should be 256-bits of random data).
the optional number of bits in the hmac key. this is optional and should
be omitted for most cases.
the number of bits to generate for the hmac key. if omitted,
the length will be determined by the hash algorithm used.
this is optional and should be omitted for most cases.
the number of iterations the pbkdf2 algorithm should make when deriving bits.
should be at least 16 random or pseudorandom bytes.
type:  must be one of , , or
the length in bits of the rsa modulus. as a best practice, this should be
at least .
the rsa public exponent. this must be a  containing a big-endian,
unsigned integer that must fit within 32-bits. the  may contain an
arbitrary number of leading zero-bits. the value must be a prime number. unless
there is reason to use a different value, use
(65537) as the public exponent.
rsaoaepparams.label
an additional collection of bytes that will not be encrypted, but will be bound
to the generated ciphertext.
the  parameter is optional.
rsaoaepparams.name
the length (in bytes) of the random salt to use.
footnotes
an experimental implementation of
as of 05 may 2022
web streams api
an implementation of the .
the  (or "web streams") defines an api for handling
streaming data. it is similar to the node.js  api but emerged later
and has become the "standard" api for streaming data across many javascript
environments.
there are three primary types of objects:
- represents a source of streaming data.
- represents a destination for streaming data.
- represents an algorithm for transforming streaming data.
this example creates a simple  that pushes the current
timestamp once every second forever. an async iterable
is used to read the data from the stream.
added in: v16.5.0
a user-defined function that is invoked immediately when
the  is created.
returns:  or a promise fulfilled with .
a user-defined function that is called repeatedly when the
internal queue is not full. the operation may be sync or
async. if async, the function will not be called again until the previously
returned promise is fulfilled.
returns: a promise fulfilled with .
a user-defined function that is called when the
is canceled.
used only when  is equal to
the maximum internal queue size before backpressure
is applied.
a user-defined function used to identify the size of each
chunk of data.
type:  set to  if there is an active reader for this
the  property is  by default, and is
switched to  while there is an active reader consuming the
stream's data.
returns: a promise fulfilled with  once cancelation has
been completed.
causes the  to be .
the  to which
will push the potentially modified data
is receives from this .
the  to which this
's data will be written.
when , errors in this
will not cause  to be aborted.
when , errors in the destination
do not cause this  to be
canceled.
when , closing this
does not cause  to be closed.
allows the transfer of data to be canceled
returns:  from .
connects this  to the pair of  and
provided in the  argument such that the
data from this  is written in to ,
possibly transformed, then pushed to . once the
pipeline is configured,  is returned.
causes the  to be  while the pipe operation
is active.
a  to which this
when , errors in the
will not cause this  to be canceled.
returns: a promise fulfilled with
returns a pair of new  instances to which this
's data will be forwarded. each will receive the
same data.
when , prevents the
from being closed when the async iterator abruptly terminates.
creates and returns an async iterator usable for consuming this
's data.
causes the  to be  while the async iterator
async iteration
the  object supports the async iterator protocol using
syntax.
the async iterator will consume the  until it terminates.
by default, if the async iterator exits early (via either a ,
, or a ), the  will be closed. to prevent
automatic closing of the , use the
method to acquire the async iterator and set the  option to
the  must not be locked (that is, it must not have an existing
active reader). during the async iteration, the  will be locked.
transferring with
a  instance can be transferred using a .
by default, calling  with no arguments
will return an instance of . the default
reader treats the chunks of data passed through the stream as opaque
values, which allows the  to work with generally any
javascript value.
creates a new  that is locked to the
cancels the  and returns a promise that is fulfilled
when the underlying stream has been canceled.
type:  fulfilled with  when the associated
is closed or rejected if the stream errors or the reader's
lock is released before the stream finishes closing.
returns: a promise fulfilled with an object:
requests the next chunk of data from the underlying
and returns a promise that is fulfilled with the data once it is
releases this reader's lock on the underlying .
the  is an alternative consumer for
byte-oriented s (those that are created with
set equal to  when the
was created).
the  is short for "bring your own buffer". this is a
pattern that allows for more efficient reading of byte-oriented
data that avoids extraneous copying.
do not pass a pooled  object instance in to this method.
pooled  objects are created using ,
or , or are often returned by various  module
callbacks. these types of s use a shared underlying
object that contains all of the data from all of
the pooled  instances. when a , ,
or  is passed in to ,
the view's underlying  is detached, invalidating
all existing views that may exist on that . this
can have disastrous consequences for your application.
every  has a controller that is responsible for
the internal state and management of the stream's queue. the
is the default controller
implementation for s that are not byte-oriented.
closes the  to which this controller is associated.
returns the amount of data remaining to fill the 's
queue.
appends a new chunk of data to the 's queue.
signals an error that causes the  to error and close.
is for byte-oriented s.
when using  in byte-oriented
streams, and when using the ,
the  property
provides access to a  instance
that represents the current read request. the object
is used to gain access to the /
that has been provided for the read request to fill,
and provides methods for signaling that the data has
been provided.
signals that a  number of bytes have been written
signals that the request has been fulfilled with bytes written
to a new , , or .
the  is a destination to which stream data is sent.
a user-defined function that is invoked when a chunk of
data has been written to the .
a user-defined function that is called to abruptly close
the  option is reserved for future use and must be
abruptly terminates the . all queued writes will be
canceled with their associated promises rejected.
closes the  when no additional writes are expected.
creates and creates a new writer instance that can be used to write
data into the .
switched to  while there is an active writer attached to this
transferring with postmessage()
creates a new  that is locked to the given
is closed or rejected if the stream errors or the writer's
the amount of data required to fill the 's queue.
type: a promise that is fulfilled with  when the
writer is ready to be used.
releases this writer's lock on the underlying .
the  manage's the 's
internal state.
called by user-code to signal that an error has occurred while processing
the  data. when called, the  will be aborted,
with currently pending writes canceled.
type:  an  that can be used to cancel pending
write or close operations when a  is aborted.
a  consists of a  and a  that
are connected such that the data written to the  is received,
and potentially transformed, before being pushed into the 's
returns:  or a promise fulfilled with
a user-defined function that receives, and
potentially modifies, a chunk of data written to ,
before forwarding that on to .
a user-defined function that is called immediately before
the writable side of the  is closed, signaling the end of
the transformation process.
the  option is reserved for future use
and must be .
the  manages the internal state
the amount of data required to fill the readable side's queue.
appends a chunk of data to the readable side's queue.
signals to both the readable and writable side that an error has occurred
while processing the transform data, causing both sides to be abruptly
closes the readable side of the transport and causes the writable side
to be abruptly closed with an error.
when , the  will include the
byte order mark in the decoded result. when , the byte order mark
will be removed from the output. this option is only used when  is
utility consumers
the utility consumer functions provide common options for consuming
streams.
they are accessed using:
returns:  fulfills with an  containing the full
contents of the stream.
returns:  fulfills with a  containing the full contents
of the stream.
returns:  fulfills with a  containing the full
returns:  fulfills with the contents of the stream parsed as a
utf-8 encoded string that is then passed through .
utf-8 encoded string.
worker threads
the  module enables the use of threads that execute
javascript in parallel. to access it:
workers (threads) are useful for performing cpu-intensive javascript operations.
they do not help much with i/o-intensive work. the node.js built-in
asynchronous i/o operations are more efficient than workers can be.
unlike  or ,  can share memory. they do
so by transferring  instances or sharing
the above example spawns a worker thread for each  call. in
practice, use a pool of workers for these kinds of tasks. otherwise, the
overhead of creating workers would likely exceed their benefit.
when implementing a worker pool, use the  api to inform
diagnostic tools (e.g. to provide asynchronous stack traces) about the
correlation between tasks and their outcomes. see
in the  documentation for an example implementation.
worker threads inherit non-process-specific options by default. refer to
to know how to customize worker thread options,
specifically  and  options.
any arbitrary, cloneable javascript value that can be used as a
within a worker thread,  returns a clone
of data passed to the spawning thread's .
every new  receives its own copy of the environment data
added in: v10.5.0
is  if this code is not running inside of a  thread.
mark an object as not transferable. if  occurs in the transfer list of
a  call, it is ignored.
in particular, this makes sense for objects that can be cloned, rather than
transferred, and which are used by other objects on the sending side.
for example, node.js marks the s it uses for its
with this.
this operation cannot be undone.
there is no equivalent to this api in browsers.
the message port to transfer.
transfer a  to a different  context. the original
object is rendered unusable, and the returned  instance
takes its place.
the returned  is an object in the target context and
inherits from its global  class. objects passed to the
listener are also created in the target context
and inherit from its global  class.
however, the created  no longer inherits from
, and only  can be used to receive
events using it.
if this thread is a , this is a
allowing communication with the parent thread. messages sent using
are available in the parent thread
using , and messages sent from the parent thread
using  are available in this thread using
receive a single message from a given . if no message is available,
is returned, otherwise an object with a single  property
that contains the message payload, corresponding to the oldest message in the
's queue.
when this function is used, no  event is emitted and the
listener is not invoked.
provides the set of js engine resource constraints inside this worker thread.
if the  option was passed to the  constructor,
this matches its values.
if this is used in the main thread, its value is an empty object.
a special value that can be passed as the  option of the
constructor, to indicate that the current thread and the worker thread should
share read and write access to the same set of environment variables.
any arbitrary, cloneable javascript value that will be cloned
and passed automatically to all new  instances. if  is passed
as , any previously set value for the  will be deleted.
the  api sets the content of
in the current thread and all new
instances spawned from the current context.
an integer identifier for the current thread. on the corresponding worker object
(if there is any), it is available as .
this value is unique for each  instance inside a single process.
an arbitrary javascript value that contains a clone of the data passed
to this thread's  constructor.
the data is cloned as if using ,
according to the .
instances of  allow asynchronous one-to-many communication
with all other  instances bound to the same channel name.
the name of the channel to connect to. any javascript value
that can be converted to a string using  is permitted.
closes the  connection.
type:  invoked with a single  argument
when a message is received.
type:  invoked with a received message cannot be
any cloneable javascript value.
opposite of . calling  on a previously ed
broadcastchannel does not let the program exit if it's the only active handle
left (the default behavior). if the port is ed, calling  again
has no effect.
calling  on a broadcastchannel allows the thread to exit if this
is the only active handle in the event system. if the broadcastchannel is
already ed calling  again has no effect.
instances of the  class represent an asynchronous,
two-way communications channel.
the  has no methods of its own.
yields an object with  and  properties, which refer to linked
instances of the  class represent one end of an
asynchronous, two-way communications channel. it can be used to transfer
structured data, memory regions and other s between different
this implementation matches s.
the  event is emitted once either side of the channel has been
disconnected.
the transmitted value
the  event is emitted for any incoming message, containing the cloned
input of .
listeners on this event receive a clone of the  parameter as passed
to  and no further arguments.
an error object
the  event is emitted when deserializing a message failed.
currently, this event is emitted when there is an error occurring while
instantiating the posted js object on the receiving end. such situations
are rare, but can happen, for instance, when certain node.js api objects
are received in a  (where node.js apis are currently
unavailable).
disables further sending of messages on either side of the connection.
this method can be called when no further communication will happen over this
the  is emitted on both  instances that
are part of the channel.
sends a javascript value to the receiving side of this channel.
is transferred in a way which is compatible with
in particular, the significant differences to  are:
may contain circular references.
may contain instances of builtin js types such as s,
s, s, s, etc.
may contain typed arrays, both using s
and s.
may contain  instances.
may not contain native (c++-backed) objects other than:
may be a list of , , and
after transferring, they are not usable on the sending side of the channel
anymore (even if they are not contained in ). unlike with
, transferring handles such as network sockets is currently
not supported.
if  contains  instances, those are accessible
from either thread. they cannot be listed in .
may still contain  instances that are not in
; in that case, the underlying memory is copied rather than moved.
the message object is cloned immediately, and can be modified after
posting without having side effects.
for more information on the serialization and deserialization mechanisms
behind this api, see the .
considerations when transferring typedarrays and buffers
all  and  instances are views over an underlying
. that is, it is the  that actually stores
the raw data while the  and  objects provide a
way of viewing and manipulating the data. it is possible and common
for multiple views to be created over the same  instance.
great care must be taken when using a transfer list to transfer an
as doing so causes all  and
instances that share that same  to become unusable.
for  instances, specifically, whether the underlying
can be transferred or cloned depends entirely on how
instances were created, which often cannot be reliably determined.
an  can be marked with  to indicate
that it should always be cloned and never transferred.
depending on how a  instance was created, it may or may
not own its underlying . an  must not
be transferred unless it is known that the  instance
owns it. in particular, for s created from the internal
pool (using, for instance  or ),
transferring them is not possible and they are always cloned,
which sends a copy of the entire  pool.
this behavior may come with unintended higher memory
usage and possible security concerns.
see  for more details on  pooling.
the s for  instances created using
or  can always be
transferred but doing so renders all other existing views of
those s unusable.
considerations when cloning objects with prototypes, classes, and accessors
because object cloning uses the ,
non-enumerable properties, property accessors, and object prototypes are
not preserved. in particular,  objects will be read as
plain s on the receiving side, and instances of javascript
classes will be cloned as plain javascript objects.
this limitation extends to many built-in objects, such as the global
opposite of . calling  on a previously ed port does
not let the program exit if it's the only active handle left (the default
behavior). if the port is ed, calling  again has no effect.
if listeners are attached or removed using , the port
is ed and ed automatically depending on whether
listeners for the event exist.
starts receiving messages on this . when using this port
as an event emitter, this is called automatically once
listeners are attached.
this method exists for parity with the web  api. in node.js,
it is only useful for ignoring messages when no event listener is present.
node.js also diverges in its handling of . setting it
automatically calls , but unsetting it lets messages queue up
until a new handler is set or the port is discarded.
calling  on a port allows the thread to exit if this is the only
active handle in the event system. if the port is already ed calling
again has no effect.
if listeners are attached or removed using , the port is
ed and ed automatically depending on whether
the  class represents an independent javascript execution thread.
most node.js apis are available inside of it.
notable differences inside a worker environment are:
streams may be redirected by the parent thread.
the  property is set to .
the  message port is available.
does not stop the whole program, just the single thread,
and  is not available.
and  methods that set group or user ids
are not available.
is a copy of the parent thread's environment variables,
unless otherwise specified. changes to one copy are not visible in other
threads, and are not visible to native add-ons (unless
is passed as the  option to the
constructor).
cannot be modified.
signals are not delivered through .
execution may stop at any point as a result of
being invoked.
ipc channels from parent processes are not accessible.
the  module is not supported.
native add-ons can only be loaded from multiple threads if they fulfill
creating  instances inside of other s is possible.
like  and the , two-way communication
can be achieved through inter-thread message passing. internally, a  has
a built-in pair of s that are already associated with each
other when the  is created. while the  object on the parent
side is not directly exposed, its functionalities are exposed through
and the  event
on the  object for the parent thread.
to create custom messaging channels (which is encouraged over using the default
global channel because it facilitates separation of concerns), users can create
a  object on either thread and pass one of the
s on that  to the other thread through a
pre-existing channel, such as the global one.
see  for more information on how messages are passed,
and what kind of javascript values can be successfully transported through
the thread barrier.
|  the path to the worker's main script or module. must
be either an absolute path or a relative path (i.e. relative to the
current working directory) starting with  or , or a whatwg
object using  or  protocol.
when using a , the data is interpreted based on mime type using
if  is , this is a string containing javascript code
rather than a path.
list of arguments which would be stringified and appended to
in the worker. this is mostly similar to the
but the values are available on the global  as if they
were passed as cli options to the script.
if set, specifies the initial value of  inside
the worker thread. as a special value,  may be used
to specify that the parent thread and the child thread should share their
environment variables; in that case, changes to one thread's
object affect the other thread as well. default: .
if  and the first argument is a , interpret
the first argument to the constructor as a script that is executed once the
worker is online.
list of node cli options passed to the worker.
v8 options (such as ) and options that affect the
process (such as ) are not supported. if set, this is provided
as  inside the worker. by default, options are
inherited from the parent thread.
if this is set to , then
provides a writable stream whose contents appear as
inside the worker. by default, no data is provided.
if this is set to , then  is
not automatically piped through to  in the parent.
any javascript value that is cloned and made
available as . the cloning
occurs as described in the , and an error
is thrown if the object cannot be cloned (e.g. because it contains
if this is set to , then the worker
tracks raw file descriptors managed through  and
, and closes them when the worker exits, similar to other
resources like network sockets or file descriptors managed through
the  api. this option is automatically inherited by all
nested s. default: .
if one or more -like objects
are passed in , a  is required for those
items or  is thrown.
an optional set of resource limits for the new js
engine instance. reaching these limits leads to termination of the
instance. these limits only affect the js engine, and no external data,
including no s. even if these limits are set, the process may
still abort if it encounters a global out-of-memory situation.
the maximum size of the main heap in
mb. if the command-line argument  is set, it
overrides this setting.
the maximum size of a heap space for
recently created objects. if the command-line argument
is set, it overrides this setting.
the size of a pre-allocated memory range
used for generated code.
the default maximum stack size for the thread.
small values may lead to unusable worker instances. default: .
the  event is emitted if the worker thread throws an uncaught
exception. in that case, the worker is terminated.
the  event is emitted once the worker has stopped. if the worker
exited by calling , the  parameter is the
passed exit code. if the worker was terminated, the  parameter is
this is the final event emitted by any  instance.
the  event is emitted when the worker thread has invoked
see the  event for more details.
all messages sent from the worker thread are emitted before the
is emitted on the  object.
the  event is emitted when the worker thread has started executing
javascript code.
returns:  a promise for a readable stream containing
a v8 heap snapshot
returns a readable stream for a v8 snapshot of the current state of the worker.
if the worker thread is no longer running, which may occur before the
is emitted, the returned  is rejected
immediately with an  error.
added in: v15.1.0, v14.17.0, v12.22.0
an object that can be used to query performance information from a worker
instance. similar to .
the same call as , except the values
of the worker instance are returned.
one difference is that, unlike the main thread, bootstrapping within a worker
is done within the event loop. so the event loop utilization is
immediately available once the worker's script begins execution.
an  time that does not increase does not indicate that the worker is
stuck in bootstrap. the following examples shows how the worker's entire
lifetime never accumulates any  time, but is still be able to process
messages.
the event loop utilization of a worker is available only after the  emitted, and if called before this, or after the , then all properties have the value of .
send a message to the worker that is received via
opposite of , calling  on a previously ed worker does
behavior). if the worker is ed, calling  again has
provides the set of js engine resource constraints for this worker thread.
if the worker has stopped, the return value is an empty object.
this is a readable stream which contains data written to
inside the worker thread. if  was not passed to the
constructor, then data is piped to the parent thread's
if  was passed to the  constructor, this is a
writable stream. the data written to this stream will be made available in
the worker thread as .
stop all javascript execution in the worker thread as soon as possible.
returns a promise for the exit code that is fulfilled when the
is emitted.
an integer identifier for the referenced thread. inside the worker thread,
it is available as .
calling  on a worker allows the thread to exit if this is the only
active handle in the event system. if the worker is already ed calling
synchronous blocking of stdio
s utilize message passing via  to implement interactions
with . this means that  output originating from a  can
get blocked by synchronous code on the receiving end that is blocking the
launching worker threads from preload scripts
take care when launching worker threads from preload scripts (scripts loaded
and run using the  command line flag). unless the  option is
explicitly set, new worker threads automatically inherit the command line flags
from the running process and will preload the same preload scripts as the main
thread. if the preload script unconditionally launches a worker thread, every
thread spawned will spawn another until the application crashes.
zlib
the  module provides compression functionality implemented using
gzip, deflate/inflate, and brotli.
to access it:
compression and decompression are built around the node.js .
compressing or decompressing a stream (such as a file) can be accomplished by
piping the source stream through a   stream into a destination
stream:
it is also possible to compress or decompress data in a single step:
threadpool usage and performance considerations
all  apis, except those that are explicitly synchronous, use the node.js
internal threadpool. this can lead to surprising effects and performance
limitations in some applications.
creating and using a large number of zlib objects simultaneously can cause
significant memory fragmentation.
in the preceding example, 30,000 deflate instances are created concurrently.
because of how some operating systems handle memory allocation and
deallocation, this may lead to significant memory fragmentation.
it is strongly recommended that the results of compression
operations be cached to avoid duplication of effort.
compressing http requests and responses
the  module can be used to implement support for the ,
and  content-encoding mechanisms defined by
the http  header is used within an http request to identify
the compression encodings accepted by the client. the
header is used to identify the compression encodings actually applied to a
message.
the examples given below are drastically simplified to show the basic concept.
using  encoding can be expensive, and the results ought to be cached.
see  for more information on the speed/memory/compression
tradeoffs involved in  usage.
by default, the  methods will throw an error when decompressing
truncated data. however, if it is known that the data is incomplete, or
the desire is to inspect only the beginning of a compressed file, it is
possible to suppress the default error handling by changing the flushing
method that is used to decompress the last chunk of input data:
this will not change the behavior in other error-throwing situations, e.g.
when the input data has an invalid format. using this method, it will not be
possible to determine whether the input ended prematurely or lacks the
integrity checks, making it necessary to manually check that the
decompressed result is valid.
memory usage tuning
for zlib-based streams
from , modified for node.js usage:
the memory requirements for deflate are (in bytes):
that is: 128k for  = 15 + 128k for  = 8
(default values) plus a few kilobytes for small objects.
for example, to reduce the default memory requirements from 256k to 128k, the
options should be set to:
this will, however, generally degrade compression.
the memory requirements for inflate are (in bytes) .
that is, 32k for  = 15 (default value) plus a few kilobytes
for small objects.
this is in addition to a single internal output slab buffer of size
, which defaults to 16k.
the speed of  compression is affected most dramatically by the
setting. a higher level will result in better compression, but
will take longer to complete. a lower level will result in less
compression, but will be much faster.
in general, greater memory usage options will mean that node.js has to make
fewer calls to  because it will be able to process more data on
each  operation. so, this is another factor that affects the
speed, at the cost of memory usage.
for brotli-based streams
there are equivalents to the zlib options for brotli-based streams, although
these options have different ranges than the zlib ones:
zlib's  option matches brotli's  option.
see  for more details on brotli-specific options.
flushing
calling  on a compression stream will make  return as much
output as currently possible. this may come at the cost of degraded compression
quality, but can be useful when data needs to be available as soon as possible.
in the following example,  is used to write a compressed partial
http response to the client:
constants
zlib constants
all of the constants defined in  are also defined on
. in the normal course of operations, it will
not be necessary to use these constants. they are documented so that their
presence is not surprising. this section is taken almost directly from the
previously, the constants were available directly from ,
for instance . accessing the constants directly from the module
is currently still possible but is deprecated.
allowed flush values.
return codes for the compression/decompression functions. negative
values are errors, positive values are used for special but normal
events.
compression levels.
compression strategy.
brotli constants
added in: v11.7.0, v10.16.0
there are several options and other constants available for brotli-based
streams:
flush operations
the following values are valid flush operations for brotli-based streams:
(default for all operations)
(default when calling )
(default for the last chunk)
this particular operation may be hard to use in a node.js context,
as the streaming layer makes it hard to know which data will end up
in this frame. also, there is currently no way to consume this data through
the node.js api.
compressor options
there are several options that can be set on brotli encoders, affecting
compression efficiency and speed. both the keys and the values can be accessed
as properties of the  object.
the most important options are:
, adjusted for utf-8 text
, adjusted for woff 2.0 fonts
ranges from  to ,
with a default of .
integer value representing the expected input size;
defaults to  for an unknown input size.
the following flags can be set for advanced control over the compression
algorithm and memory usage tuning:
with a default of , or up to
if the  flag
ranges from  to .
boolean flag that decreases compression ratio in favour of
decompression speed.
boolean flag enabling “large window brotli” mode (not compatible with the
brotli format as standardized in ).
ranges from  to  in steps of .
decompressor options
these advanced options are available for controlling decompression:
boolean flag that affects internal memory allocation patterns.
each zlib-based class takes an  object. no options are required.
some options are only relevant when compressing and are
ignored by the decompression classes.
(compression only)
|  |  |  (deflate/inflate only,
empty dictionary by default)
(if , returns an object with  and .)
limits output size when using
. default:
see the  documentation for more
each brotli-based class takes an  object. all options are optional.
key-value object containing indexed .
compress data using the brotli algorithm.
decompress data using the brotli algorithm.
compress data using deflate.
compress data using deflate, and do not append a  header.
decompress a gzip stream.
compress data using gzip.
decompress a deflate stream.
decompress a raw deflate stream.
decompress either a gzip- or deflate-compressed stream by auto-detecting
the header.
not exported by the  module. it is documented here because it is the
base class of the compressor/decompressor classes.
this class inherits from , allowing  objects to
be used in pipes and similar stream operations.
added in: v8.1.0deprecated since: v10.0.0
the  property specifies the number of bytes written to
the engine, before the bytes are processed (compressed or decompressed,
as appropriate for the derived class).
close the underlying handle.
default:  for zlib-based streams,
for brotli-based streams.
flush pending data. don't call this frivolously, premature flushes negatively
impact the effectiveness of the compression algorithm.
calling this only flushes data from the internal  state, and does not
perform flushing of any kind on the streams level. rather, it behaves like a
normal call to , i.e. it will be queued up behind other pending
writes and will only produce output when data is being read from the stream.
this function is only available for zlib-based streams, i.e. not brotli.
dynamically update the compression level and compression strategy.
only applicable to deflate algorithm.
reset the compressor/decompressor to factory defaults. only applicable to
the inflate and deflate algorithms.
added in: v7.0.0
provides an object enumerating zlib-related constants.
creates and returns a new  object.
an upgrade of zlib from 1.2.8 to 1.2.11 changed behavior when
is set to 8 for raw deflate streams. zlib would automatically set
to 9 if was initially set to 8. newer versions of zlib will throw an exception,
so node.js restored the original behavior of upgrading a value of 8 to 9,
since passing  to zlib actually results in a compressed stream
that effectively uses an 8-bit window only.
convenience methods
all of these take a , , ,
or string as the first argument, an optional second argument
to supply options to the  classes and will call the supplied callback
every method has a  counterpart, which accept the same arguments, but
without a callback.
compress a chunk of data with .
decompress a chunk of data with .
welcome to the strapi v4 developer documentation!
this documentation contains all technical documentation related to the setup, deployment, update and customization of your strapi application.
🤓 can't wait to start using strapi?you can directly head to the !  if demos are more your thing, we have a , or you can request a !
the original purpose of the project was to help bootstrap your api: that's how strapi was created. now, strapi is an open-source headless cms that gives developers the freedom to choose their favorite tools and frameworks and allows editors to manage and distribute their content using their application's admin panel. based on a plugin system, strapi is a flexible cms whose admin panel and api are extensible - and which every part is customizable to match any use case. strapi also has a built-in user system to manage in detail what the administrators and end users have access to.
open-source & contribution
strapi is an open-source project (see  file for more information). the core project, as well as the documentation and any related tool can be found in the  github organization.
as it goes hand in hand with the open-source ecosystem, strapi is open to contributions. the strapi team appreciates every contribution, be it a feature request, bug report, or pull request. the following github repositories are open-source and contributions-friendly:
: main repository of strapi, which contains the core of the project. you can find the admin panel, core plugins, plugin providers, and the whole code that runs your strapi application. please read the  file to have more information about contributions to the main repository. : contains the whole documentation of strapi. please read the  to have more information about contributions to the strapi documentation. : is the design system that is used in the admin panel. it brings consistency between the different admin plugins. : contains everything the community built and all managed plugins. it is used as a central place to find and submit new packages such as plugins, middlewares, hooks, and general enhancements to the core of strapi.
strapi community
strapi is a community-oriented project with an emphasis on transparency. the strapi team has at heart to share their vision and build the future of strapi with the strapi community. this is why the  is open: as all insights are very important and will help steer the project in the right direction, any community member is most welcome to share ideas and opinions there.
community members also take great part in providing the whole community a plethora of resources about strapi. you can find  on the strapi website, where you can also create your own. also, as an open-source project, the technical documentation of strapi is open to contributions (see ).
🤓 want to join the community?you can join , the , and the  to share your ideas and opinions with other community members and members of the strapi team. if you're looking for news and updates about strapi,  and the  are pretty good places to start!
support
strapi is offered as free and open-source for users who wish to self-host the software. when having an issue or a question, the  is great first place to reach out for help. both the strapi community and core developers often check this platform and answer posts.
for enterprise support, please see our , please note that you will need to have an active enterprise license to place tickets.
quick start guide
strapi offers a lot of flexibility. whether you want to go fast and quickly see the final result, or would rather dive deeper into the product, we got you covered.
prerequisites the installation requires the following software to be already installed on your computer: : only lts versions are supported (v14 and v16). other versions of node.js may not be compatible with the latest release of strapi. the 16.x version is most recommended by strapi.  (v6 only) or  to run the cli installation scripts.  when using a sqlite database
👇 let's get started! using the big buttons below, please choose between:
the hands-on path for a diy approach to run your project or the starters path for the quickest way to spin up a fullstack application powered by a strapi back end.
🚀 part a: create a new project with strapi  step 1: run the installation script run the following command in a terminal:  ✏️ notethe quick start installation sets up strapi with a sqlite database. other databases and installation options are available (see ). step 2: register the first administrator user once the installation is complete, your browser automatically opens a new tab. by completing the form, you create your own account. once done, you become the first administrator user of this strapi application. welcome aboard, commander! you now have access to the :  🥳 congratulations! you have just created a new strapi project! you can start playing with strapi and discover the product by yourself using our , or proceed to part b below.  🛠 part b: build your content the installation script has just created an empty project. we will now guide you through creating a restaurants directory, inspired by our  example application. in short, we will create a data structure for your content, then add some entries and publish them, so that the api for your content can be consumed. the admin panel of strapi runs at . this is where you will spend most of your time creating and updating content. 💡 tipif the server is not already running, in your terminal,  into the  folder and run  (or ) to launch it. step 1: create collection types with the content-type builder the content-type builder plugin helps you create your data structure. when creating an empty project with strapi, this is where to get the party started!  create a "restaurant" collection type your restaurants directory will eventually include many restaurants, so we need to create a "restaurant" collection type. then we can describe the fields to display when adding a new restaurant entry: go to plugins   in the main navigation. click on create new collection type. type  for the display name, and click continue. click the text field. type  in the name field. switch to the advanced settings tab, and check the required field and the unique field settings. click on add another field. choose the rich text field. type  under the name field, then click finish. finally, click save and wait for strapi to restart.  once strapi has restarted, "restaurant" is listed under  content manager > collection types in the navigation. wow, you have just created your very first content-type! it was so cool — let's create another one right now, just for pleasure.  create a "category" collection type it would help getting a bit more organized if our restaurants directory had some categories. let's create a "category" collection type: go to plugins   in the main navigation. click on create new collection type. type  for the display name, and click continue. click the text field. type  in the name field. switch to the advanced settings tab, and check the required field and the unique field settings. click on add another field. choose the relation field. on the right side, click the category relational fields box and select "restaurant". in the center, select the icon that represents "many-to-many" . the text should read .  finally, click finish, then the save button, and wait for strapi to restart.  step 2: use the collection types to create new entries now that we have created a basic data structure with 2 collection types, "restaurant" and "category", let's use them to actually add content by creating new entries.  create an entry for the "restaurant" collection type go to   in the navigation. click on add new entry. type the name of your favorite local restaurant in the name field. let's say it's . in the description field, write a few words about it. if you're lacking some inspiration, you can use  click save.  the restaurant is now listed in the  view.  add categories let's go to   and create 2 categories: click on add new entry. type  in the name field. click save. go back to collection types - category, then click again on add new entry. type  in the name field, then click save.  the "french food" and "brunch" categories are now listed in the  view.  add a category to a restaurant go to   in the navigation, and click on "biscotte restaurant". in the right sidebar, in the categories drop-down list, select "brunch". click save.  step 3: set roles & permissions we have just added a restaurant and 2 categories. we now have enough content to consume (pun intended). but first, we need to make sure that the content is publicly accessible through the api: click on general  settings at the bottom of the main navigation. under users & permissions plugin, choose . click the public role. scroll down under permissions. in the permissions tab, find restaurant and click on it. click the checkboxes next to find and findone. repeat with category: click the checkboxes next to find and findone. finally, click save.   step 4: publish the content by default, any content you create is saved as a draft. let's publish our categories and restaurant. first, navigate to  . from there: click the "brunch" entry. on the next screen, click publish. in the confirmation window, click yes, publish. then, go back to the categories list and repeat for the "french food" category. finally, to publish your favorite restaurant, go to  , click the restaurant entry, and publish it.   step 5: use the api ok dear gourmet, we have just finished creating our content and making it accessible through the api. you can give yourself a pat on the back — but you have yet to see the final result of your hard work. there you are: the list of restaurants is accessible at . try it now! the result should be similar to the example response below 👇.   12345678910111213141516171819202122 🥳 congratulations! now your content is created, published, and you have permissions to request it through the api.
keep on creating amazing content!  ⏩ what to do next? now that you know the basics of creating and publishing content with strapi, we encourage you to explore and dig deeper into some strapi features: 👉  to restrict access to your api, 👉 learn how to use strapi's  and  apis to query the content, 👉 and  and .  🚀 part a: create a new project with strapi starters strapi  are the fastest way to kickstart your project. they cover many use cases (blog, e-commerce solution, corporate website, portfolio) and integrate with various technologies (next, gridsome, nuxt). this quick start guide has been specifically tailored to use the . we highly recommend you to follow along with this starter. once you get a better understanding of strapi, you will be able to play with other starters on your own.  step 1: run the installation script to create a  blog using strapi, run the following command in a terminal:  1  1 during the installation, when terminal asks : select the default  option by pressing enter. the installation then resumes — just let the magic happen!  step 2: register & have a look at your blog once the installation is complete, your browser automatically opens a tab at (). it's for strapi's admin panel, the back end of your application. by completing the form in the admin panel tab, you create your own account. once done, you become the first administrator user of this strapi application. welcome aboard, commander! now, open  in another tab. this is the front end of your application, and you can already see the next blog in action.   congratulations! 🥳 your blog is ready! you can start playing with strapi and discover the product by yourself using our , or proceed to part b below. writing a blog is not your cup of tea? you can leave this guide and play with other  on your own.  🎨 part b: play with your content strapi  build a full stack application and a data structure for you, so you can start playing with your content faster. we are about to contribute to the blog we have just created. let's play with your application, by adding yourself as a writer and creating your own article, updating the homepage, and restarting the servers to look at the final result. 💡 tipif the strapi server is not already running, in your terminal,  into the  folder and run  (or ) to launch it. step 1: add yourself as a writer you have several ideas for great articles in mind. but first, the world needs to know who you are! click on   in the navigation, and click the add new entry button.  add your name and email in the corresponding fields. add your favorite selfie in the picture field. you can either drag and drop an image or click the field and upload a file. saying 'cheese!' during the process is optional. 😄 click save.  step 2: write & publish your first article to write an article, we need to add a new entry to the "article" collection type and fill in a few fields.  click on   in the main navigation, and click the add new entry button.  give your article a title, a description, and add some content type  in the title field. type  in the description field. write a few lines in the content field. if you're lacking some inspiration, just type . scroll down and add a picture in the image field.  choose an author and a category for your article in the sidebar on the right, choose your name in the author drop-down list. you have just signed your first article with strapi. take a few seconds to contemplate this historic moment! while there, you might also want to choose a category for your article from the list. 🤓 don't forget to click on save to save the article.  turn your draft into a publication by default, your new article would be saved as a draft. let's not be too shy and publish it right away. to publish an article, click the publish button at the top of the window. you have just created and published your first article, "hello world!". you can find it in the   view.  step 3: update the  single type it's time to make this blog a bit more yours. click on   in the main navigation. let's edit this homepage: replace the image in the shareimage field. at the bottom of the page, update the title to  in the hero field group. click save.  step 4: restart the servers to reflect latest changes next is a static-site generator. it means that you need to restart the servers for changes to appear on the front end: in your terminal, press  to stop the servers. make sure you are in the  folder. if not, type  and press enter. restart the servers by typing  (or ) and press enter. after a few moments, you should see your blog with its updated title running at . the "hello world!" article you have just created is also visible at the bottom of the page.  congratulations! 🥳 now you know how to use strapi to create and update your blog. keep on creating amazing content!  ⏩ what to do next? now that you know the basics of kickstarting your project with a strapi starter, we encourage you to explore and play a bit further: 👉  start another project! we have lots of other  you can use to kickstart your blog, e-commerce, corporate website, or portfolio project. 👉  read more about the  on our blog.
below are answers and solutions to most common issues that you may experience when working with strapi.
why can't i create or update content-types in production/staging?
strapi stores model configuration files (what defines the model schema) in files such as . due to how node.js works, in order for changes to take effect, that would require node to restart the server. this could potentially cause downtime of your production service and likewise these changes should be tracked in some kind of source control.
generally your "flow" of development would follow the following path:
development - develop your strapi application locally on your host machine, then push changes into source control staging - deploy changes from source control to a "production-like" environment for testing production - if no other changes are needed, deploy into production repeat as needed, it is recommended that you properly version and test your application as you go
at this time and in the future there is no plan to allow model creating or updating while in a production environment, and there is currently no plans to move model settings into the database. there are no known nor recommended workarounds for this.
does strapi handle deploying or migrating of content?
strapi does not currently provide any tools for migrating or deploying your data changes between different environments (ie. from development to production). with the exception being the content-manager settings, to read more about this option please see the following .
user can't login to the admin panel
with the release of the strapi beta version a fundamental change occurred in that the end users (rest and graphql users) were split from the administrators (admin panel users) in such a way that normal users can not be given access to the admin panel. if you would like to read more on why this change was done, you can read the strapi  about it.
strapi has released the new admin & permissions (rbac - role based access control) that does allow for some degree of control over what users can access within the admin panel and includes some field level permissions. you can now also give roles specific permissions for things like content-types, single types, plugins, and settings.
when this new plugin release, there is two versions:
community edition enterprise edition
by default, the community edition includes 3 pre-defined roles (administrators, editor, author). upgrading to the enterprise edition will unlock an unlimited number of roles. there will be certain other field level permission limitations based on the edition and we will be building a detailed guide as to what is included within the "basic" vs "advanced" rbac features. to learn more about what is included as well as pricing please see our .
relations aren't maintaining their sort order
with the components there is a hidden field called  that allows entries to maintain their sort, however with relations there is no such field. if you consider the typical count of of component entries vs relational based entries (in retrospect they function in the backend the same) there is generally a much higher number of relations. if relations were to have an  field applied to them as well it could cause significant performance degradation when trying to update the order, and likewise in the case where a relation could be attached to multiple entries it would be quite difficult to maintain the order.
for the time being there is no recommended way to handle this automatically and instead it may be required for you to create custom controllers to handle this within your own project.
we are evaluating if we will natively add support for this in the future. we will add more details when they become available.
why are my application's database and uploads resetting on paas?
if you used  to create your strapi project, by default this uses the sqlite database. paas systems (heroku, digitalocean apps, google app engine, etc.) file systems are typically  or read-only meaning that each time a dyno (container) is reset all filesystem changes are lost. and since both sqlite and local uploads are stored on the filesystem, any changes made to these since the last dyno reset will be deleted. typically dynos are reset at least once a day, and in most cases multiple times per day or when new code is pushed to these services.
it is recommended you use a database add-on like heroku's postgresql. for file uploads, you will need to use one of the 3rd party providers such as cloudinary or aws s3.
can strapi be run in serverless environments?
strapi is not well suited for serverless environments due to how the application is structured. several actions happen while strapi is booting that can take several seconds. serverless deployment usually requires an application to cold boot very quickly. strapi is designed to run as an always-on service, and we don't plan to decrease the cold boot time for the foreseeable future. therefore, running strapi in serverless environments is not a great experience, as every request will take seconds to respond to instead of milliseconds. choosing between a cold boot or a warm boot is an architectural decision that many software developers need to take from a very early stage, so please consider this when choosing to use strapi.
can i store my content manager layout configurations in the model settings?
currently strapi does not support this, a  and  command has been added to make migration of these settings easier when moving between different deployments and environments.
we don't offer the ability to store these configurations in the model settings for several reasons:
it will create conflicts in case of content internationalization and translations in the admin interface. the layout might be different according to the roles and permissions. while the model is the same whatever the content created, the contribution interface can be different. for instance, we have an idea to create a mobile application for contributors only. the labels and layout configurations could be different according the device & interface.
for all these reasons, and others, we think it'll be a mistake and might confuse users if we store the configuration in the model settings file. the final solution is to make the migration and deployment across environment easier.
how do i customize a plugin?
strapi uses a system called  as plugins are stored in the  folder. due to this extensions work by strapi detecting newer versions of files and using that as a replacement for the ones stored within the .
you gain the ability to modify these files without forking the plugin package, however you lose the ability to easily update. after each version release you will need to compare your changes to those in the new version and modify your version of the files accordingly.
can i add my own 3rd party auth provider?
yes, you can either follow the following  or you can take a look at the  and submit a pull request to include the provider for everyone. eventually strapi does plan to move from the current grant/purest provider to a split natured system similar to the upload providers.
there is currently no eta on this migration however.
does strapi allow me to change the default id type or name?
no, currently does not have the ability to allow for changing the default id name nor does it allow you to switch the data type (such as uuid in postgresql), support for this is being looked at in future.
can you filter and/or deep filter on dynamic zones and polymorphic relations?
at this time we do not plan to allow for filtering on dynamic zones or polymorphic relations due to various complexity and performance issues that come from doing so.
how do i setup ssl with strapi?
strapi implements no ssl solution natively, this is due to the fact that it is extremely insecure to directly offer a node.js application to the public web on a low port.
on linux based operating systems you need root permissions to bind to any port below 1024 and with typical ssl being port 443 you would need to run your application as root.
likewise since strapi is node.js based, in order for changes with the ssl certificate to take place (say when it expires) you would need to restart your application for that change to take effect.
due to these two issues, it is recommended you use a proxy application such as , , , apache, traefik, or many others to handle your edge routing to strapi. there are settings in the environment  to handle upstream proxies. the proxy block requires all settings to be filled out and will modify any backend plugins such as authentication providers and the upload plugin to replace your standard  with the proxy url.
can i use typescript in a strapi project?
typescript is supported in strapi projects from v4.2.0-beta.1 typescript code examples are available throughout the core developer documentation and a .
is x feature available yet?
you can see the  to see which feature requests are currently being worked on and which have not been started yet, and to add new feature requests.
collected usage information
we are committed to providing a solution, with strapi, that exceeds the expectations of the users and community. we are also committed to continuing to develop and make strapi even better than it is today. to that end, strapi contains a feature in which anonymous and otherwise non-sensitive data is collected. this data is collectively aggregated for all our users, which when taken together give us a better global understanding of how users are interacting and using strapi.
the number of developers using strapi is growing significantly. as mentioned earlier, we are committed to providing the best experience to our users. we will always continue to do hands-on ui/ux testing, surveys, issue tracking, roadmap votes, etc... and otherwise talk with the strapi community while striving to understand and deliver what is being asked for and what is needed, by any means available.
however, these above actions alone are often insufficient to maintain an overall picture of some aspects of the global usage of strapi and its features. globally aggregated data helps us answer and make choices around questions like these:
are our users using a particular feature or not? for those who are using it, what do they use it for? is it activated and used alongside another plugin? which specific plugin? or something else, like, only in development/production? how long does setting up a project take? if the global install time increases, does it mean that users are encountering issues or the process is simply too complicated? what type of errors our users are facing? what are the most used plugins? should we focus our efforts on being compatible with node 16? maybe our community uses version 16 in greater percentages than the global node.js community? and more...
without these metrics, we wouldn't be able to make the right choices as we continue to move forward with the roadmap and provide what you, the community and users, are asking for.
collected data
the following data is collected:
unique project id (generated with uuid) unique machine id (generated with ) environment state (development, staging, production) system information (os) build configurations
✋ gdprthe collected data are of a non-sensitive nature and no personal data is collected by strapi's telemetry system. we are compliant with the european gdpr recommendations (see our ). we do not collect databases configurations, password or custom variables. any data collected (as above) is secured, encrypted and anonymized.
✏️ noteif you check the box "keep me updated about new features & upcoming improvements (by doing this, you accept the terms and the privacy policy)" on the first registration screen, your email address, first name, and role in the company will be sent to our marketing team. these data are used for marketing-related purposes only (e.g., the strapi newsletter) and are not sent to strapi's telemetry system. this collected usage information is opt-out by default: the data aren't sent if you don't check the box.
opt-out
✋ cautionstrapi previously recommended disabling data collection by removing the  property in the  file located in the project root. while this method will still work it is discouraged since the  might be required for certain project functionality and adding a  at a later date would re-enable data collection without informing the user.
the default data collection feature can be disabled using the following cli command:
alternatively, the  flag in the project  file will also disable data collection.
data collection can later be re-enabled by deleting the flag or setting it to false, or by using the  command.
✏️ noteif you have any questions or concerns regarding data collection, please contact us at the following email address .
project structure
✏️ noteif the strapi project was created with the , its structure includes both a  and  folder, where the  folder has the default structure.   1234
the default structure of a strapi project created without the starter cli depends on whether the project was created with vanilla javascript or with , and looks like the following:
admin panel customization
the admin panel is a  that is similar to a plugin, except that it encapsulates all the installed plugins of a strapi application. some of its aspects can be , and plugins can also  it.
to toggle hot reloading and get errors in the console while developing, start strapi in front-end development mode by running the application with the  flag:
customization options
customizing the admin panel is helpful to better reflect your brand identity or to modify some default strapi behavior:
the  can be modified through the server configuration. the  allows replacing the logos and favicon, defining locales and extending translations, extending the theme, and disabling some strapi default behaviors like displaying video tutorials or notifications about new strapi releases. the  can be replaced or customized. the  should be customized using the users and permissions plugin. the  based on webpack 5 can also be extended for advanced customization
access url
by default, the administration panel is exposed via . for security reasons, this path can be updated.
to make the admin panel accessible from , use this in the  file:
🤓 advanced settingsfor more advanced settings please see the  documentation.
host and port
by default, the front end development server runs on  but this can be modified:
prerequisites before configuring any admin panel customization option, make sure to: rename the default  file into , and create a new  folder in . strapi projects already contain by default another  folder in  but it is for plugins extensions only (see ).
the  object found at  stores the admin panel configuration.
any file used by the  object (e.g. a custom logo) should be placed in a  folder and imported inside .
the  object accepts the following parameters:
locales
to update the list of available locales in the admin panel, use the  array:
✏️ notesthe  locale cannot be removed from the build as it is both the fallback (i.e. if a translation is not found in a locale, the  will be used) and the default locale (i.e. used when a user opens the administration panel for the first time). the full list of available locales is accessible on .
extending translations
translation key/value pairs are declared in  files. these keys can be extended through the  key:
a plugin's key/value pairs are declared independently in the plugin's files at . these key/value pairs can similarly be extended in the  key by prefixing the key with the plugin's name (i.e. ) as in the following example:
if more translations files should be added, place them in  folder.
logos
the strapi admin panel displays a logo in 2 different locations, represented by 2 different keys in the :
to update the logos, put image files in the  folder and update the corresponding keys. there is no size limit for image files set through the configuration files.
✏️ notethe logo displayed in the main navigation of the admin panel can also be customized directly via the admin panel (see ). however, the logo displayed in the login page can only be customized via the configuration files for now.
note also that the main navigation logo uploaded via the admin panel supersedes any logo set through the configuration files.
favicon
to replace the favicon, use the following procedure:
(optional) create a  folder if the folder does not already exist. upload your favicon into . replace the existing favicon.ico file at the strapi application root with a custom  file. update  with the following:  123456789101112 rebuild, launch and revisit your strapi app by running  in the terminal.
💡 tipthis same process may be used to replace the login logo (i.e. ) and menu logo (i.e. ) (see ).
✋ cautionmake sure that the cached favicon is cleared. it can be cached in your web browser and also with your domain management tool like cloudflare's cdn.
tutorial videos
to disable the information box containing the tutorial videos, set the  key to .
releases notifications
to disable notifications about new strapi releases, set the  key to .
theme extension
strapi applications can be displayed either in light or dark mode (see ), and both can be extended through custom theme settings.
to extend the theme, use either:
the  key for the light mode the  key for the dark mode
🤓 strapi design systemthe default  defines various theme-related keys (shadows, colors…) that can be updated through the  and  keys in . the  is fully customizable and has a dedicated  documentation.
✋ cautionthe former syntax for  without  or  keys is deprecated and will be removed in the next major release. we encourage you to update your custom theme to use the new syntax that supports light and dark modes.
wysiwyg editor
to change the current wysiwyg, you can install a , create your own plugin (see ) or take advantage of the  and the  system:
email templates
email templates should be edited through the admin panel, using the .
webpack configuration
prerequisites make sure to rename the default  file into  before customizing webpack.
in order to extend the usage of webpack v5, define a function that extends its configuration inside :
123456789101112
✏️ noteonly  and the files under the  folder are being watched by the webpack dev server.
extension
there are 2 use cases to extend the admin panel:
a plugin developer wants to develop a strapi plugin that extends the admin panel everytime it's installed in any strapi application. this can be done by taking advantage of the . a strapi user only needs to extend a specific instance of a strapi application. this can be done by directly updating the  file, which can import any file located in .
the administration is a react front-end application calling an api. the front end and the back end are independent and can be deployed on different servers, which brings us to different scenarios:
deploy the entire project on the same server. deploy the administration panel on a server (aws s3, azure, etc) different from the api server.
build configurations differ for each case.
before deployment, the admin panel needs to be built, by running the following command from the project's root directory:
this will replace the folder's content located at . visit  to make sure customizations have been taken into account.
same server
deploying the admin panel and the api on the same server is the default behavior. the build configuration will be automatically set. the server will start on the defined port and the administration panel will be accessible through .
💡 tipyou might want to .
different servers
to deploy the front end and the back end on different servers, use the following configuration:
after running  with this configuration, the  folder will be created/overwritten. use this folder to serve it from another server with the domain of your choice (e.g. ).
the administration url will then be  and every request from the panel will hit the backend at .
✏️ noteif you add a path to the  option, it won't prefix your app. to do so, use a proxy server like nginx (see ).
plugins extension
strapi comes with  that can be installed from the  or as npm packages. you can also create your own plugins (see ) or extend the existing ones.
plugin extensions code is located in the  folder (see ). some plugins automatically create files there, ready to be modified.
12345678910
plugins can be extended in 2 ways:
(e.g. to add controllers, services, policies, middlewares and more)
️❗️ warningnew versions of strapi are released with , but these guides might not cover unexpected breaking changes in your plugin extensions. consider forking a plugin if extensive customizations are required. currently, the admin panel part of a plugin can only be extended using , but please consider that doing so might break your plugin in future major versions of strapi.
extending a plugin's content-types
a plugin's content-types can be extended in 2 ways: using the programmatic interface within  and by overriding the content-types schemas.
the final schema of the content-types depends on the following loading order:
the content-types of the original plugin, the content-types overridden by the declarations in the  defined in  the content-types declarations in the  the content-types declarations in the  of the strapi application
to overwrite a plugin's :
(optional) create the  folder at the root of the app, if the folder does not already exist. create a subfolder with the same name as the plugin to be extended. create a  subfolder. inside the  subfolder, create another subfolder with the same  as the content-type to overwrite. inside this  subfolder, define the new schema for the content-type in a  file (see  documentation). (optional) repeat steps 4 and 5 for each content-type to overwrite.
extending a plugin's interface
when a strapi application is initializing, plugins, extensions and global lifecycle functions events happen in the following order:
plugins are loaded and their interfaces are exposed. files in  are loaded. the  and  functions in  are called.
a plugin's interface can be extended at step 2 (i.e. within ) or step 3 (i.e. inside ).
within the extensions folder
to extend a plugin's server interface using the  folder:
(optional) create the  folder at the root of the app, if the folder does not already exist. create a subfolder with the same name as the plugin to be extended. create a  file to extend a plugin's back end using the . within this file, define and export a function.  the function receives the  interface as an argument so it can be extended.
123456789101112131415
within the register and bootstrap functions
to extend a plugin's interface within , use the  and   of the whole project, and access the interface programmatically with .
plugins development
strapi allows the development of local plugins that work exactly like the external plugins available from the .
🤓 extending pluginsif you would rather extend an existing plugin than create a new one, see the  documentation.
create a plugin
strapi provides a  for creating plugins. to create a plugin:
navigate to the root of a strapi project. run  or  in a terminal window to start the interactive cli. choose "plugin" from the list, press enter, and give the plugin a name in kebab-case (e.g. ) choose either  or  for the plugin language. enable the plugin by adding it to the  file:
(typescript-specific) run  or  in the newly-created plugin directory. (typescript-specific) run  or  in the plugin directory. this step transpiles the typescript files and outputs the javascript files to a  directory that is unique to the plugin. run  or  at the project root. run  or  at the project root.
plugins created using the preceding directions are located in the  directory of the application (see ).
✏️ noteduring plugin development it is helpful to use the  flag to toggle hot reloading of the admin panel. see the  documentation for more details. (typescript specific) while developing your plugin, you can run  or  in the plugin directory to watch the changes to the typescript server files.
💡 tipcheck  to learn how to publish your strapi plugin on npm.
add features to a plugin
strapi provides programmatic apis for plugins to hook into some of strapi's features.
plugins can register with the server and/or the admin panel, by looking for entry point files at the root of the package:
for the server (see ),  for the admin panel (see ).
🤓 custom fields pluginsplugins can also be used to add  to strapi.
custom fields
custom fields extend strapi’s capabilities by adding new types of fields to content-types and components. once created or installed, custom fields can be used in the content-type builder and content manager just like built-in fields.
the present documentation is intended for custom field creators: it describes which apis and functions developers must use to create a new custom field. the  describes how to install and use custom fields from strapi's admin panel.
it is recommended that you develop a dedicated  for custom fields. custom-field plugins include both a server and admin panel part. the custom field must be registered in both parts before it is usable in strapi's admin panel.
once created and used, custom fields are defined like any other attribute in the model's schema. an attribute using a custom field will have its type represented as  (i.e. ). depending on the custom field being used a few additional properties may be present in the attribute's definition (see ).
✏️ notesthough the recommended way to add a custom field is through creating a plugin, app-specific custom fields can also be registered within the global   found in  and  files. custom fields can only be shared using plugins.
registering a custom field on the server
prerequisites registering a custom field through a plugin requires creating and enabling a plugin (see ).
strapi's server needs to be aware of all the custom fields to ensure that an attribute using a custom field is valid.
the  object exposes a  method on the  instance. this method is used to register custom fields on the server during the plugin's server .
registers one or several custom field(s) on the server by passing an object (or an array of objects) with the following parameters:
✏️ notecurrently, custom fields cannot add new data types to strapi and must use existing, built-in strapi data types described in the  documentation. special data types unique to strapi, such as relation, media, component, or dynamic zone data types, cannot be used in custom fields.
in the following example, the  plugin was created using the cli generator (see ):  1234567891011the custom field could also be declared directly within the  file if you didn't have the plugin code scaffolded by the cli generator:  1234567891011
registering a custom field in the admin panel
custom fields must be registered in strapi's admin panel to be available in the content-type builder and the content manager.
the  object exposes a  method on the  instance. this method is used to register custom fields in the admin panel during the plugin's admin .
registers one or several custom field(s) in the admin panel by passing an object (or an array of objects) with the following parameters:
in the following example, the  plugin was created using the cli generator (see ):  123456789101112131415161718192021222324252627282930313233
must pass a  object with an  react component to use in the content manager's edit view.
in the following example, the  plugin was created using the cli generator (see ):  12345678910111213
💡 tipthe  react component receives several props. the  in the strapi codebase gives you an example of how they can be used.
can pass an additional  object with the following parameters:
both  and  settings accept an object or an array of objects, each object being a settings section. each settings section could include:
a  to declare the title of the section as an  and a list of  as an array of objects.
each object in the  array can contain the following parameters:
in the following example, the  plugin was created using the cli generator (see ):  1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677
💡 tipthe strapi codebase gives an example of how settings objects can be described: check the  file for the  settings and the  file for the  settings. the base form lists the settings items inline but the advanced form gets the items from an  file.
typescript development
typescript adds an additional type system layer above javascript, which means that existing javascript code is also typescript code. strapi supports typescript in new projects on v4.3.0 and above. existing javascript projects can  through a conversion procedure. typescript-enabled projects allow developing plugins with typescript as well as using typescript typings.
🤓 getting started with typescriptto start developing in typescript, use the  to create a new typescript project. for existing projects,  with the provided conversion steps. additionally, the  and  sections have typescript-specific resources for understanding and configuring an application.
start developing in typescript
starting the development environment for a typescript-enabled project requires building the admin panel prior to starting the server. in development mode, the application source code is compiled to the  directory and recompiled with each change in the content-type builder. to start the application, run the following commands in the root directory:
use typescript typings
strapi provides typings on the  class to improve the typescript developing experience. these typings come with an autocomplete feature that automatically offers suggestions while developing.
to experience typescript-based autocomplete while developing strapi applications, you could try the following:
from your code editor, open the  file. in the  method, declare the  argument as of type :  123456789 within the body of the  method, start typing  and use keyboard arrows to browse the available properties. choose  from the list. when the  method is added, a list of available lifecycle types (i.e. ,  and ) are returned by the code editor. use keyboard arrows to choose one of the lifecycles and the code will autocomplete.
generate typings for project schemas
to generate typings for your project schemas use the . the  command creates the file , at the project root, which stores the schema typings. the optional  flag returns a detailed table of the generated schemas.
to use run the following code in a terminal at the project root:
develop a plugin using typescript
new plugins can be generated following the . there are 2 important distinctions for typescript applications:
after creating the plugin, run  or  in the plugin directory  to install the dependencies for the plugin. run  or  in the plugin directory  to build the admin panel including the plugin.
✏️ noteit is not necessary to repeat the  or  command after the initial installation. the  or  command is necessary to implement any plugin development that affects the admin panel.
start strapi programmatically
to start strapi programmatically in a typescript project the strapi instance requires the compiled code location. this section describes how to set and indicate the compiled code directory.
use the  factory
strapi can be run programmatically by using the  factory. since the code of typescript projects is compiled in a specific directory, the parameter  should be passed to the factory to indicate where the compiled code should be read:
12345
use the  function
the  function should be mostly used for developing tools that need to start a strapi instance and detect whether the project includes typescript code.  automatically detects the project language. if the project code contains any typescript code,  compiles the code and returns a context with specific values for the directories that strapi requires:
add typescript support to an existing strapi project
adding typescript support to an existing project requires adding 2  files and rebuilding the admin panel. additionally, the  and  files can be optionally removed. the typescript flag  should be set to  in the root  file to incrementally add typescript files to existing javascript projects. the  flag allows  and  files to coexist with javascript files.
typescript support can be added to an existing strapi project using the following procedure:
add a  file at the project root and copy the following code, with the  flag, to the file:
1234567891011121314151617181920212223242526
add a  file in the  directory and copy the following code to the file:
12345678910111213141516
(optional) delete the  and  files from the project root. add an additional  to the  property in the  configuration file (only required for sqlite databases):
1234567891011121314
rebuild the admin panel and start the development server:
after completing the preceding procedure a  directory will be added at the project root and the project has access to the same typescript features as a new typescript-supported strapi project.
providers
certain  can be extended via the installation and configuration of additional .
providers add an extension to the core capabilities of the plugin, for example to upload media files to aws s3 instead of the local server, or using amazon ses for emails instead of sendmail.
✏️ noteonly the  and  plugins are currently designed to work with providers.
for the relevant plugins, there are both official providers maintained by strapi — discoverable via the  — and many community maintained providers available via .
installing providers
new providers can be installed using  or  using the following format .
configuring providers
newly installed providers are enabled and configured in the  file. if this file does not exist you must create it.
each provider will have different configuration settings available. review the respective entry for that provider in the  or  to learn more.
below are example configurations for the upload and email plugins.
12345678910111213141516171819✏️ notestrapi has a default  that has a very strict  that limits loading images and media to  only, see the example configuration on the  or the  for more information.  12345678910111213141516171819keep in mind that: when using a different provider per environment, specify the correct configuration in  (see ). only one email provider will be active at a time. if the email provider setting isn't picked up by strapi, verify the  file is in the correct folder. when testing the new email provider with those two email templates created during strapi setup, the shipper email on the template defaults to  and needs to be updated according to your email provider, otherwise it will fail the test (see ).
configuration per environment
when configuring your provider you might want to change the configuration based on the  environment variable or use environment specific credentials.
you can set a specific configuration in the  configuration file and it will be used to overwrite the default configuration.
creating providers
to implement your own custom provider you must .
the interface that must be exported depends on the plugin you are developing the provider for. below are templates for the upload and email plugins:
12345678910111213141516171819  1234567in the send function you will have access to:  that contains configurations written in   that contains configurations written in   that contains options you send when you call the send function from the email plugin service
you can review the  for example implementations.
after creating your new provider you can  to share with the community or  for your project only.
local providers
if you want to create your own provider without publishing it on npm you can follow these steps:
create a  folder in your application. create your provider (e.g. ) then update your  to link your  dependency to the  of your new provider.
12345678
update your  file to . finally, run  or  to install your new custom provider.
graphql api
prerequisites to use the graphql api, install the  plugin.
the graphql api allows performing queries and mutations to interact with the  through strapi's . results can be ,  and .
unified response format
responses are unified with the graphql api in that:
queries and mutations that return information for a single entry mainly use a  type queries and mutations that return i️nformation for multiple entries mainly use a  type, which includes  information (with ) in addition to the data itself
responses can also include an  (see ).
12345678910111213141516171819
queries
queries in graphql are used to fetch data without modifying it.
we assume that the  feature is enabled. for each model, the graphql plugin auto-generates queries and mutations that mimics basic crud operations (findmany, findone, create, update, delete).
fetch a single entry
single entries can be found by their .
example query: find the entry with id 1  12345678910111213141516171819
fetch multiple entries
example query: find all documents and populate 'categories' relation with the `name` attribute  123456789101112131415161718192021222324252627
fetch dynamic zone data
dynamic zones are union types in graphql so you need to use fragments to query the fields.
query  1234567891011121314
mutations
mutations in graphql are used to modify data (e.g. create, update, delete data).
create a new entry
mutation  12345678910
the implementation of the mutations also supports relational attributes. for example, you can create a new  and attach many  to it by writing your query like this:
mutation  123456789101112131415161718192021222324252627
update an existing entry
you can also update relational attributes by passing an id or an array of ids (depending on the relationship).
mutation  12345678910111213141516171819202122
delete an entry
filters
queries can accept a  parameter with the following syntax:
logical operators (, , ) can also be used and accept arrays of objects.
the following operators are available:
example query with filters  1234567
sorting
to sort based on a single value:  to sort based on multiple values:
the sorting order can be defined with  (ascending order, default, can be omitted) or  (for descending order).
example request: sorting on title by ascending order  1234567
example request: sorting on title by descending order  1234567
example request: sorting on title by ascending order, then on price by descending order  1234567
pagination
queries can accept a  parameter. results can be paginated either by page or by offset.
✏️ notepagination methods can not be mixed. always use either  with  or  with .
pagination by page
example query: pagination by page  123456789101112131415
pagination by offset
example query: pagination by offset  12345678910111213
💡 tipthe default and maximum values for  can be  file with the  and  keys.
command line interface (cli)
strapi comes with a full featured command line interface (cli) which lets you scaffold and manage your project in seconds.
✏️ noteit is recommended to install strapi locally only, which requires prefixing all of the following  commands with the package manager used for the project setup (e.g  or ) or a dedicated node package executor (e.g. ).
strapi new
create a new project.
strapi new <name>
generates a new project called <name> and installs the default plugins through the npm registry. strapi new <name> --debug
will display the full error message if one is fired during the database connection. strapi new <name> --quickstart
use the quickstart system to create your app. strapi new <name> --quickstart --no-run
use the quickstart system to create your app, and do not start the application after creation. strapi new <name> --dbclient=<dbclient> --dbhost=<dbhost> --dbport=<dbport> --dbname=<dbname> --dbusername=<dbusername> --dbpassword=<dbpassword> --dbssl=<dbssl> --dbauth=<dbauth> --dbforce generates a new project called <name> and skip the interactive database configuration and initialize with these options. <dbclient> can be , . --dbforce allows you to overwrite content if the provided database is not empty. only available for , , and is optional.
strapi develop
alias:
start a strapi application with autoreload enabled.
strapi modifies/creates files at runtime and needs to restart when new files are created. to achieve this,  adds a file watcher and restarts the application when necessary.
starts your application with the autoreload enabled strapi develop --no-build
starts your application with the autoreload enabled and skip the administration panel build process strapi develop --watch-admin
starts your application with the autoreload enabled and the front-end development server. it allows you to customize the administration panel. strapi develop --watch-admin --browser 'google chrome'
starts your application with the autoreload enabled and the front-end development server. it allows you to customize the administration panel. provide a browser name to use instead of the default one,  means stop opening the browser.
️❗️ warningyou should never use this command to run a strapi application in production.
strapi start
start a strapi application with autoreload disabled.
this command is to run a strapi application without restarts and file writes, primarily for use in production.
certain features such as the content-type builder are disabled in the  mode because they require application restarts. the  command can be prefaced with  to customize the application start.
strapi build
builds your admin panel.
builds the administration panel and delete the previous build and .cache folders strapi build --no-optimization
builds the administration panel without minimizing the assets. the build duration is faster.
strapi watch-admin
starts the admin server. strapi should already be running with .
strapi configuration:dump
dumps configurations to a file or stdout to help you migrate to production.
the dump format will be a json array.
all these examples are equivalent.
✋ cautionwhen configuring your application you often enter credentials for third party services (e.g authentication providers). be aware that those credentials will also be dumped into the output of this command.
in case of doubt, you should avoid committing the dump file into a versioning system. here are some methods you can explore: copy the file directly to the environment you want and run the restore command there. put the file in a secure location and download it at deploy time with the right credentials. encrypt the file before committing and decrypt it when running the restore command.
strapi configuration:restore
restores a configuration dump into your application.
the input format must be a json array.
strategies
when running the restore command, you can choose from three different strategies:
replace: will create missing keys and replace existing ones. merge: will create missing keys and merge existing keys with their new value. keep: will create missing keys and keep existing keys as is.
strapi admin:create-user
alias
creates an administrator.
administrator's first name, last name, email, and password can be:
passed as options or set interactively if you call the command without passing any option.
strapi admin:reset-user-password
reset an admin user's password.
you can pass the email and new password as options or set them interactively if you call the command without passing the options.
strapi generate
run a fully interactive cli to generate apis, , , , ,  and .
strapi templates:generate
create a template from the current strapi project.
strapi templates:generate <path>
generates a strapi template at  example:  will copy the required files and folders to a  directory inside
strapi ts:generate-types
generate  typings for the project schemas.
strapi ts:generate-types --verbose
generate typings with the verbose mode enabled, displaying a detailed table of the generated schemas. strapi ts:generate-types --silent or strapi ts:generate-types -s
generate typings with the silent mode enabled, completely removing all the logs in the terminal. strapi ts:generate-types --out-dir <path> or strapi ts:generate-types -o <path>
generate typings specifying the output directory in which the file will be created. strapi ts:generate-types --file <filename> or strapi ts:generate-types -f <filename>
generate typings specifying the name of the file to contain the types declarations.
strapi routes:list
display a list of all the available .
strapi policies:list
display a list of all the registered .
strapi middlewares:list
strapi content-types:list
display a list of all the existing .
strapi hooks:list
display a list of all the available hooks.
strapi controllers:list
strapi services:list
strapi install
install a plugin in the project.
strapi install <name>
installs a plugin called <name>. example:  will install the plugin
✋ cautionsome plugins have admin panel integrations, your admin panel might have to be rebuilt. this can take some time.
strapi uninstall
uninstall a plugin from the project.
strapi uninstall <name>
uninstalls a plugin called <name>. example:  will remove the plugin  strapi uninstall <name> --delete-files
uninstalls a plugin called <name> and removes the files in  example:  will remove the plugin  and all the files in
strapi telemetry:disable
disable data collection for the project (see ).
strapi telemetry:enable
re-enable data collection for the project after it was disabled (see ).
strapi console
start the server and eval commands in your application in real time.
strapi version
print the current globally installed strapi version.
strapi help
list cli commands.
strapi is natively handling errors with a standard format.
there are 2 use cases for error handling:
as a developer querying content through the  or  apis, you might  in response to the requests. as a developer customizing the backend of your strapi application, you could use controllers and services to .
receiving errors
errors are included in the response object with the  key and include information such as the http status code, the name of the error, and additional information.
rest errors
errors thrown by the rest api are included in the  that has the following format:
1234567891011
graphql errors
errors thrown by the graphql api are included in the  that has the following format:
1234567891011121314151617
throwing errors
controllers and middlewares
the recommended way to throw errors when developing any custom logic with strapi is to have the  or  respond with the correct status and body.
this can be done by calling an error function on the context (i.e. ). available error functions are listed in the  but their name should be lower camel-cased to be used by strapi (e.g. ).
error functions accept 2 parameters that correspond to the  and  attributes  by a developer querying the api:
the first parameter of the function is the error  and the second one is the object that will be set as  in the response received
services and models lifecycles
once you are working at a deeper layer than the controllers or middlewares there are dedicated error classes that can be used to throw errors. these classes are extensions of  and are specifically targeted for certain use-cases.
these error classes are imported through the  package and can be called from several different layers. the following examples use the service layer but error classes are not just limited to services and model lifecycles. when throwing errors in the model lifecycle layer, it's recommended to use the  class so that proper error messages are shown in the admin panel.
✏️ notesee the  section for more information on the error classes provided by strapi.
this example shows wrapping a  and doing a custom validation on the  method:
this example shows building a  and being able to throw an error that stops the request and will return proper error messages to the admin panel. generally you should only throw an error in  lifecycles, not  lifecycles.
are a special type of middleware that are executed before a controller. they are used to check if the user is allowed to perform the action or not. if the user is not allowed to perform the action and a  is used then a generic error will be thrown. as an alternative, you can throw a custom error message using a nested class extensions from the strapi  class,  class (see  for both classes), and finally the .
the  class is available from  package and accepts 2 parameters:
the first parameter of the function is the error  (optional) the second parameter is the object that will be set as  in the response received; a best practice is to set a  key with the name of the policy that threw the error.
this example shows building a  that will throw a custom error message and stop the request.
default error classes
the default error classes are available from the  package and can be imported and used in your code. any of the default error classes can be extended to create a custom error class. the custom error class can then be used in your code to throw errors.
the  class is a generic error class for application errors and is generally recommended as the default error class. this class is specifically designed to throw proper error messages that the admin panel can read and show to the user. it accepts the following parameters:   1 the  class is a specific error class that is typically used when parsing the pagination information from , , or the . it accepts the following parameters:   1 the  class is a generic error class for throwing  status code errors. it accepts the following parameters:   1 the  class is a specific error class used when a user either doesn't provide any or the correct authentication credentials. it accepts the following parameters:   1 the  class is a specific error class used when a user doesn't have the proper role or permissions to perform a specific action, but has properly authenticated. it accepts the following parameters:   1 the  class is a specific error class used when the incoming request body or attached files exceed the limits of the server. it accepts the following parameters:   1 the  class is a specific error designed to be used with . the best practice recommendation is to ensure the name of the policy is passed in the  parameter. it accepts the following parameters:   1
unit testing
in this guide we will see how you can run basic unit tests for a strapi application using a testing framework.
💡 tipin this example we will use  testing framework with a focus on simplicity and
super-agent driven library for testing node.js http servers using a fluent api
✋ cautionplease note that this guide will not work if you are on windows using the sqlite database due to how windows locks the sqlite file.
install test tools
contains a set of guidelines or rules used for creating and designing test cases - a combination of practices and tools that are designed to help testers test more efficiently.
allows you to test all the  routes as they were instances of
is used to create an on-disk database that is created and deleted between tests.
once this is done add this to  file
add  command to  section
1234567
and add those line at the bottom of file
those will inform  not to look for test inside the folder where it shouldn't.
testing environment
test framework must have a clean empty environment to perform valid test and also not to interfere with current database.
once  is running it uses the   (switching  to )
so we need to create a special environment setting for this purpose.
create a new config for test env  and add the following value  - the reason of that is that we want to have a separate sqlite database for tests, so our test will not touch real data.
this file will be temporary, each time test is finished, we will remove that file that every time tests are run on the clean database.
the whole file will look like this:
path —
strapi instance
in order to test anything we need to have a strapi instance that runs in the testing environment,
basically we want to get instance of strapi app as object, similar like creating an instance for .
these tasks require adding some files - let's create a folder  where all the tests will be put and inside it, next to folder  where main strapi helper will be in file strapi.js.
12345678910111213141516171819202122232425262728293031323334
test strapi instance
we need a main entry file for our tests, one that will also test our helper file.
actually this is all we need for writing unit tests. just run  and see a result of your first test
💡 tipif you receive a timeout error for jest, please add the following line right before the  method in the  file:  and adjust the milliseconds value as you need.
testing basic endpoint controller.
💡 tipin the example we'll use and example   endpoint from  section.
some might say that api tests are not unit but limited integration tests, regardless of nomenclature, let's continue with testing first endpoint.
we'll test if our endpoint works properly and route  does return
let's create a separate test file where  will be used to check if endpoint works as expected.
then include this code to  at the bottom of that file
and run  which should return
💡 tipif you receive an error  check  version as  works without an issue.
testing  endpoint controller.
in this scenario we'll test authentication login endpoint with two tests
test  that should login user and return  token test  that should return users data based on  header
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465
all the tests above should return an console output like
123456789101112131415161718
database migrations
database migrations exist to run one-time queries against the database, typically to modify the tables structure or the data when upgrading the strapi application. these migrations are run automatically when the application starts and are executed before the automated schema migrations that strapi also performs on boot.
🚧  experimental feature database migrations are experimental. this feature is still a work in progress and will continue to be updated and improved. in the meantime, feel free to ask for help on the  or on the community .
understanding database migration files
migrations are run using javascript migration files stored in .
strapi automatically detects migration files and run them once at the next startup in alphabetical order. every new file is executed once. migrations are run before the database tables are synced with the content-types schemas.
️❗️ warningcurrently strapi does not support down migrations. this means that if you need to revert a migration, you will have to do it manually. it is planned to implement down migrations in the future but no timeline is currently available.
migration files should export the function , which is used when upgrading (e.g. adding a new table ).
the  function runs in a database transaction which means if a query fails during the migration, the whole migration is cancelled, and no changes are applied to the database. if another transaction is created within the migration function, it will act as a nested transaction.
✏️ notethere is no cli to manually execute the database migrations.
creating a migration file
to create a migration file:
in the  folder, create a new file named after the date and the name of the migration (e.g. ). make sure that the file name follows this naming pattern, because the alphabetical order of the files defines the order in which the migrations have to run. copy and paste the following template in the previously created file:
fill in the template by adding actual migration code inside the  function.
receives a , already in a transaction state, that can be used to run the database queries.
graphql
by default strapi create  for each of your content-types. with the graphql plugin, you will be able to add a graphql endpoint to fetch and mutate your content.
🤓 looking for the graphql api documentation?the  describes queries, mutations and parameters you can use to interact with your api using strapi's graphql plugin.
to get started with graphql in your application, please install the plugin first. to do that, open your terminal and run the following command:
then, start your app and open your browser at . you should now be able to access the graphql playground that will help you to write your graphql queries and mutations.
✏️ notethe graphql playground is enabled by default for both the development and staging environments, but disabled in production environments. set the  configuration option to  to also enable the graphql playground in production environments (see ).
plugins configuration are defined in the  file. this configuration file can include a  object to define specific configurations for the graphql plugin (see ).
options can be set with the  . apollo server options can be used for instance to enable the , which is supported by the graphql playground to track the response time of each part of your query. from  version 3.9 default cache option is . you can change it in the  configuration. for more information visit .
✋ cautionthe maximum number of items returned by the response is limited to 100 by default. this value can be changed using the  configuration option, but should only be changed after careful consideration: a large query can cause a ddos (distributed denial of service) and may cause abnormal load on your strapi server, as well as your database server.
shadow crud
to simplify and automate the build of the graphql schema, we introduced the shadow crud feature. it automatically generates the type definitions, queries, mutations and resolvers based on your models.
if you've generated an api called  using  or the administration panel, your model looks like this:
123456789101112131415161718192021222324252627
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566
customization
strapi provides a programmatic api to customize graphql, which allows:
disabling some operations for the   to return information about allowed operations registering and using an  object to  (e.g. extend types or define custom resolvers, policies and middlewares)
disabling operations in the shadow crud
the   provided with the graphql plugin exposes functions that can be used to disable operations on content-types:
actions can also be disabled at the field level, with the following functions:
using getters
the following getters can be used to retrieve information about operations allowed on content-types:
the following getters can be used to retrieve information about operations allowed on fields:
extending the schema
the schema generated by the content api can be extended by registering an extension.
this extension, defined either as an object or a function returning an object, will be used by the  function exposed by the   provided with the graphql plugin.
the object describing the extension accepts the following parameters:
💡 tipthe  and  parameters are based on . to use them, register the extension as a function that takes  as a parameter:
custom configuration for resolvers
a resolver is a graphql query or mutation handler (i.e. a function, or a collection of functions, that generate(s) a response for a graphql query or mutation). each field has a default resolver.
when , the  key can be used to define a custom configuration for a resolver, which can include:
with the  key  key and  key
authorization configuration
by default, the authorization of a graphql request is handled by the registered authorization strategy that can be either  or through the . the users & permissions plugin offers a more granular control.
with the users & permissions plugin, a graphql request is allowed if the appropriate permissions are given. for instance, if a 'category' content-type exists and is queried through graphql with the  handler, the request is allowed if the appropriate  permission for the 'categories' content-type is given. to query a single category, which is done with the  handler, the request is allowed if the the  permission is given. please refer to the user guide on how to .
to change how the authorization is configured, use the resolver configuration defined at . the authorization can be configured:
either with  to fully bypass the authorization system and allow all requests, or with a  attribute that accepts an array of strings to define the permissions required to authorize the request.
can be applied to a graphql resolver through the  key.
the  key is an array accepting a list of policies, each item in this list being either a reference to an already registered policy or an implementation that is passed directly (see ).
policies directly implemented in  are functions that take a  object and the  instance as arguments.
the  object gives access to:
the , ,  and  arguments of the graphql resolver, koa's  with  and  with .
middlewares
the  key is an array accepting a list of middlewares, each item in this list being either a reference to an already registered policy or an implementation that is passed directly (see ).
middlewares directly implemented in  can take the graphql resolver's  as arguments.
💡 tipmiddlewares with graphql can even act on nested resolvers, which offer a more granular control than with rest.
usage with the users & permissions plugin
the  is an optional plugin that allows protecting the api with a full authentication process.
registration
usually you need to sign up or register before being recognized as a user then perform authorized requests.
mutation  123456789
you should see a new user is created in the  collection type in your strapi admin panel.
to perform authorized requests, you must first get a jwt:
mutation  12345
then on each request, send along an  header in the form of . this can be set in the http headers section of your graphql playground.
🌍 internationalization (i18n)
the internationalization (i18n) plugin allows strapi users to create, manage and distribute localized content in different languages, called "locales". for more information about the concept of internationalization, please refer to the
the i18n plugin:
allows admin panel users to create several localized versions of their content (see ) allows developers to build localized projects by fetching and consuming the right content depending on the country/language of the audience.
✏️ notethe i18n plugin does not automatically translate the users' content nor adapt the admin interface to languages specificities (e.g. displaying the admin panel in right to left format).
prerequisites the internationalization plugin is installed by default on all strapi applications running on version 3.6.0 or higher. for lower versions, a migration is needed (see ), as well as a manual installation of the plugin.
the plugin can be installed:
, or using the terminal, by running one of the following commands:
configuration of the default locale
a   can be configured to set the default locale for your environment. the value used for this variable should be an iso country code (see ).
this is useful when a strapi application is deployed in production, with the i18n plugin installed and enabled for your content types the first time. on a fresh i18n plugin installation,  is set as default locale. if the database does not contain any locale, and no  is set for the environment, all entities of the content types, which have localization enabled, will be automatically migrated to the  locale.
usage with the rest api
the i18n plugin adds new features to the :
a new  parameter to fetch content only for a specified locale the ability to create a localized entry, either  or
getting localized entries with the  parameter
the   can be used to fetch entries only for a specified locale. it takes a locale code as value (see ).
💡 tipto fetch content for a locale, make sure it has been already .
the format for a get request is the following:
request
use  as a value for the locale code, as in , to fetch entries for all locales that have been configured in the admin panel.
if the  parameter isn't defined, it will be set to the default locale.  is the default locale when the i18n plugin is installed, so by default a get request to  will return the same response as a request to .
💡 tipanother locale can be  in the admin panel.
when the i18n plugin is installed, the response to requests can include fields that are specific to internationalization:
the  (string) field is always included, it's the locale code for the content entry. the  (object) can be included if specifically requested by appending  to the url (see . it includes a  array with a list of objects, each of them including the  and  of the localization.
example request  example response  1234567891011121314151617181920212223242526272829303132
in the example response above:
restaurant with  is a french () localization of the existing restaurant with  (created for the default  locale). restaurant with  was created from scratch using the api, passing the  in the request body (see ).
creating a new localized entry
to create a localized entry from scratch, send a post request to the content api.
if no locale has been passed in the request body, the entry is created using the default locale for the application:
example request   123456 example response  1234567891011121314
to create a localized entry for a locale different from the default one, add the  attribute to the body of the post request:
example request   1234567 example response  1234567891011121314
creating a localization for an existing entry
to create another localization for an existing localized entry, send a post request to the appropriate url depending on the type of content:
when creating a localization for existing localized entries, the body of the post request can only accept localized fields.
💡 tipthe content-type should have the  enabled, otherwise the post request will return a  status.
creating a localization for a collection type
when sending a post request to a collection type, strapi will:
use the  as a base entry for the non-localized fields and copy them in the new entry then create a new entry for the given locale and link it with the base entry.
example request   12345this request: creates a new entry in  links the created entry with  (they will share the same  object) copies every non-localized fields from  into the new entry and keeps the localized fields from the request's body example response  1234567891011121314151617181920
creating a localization for a single type
example request   1234 example response  12345678910111213
updating an entry
to update an existing localized entry, send a  request to the appropriate url depending on the type of content:
when updating a localization for existing localized entries, the body of the  request can only accept localized fields.
💡 tipit is not possible to change the locale of an existing localized entry. when updating a localized entry, if you set a  attribute in the request body it will be ignored.
usage with the graphql plugin
the  and  fields are added to the  schema. the graphql  can be used to:
on collection types and single types  for collection types and single types  and  a localization with a mutation on single types
getting localized entries with graphql
queries can use the  argument to fetch entries only for a specified locale.
💡 tipto fetch entries for all locales, use  in the query.
fetching a collection type
example query  1234567891011121314151617181920 example response  12345678910111213141516171819202122232425262728293031
fetching a single type
example query  12345678910 example response  123456789101112
creating new localized entries with graphql
the  field can be passed in the  object of the mutation to create a localized entry for this specific locale (for more information, see ).
creating a new localization for a collection type
example mutation  123456789101112131415161718192021222324 example response  123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
creating a new localization for a single type
example mutation  1234567891011 example response  12345678910111213
updating a localized single type with graphql
a  argument can be passed in the mutation to update content for a given locale (for more information, see ).
currently, it is not possible to change the locale of an existing localized entry. if you set a  field in the  object of the mutation, it will be ignored.
example mutation  12345678910 example response  123456789101112
deleting a localization for a single type with graphql
pass the  argument in the mutation to delete a specific localization for a single type:
example mutation  12345678910 example response  123456789101112the response returns the entry that has just been deleted.
users & permissions
this plugin provides a full authentication process based on  to protect your api. it also provides an access-control list (acl) strategy that enables you to manage permissions between groups of users.
to access the plugin admin panel, click on the settings link in the left menu of your strapi application dashboard and under the users & permissions plugin section you will find sections for managing roles, providers, email templates, and advanced settings.
concept
when this plugin is installed, it adds an access layer on your application.
the plugin uses  to authenticate users. your jwt contains your user id, which is matched to the group your user is in and used to determine whether to allow access to the route.
each time an api request is sent the server checks if an  header is present and verifies if the user making the request has access to the resource.
manage role permissions
public role
this is the default role used when the server receives a request without an  header. any permissions (i.e. accessible endpoints) granted to this role will be accessible by anyone.
it is common practice to select  /  endpoints when you want your front-end application to access all the content without requiring user authentication and authorization.
authenticated role
this is the default role that is given to every new user at creation if no role is provided. in this role you define routes that a user can access.
permissions management
by clicking on the role name, you can see all functions available in your application (with these functions related to the specific route displayed).
if you check a function name, it makes this route accessible by the current role you are editing. on the right sidebar you can see the url related to this function.
update the default role
when you create a user without a role, or if you use the  route, the  role is given to the user.
to change the default role, go to the  tab and update the  option.
login
submit the user's identifier and password credentials for authentication. on successful authentication the response data will have the user's information along with an authentication token.
the  param can be an email or username.
123456789101112131415161718 if you use postman, set the body to raw and select json as your data format:  1234if the request is successful you will receive the user's jwt in the  key:  12345678
token usage
the  may then be used for making permission-restricted api requests. to make an api request as a user place the jwt into an  header of the  request.
any request without a token will assume the  role permissions by default. modify the permissions of each user's role in the admin dashboard.
authentication failures return a  error.
the  variable is the  received when logging in or registering.
jwt configuration
you can configure the jwt generation by using the .
strapi uses  to generate the jwt.
available options:
: random string used to create new jwts, typically set using the  . : expressed in seconds or a string describing a time span.
eg: 60, "45m", "10h", "2 days", "7d", "2y". a numeric value is interpreted as a seconds count. if you use a string be sure you provide the time units (minutes, hours, days, years, etc), otherwise milliseconds unit is used by default ("120" is equal to "120ms").
️❗️ warningsetting jwt expiry for more than 30 days is not recommended due to security concerns.
creates a new user in the database with a default role as 'registered'.
1234567891011121314151617181920
thanks to  and , you can use oauth and oauth2 providers to enable authentication in your application.
for better understanding, review the following description of the login flow. we use  as the provider but it works the same for other providers.
understanding the login flow
let's say that:
strapi's backend is located at: , and your app frontend is located at:
the user goes on your frontend app () and clicks on your button . the frontend redirects the tab to the backend url: . the backend redirects the tab to the github login page where the user logs in. once done, github redirects the tab to the backend url:. the backend uses the given  to get an  from github that can be used for a period of time to make authorized requests to github to get the user info. then, the backend redirects the tab to the url of your choice with the param  (example: ). the frontend () calls the backend with  that returns the strapi user profile with its .  (under the hood, the backend asks github for the user's profile and a match is done on github user's email address and strapi user's email address). the frontend now possesses the user's , which means the user is connected and the frontend can make authenticated requests to the backend!
an example of a frontend app that handles this flow can be found here: .
setting up the server url
before setting up a provider you must specify the absolute url of your backend in .
example -
💡 tiplater you will give this url to your provider.  for development, some providers accept the use of localhost urls but many don't. in this case we recommend to use  () that will make a proxy tunnel from a url it created to your localhost url (ex: ).
setting up the provider - examples
instead of a generic explanation we decided to show an example for each provider.
in the following examples, the frontend app will be the .
it (the frontend app) will be running on .
strapi (the backend) will be running on .
using ngrok github doesn't accept  urls.
use  to serve the backend app.  1don't forget to update the server url in the backend config file  and the server url in your frontend app (environment variable  if you use ) with the generated ngrok url.  github configuration visit the oauth apps list page  click on new oauth app button fill the information (replace with your own ngrok url):
application name: strapi github auth homepage url:  application description: strapi provider auth description authorization callback url:   strapi configuration visit the user permissions provider settings page   click on the github provider fill the information (replace with your own client id and secret):
enable:  client id: 53de5258f8472c140917 client secret: fb9d0fe1d345d9ac7f83d7a1e646b37c554dae8b the redirect url to your front-end app:   using ngrok facebook doesn't accept  urls.
use  to serve the backend app.  1don't forget to update the server url in the backend config file  and the server url in your frontend app (environment variable  if you use ) with the generated ngrok url.  facebook configuration visit the developer apps list page   click on add a new app button fill the display name in the modal and create the app setup a facebook login product click on the products > facebook login > settings link in the left menu fill the information and save (replace with your own ngrok url):
valid oauth redirect uris:  then, click on settings in the left menu then on basic link you should see your application id and secret, save them for later  strapi configuration visit the user permissions provider settings page   click on the facebook provider fill the information (replace with your own client id and secret):
enable:  client id: 2408954435875229 client secret: 4fe04b740b69f31ea410b9391ff3b5b0 the redirect url to your front-end app:   using ngrok google accepts the  urls.
the use of  is not needed.  google configuration visit the google developer console   click on the select a project dropdown in the top menu then click new project button fill the project name input and create wait a few seconds while the application is created. on the project dropdown, select your new project click on go to apis overview under the apis card then click on the credentials link in the left menu click on oauth consent screen button choose external and click on create fill the application name and save then click on create credentials button choose oauth client id option fill the information:
name:  authorized redirect uris:  click on oauth 2.0 client ids name of the client you just created you should see your application id and secret, save them for later  strapi configuration visit the user permissions provider settings page   click on the google provider fill the information (replace with your own client id and secret):
enable:  client id: 226437944084-o2mojv5i4lfnng9q8kq3jkf5v03avemk.apps.googleusercontent.com client secret: aitbmoiujqflsby6uqrfgsni the redirect url to your front-end app:   using ngrok aws cognito accepts the  urls.
the use of  is not needed.  aws cognito configuration visit the aws management console   if needed, select your region in the top right corner next to the support dropdown select the services dropdown in the top left corner click on cognito in the  section then click on the manage user pools button if applicable either create or use an existing user pool. you will find hereafter a tutorial to create a user pool   go to the app clients section in your cognito user pool and create a new client with the name  and set all the parameters and then click on create app client you should now have an app client id and by clicking on the button show details you will be able to see the app client secret. do copy those two values app client id and app client secret somewhere for later use when configuring the aws cognito provider in strapi. go to the app integration section and click on app client settings look for your app client named  and enable cognito user pool by checking it in the enabled identity providers section of your newly created app client fill in your callback url and sign out url with the value  or the one provided by your aws cognito provider in strapi in the oauth 2.0 section select  and  for the allowed oauth flows and select ,  and  for the allowed oauth scopes you can now click on save changes and if you have already configured your domain name then you should be able to see a link to the launch hosted ui. you can click on it in order to display the aws cognito login page. in case you haven't yet configured your domain name, use the link choose domain name at the bottom right of the page in order to configure your domain name. on that page you will have an  section where a  is already setup. type a domain prefix to use for the sign-up and sign-in pages that are hosted by amazon cognito, this domain prefix together with the  will be the host uri (subdomain) value for your strapi configuration later on.  strapi configuration visit the user permissions provider settings page   click on the cognito provider fill the information (replace with your own client id and secret):
enable:  client id: fill in the app client id () client secret: fill in the app client secret () host uri (subdomain): fill in the url value that you copied earlier () the redirect url to your front-end app: if you are using strapi react-login  use  but if you do not yet have a front-end app to test your cognito configuration you can then use the following url   using ngrok twitter doesn't accept  urls.
use  to serve the backend app.  1don't forget to update the server url in the backend config file  and the server url in your frontend app (environment variable  if you use ) with the generated ngrok url.  twitter configuration visit the apps list page   click on create an app button fill the information (replace with your own ngrok url):
app name: strapi twitter auth application description: this is a demo app for strapi auth tell us how this app will be used: - here write a message enough long - at the end of the process you should see your application id and secret, save them for later go to you app setting and click on edit authentication settings enable 3rd party authentication and request email address from users fill the information (replace with your own ngrok url):
callback urls:  website url:  privacy policy:  terms of service:   strapi configuration visit the user permissions provider settings page   click on the twitter provider fill the information (replace with your own client id and secret):
enable:  client id: yfn4ycggmkxis1njtiyxun5ih client secret: nag1en8s4vwqurbvlw5oafyklzqrxfeywhph6czlpga2v3vr3t the redirect url to your front-end app:   using ngrok discord accepts the  urls.
the use of  is not needed.  discord configuration visit the apps list page on the developer portal   click on new application button fill the name and create click on oauth2 in the left menu and click on add redirect button fill the redirect input with  url and save click on general information in the left menu you should see your application id and secret, save them for later  strapi configuration visit the user permissions provider settings page   click on the discord provider fill the information (replace with your own client id and secret):
enable:  client id: 665118465148846081 client secret: ijbr7mkyqyut-j2hgvvsdch_5dw5u77j the redirect url to your front-end app:   using ngrok twitch accepts the  urls.
the use of  is not needed.  twitch configuration visit the apps list page on the developer console   click on register your application button fill the information:
name: strapi auth oauth redirect urls:  category: choose a category click on manage button of your new app generate a new client secret with the new secret button you should see your application id and secret, save them for later  strapi configuration visit the user permissions provider settings page   click on the twitch provider fill the information (replace with your own client id and secret):
enable:  client id: amuy279g8wt68qlht3u4gek4oykh5j client secret: dapssh10uo97gg2l25qufr8wen3yr6 the redirect url to your front-end app:   using ngrok facebook doesn't accept  urls.
use  to serve the backend app.  1don't forget to update the server url in the backend config file  and the server url in your frontend app (environment variable  if you use ) with the generated ngrok url.  instagram configuration visit the developer apps list page   click on add a new app button fill the display name in the modal and create the app setup an instagram product click on the products > instagram > basic display link in the left menu then click on the create new application button (and valid the modal) fill the information (replace with your own ngrok url):
valid oauth redirect uris:  deauthorize:  data deletion requests:  on the app review for instagram basic display click on add to submission for instagram_graph_user_profile. you should see your application id and secret, save them for later  strapi configuration visit the user permissions provider settings page   click on the instagram provider fill the information (replace with your own client id and secret):
enable:  client id: 563883201184965 client secret: f5ba10a7dd78c2410ab6b8a35ab28226 the redirect url to your front-end app:   using ngrok discord accepts the  urls.
the use of  is not needed.  vk configuration visit the apps list page   click on create app button fill the information:
title: strapi auth platform: choose website option website address:  base domain:  click on the settings link in the left menu click on the open api link to enable this option fill the information:
authorized redirect url:   strapi configuration visit the user permissions provider settings page   click on the vk provider fill the information:
enable:  client id: 7276416 client secret: cfbusghlxguxqncyw1n3 the redirect url to your front-end app:   using ngrok linkedin accepts the  urls.
the use of  is not needed.  linkedin configuration visit the apps list page   click on create app button fill the information:
app name: strapi auth linkedin page: enter a linkedin page name to associate with the app or click create a new linkedin page to create a new one app logo: upload a square image that is at least 100x100 pixels. click on the create app to create the app on the app page click on auth tab fill the information:
authorized redirect url:  on the app page click on products tab. select  from the product list to enable it.  strapi configuration visit the user permissions provider settings page   click on the linkedin provider fill the information:
enable:  client id: 84witsxk641rlv client secret: hdxo7a7mkru5a6wn the redirect url to your front-end app:   using ngrok a remote cas server can be configured to accept  urls or you can run your own cas server locally that accepts them. the use of  is not needed.  cas configuration  is an sso server that supports many different methods of verifying a users identity,
retrieving attributes out the user and communicating that information to applications via protocols such as saml, oidc, and the cas protocol. strapi can use a cas server for authentication if cas is deployed with support for oidc.  could already be used by your company or organization or you can setup a local cas server by cloning the  project or using the newer  to create an overlay project. the cas server must be configured so it can act as an  cas version 6.3.x and higher is known to work with strapi but older versions that support oidc may work. define a cas oidc service for strapi and store it in whichever cas service registry is being used. the cas service definition might look something like this for a local strapi deployment:  123456789101112131415161718 strapi configuration visit the user permissions provider settings page   click on the cas provider fill the information:
enable:  client id: thestrapiclientid client secret: thestrapiclientsecret the redirect url to your front-end app:  the provider subdomain such that the following urls are correct for the cas deployment you are targeting:  123for example, if running cas locally with a login url of: , the value for the provider subdomain would be .  using ngrok reddit accepts the  urls.
the use of  is not needed.  reddit configuration visit the reddit authorized applications preferences page   click on the create another app... button near the bottom select web app for the type fill the name and redirect uri input in click the create app button note that the client id is located under the app type (web app)  strapi configuration visit the user permissions provider settings page   click on the reddit provider fill the information (replace with your own client id and secret):
enable:  client id: hmxsboit0scjsq client secret: gwr9hcjk_pmyvyngedls4wlb8g7xqg the redirect url to your front-end app:   using ngrok auth0 accepts the  urls.
the use of  is not needed.  auth0 configuration visit your auth0 tenant dashboard in api section, create a new api in application, create a  application and select the api that you have just created in settings of this app set these values:
allowed callback urls:  allowed logout urls:  allowed web origins:  at the bottom of settings, show "advanced settings" and go to the "grant types". ensure that these grants are checked/enabled:
implicit authorization code refresh token client credentials  strapi configuration visit the user permissions provider settings page   click on the auth0 provider fill the information:
enable:  client id:  client secret:  subdomain: , example it is the part in bold in the following url: https://my-tenant.eu.auth0.com/ the redirect url to your front-end app:
your configuration is done.
launch the backend and the , go to  and try to connect to the provider your configured. it should work 🎉
what you have to do in your frontend
once you have configured strapi and the provider, in your frontend app you have to :
create a button that links to  (ex: ). create a frontend route like  that have to handle the  param and that have to request  with the  param.
the json request response will be .
now you can make authenticated requests 🎉 more info here: .
✋ troubleshootingerror 429: it's most likely because your login flow fell into a loop. to make new requests to the backend, you need to wait a few minutes or restart the backend. grant: missing session or misconfigured provider: it may be due to many things.
the redirect url can't be built: make sure you have set the backend url in :  a session/cookie/cache problem: you can try again in a private tab. the incorrect use of a domain with ngrok: check your urls and make sure that you use the ngrok url instead of . don't forget to check the backend url set in the example app at . you can't access your admin panel: it's most likely because you built it with the backend url set with a ngrok url and you stopped/restarted ngrok. you need to replace the backend url with the new ngrok url and run  or  again.
reset password
can only be used for users registered using the email provider.
the assumed general flow: the user goes to your forgotten password page. the user enters their email address. your forgotten password page sends a request to the backend to send an email with the reset password link to the user. the user receives the email and clicks on the special link. the link redirects the user to your reset password page. the user enters their new password. the reset password page sends a request to the backend with the new password. if the request contains the code contained in the link at step 3, the password is updated. the user can log in with the new password. in the following section we will detail steps 3. and 7..  forgotten password: ask for the reset password link this action sends an email to a user with the link to your reset password page. the link will be enriched with the url param  that is needed for the  at step 7. first, you must specify the following: in the admin panel: settings > users & permissions plugin > advanced settings > reset password page, the  to your reset password page. in the admin panel: settings > users & permissions plugin > email template page, the shipper email. then, your forgotten password page has to make the following request to your backend:  12345678910111213 reset password: send the new password this action will update the user password.
this also works with the , with the  mutation. your reset password page has to make the following request to your backend:  123456789101112131415congrats, you're done! you can also update an authenticated user password through the  api endpoint:  12345678910111213141516
email validation
✏️ notein production, make sure the  config property is set. otherwise the validation link will redirect to . more info on the config .
after registering, if you have set enable email confirmation to on, the user will receive a confirmation link by email. the user has to click on it to validate their registration.
example of the confirmation link:
if needed you can re-send the confirmation email by making the following request:
12345678910111213
user object in strapi context
the  object is available to successfully authenticated requests.
the authenticated  object is a property of .
templating emails
by default this plugin comes with two templates: reset password and email address confirmation. the templates use lodash's  method to populate the variables.
you can update these templates under plugins > roles & permissions > email templates tab in the admin panel.
(object)
corresponds to the token generated to be able to reset the password.  is the link where the user will be redirected after clicking on it in the email.
email address confirmation
corresponds to the code generated to be able confirm the user email.  is the strapi backend url that confirms the code (by default ).
security configuration
jwts can be verified and trusted because the information is digitally signed. to sign a token a secret is required. by default strapi generates and stores it in .
this is useful during development but for security reasons it is recommended to set a custom token via an environment variable  when deploying to production.
by default you can set a  environment variable and it will be used as secret. if you want to use another variable you can update the configuration file.
💡 tipyou can learn more about configuration .
the email plugin enables applications to send emails from a server or an external provider. the email plugin uses the strapi global api, meaning it can be called from anywhere inside a strapi application. two of the most common use cases are in the strapi back end and in the admin panel. the following documentation describes how to use the email plugin in a controller or service for back-end use cases and using a lifecycle hook for admin panel use cases.
prerequisites the email plugin requires a provider and a provider configuration in the  file. see the  documentation for detailed installation and configuration instructions.
✏️ note is the default email provider in the strapi email plugin. it provides functionality for the local development environment but is not production-ready in the default configuration. for production stage applications you need to further configure  or change providers. the  documentation has instructions for changing providers, configuring providers, and creating a new email provider.
sending emails with a controller or service
the email plugin has an   that contains 2 functions to send emails:
directly contains the email contents,  consumes data from the content manager to populate emails, streamlining programmatic emails.
using the  function
to trigger an email in response to a user action add the  function to a  or . the send function has the following properties:
the  function is used to compose emails from a template. the function compiles the email from the available properties and then sends the email. to use the  function, define the  object and add the function to a controller or service. the function calls the  object, and can optionally call the  and  objects:
12345678910111213141516171819202122
sending emails with a lifecycle hook
to trigger an email based on administrator actions in the admin panel use  and the . for example, to send an email each time a new content entry is added in the content manager use the  lifecycle hook:
123456789101112131415161718192021222324
upload
the upload plugin is the backend powering the media library plugin available by default in the strapi admin panel. using either the media library from the admin panel or the upload api directly, you can upload any kind of file for use in your strapi application.
by default strapi provides a  that uploads files to a local directory. additional providers are available should you want to upload your files to another location.
the providers maintained by strapi include:
this section details configuration options for the default upload provider. if using another provider (e.g. aws s3 or cloudinary), see the available configuration parameters in that provider's documentation.
local server
by default strapi accepts  configurations for locally uploaded files. these will be passed as the options for .
you can provide them by creating or editing the  file. the following example sets the  header.
max file size
currently the strapi middleware in charge of parsing requests needs to be configured to support file sizes larger than the default of 200mb in addition to provider options passed to the upload plugin for sizelimit.
✋ cautionyou may also need to adjust any upstream proxies, load balancers, or firewalls to allow for larger file sizes.
(e.g.  has a config setting called  that will need to be adjusted since it's default is only 1mb.)
the library we use is , and it uses the  library to process files.
you can pass configuration to the middleware directly by setting it in the  configuration in :
in addition to the middleware configuration, you can pass the , which is an integer in bytes, in the  of the  in :
responsive images
when the  setting is enabled in the settings panel the plugin will generate the following responsive image sizes:
these sizes can be overridden in :
✋ cautionbreakpoint changes will only apply to new images, existing images will not be resized or have new sizes generated.
endpoints
✏️ note are an admin panel feature and are not part of the rest or the graphql api. files uploaded through the rest or graphql api are located in the automatically created "api uploads" folder.
upload files
upload one or more files to your application.
the following parameters are accepted:
: the file(s) to upload. the value(s) can be a buffer or stream.
✋ cautionyou have to send formdata in your request body.
upload entry files
upload one or more files that will be linked to a specific entry.
for example, given the  model attributes:
the corresponding code would be:
123456789101112131415161718192021
upload files at entry creation
you can also add files during your entry creation.
1234567891011121314151617181920212223242526272829303132333435
your entry data has to be contained in a  key and you need to  this object. the keys for files need to be prefixed with  (e.g. for a cover attribute: ).
💡 tipif you want to upload files for a component, you will have to specify the index of the item you want to add the file to:
models definition
adding a file attribute to a model (or the model of another plugin) is like adding a new association.
in the first example below, you will be able to upload and attach one file to the avatar attribute.
in our second example, you can upload and attach multiple pictures to the restaurant.
sentry
this plugin enables you to track errors in your strapi application using .
by using the sentry plugin you can:
initialize a sentry instance upon startup of a strapi application send strapi application errors as events to sentry include additional metadata in sentry events to assist in debugging expose a
begin by first  the sentry plugin, and then  the plugin to enable your strapi application to send events to the sentry instance.
install the sentry plugin by adding the dependency to your strapi application as follows:
create or edit your  file to configure the sentry plugin. the following properties are available:
an example configuration:
environment configuration
using the , you can enable or disable the sentry plugin based on the environment. for example, to only enable the plugin in your  environment:
123456789
global sentry service access
after installing and configuring the plugin, you can access a sentry service in your strapi application as follows:
this service exposes the following methods:
below are examples for each method.
12345678910111213141516171819  1234
api documentation
✋ cautionthe documentation plugin documentation may not be fully updated for strapi v4. the strapi team is currently improving that plugin and documentation, which should be completed by the end of september 2022.
now that you have created your api it's really important to document its available end-points. the documentation plugin takes out most of your pain to generate your documentation. this plugin uses  to visualize your api's documentation.
if installed, this plugin will scan all the routes available from your  folder and will try to create the appropriate documentation, infer on the parameters needed to create data, the responses you will receive.
you'll be able to visualize all your end-points directly from the swagger ui.
as usual run the following in your terminal:
when your plugin is installed, you just have to start your application and it will generate your api documentation.
administration panel
this plugin comes with an interface that is available in your administration panel and a configuration file.
restrict the access to your api's documentation
by default, your documentation will be accessible by anyone.
if you want to restrict the access to the documentation you have to enable the restricted access option.
click the  of restricted access select a password in the  input save your settings
now if you try to access your documentation, you will have to enter the password you set.
retrieve your jwt token
strapi is secured by default which means that most of your end-points require your user to be authorized. you will need to paste this token in your swagger ui to try out your end-points.
click on the retrieve your jwt token input to copy the token visit your documentation click on the authorize button on the right past your token in the  input
regenerate a documentation
if you update your api, the documentation will not be updated automatically.
you will have to click on the regenerate button of the documentation version you want to update.
it will regenerated to specified version with the current api documentation.
settings
you need to create the  file manually to customize the swagger ui settings.
here are the file that needs to be created in order to change the documentation version, the server url and so on.
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253
the , , , ,  and  fields are located in the  file. here you can specify all your environment variables, licenses, external documentation and so on...
you can add all the entries listed in the .
️❗️ warningdo not change the  field of the
✏️ notewhen you change a field in the settings.json file you need to manually restart your server.
create a new version of the documentation
to create a new version of your documentation, you will have to update the  key.
change the documentation path
to access your documentation on a custom path, you will have to update the  key.
indicate which plugins' documentation to generate
to generate documentation for specific plugins, you will need to indicate the list of all the plugins for which you wish to generate documentation. in order to do that you need to update the  key. leaving this key with an empty array  means that not any plugin documentation will be generated. if you wish to generate documentation for all plugins, you just have to remove the key from the  file.
in the previous example, you will generate documentation for the upload, email and users permissions (permissions and roles) plugins.
default response
sometimes, an operation can return multiple errors with different http status codes, but all of them have the same response structure. you can use the default response to describe these errors collectively, not individually. “default” means this response is used for all http codes that are not covered individually for this operation.
this is how it would looks like
you can set the generation of the default response with the following attribute
note: this is configurable as some api gateways does not support a default response.
this plugin follows the openapi specifications () and generates an openapi document called .
plugin's architecture
generated files
when you start your server with this plugin installed it will automatically create the following files in your apis (we will see how it works for the plugins). the plugin scans all the routes available in your model to create the  field.
overriding the suggested documentation
currently the plugin writes a json file for each api.
in order to customize the responses or to add information to a path you need to create a file in the associated  (the name of the file matters so make sure they are similar). then you just need to identify the path you want to modify.
you can modify the default generated tags by adding a new one at the end of the file, it works the same way for the components.
note 1
overriding the  is a bad idea since it will be regenerated each time you change a model.
note 2
you can easily modify the description, summary, parameters of a path however, for a response like the  you will need to write the full object. take a look at the  for a complete example.
note 3
to modify your generated swagger files security on a specific model, for example to allow the public to use it, you will need to override the security for each path's action. for example with the route  typically all routes are protected by strapi, however if you allow the public role to use this without authentication you will need to override it in your model. see the below example:
as you can see in that example, you are defining "no security" whereas normally you would need a bearer token to access. you will need to do this manually as the documentation plugin rewrites files and cannot pull permissions from the database as this would require a server restart each time the docs are updated.
and  files
this plugin is able to serve up these files. you need to grant the permission to call the controller action to the roles that can have access.
if the default  base url is used, the endpoints exposed are:
or : the openapi spec for the latest version of your api. both endpoints serve the same
document.  or : request the spec for a specific version of your api. for example:
how does it generate the other plugins' documentation?
in order to display a plugin's end-point in the documentation you need to add a  key in the  object.
for example this is the plugin email  file:
1234567891011121314151617181920212223242526272829303132333435363738394041
in this file we have only one route that we want to reference in our documentation (). usually, the tag object is used for the swagger ui, it will group this route under the  dropdown in the documentation. furthermore, the algorithm will try to find the model to generate the best response possible. if the model is unknown it generates a response like the following  that you can easily override later.
there's another property to guide the algorithm to create the best response possible, the  key.
when we can't know by the controller name the type of the returned response (like  and ) you can specify it with this key. here's an example from the  route file.
i have created a route in a common api (like product) that queries another model. how to automate this ?
you can use the  key in your route. if you provide a  which is a string like  the algorithm will know that the end-point retrieves data from the  table. creating a tag object  will result in generating a response with the  model from the plugin users-permissions.
upgrade strapi version
strapi periodically releases code improvements through upgrades. upgrades contain no breaking changes and are announced in both the terminal and in the administration panel.  are provided whenever a new strapi version includes breaking changes.
✋ caution that create custom code or modify existing code will need to be updated and compared to the changes in the repository. not updating the plugin extensions could break the application.
upgrading the dependencies
prerequisites stop the server before starting the upgrade. confirm there are no  between the current and ultimate strapi versions.
upgrade all of the strapi packages in the .
save the edited  file. run either  or  to install the upgraded version.
💡 tipif the operation doesn't work, try removing your  or . if that doesn't help, remove the  folder as well and try again.
reinitializing the application
rebuild the administration panel and start the application:
installing from cli
strapi cli (command line interface) installation scripts are the fastest way to get strapi running locally. the following guide is the installation option most recommended by strapi.
preparing the installation
the installation requires the following software to be already installed on your computer:
: only lts versions are supported (v14 and v16). other versions of node.js may not be compatible with the latest release of strapi. the 16.x version is most recommended by strapi.  (v6 only) or  to run the cli installation scripts.  when using a sqlite database
a database is also required for any strapi project. strapi currently supports the following databases:
✋ cautionstrapi v4 does not support mongodb.
creating a strapi project
🤓 cli installation optionsthe following installation guide covers the most basic installation option using the cli. there are however other options that can be used when creating a new strapi project: using the  flag at the end of the command to directly create the project in quickstart mode. using the  flag at the end of the command to create a project with pre-made strapi configurations (see ). using the  flag (or the shorter version ) at the end of the command to create a project in . using the  flag will prevent strapi from automatically starting the server (useful in combination with ) for more information on available flags, see our . strapi also offers a starters cli to create a project with a pre-made frontend application (see ).
in a terminal, run the following command:  choose an installation type: , which uses the default database (sqlite) , which allows to choose your preferred database (custom installation type only) among the list of databases, choose a database for your strapi project. (custom installation type only) name your project's database.
💡 tipexperimental strapi versions are released every tuesday through saturday at midnight gmt. you can create a new strapi application based on the latest experimental release using . please use these experimental builds at your own risk. it is not recommended to use them in production.
running strapi
to start the strapi application, run the following command in the project folder:
running strapi in a docker container
✋ cautionstrapi does not build any official container images. the following instructions are provided as a courtesy to the community. if you have any questions please reach out on .
the following documentation will guide you through building a custom  container with an existing strapi project.
docker is an open platform that allows developing, shipping, and running applications by using containers (i.e. packages containing all the parts an application needs to function, such as libraries and dependencies). containers are isolated from each other and bundle their own software, libraries, and configuration files; they can communicate with each other through well-defined channels.
prerequisites  installed on your machine a  an existing strapi v4 project, or a new one created with the  (optional)  installed on your machine (optional)  installed on your machine
development and/or staging environments
for working with strapi locally on your host machine you can use the , and if needed the  can also be used to start up a database container.
both methods require an existing strapi project or a new one created (see  guide).
development dockerfile
the following  can be used to build a non-production docker image for a strapi project.
✏️ noteif you are using , you can skip setting the environment variables manually, as they will be set in the  file or a  file.
the following environment variables are required in order to run strapi in a docker container:
you can also set some .
for more information on the  and its commands, please refer to the .
sample :
(optional) docker compose
the following  can be used to start up a database container and a strapi container along with a shared network for communication between the two.
✏️ notefor more information about running docker compose and its commands, please refer to the .
production environments
the docker image in production is different from the one used in development/staging environments because of the differences in the admin build process in addition to the command used to run the application. typical production environments will use a reverse proxy to serve the application and the admin panel. the docker image is built with the production build of the admin panel and the command used to run the application is .
once the  is created, the  can be built. optionally, the container can be published to a  to make it available to the community.  can help you
in the process of building a production docker image and deploying it to a production environment.
production dockerfile
the following  can be used to build a production docker image for a strapi project.
building the production container
building production docker images can have several options. the following example uses the  command to build a production docker image for a strapi project. however, it is recommended you review the  for more information on building docker images with more advanced options.
to build a production docker image for a strapi project, run the following command:
(optional) publishing the container to a registry
after you have built a production docker image for a strapi project, you can publish the image to a docker registry. ideally for production usage this should be a private registry as your docker image will contain sensitive information.
depending on your hosting provider you may need to use a different command to publish your image. it is recommended you review the  for more information on publishing docker images with more advanced options.
some popular hosting providers are:
community tools
several community tools are available to assist you in deploying strapi to various cloud providers and setting up docker in a development or production environment.
we strongly support our community efforts and encourage you to check out the following tools, please help support them by contributing to their development.
if you would like to add your tool to this list, please open a pull request on the .
@strapi-community/dockerize
the  package is a cli tool that can be used to generate a  and  file for a strapi project.
to get started run  within an existing strapi project folder and follow the cli prompts.
for more information please see the official  or the .
@strapi-community/deployify
the  package is a cli tool that can be used to deploy your application to various cloud providers and hosting services. several of these also support deploying a strapi project with a docker container and will call on the  package to generate the required files if they don't already exist.
docker faq
why doesn't strapi provide official docker images?
strapi is a framework that can be used to build many different types of applications. as such, it is not possible to provide a single docker image that can be used for all use cases.
why do we have different dockerfiles for development and production?
the primary reason for various docker images is due to the way our admin panel is built. the admin panel is built using react and is bundled into the strapi application during the build process. this means that the strapi backend is acting as a web server to serve the admin panel and thus certain environment variables are statically compiled into the built admin panel.
it is generally considered a best practice with strapi to build different docker images for development and production environments. this is because the development environment is not optimized for performance and is not intended to be exposed to the public internet.
database configuration
the  file (or the  file for typescript) is used to define database connections that will be used to store the application content.
️❗️ warningstrapi applications are not meant to be connected to a pre-existing database, not created by a strapi application, nor connected to a strapi v3 database. the strapi team will not support such attempts. attempting to connect to an unsupported database may, and most likely will, result in lost data.
🤓 supported databasesthe cli installation guide details .
configuration structure
the  (or  for typescript) accepts 2 main configuration objects:
for database configuration options passed to   for strapi-specific database settings
configuration object
connection parameters
the  object found in  (or  for typescript) is used to pass database connection information and accepts the following parameters:
database pooling options
the  object optionally found in  (or  for typescript) is used to pass  database pooling options and accepts the following parameters:
✋ cautionwhen using docker, change the pool  value to  as docker will kill any idle connections, making it impossible to keep any open connections to the database (see  settings used by knex.js for more information).
the  object found in  (or  for typescript) is used to configure strapi-specific database settings and accepts the following parameter:
configuration examples
12345678910111213141516171819✋ cautionstrapi is aware that there is an issue regarding ssl support for the server.
in order to fix it, you have to set the  object as a boolean in order to disable it. see below for example:  123456789please note that if you need client side ssl ca verification you will need to use the  object with the fs module to convert your ca certificate to a string. you can see an example below:  12345678910111213  123456789101112131415161718
configuration in database
configuration files are not multi-server friendly. to update configurations in production you can use a data store to get and set settings.
get settings
(string): sets the environment you want to store the data in. by default it's current environment (can be an empty string if your configuration is environment agnostic).  (string): sets if your configuration is for an ,  or . by default it's .  (string): you have to set the plugin or api name if  is  or .  (string, required): the name of the key you want to store.
set settings
(any, required): the value you want to store.
databases installation guides
strapi gives you the option to choose the most appropriate database for your project. it currently supports postgresql, sqlite, mysql and mariadb.
the following documentation covers how to install sqlite locally (for development purposes):
✋ cautioninstallation guides for other databases (mysql, mariadb) are being reworked.  are most welcome.
server configuration
the  is used to define server configuration for the strapi application.
✋ cautionchanges to the  file require rebuilding the admin panel. after saving the modified file run either  or  in the terminal to implement the changes.
available options
the  file can include the following parameters:
configurations
the  file should at least include a minimal configuration with the  and  parameters. additional parameters can be included for a full configuration.
✏️ note (i.e. using the  helper) do not need to contain all the values so long as they exist in the default .
the default configuration created with any new project should at least include the following:  the following is an example of a full configuration file. not all of these keys are required (see ).
admin panel configuration
the  is used to define admin panel configuration for the strapi application.
the  file should at least include a minimal configuration with required parameters for authentication and . additional parameters can be included for a full configuration.
the default configuration created with any new project should at least include the following:
middlewares configuration
🤓 different types of middlewaresin strapi, 2 middleware concepts coexist: strapi middlewares are configured and enabled as global middlewares for the entire strapi server application. the present documentation describes how to configure strapi middlewares.strapi also offers the ability to implement your own custom middlewares (see ). route middlewares have a more limited scope and are configured and used as middlewares at the route level. they are described in the .
the  file is used to define all the strapi middlewares that should be applied by the strapi server.
only the middlewares present in  are applied. loading middlewares happens in a specific , with some  and an  for each middleware.
strapi pre-populates the  file with built-in, internal middlewares that all have their own .
loading order
the  file exports an array, where order matters and controls the execution order of the middleware stack:
💡 tipif you aren't sure where to place a middleware in the stack, add it to the end of the list.
naming conventions
strapi middlewares can be classified into different types depending on their origin, which defines the following naming conventions:
optional configuration
middlewares can have an optional configuration with the following parameters:
internal middlewares configuration reference
strapi's core includes the following internal middlewares, mostly used for performances, security and error handling:
the  middleware is based on . it accepts the following options:
for a full list of available options for , check the .
this security middleware is about cross-origin resource sharing (cors) and is based on . it accepts the following options:
the errors middleware handles  thrown by the code. based on the type of error it sets the appropriate http status to the response. by default, any error not supposed to be exposed to the end user will result in a 500 http response.
the middleware doesn't have any configuration options.
the  middleware serves the favicon and is based on . it accepts the following options:
the  middleware is an ip filter middleware based on . it accepts the following options:
💡 tipthe  and  options support wildcards (e.g. ) and spreads (e.g. ).
the  middleware is used to log requests.
to define a custom configuration for the  middleware, create a dedicated configuration file (). it should export an object that must be a complete or partial  logger configuration. the object will be merged with strapi's default logger configuration on server start.
the  middleware adds a  parameter to the response header. it accepts the following options:
the  middleware is a query parser based on . it accepts the following options:
the  middleware enables the  (in milliseconds) for the response header.
the  middleware is a static file serving middleware, based on . it accepts the following options:
💡 tipyou can customize the path of the public folder by editing the .
the security middleware is based on . it accepts the following options:
💡 tipwhen using any 3rd party upload provider, generally it's required to set a custom configuration here. please see the provider documentation for which configuration options are required.
✏️ notethe default directives include a  value. this value is set for the  and is safe to keep.
1234567891011121314151617181920212223242526272829303132
the  middleware allows the use of cookie-based sessions, based on . it accepts the following options:
strapi cloud
️❗️ warningstrapi cloud is currently in closed beta release. features and functionality may change prior to general availability.
this is a step-by-step guide for deploying your strapi application on strapi cloud. before you are able to access strapi cloud, you must first  to the beta release.
requesting access
to request access you must sign up for the waitlist . be sure to enter your correct github username and discord id as these are used to grant access to the strapi cloud beta.
onboarding of users will be done in batches. you will be notified via email when you have been granted access to strapi cloud.
prerequisites
before you can deploy your strapi application on strapi cloud, you need to have the following prerequisites:
a  account github repository for your strapi application(s) strapi version 4.1+
💡 tipthe connected repository can contain multiple strapi applications. each strapi app must be in a separate directory.
navigate to the  login page.  you are prompted to log in with github. your strapi cloud account is created during this initial login. once logged in, you will be redirected to the strapi cloud projects page. from here you can create your first strapi cloud project.
create a project
from the projects page, click the create project button. you are prompted to connect with github. 💡 tipconnect the github account and/or organizations that own the repository or repositories you want to deploy. this can be different from the account that owns the strapi cloud account. you will be redirected to github to authorize strapi cloud to access your repository. after granting the required access from github, from the projects page select your desired repository to install strapi cloud.  click next to proceed to the project set up page and enter the following information: project name: the name of your strapi app, this is fetched from the repository name but can be edited. it is automatically converted to slug format (). github branch: the default branch to use for this deployment. this uses the  of the repository but can be changed via the drop-down. deploy on push: when enabled, this will automatically deploy the latest changes from the selected branch. when disabled, you will need to manually deploy the latest changes.  (optional) select show advanced settings to configure the following options: : environment variables are used to configure your strapi app. base directory: the directory where your strapi app is located in the repository. this is useful if you have multiple strapi apps in the same repository or if you have a monorepo.  click create to finalize the project creation. an initial deployment is triggered and you are redirected to the projects page.
managing projects
the projects page displays a list of all your strapi cloud projects. from here you can manage your projects and access the corresponding applications.
each project card displays the following information:
project name status: displays a disconnected warning if the project repository is not connected to strapi cloud. last deployment date: timestamp of the last deployment.
project details
from the projects page, click on any project card to access that project's details page.
the project details page displays the following tabs: deploys and settings.
deploys
the deploys tab displays a chronological list of cards with the details of all historical deployments for the project.
each card displays the following information:
commit sha commit message deployment status: whether the project is
deploying done cancelled build failed deploy failed last deployment time: when the deployment was triggered and the duration. production branch options menu (): the available options vary depending on the deployment status.
for done status: no further options. for deploying status, you can:
cancel deploy for build failed status, you can:
download error logs for deploy failed status, you can:
download error logs
from this page you can also trigger a new deployment and access the application using the corresponding buttons.
the settings tab enables you to edit the following details for the project:
project name: the name of your strapi app, used to identify the project on the cloud dashboard, strapi cli, and deployment urls. production branch: the branch of the linked repository to use for production deployments. environment variables: environment variables are used to configure the environment of your strapi app. connected github repository: the git repository linked to the project. delete project: this will permanently and irreversibly delete the project and all its associated data.
routes
requests sent to strapi on any url are handled by routes. by default, strapi generates routes for all the content-types (see ). routes can be  and configured:
with , which are a way to block access to a route, and with , which are a way to control and change the request flow and the request itself.
once a route exists, reaching it executes some code handled by a controller (see ).
implementing a new route consists in defining it in a router file within the  folder (see ).
there are 2 different router file structures, depending on the use case:
configuring  or creating .
configuring core routers
core routers (i.e. , , , , and ) correspond to  automatically created by strapi when a new  is created.
strapi provides a  factory function that automatically generates the core routers and allows:
passing in configuration options to each router and disabling some core routers to .
a core router file is a javascript file exporting the result of a call to  with the following parameters:
generic implementation example:
this only allows a  request on the  path from the core   without authentication.
creating custom routers
creating custom routers consists in creating a file that exports an array of objects, each object being a route with the following parameters:
dynamic routes can be created using parameters and regular expressions. these parameters will be exposed in the  object. for more details, please refer to the  documentation.
✋ cautionroutes files are loaded in alphabetical order. to load custom routes before core routes, make sure to name custom routes appropriately (e.g.  and ).
in the following example, the custom routes file name is prefixed with  to make sure the route is reached before the core routes.  12345678910111213141516  12345678910111213141516
both  and  have the same configuration options. the routes configuration is defined in a  object that can be used to handle  and  or to .
can be added to a route configuration:
by pointing to a policy registered in , with or without passing a custom configuration or by declaring the policy implementation directly, as a function that takes  to extend  () and the  instance as arguments (see )
by pointing to a middleware registered in , with or without passing a custom configuration or by declaring the middleware implementation directly, as a function that takes  () and the  instance as arguments:
public routes
by default, routes are protected by strapi's authentication system, which is based on  or on the use of the .
in some scenarios, it can be useful to have a route publicly available and control the access outside of the normal strapi authentication system. this can be achieved by setting the  configuration parameter of a route to :
policies are functions that execute specific logic on each request before it reaches the . they are mostly used for securing business logic.
each  of a strapi project can be associated to an array of policies. for example, a policy named  could check that the request is sent by an admin user, and restrict access to critical routes.
policies can be global or scoped.  can be associated to any route in the project. scoped policies only apply to a specific  or .
a new policy can be implemented:
with the  or manually by creating a javascript file in the appropriate folder (see ):
for global policies  for api policies  for plugin policies
global policy implementation example:
is a wrapper around the  context. it adds some logic that can be useful to implement a policy for both rest and graphql.
policies can be configured using a  object:
to apply policies to a route, add them to its configuration object (see ).
policies are called different ways depending on their scope:
use  for  use  for  use  for
💡 tipto list all the available policies, run .
global policies
global policies can be associated to any route in a project.
plugin policies
can add and expose policies to an application. for example, the  comes with policies to ensure that the user is authenticated or has the rights to perform an action:
api policies
api policies are associated to the routes defined in the api where they have been declared.
to use a policy in another api, reference it with the following syntax: :
middlewares customization
🤓 different types of middlewaresin strapi, 2 middleware concepts coexist: strapi middlewares are  for the entire strapi server application. these middlewares can be applied at the application level or at the api level. the present documentation describes how to implement them.plugins can also add strapi middlewares (see ). route middlewares have a more limited scope and are configured and used as middlewares at the route level. they are described in the .
a new application-level or api-level middleware can be implemented:
for application-level middlewares  for api-level middlewares  for
middlewares working with the rest api are functions like the following:
once created, custom middlewares should be added to the  or strapi won't load them.
the graphql plugin also allows , with a different syntax.
middlewares are called different ways depending on their scope:
use  for application-level middlewares use  for api-level middlewares use  for plugin middlewares
💡 tipto list all the registered middlewares, run .
controllers
controllers are javascript files that contain a set of methods, called actions, reached by the client according to the requested . whenever a client requests the route, the action performs the business logic code and sends back the . controllers represent the c in the model-view-controller (mvc) pattern.
in most cases, the controllers will contain the bulk of a project's business logic. but as a controller's logic becomes more and more complicated, it's a good practice to use  to organize the code into re-usable parts.
controllers can be . strapi provides a  factory function that automatically generates core controllers and allows building custom ones or .
adding a new controller
a new controller can be implemented:
with the  or manually by creating a javascript file:
in  for api controllers (this location matters as controllers are auto-loaded by strapi from there) or in a folder like  for plugin controllers, though they can be created elsewhere as long as the plugin interface is properly exported in the  file (see )
each controller action can be an  or  function.
every action receives a context object () as a parameter.  contains the  and the .
a specific   is defined, the name of the router file (i.e. ) is used to call the controller handler (i.e. ). every time a  request is sent to the server, strapi calls the  action in the  controller, which returns :
✏️ notewhen a new  is created, strapi builds a generic controller with placeholder code, ready to be customized.
extending core controllers
default controllers and actions are created for each content-type. these default controllers are used to return responses to api requests (e.g. when  is accessed, the  action of the default controller for the "article" content-type is called). default controllers can be customized to implement your own logic. the following code examples should help you get started.
💡 tipan action from a core controller can be replaced entirely by  and naming the action the same as the original action (e.g. , , , , or ).
1234567  1234567  1234567  1234567  1234567
1234567  1234567  1234567
attaching a controller to a route
controllers are declared and attached to a route. controllers are automatically called when the route is called, so controllers usually do not need to be called explicitly (see ).
💡 tipyou may also create routes, controllers, services, and bind them together, by using
usage with services
can call controllers, and in this case the following syntax should be used:
1234
💡 tipto list all the available controllers, run .
requests & responses
requests
the context object () contains all the requests related information. they are accessible through , from  and .
strapi passes the  on  and  through
for more information, please refer to the .
responses
the context object () contains a list of values and functions useful to manage server responses. they are accessible through , from  and .
accessing the request context anywhere
✨ new in v4.3.9 the  works with strapi v4.3.9+.
strapi exposes a way to access the current request context from anywhere in the code (e.g. lifecycle functions).
you can access the request as follows:
you should only use this inside of functions that will be called in the context of an http request.
✏️ notestrapi uses a node.js feature called  to make the context available anywhere.
services
services are a set of reusable functions. they are particularly useful to respect the "don’t repeat yourself" (dry) programming concept and to simplify  logic.
services can be . strapi provides a  factory function that automatically generates core services and allows building custom ones or .
adding a new service
a new service can be implemented:
for api services or  for .
to manually create a service, export a factory function that returns the service implementation (i.e. an object with methods). this factory function receives the  instance:
🤓 entity service apito get started creating your own services, see strapi's built-in functions in the  documentation.
the goal of a service is to store reusable functions. a  service could be useful to send emails from different functions in our codebase that have a specific purpose:  the service is now available through the  global variable. it can be used in another part of the codebase, like in the following controller:
✏️ notewhen a new  is created, strapi builds a generic service with placeholder code, ready to be customized.
extending core services
core services are created for each content-type and could be used by  to execute reusable logic through a strapi project. core services can be customized to implement your own logic. the following code examples should help you get started.
💡 tipa core service can be replaced entirely by  and naming it the same as the core service (e.g. , , , , or ).
once a service is created, it's accessible from  or from other services:
💡 tipto list all the available services, run .
models
as strapi is a headless content management system (cms), creating a data structure for the content is one of the most important aspects of using the software. models define a representation of the data structure.
there are 2 different types of models in strapi:
content-types, which can be collection types or single types, depending on how many entries they manage, and components that are data structures re-usable in multiple content-types.
if you are just starting out, it is convenient to generate some models with the  directly in the admin panel. the user interface takes over a lot of validation tasks and showcases all the options available to create the content's data structure. the generated model mappings can then be reviewed at the code level using this documentation.
model creation
content-types and components models are created and stored differently.
content-types
content-types in strapi can be created:
with the , or with  command.
the content-types use the following files:
for the model's  definition. (generated automatically, when creating content-type with either method)  for . this file must be created manually.
these models files are stored in , and any javascript or json file found in these folders will be loaded as a content-type's model (see ).
✏️ notein -enabled projects, schema typings can be generated using the  command.
component models can't be created with cli tools. use the  or create them manually.
components models are stored in the  folder. every component has to be inside a subfolder, named after the category the component belongs to (see ).
model schema
the  file of a model consists of:
, such as the kind of content-type the model represents or the table name in which the data should be stored, , mostly used to display the model in the admin panel and access it through the rest and graphql apis, , which describe the data structure of the model, and  used to defined specific behaviors on the model.
model settings
general settings for the model can be configured with the following parameters:
123456
model information
the  key in the model's schema describes information used to display the model in the admin panel and access it through the content api. it includes the following parameters:
model attributes
the data structure of a model consists of a list of attributes. each attribute has a  parameter, which describes its nature and defines the attribute as a simple piece of data or a more complex structure used by strapi.
many types of attributes are available:
scalar types (e.g. strings, dates, numbers, booleans, etc.), strapi-specific types, such as:
for files uploaded through the   to describe a  between content-types  to describe  and their specific keys  to define a  (i.e. a data structure usable in multiple content-types)  to define a  (i.e. a flexible space based on a list of components) and the  and  types, only used by the
the  parameter of an attribute should be one of the following values:
validations
basic validations can be applied to attributes using the following parameters:
1234567891011121314151617181920212223
database validations and settings
✋ 🚧 this api is considered experimental.these settings should be reserved to an advanced usage, as they might break some features. there are no plans to make these settings stable.
database validations and settings are custom options passed directly onto the  knex.js function during schema migrations. database validations allow for an advanced degree of control for setting custom column settings. the following options are set in a  object per attribute:
12345678910111213141516171819202122232425262728293031323334353637
the  type is used to automatically prefill the field value in the admin panel with a unique identifier (uid) (e.g. slugs for articles) based on 2 optional parameters:
(string): if used, the value of the field defined as a target is used to auto-generate the uid.  (string): if used, the uid is generated based on a set of options passed to . the resulting  must match the following regular expression pattern: .
relations
relations link content-types together. relations are explicitly defined in the   of a model with   and accept the following additional parameters:
one-to-one relationships are useful when one entry can be linked to only one other entry. they can be unidirectional or bidirectional. in unidirectional relationships, only one of the models can be queried with its linked item.  a blog article belongs to a category. querying an article can retrieve its category, but querying a category won't retrieve the owned article.  1234567891011  a blog article belongs to a category. querying an article can retrieve its category, and querying a category also retrieves its owned article.  123456789101112131415161718192021222324252627 one-to-many relationships are useful when: an entry from a content-type a is linked to many entries of another content-type b, while an entry from content-type b is linked to only one entry of content-type a. one-to-many relationships are always bidirectional, and are usually defined with the corresponding many-to-one relationship:  a person can own many plants, but a plant is owned by only one person.  12345678910111213141516171819202122232425 many-to-one relationships are useful to link many entries to one entry. they can be unidirectional or bidirectional. in unidirectional relationships, only one of the models can be queried with its linked item.  a book can be written by many authors.  123456789101112  an article belongs to only one category but a category has many articles.  1234567891011121314151617181920212223242526 many-to-many relationships are useful when: an entry from content-type a is linked to many entries of content-type b, and an entry from content-type b is also linked to many entries from content-type a. many-to-many relationships can be unidirectional or bidirectional. in unidirectional relationships, only one of the models can be queried with its linked item.   123456789  an article can have many tags and a tag can be assigned to many articles.  1234567891011121314151617181920212223242526
extend strapi’s capabilities by adding new types of fields to content-types. custom fields are explicitly defined in the  of a model with .
custom fields' attributes also accept:
custom fields' attributes also show the following specificities:
a  attribute whose value acts as a unique identifier to indicate which registered custom field should be used. its value follows:
either the  format if a plugin created the custom field or the  format for a custom field specific to the current strapi application and additional parameters depending on what has been defined when registering the custom field (see ).
component fields create a relation between a content-type and a component structure. components are explicitly defined in the  of a model with  and accept the following additional parameters:
dynamic zones
dynamic zones create a flexible space in which to compose content, based on a mixed list of .
dynamic zones are explicitly defined in the   of a model with . they also accepts a  array, where each component should be named following this format: .
model options
the  key is used to define specific behaviors and accepts the following parameter:
lifecycle hooks
lifecycle hooks are functions that get triggered when strapi queries are called. they are triggered automatically when managing content through the administration panel or when developing custom code using ·
lifecycle hooks can be customized declaratively or programmatically.
✋ cautionlifecycles hooks are not triggered when using directly the  library instead of strapi functions.
available lifecycle events
the following lifecycle events are available:
hook  object
lifecycle hooks are functions that take an  parameter, an object with the following keys:
declarative and programmatic usage
to configure a content-type lifecycle hook, create a  file in the  folder.
each event listener is called sequentially. they can be synchronous or asynchronous.
using the database layer api, it's also possible to register a subscriber and listen to events programmatically:
12345678910111213141516171819202122232425262728
webhooks
webhook is a construct used by an application to notify other applications that an event occurred. more precisely, webhook is a user-defined http callback. using a webhook is a good way to tell third party providers to start some processing (ci, build, deployment ...).
the way a webhook works is by delivering information to a receiving application through http requests (typically post requests).
user content-type webhooks
to prevent from unintentionally sending any user's information to other applications, webhooks will not work for the user content-type.
if you need to notify other applications about changes in the users collection, you can do so by creating  inside the file .
available configurations
you can set webhook configurations inside the file .
: you can set default headers to use for your webhook requests. this option is overwritten by the headers set in the webhook itself.
example configuration
securing your webhooks
most of the time, webhooks make requests to public urls, therefore it is possible that someone may find that url and send it wrong information.
to prevent this from happening you can send a header with an authentication token. using the admin panel you would have to do it for every webhook.
another way is to define  to add to every webhook requests.
you can configure these global headers by updating the file at :
if you are developing the webhook handler yourself you can now verify the token by reading the headers.
available events
by default strapi webhooks can be triggered by the following events:
*only when  is enabled on this content type.
payloads
💡 noteprivate fields and passwords are not sent in the payload.
when a payload is delivered to your webhook's url, it will contain specific headers:
this event is triggered when a new entry is created.
example payload
this event is triggered when an entry is updated.
this event is triggered when an entry is deleted.
this event is triggered when an entry is published.
this event is triggered when an entry is unpublished.
this event is triggered when you upload a file on entry creation or through the media interface.
this event is triggered when you replace a media or update the metadata of a media through the media interface.
this event is triggered only when you delete a media through the media interface.
rest api
the rest api allows accessing the  through api endpoints. strapi automatically creates  when a content-type is created.  can be used when querying api endpoints to refine the results.
✏️ notethe rest api by default does not populate any relations, media fields, components, or dynamic zones. use the  to populate specific fields.
for each content-type, the following endpoints are automatically generated:
content type   content type
✏️ note don't have api endpoints.
requests return a response as an object which usually includes the following keys:
: the response data itself, which could be: a single entry, as an object with the following keys:
(number)  (object)  (object) a list of entries, as an array of objects a custom response  (object): information about pagination, publication state, available locales, etc.  (object, optional): information about any  thrown by the request
✏️ notesome plugins (including users & permissions and upload) may not follow this response format.
get entries
returns entries matching the query filters (see  documentation).
example request  example response  1234567891011121314151617181920212223242526
get an entry
returns an entry by .
example request  example response  1234567891011121314
create an entry
creates an entry and returns its value.
if the  is installed, it's possible to use post requests to the rest api to .
example request   1234567891011 example response  12345678
update an entry
partially updates an entry by  and returns its value.
fields that aren't sent in the query are not changed in the database. send a  value to clear fields.
example request   1234567 example response  12345678
✏️ noteeven with the  installed, it's currently not possible to .
deletes an entry by  and returns its value.
example request  example response  12345678
entity service api: crud operations
the  is built on top of the the  and uses it to perform crud operations on entities.
the  parameter used in function calls for this api is a  built with the following format:  where  is one of: ,  or .
a correct  to get users of the strapi admin panel is . a possible  for the upload plugin could be . as the s for user-defined custom content-types follow the  syntax, if a content-type  exists, it is referenced by .
💡 tiprun the  command in a terminal to display all possible content-types' s for a specific strapi instance.
findone()
finds the first entry matching the parameters.
syntax:  ⇒
parameters
findmany()
finds entries matching the parameters.
create()
creates one entry and returns it
update()
updates one entry and returns it.
✏️ note only performs a partial update, so existing fields that are not included won't be replaced.
delete()
deletes one entry and returns it.
entity service api: filtering
the  offers the ability to filter results found with its  method.
results are filtered with the  parameter that accepts  and . every operator should be prefixed with .
logical operators
all nested conditions must be .
will be used implicitly when passing an object with nested conditions:
one or many nested conditions must be .
negates the nested conditions.
✏️ note can be used: as a logical operator (e.g. in ) or  (e.g. in ).
💡 tip,  and  operators are nestable inside of another ,  or  operator.
attribute operators
✋ cautionusing these operators may give different results depending on the database's implementation, as the comparison is handled by the database and not by strapi.
negates the nested condition(s).
attribute equals input value.
can be omitted:
attribute equals input value (case-insensitive).
attribute does not equal input value.
attribute is contained in the input list.
can be ommited when passing an array of values:
attribute is not contained in the input list.
attribute is less than the input value.
attribute is less than or equal to the input value.
attribute is greater than the input value.
attribute is greater than or equal to the input value.
attribute is between the 2 input values.
attribute contains the input value (case-sensitive).
attribute does not contain the input value (case-sensitive).
attribute contains the input value.  is not case-sensitive, while  is.
attribute does not contain the input value.  is not case-sensitive, while  is.
attribute starts with input value.
attribute ends with input value.
attribute is .
attribute is not .
entity service api: populating
the  does not populate relations, components or dynamic zones by default.
basic populating
to populate all the root level relations, use :
populate various component or relation fields by passing an array of attribute names:
advanced populating
an object can be passed for more advanced populating:
complex populating can be achieved by using the  and select or populate nested relations or components:
entity service api: ordering & pagination
the  offers the ability to  and  results found with its  method.
ordering
to order results returned by the entity service api, use the  parameter. results can be ordered based on a  or on  attribute(s) and can also use .
single
to order results by a single field, just pass it to the  parameter:
either as a string to sort with the default ascending order or as an object to define both the field name and the order (i.e.  for ascending order or  for descending order)
multiple
to order results by multiple fields, pass the fields as an array to the  parameter:
either as an array of strings to sort multiple fields using the default ascending order or as an array of objects to define both the field name and the order (i.e.  for ascending order or  for descending order)
relational ordering
fields can also be sorted based on fields from relations:
to paginate results returned by the entity service api, use the  and  parameters:
entity service api: components and dynamic zones
the  is the layer that handles  and  logic. with the entity service api, components and dynamic zones can be  and  while creating or updating entries.
a  can be created while creating an entry with the entity service api:
a  (i.e. a list of components) can be created while creating an entry with the entity service api:
update
a  can be updated while updating an entry with the entity service api. if a component  is specified, the component is updated, otherwise the old one is deleted and a new one is created:
a  (i.e. a list of components) can be updated while updating an entry with the entity service api. if a component  is specified, the component is updated, otherwise the old one is deleted and a new one is created:
query engine api: single operations
syntax:
findwithcount()
finds and counts entries matching the parameters.
creates one entry and returns it.
query engine api: bulk operations
✋ cautionto avoid performance issues, bulk operations are not allowed on relations.
createmany()
creates multiple entries.
updatemany()
updates multiple entries matching the parameters.
deletemany()
deletes multiple entries matching the parameters.
aggregations
count()
counts entries matching the parameters.
query engine api: filtering
is used implicitly when passing an object with nested conditions:
negates nested condition(s).
can be omitted when passing an array of values:
query engine api: populating
relations and components have a unified api for populating them.
select which data to populate by passing an array of attribute names:
an object can be passed for more advanced usage:
complex populating can also be achieved by applying  filters and select or populate nested relations:
query engine api: ordering & paginating
the  offers the ability to  and  results.
to order results returned by the query engine, use the  parameter. results can be ordered based on a  or on  attributes and can also use .
to paginate results returned by the query engine api, use the  and  parameters:
getting started with react
✋ cautionthis third-party integration guide might not be up-to-date with strapi v4.  are most welcome.
this integration guide is following the . we assume that you have fully completed its "hands-on" path, and therefore can consume the api by browsing this .
if you haven't gone through the quick start guide, the way you request a strapi api with  remains the same except that you will not fetch the same content.
create a react app
create a basic react application using .
use an http client
many http clients are available but in this documentation we'll use  and .
1 no installation needed
get request your collection type
execute a  request on the  collection type in order to fetch all your restaurants.
be sure that you activated the  permission for the  collection type
example get request with axios  12345 example get request with fetch  12345678 example response  12345678910111213141516171819202122232425262728293031  example   1234567891011121314151617181920212223242526272829   123456789101112131415161718192021222324252627282930313233343536373839404142434445  post request your collection type execute a  request on the  collection type in order to create a restaurant. be sure that you activated the  permission for the  collection type and the  permission fot the  collection type. in this example a  category has been created which has the id: 3. example post request with axios  1234567891011 example post request with fetch  12345678910111213
example response  12345678910111213141516171819
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120
put request your collection type
execute a  request on the  collection type in order to update the category of a restaurant.
be sure that you activated the  permission for the  collection type.
we consider that the id of your restaurant is , and the id of your category is . example put request with axios  123456789 example put request with fetch  12345678910111213
starter
conclusion
here is how to request your collection types in strapi using react. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
we just used the get, post and put methods here but you can , and  an entry too. learn more about .
getting started with vue.js
create a vue.js app
create a basic vue.js application using .
example get request with axios  12345 example get request with fetch  12345678
example response  12345678910111213141516171819202122232425262728293031
12345678910111213141516171819202122232425262728293031323334   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051
post request your collection type
execute a  request on the  collection type in order to create a restaurant.
be sure that you activated the  permission for the  collection type and the  permission for the  collection type.
in this example a  category has been created which has the id: 3.
example post request with axios  1234567891011 example post request with fetch  12345678910111213
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172   1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495
we consider that the id of your restaurant is .
and the id of your category is . example put request with axios  123456789 example put request with fetch  12345678910111213
here is how to request your collection types in strapi using vue.js. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
getting started with angular
create a angular app
create a basic angular application using .
12345678910111213141516171819202122   1234567891011121314151617181920212223242526272829303132333435363738394041   1234567
be sure that you activated the  permission for the  collection type and the  permission fot the  collection type.
12345678910111213  12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990   123456789101112131415161718192021222324252627282930
here is how to request your collection types in strapi using angular. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
getting started with next.js
create a next.js app
create a basic next.js application.
1 no installation needed.
example response  12345678910111213141516171819202122
1234567891011121314151617181920212223242526   123456789101112131415161718192021222324252627282930313233343536373839404142434445
add the  query parameter to return categories with the response.
example post request with axios  1234567891011121314 example post request with fetch  12345678910111213141516
example response  1234567891011121314151617181920212223242526
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126
and the id of your category is . example put request with axios  1234567891011 example put request with fetch  123456789101112131415
here is how to request your collection types in strapi using next.js. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
getting started with nuxt.js
create a nuxt.js app
create a basic nuxt.js application with .
for this example we are using the awesome  module.  1add  to the module section of  with the following settings.  12345  1 no installation needed
example get request with @nuxtjs/strapi  12345 example get request with axios  12345 example get request with fetch  12345678
12345678910111213141516171819202122232425262728293031   12345678910111213141516171819202122232425262728293031323334   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051
example post request with @nuxtjs/strapi  123456789 example post request with axios  1234567891011 example post request with fetch  12345678910111213
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172   1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495
and the id of your category is . example put request with @nuxtjs/strapi  1234567 example put request with axios  123456789 example put request with fetch  12345678910111213
starters
here is how to request your collection types in strapi using nuxt.js. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
we just used the get, post and put methods here but you can ,  and  an entry too. learn more about .
getting started with graphql
install the graphql plugin
install the graphql plugin in your strapi project.
fetch your restaurant collection type
play with the  to fetch your content.
example query  12345678910 example response  12345678910111213141516
these examples do not guide you to configure your client with apollo for your . please follow the associated documentation for each client: ( and  here)
using  and   123456789101112131415161718192021222324252627 using  and   12345678910111213141516171819202122232425262728293031323334
fetch your category collection type
example request  1234567891011 example response  123456789101112131415
using  and   12345678910111213141516171819202122232425262728293031323334 using  and   123456789101112131415161718192021222324252627282930313233343536373839404142434445
this is how you request your collection types in strapi using graphql.
feel free to explore more about .
getting started with gatsby
create a gatsby app
create a basic gatsby application using the .
configure gatsby
gatsby is a  and will fetch your content from strapi at build time. you need to configure gatsby to communicate with your strapi application.
add the  to the plugins section in the  file:
example get request  123456789101112131415 example response  1234567891011121314151617181920212223
123456789101112131415161718192021222324252627282930313233343536
execute a  request on the  collection type in order to fetch a specific category with all the associated restaurants.
example get request  1234567891011121314 example response  123456789101112131415161718192021
123456789101112131415161718192021222324252627282930313233343536373839
we can generate pages for each category.
tell gatsby to generate a page for each category by updating the  file with the following:
create a  file that will display the content of each one of your category:
you can find your restaurant categories by browsing .
feel free to do the same for your restaurants!
here is how to request your collection types in strapi using gatsby.
getting started with gridsome
create a gridsome app
create a basic gridsome application using the .
configure gridsome
gridsome is a  and will fetch your content from strapi at build time. you need to configure gridsome to communicate with your strapi application.
example get request  1234567891011 example response  12345678910111213141516
12345678910111213141516171819202122232425262728293031
example get request  12345678910 example response  123456789101112131415
tell gridsome to generate a page for each category by updating the  file with the following:
here is how to request your collection types in strapi using gridsome.
learn more about
getting started with jekyll
create a jekyll app
be sure to have  on your computer.
configure jekyll
jekyll is a  and will fetch your content from strapi at build time. you need to configure jekyll to communicate with your strapi application.
add  to your
add  to your plugins in .
add the configuration of strapi at the end of the .
run  to install your gems.
run your application with:
tell jekyll to generate a page for each category by updating the  file with the following:
after building your application, you'll be able to see a  folder in your  folder.
here is how to request your collection types in strapi using jekyll.
learn more about jekyll with their .
getting started with 11ty
create an 11ty app
create a  file, install and save eleventy into your project.
12  12
make sure installation went ok:
configure 11ty
11ty does not create any file structure for you. it's up to you to do it.
create a  folder containing a  and a  file. they will be used to fetch your content from strapi. create a  folder containing a  file. it will be the default template of your project.
create a ,  and a  file. they will define how you'll present the data. create a  file containing the following (make sure to prefix the file's name with the dot):
finally, add the following packages to your application:
1  1
example get request  12345678910 example response  1234567891011121314151617181920212223242526272829303132333435
example get request  12345678910 example response  1234567891011121314151617181920212223242526272829303132333435363738394041424344454647
you can find your restaurants and categories by browsing  and .
user stories
how  switched from wordpress to strapi and eleventy.
here is how to request your collection types in strapi using 11ty.
getting started with svelte
create a svelte app
first, install degit by running  in your command-line interface (cli).
“degit makes copies of git repositories and fetches the latest commit in the repository. this is a more efficient approach than using git clone, because we’re not downloading the entire git history.”
create a basic svelte application using webpack:
example get request with axios  12345 example get request with fetch  12345678 example response  12345678910111213141516171819202122232425262728293031  example   1234567891011121314151617181920212223242526272829   12345678910111213141516171819202122232425262728293031323334353637383940414243444546  post request your collection type execute a  request on the  collection type in order to create a restaurant. be sure that you activated the  permission for the  collection type and the  permission for the  collection type. in this example a  category has been created which has the id: 3. example post request with axios  1234567891011 example post request with fetch  12345678910111213
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778
here is how to request your collection types in strapi using svelte.
learn more about svelte with their .
getting started with sapper
create a sapper app
create a basic sapper application using webpack:
1234567891011121314151617181920212223242526272829   12345678910111213141516171819202122232425262728293031323334353637383940414243444546
here is how to request your collection types in strapi using sapper.
learn more about svelte with their /
getting started with ruby
create a ruby file
many http clients are available but in this documentation we'll use .
create a  containing the following:
install your gems by running the following command:
example get request  1 example response  12345678910111213141516171819202122
add the  query parameter to return the categories with the response.
example post request  12345678910111213141516 example response  1234567891011121314151617181920212223242526
example put request  1234567891011121314 example response  1234567891011121314151617181920212223242526
here is how to request your collection types in strapi using ruby. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
getting started with python
create a python file
example post request  1234567891011121314 example response  1234567891011121314151617181920212223242526
1234567891011121314151617181920212223242526272829303132333435363738
example put request  123456789101112 example response  1234567891011121314151617181920212223242526
here is how to request your collection types in strapi using python. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
getting started with dart
create a dart file
we'll use  for making http requests.
create a  file containing the following:
install your dependencies by running the following command:
example get request  1234567891011 example response  12345678910111213141516171819202122232425262728293031
example post request  1234567891011121314 example response  12345678910111213141516171819
example put request  123456789101112 example response  12345678910111213141516171819
here is how to request your collection types in strapi using dart. when you create a collection type or a single type you will have a certain number of  available to interact with.
getting started with flutter
create a flutter application
update the  file with the following:
here is how to request your collection types in strapi using dart/flutter. when you create a collections type or a single type you will have a certain number of  available to interact with.
getting started with go
create a go file
go has built-in packages to make http requests like get, post, put, and delete.
we will use the "net/http" package along with other packages.
example get request  1 example response  1234567891011121314151617181920212223
example post request  123456 example response  12345678
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647
put request is sligtly different as we need to target the particular thing we want update. we do this by first making a request to http://localhost:1337/api/restaurants/1 and then update what we want to update. in this example, we are going to update  "biscotte restaurant" to "restaurant home".
example put request  1234567 example response  1234567891011121314151617181920212223
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374
here is how to request your collection types in strapi using go. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
getting started with php
create a php file
we will use curl, a built-in php extension that allows us to receive and send information via the url syntax.
example get request  1running the php file on the browser  will give you this response: example response  1234567891011121314151617181920212223
example post request  123456789101112131415running the php file on the browser  will give you this response: example response  12345678910111213141516
123456789101112131415161718192021222324252627282930313233343536373839404142434445
put request is slightly different as we need to target the particular entry we want update. we do this by first making a request to http://localhost:1337/api/restaurants/1 and then update what we want to update. in this example, we are going to update  "biscotte restaurant" to "femoni kitchen".
example put request  123456789 example response running the php file on the browser  will give you this response:  12345678910111213141516
12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364
running an authentication request (getting jwt):
response  123456789101112131415161718192021222324
running an authenticated post request with jwt
example response  12345678910111213141516
here is how to request your collection types in strapi using php. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
getting started with laravel
should you wish to use standalone php, you may also be interested in the .
this guide assumes you already have  and are familiar with the basics of the framework.
using the native laravel http client
following the official laravel documentation, you can easily make a strapi macro to integrated it to the http client from laravel :
https://laravel.com/docs/9.x/http-client#macros :
in app\providers\appserviceprovider (or your serviceprovider) :
once your macro has been configured, you may invoke it from anywhere in your application to create a pending request with the specified configuration:
install the laravel-strapi laravel package
this will install , a laravel specific package for interacting with strapi.
you will need to publish a config file:
you will also need to define your  and  in the  file:
get your collection type
example get request  12
you may now iterate over the $restaurants array which will contain all your restaurants. more options are available as well:
accessing single type items
you may also access single type items as follows:
collection by field
single item from collection
here is how to request your collection types in strapi using laravel. when you create a collection type or a single type you will have a certain number of rest api endpoints available to interact with.
there is more documentation available in the  or in the .
migration guides
migrations are necessary when upgrades to strapi include breaking changes. the migration guides are sequential, meaning if there is more than 1 migration guide between your current version and the latest release, follow each guide in order. if there is no specific migration guide between your current version and the latest release follow the .
v4 migration guides
v3 to v4 migration guides
🚧  migration guides this section is still a work in progress and will continue to be updated and improved. in the meantime, feel free to ask for help on the  or on the community .
migrating from v3.6.x to v4.0.x revolves around 3 topics:
the  helps migrating the built-in back-end and front-end code of the strapi application to v4. the  helps migrating the database content to v4. the  helps migrating a plugin to v4.
v3 migration guides
v3 migration guides are available in the .
strapi projects and applications can be installed either locally on a computer or on a remote server. the following installation guides will guide you step-by-step instructions on how to install and create a new strapi project.
local installations
the application configuration lives in the  folder (see ). all the configuration files are loaded on startup and can be accessed through the configuration provider.
if the  file has the following config:
the  key can be accessed as:
nested keys are accessible with the .
✏️ notethe filename is used as a prefix to access the configurations.
configuration files can either be  or  files.
when using a  file, the configuration can be exported:
either as an object:  123 or as a function returning a configuration object (recommended usage). the function will get access to the :  12345
required configurations
some parts of strapi must be configured for the strapi application to work properly:
the , the , the , and the .
optional configurations
strapi also offers the following optional configuration options for specific features:
strapi provides many deployment options for your project or application. your strapi applications can be deployed on traditional hosting servers or your preferred hosting provider.
the following documentation covers how to develop locally with strapi and deploy strapi with several common hosting options.
☁️ strapi cloud don't want to deploy strapi by yourself?  to soon get access to a platform to easily deploy and host your project.
🤓 community guidesin addition to the official deployment guides maintained by strapi that are found here, community-maintained guides for additional providers are available in the .
general guidelines
hardware and software requirements
to provide the best possible environment for strapi the following requirements apply to development (local) and staging and production workflows.
node lts (v14 or v16) odd-number releases of node are not supported (e.g. v13, v15). npm v6 (or the version shipped with the lts node versions) standard build tools for your os (the  package on most debian-based systems) hardware specifications for your server (cpu, ram, storage):
a supported database version:
🤓 database deploymentdeploying databases along with strapi is covered in the .
a supported operating system:
application configuration
1. configure
we recommend using environment variables to configure your application based on the environment, for example:
then you can create a  file or directly set environment variables in your chosen deployment platform:
💡 tipto learn more about configuration details, see the  documentation.
2. launch the server
before running your server in production you need to build your admin panel for production:
1  1  1then in your  scripts section:  1and run:  1
run the server with the  settings:
✋ cautionwe highly recommend using  to manage your process.
if you need a server.js file to be able to run  instead of  then create a  file as follows:
advanced configurations
if you want to host the administration on another server than the api, .
hosting provider guides
manual guides for deployment on the following platforms:
optional software guides
additional guides for optional software additions that compliment or improve the deployment process when using strapi in a production or production-like environment.
back-end customization
strapi runs an http server based on , a back end javascript framework.
🤓 what is koa?if you are not familiar with the koa back end framework, we highly recommend you to read the .
each part of strapi's back end can be customized:
the  received by the strapi server, the  that handle the requests and trigger the execution of their controller handlers, the  that can block access to a route, the  that can control the request flow and the request before moving forward, the  that execute code once a route has been reached, the  that are used to build custom logic reusable by controllers, the  that are a representation of the content data structure, the  sent to the application that sent the request, and the  that are used to notify other applications of events that occurred.
entity service api
strapi provides an entity service api, built on top of the . the entity service is the layer that handles strapi's complex data structures like  and , and uses the query engine api under the hood to execute database queries.
🤓 entity service api vs. query engine apistrapi v4 offers several layers to interact with the backend and build your queries: the entity service api is the recommended api to interact with your application's database. the entity service is the layer that handles strapi's complex data structures like components and dynamic zones, which the lower-level layers are not aware of. the query engine api interacts with the database layer at a lower level and is used under the hood to execute database queries. it gives unrestricted internal access to the database layer, but should be used only if the entity service api does not cover your use case. if you need direct access to  functions, use .
the entity service is available through :
available operations
the entity service api allows:
(e.g. , , , , ) with the ability to , , and  the
query engine api
strapi provides a query engine api to interact with the database layer at a lower level. it should mostly be used by plugin developers and developers adding custom business logic to their applications. in most use cases, it's recommended to use the  instead.
the query engine is available through :
the query engine allows operations on database entries, such as:
crud operations on  or  ,  and
integrations
strapi generates an api for you to access your content. but how can you connect a react, ruby, gatsby application to it?
it is important to know what is an api.
what is an api?
api is the acronym for application programming interface, which is a software intermediary that allows two applications to talk to each other.
in case you want to connect a react application with strapi, we say that react is the client and strapi the system. indeed, react will communicate to strapi, by making http requests. strapi will then give a response back to your client.
if your strapi application contains restaurants and you want to list them in your react application, all you need to do is to make an http request to strapi which will take care to give you a response containing your restaurants.
the  documentation will give you all the keys in hand to interact with your strapi api.
get started
today, any programming language has an http client allowing you to execute requests to an api and therefore interact with it. javascript has , , ruby has , , python has  etc...
integrate strapi with a multitude of frameworks, frontend or backend programming languages just below.
strapi plugins
strapi comes with these officially supported plugins:
automatic plugins discovery
strapi automatically loads plugins installed with npm. under the hood, strapi scans every  file of the project dependencies, and looks for the following declaration:
installed plugins can also be manually enabled or disabled.
manual enabling/disabling
to disable a plugin without uninstalling it, switch its  key to  in the .
🤓 to go furtherto know more about plugins installation, see the . existing plugins can be , or you can even !